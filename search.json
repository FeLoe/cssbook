[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cssbook_quarto",
    "section": "",
    "text": "Computational Analysis of Communication"
  },
  {
    "objectID": "index.html#wouter-van-atteveldt-damian-trilling-carlos-arcila-calderon",
    "href": "index.html#wouter-van-atteveldt-damian-trilling-carlos-arcila-calderon",
    "title": "cssbook_quarto",
    "section": "Wouter van Atteveldt, Damian Trilling, Carlos Arcila Calderon",
    "text": "Wouter van Atteveldt, Damian Trilling, Carlos Arcila Calderon\nThis is the online version of the book Computational Analysis of Communication …\n[Book][https://media.wiley.com/product_data/coverImage300/39/11196802/1119680239.jpg]]\n\nChapter 1"
  },
  {
    "objectID": "chapter01.html#sec-ccs",
    "href": "chapter01.html#sec-ccs",
    "title": "1  Introduction",
    "section": "1.1 The Role of Computational Analysis in the Social Sciences",
    "text": "1.1 The Role of Computational Analysis in the Social Sciences\nThe use of computers is nothing new in the social sciences. In fact, one could argue that some disciplines within the social sciences have even been early adopters of computational approaches. Take the gathering and analyzing of large-scale survey data, dating back to the use of the Hollerith Machine in the 1890 US census. Long before every scholar had a personal computer on their desk, social scientists were using punch cards and mainframe computers to deal with such data. If we think of the analysis of communication more specifically, we already see attempts to automate content analysis in the 1960’s (see, e.g. Scharkow 2017).\nHowever, something has profoundly changed in recent decades. The amount and type of data we can collect as well as the computational power we have access to have increased dramatically. In particular, digital traces that we leave when communicating online, from access logs to comments we place, have required new approaches (e.g., Trilling 2017). At the same time, better computational facilities now allow us to ask questions we could not answer before.\nGonzález-Bailón (2017), for instance, argued that the computational analysis of communication now allows us to test theories that were formulated a century ago, such as Tarde’s theory of social imitation. Salganik (2019) tells an impressive methodological story of continuity in showing how new digital research methods build on and relate to established methods such as surveys and experiments, while offering new possibilities by observing behavior in new ways.\nA frequent misunderstanding, then, about computational approaches is that they would somehow be a-theoretical. This is probably fueled by clichés coined during the “Big Data”-hype in the 2010’s, such as the infamous saying that in the age of Big Data, correlation is enough (Mayer-Schönberger and Cukier 2013); but one could not be more wrong: as the work of Kitchin shows (Kitchin 2014a, 2014b), computational approaches can be well situated within existing epistemologies. For the field to advance, computational and theoretical work should be symbiotic, with each informing the other and with neither claiming superiority (Margolin 2019). Thus, the computational scientists’ toolbox includes both more data-driven and more theory-driven techniques; some are more bottom-up and inductive, others are more top-down and deductive. What matters here, and what is often overlooked, is in which stage of the research process they are employed. In other words, both inductive and deductive approaches as they are distinguished in more traditional social-science textbooks (e.g., Bryman 2012) have their equivalent in the computational social sciences.\nTherefore, we suggest that the data collection and data analysis process is thought of as a pipeline. To test, for instance, a theoretically grounded hypothesis about personalization in the news, we could imagine a pipeline that starts with scraping online news, proceeds with some natural-language processing techniques such as Named Entity Recognition, and finally tests whether the mentioning of persons has an influence on the placement of the stories. We can distinguish here between parts of the pipeline that are just necessary but not inherently interesting to us, and parts of the pipeline that answer a genuinely interesting question. In this example, the inner workings of the Named Entity Recognition step are not genuinely interesting for us – we just need to do it to answer our question. We do care about how well it works and especially which biases it may have that could affect our substantive outcomes, but we are not really evaluating any theory on Named Entity Recognition here. We are, however, answering a theoretically interesting question when we look at the pipeline as a whole, that is, when we apply the tools in order to tackle a social scientific problem. Of course, what is genuinely interesting depends on one’s discipline: For a computational linguist, the inner workings of the Named Entity Recognition may actually be the interesting part, and our research question just one possible “downstream task”.\nThis distinction is also sometimes referred to as “building a better mousetrap” versus “understanding”. For instance, Breiman (2001) remarked: “My attitude toward new and/or complicated methods is pragmatic. Prove that you’ve got a better mousetrap and I’ll buy it. But the proof had better be concrete and convincing.” (p. 230). In contrast, many social scientists are using statistical models to test theories and to understand social processes: they want to specifically understand how \\(x\\) relates to \\(y\\), even if \\(y\\) may be better predicted by another (theoretically uninteresting) variable.\nThis book is to some extent about both building mousetraps and understanding. When you are building a supervised machine learning classifier to determine the topic of each text in a large collection of news articles or parliamentary speeches, you are building a (better) mousetrap. But as a social scientist, your work does not stop there. You need to use the mousetrap to answer some theoretically interesting question.\nActually, we expect that the contents of this book will provide a background that helps you to face the current research challenges in both academia and industry. On the one hand, the emerging field of Computational Social Science has become one of the most promising areas of knowledge and many universities and research institutes are looking for scholars with this profile. On the other hand, it is widely known that nowadays the computational skills will increase your job opportunities in private companies, public organizations, or NGOs, given the growing interest in data-driven solutions.\nWhen planning this book, we needed to make a couple of tough choices. We aimed to at least give an introduction to all techniques that students and scholars who want to computationally analyze communication will probably be confronted with. Of course, specific – technical – literature on techniques such as, for instance, machine learning can cover the subject in more depth, and the interested student may indeed want to dive into one or several of the techniques we cover more deeply. Our goal here is to offer enough working knowledge to apply these techniques and to know what to look for. While trying to cover the breadth of the field without sacrificing too much depth when covering each technique, we still needed to draw some boundaries. One technique that some readers may miss is agent-based modeling (ABM). Arguably, such simulation techniques are an important technique in the computational social sciences more broadly (Cioffi-Revilla 2014), and they have recently been applied to the analysis of communication as well (Waldherr 2014; Wettstein 2020). Nevertheless, when reviewing the curricula of current courses teaching the computational analysis of communication, we found that simulation approaches do not seem to be at the core of such analyses (yet). Instead, when looking at the use of computational techniques in fields such as journalism studies (e.g., Boumans and Trilling 2016), media studies (e.g., Rieder 2017), or the text-as-data movement (Grimmer and Stewart 2013), we see a core of techniques that are used over and over again, and that we have therefore included in our book. In particular, besides general data analysis and visualization techniques, these are techniques for gathering data such as web scraping or the use of API’s; techniques for dealing with text such as natural language processing and different ways to turn text into numbers; supervised and unsupervised machine learning techniques; and network analysis."
  },
  {
    "objectID": "chapter01.html#sec-whypythonr",
    "href": "chapter01.html#sec-whypythonr",
    "title": "1  Introduction",
    "section": "1.2 Why Python and/or R?",
    "text": "1.2 Why Python and/or R?\nBy far most work in the computational social sciences is done using Python and/or R. Sure, for some specific tasks there are standalone programs that are occasionally used; and there are some useful applications written in other languages such as C or Java. But we believe it is fair to say that it is very hard to delve into the computational analysis of communication without learning at least either Python or R, and preferably both of them. There are very few tasks that you cannot do with at least one of them.\nSome people have strong beliefs as to which language is “better” – we do not subscribe to that view. Most techniques that are relevant to us can be done in either language, and personal preference is a big factor. R started out as a statistical programming environment, and that heritage is still visible, for instance in the strong emphasis on vectors, factors, et cetera, or the possibility to estimate complex statistical models in just one line of code. Python started out as a general-purpose programming language, which means that some of the things we do feel a bit more `low-level’ – Python abstracts away less of the underlying programming concepts than R does. This sometimes gives us more flexibility – at the cost of being more wordy. In recent years, however, Python and R have been growing closer to each other: with modules like pandas and statsmodels, Python now has R-like functionality handling data frames and estimating common statistical models on them; and with packages such as quanteda, handling of text – traditionally a strong domain of Python – has become more accessible in R.\nThis is the main reason why we decided to write this “bi-lingual” book. We wanted to teach techniques for the computational analysis of communication, without enforcing a specific implementation. We hope that the reader will learn from our book, say, how to transform a text into features and how to choose an appropriate machine learning model, but will find it of less importance in which language this happens.\nHowever, sometimes, there are good reasons to choose one language above the other. For instance, many machine learning models in the popular caret package in R under the hood create a dense matrix, which severely limits the number of documents and features one can use; also, some complex web scraping tasks are maybe easier to realize in Python. On the other hand, R’s data wrangling and visualization techniques in the tidyverse environment are known for their user-friendliness and quality. In the rare cases where we believe that R or Python is clearly superior for a given task, we indicate this; for the rest, we believe that it is up to the reader to choose."
  },
  {
    "objectID": "chapter01.html#sec-howtouse",
    "href": "chapter01.html#sec-howtouse",
    "title": "1  Introduction",
    "section": "1.3 How to use this book",
    "text": "1.3 How to use this book\nThis book differs from more technically oriented books on the one hand and more conceptual books on the other hand. We do cover the technical background that is necessary to understand what is going on, but we keep both computer science concepts and mathematical concepts to a minimum. For instance, if we had written a more technical book about programming in Python, we would have introduced rather early and in detail concepts such as classes, inheritance, and instances of classes. Instead, we decided to provide such information only as additional background where necessary, and to focus, rather pragmatically, on the application of techniques for the computational analysis of communication. Vice versa, if we had written a more conceptual book on new methods in our field, we would have given more emphasis to epistemological aspects, and had skipped the programming examples, which are now at the core of this book.\nWe do not expect much prior knowledge from the readers of this book. Sure, some affinity with computers helps, but there is no strict requirement on what you need to know. Also in terms of statistics, it helps if you have heard of concepts such as correlation or regression analysis, but even if your knowledge here is rather limited, you should be able to follow along.\nThis also means that you may be able to skip chapters. For instance, if you already work with R and/or Python, you may not need our detailed instructions at the beginning. Still, the book follows a logical order in which chapters build on previous ones. For instance, when explaining supervised machine learning on textual data, we expect you to be familiar with previous chapters that deal with machine learning in general, or with the handling of textual data.\nThis book is designed in such a way that it can be used as a text book for introductory courses on the computational analysis of communications. Often, such courses will be on the graduate level, but it is equally possible to use this book in an undergraduate course; maybe skipping some parts that may go too deep. All code examples are not only printed in this book, but also available online. Students as well as social-scientists who want to brush up their skillset should therefore also be able to use this book for self-study, without a formal course around it. Lastly, this book can also be a reference for readers asking themselves: “How do I do this again?”. In particular, if the main language you work in is R, you can look up how to do similar things in Python and vice versa.\n\n\n\n\n\n\nCode examples\n\n\n\n\n\nRegardless of the context in which you use this book, one thing is for sure: The only way to learn computational analysis methods is by practicing and playing around. For this reason, the code examples are probably the most important part of the book. Where possible, the examples use real world data that is freely available on the Internet. To make sure that the examples still work in five years’ time, we generally provide a copy of this data on the book website, but we also provide a link to the original source.\nOne thing to note is that to avoid unnecessary repetition the examples are sometimes designed to continue on earlier snippets from that chapter. So, if you seem to be missing a data set, or if some package is not imported yet, make sure you run all the code examples from that chapter.\nNote that although it is possible to copy-paste the code from the website accompanying this book1, we would actually recommend typing the examples yourself. That way, you are more conscious about the commands you are using and you are adding them to your `muscle memory’.\nFinally, realize that the code examples in this book are just examples. There’s often more ways to do something, and our way is not necessarily the only good (let alone the best) way. So, after you get an example to work, spend some time to play around with it: try different options, maybe try it on your own data, or try to achieve the same result in a different way. The most important thing to remember is: you can’t break anything! So just go ahead, have fun, and if nothing works anymore you can always start over from the code example from the book."
  },
  {
    "objectID": "chapter01.html#sec-installing",
    "href": "chapter01.html#sec-installing",
    "title": "1  Introduction",
    "section": "1.4 Installing R and Python",
    "text": "1.4 Installing R and Python\nR and Python are the most popular programming languages that data scientists and computational scholars have adopted to conduct their work. While many develop a preference for one or the other language, the chances are good that you will ultimately switch back and forth between them, depending on the specific task at hand and the project you are involved in.\nBefore you can start with analyzing data and communication in Python or R, you need to install interpreters for these languages (i.e., programs that can read code in these languages and execute it) on your computer. Interpreters for both Python and R are open source and completely free to download and use. Although there are various web-based services on which you can run code for both languages (such as Google Colab or RStudio Cloud), it is generally better to install an interpreter on your own computer.\nAfter installing Python or R, you can execute code in these languages, but you also want a nice Integrated Development Environment (IDE) to develop your data analysis scripts. For R we recommend RStudio, which is free to install and is currently the most popular environment for working with R. For Python we recommend starting with JupyterLab or JupyterNotebook, which is a browser-based environment for writing and running Python code. All of these tools are available and well documented for Windows, MacOS, and Linux. After explaining how to install R and Python, there is a very important section on installing packages. If you plan to only use either R or Python (for now), feel free to skip the part about the other language.\nIf you are writing longer Python programs (as opposed to, for instance, short data analysis scripts) you probably want to install a full-blown IDE as well. We recommend PyCharm2 for this, which has a free version that has everything you need, and the premium version is also free for students and academic or open source developers. See their website for download and installation instructions.\n\n\n\n\n\n\nAnaconda\n\n\n\n\n\n. An alternative to installing R, Python, and optional libraries separately and as you need them (which we will explain later in this chapter) is to install the so-called Anaconda distribution, one of the most used and extensive platforms to perform data science. Anaconda is free and open-source, and is conceived to run Python and R code for data analysis and machine learning. Installing the complete Anaconda Distribution on your computer3 provides you with everything that you need to follow the examples in this book and includes development environments such as Spyder, Jupyter, and RStudio. It also includes a large set of pre-installed packages often used in data science and its own package manager, conda, which will help you to install and update other libraries or dependencies. In short, Anaconda bundles almost all the important software to perform computational analysis of communication.\nSo, should you install Anaconda, or should you install all software separately as outlined in this chapter? It depends. On the pro side, by downloading Anaconda you have everything installed at once and do not have to worry about dependencies (e.g., Windows users usually do not have a C compiler installed, but some packages may need it). On the con side, it is huge and also installs many things you do not need, so you essentially get a non-standard installation, in which programs and packages are stored in different locations than those you (or your computer) may expect. Nowadays, as almost all computers actually already have some version of Python installed (even though you may not know it), you also end up in a possibly confusing situation where it may be unclear which version you are actually running, or for which version you installed a package. For this reason, our recommendation is to not use Anaconda unless it is already installed or you have a specific reason to do so (for example, if your professor requires you to use it).\n\n\n\n\n1.4.1 Installing R and RStudio\nFirstly, we will install R and its most popular IDE RStudio, and we will learn how to install additional packages and how to run a script. R is an object-based programming language orientated to statistical computing that can be used for most of the stages of computational analysis of communication. If you are completely new to R, but familiar with other popular statistical packages in social sciences (such as SPSS or STATA), you will find that you can perform in R many already-known statistical operations. If you are not familiar with other statistical packages, do not panic, we will guide you from the very beginning. Unlike much traditional software that requires just one complete and initial installation, when working with R, we will first install the raw programming language and then we will continue to install additional components during our journey. It might sound cumbersome, but in fact it will make your work more powerful and flexible, since you will be able to choose the best way to interact with R and especially you will select the packages that are suitable for your project.\nNow, let us install R. The easiest way is to go to the RStudio CRAN page at cran.rstudio.com/. 4 Click on the link for installing R for your operating system, and install the latest version. If you use Linux, you may want to install R via your package manager. For Ubuntu linux, it is best to follow the instructions on cran.r-project.org/bin/linux/ubuntu/.\nAfter installing R, let us immediately install RStudio Desktop (the free version). Go to rstudio.com/products/rstudio/download/#download and download and run the installer for your computer. If you open RStudio you should get a screen similar to Figure 1.1. If this is the first time you open RStudio you probably won’t see the top left pane (the scripts), you can create that pane by creating a new R Script via the file menu or with the green plus icon in the top left corner.\n\n\n\nFigure 1.1: RStudio Desktop.\n\n\nOf the four panes in RStudio, you will probably spend most time in the top left pane, where you can view and edit your analysis scripts. A script is simply a list of commands that the computer should execute one after the other, for example: open your data, do some computations, and make a nice graph.\nTo run a line of code, you can place your cursor anywhere on that line and click the Run icon or press control+Enter. To try that, type the following into your newly opened script:\nprint(\"Hello world\")\nNow, place your cursor on that line and press Run (or control+Enter). What happens is that the line is copied to the Console in the bottom left corner and executed. So, the results of your commands (and any error messages) will be shown in this console view.\nIn contrast to most traditional programming languages, the easiest way to run R code is line by line. You can simply place your cursor on the first line, and repeatedly press control+Enter, which executes a line and then places the cursor on the next line. You can also select multiple lines (or part of a line) to execute those commands together, but in general it is easier to check that everything is going as planned if you run the code line by line.\nYou can also write commands directly in the console and execute them (by pressing Enter). This can be useful for trying things out or to run things that only need to be run once, but in general we would strongly recommend typing all your commands in a script and then executing them. That way, the script serves as a log of the commands you used to analyze your data, so you (or a colleague) can read and understand how you did the analyses.\n\n\n\n\n\n\nRStudio Projects\n\n\n\n\n\nA very good idea to organize your data and code is to work with RStudio Projects. In fact, we would recommend you to now create a new empty project for the examples in this book. To do this, click on the Project button in the top right and select “New Project”. Then, select New Directory and New Project and enter a name for this project and a parent folder for the project if you don’t want it in your Documents. Using a project means that the scripts and data files for your project are all in the same location and you don’t need to mess around with specifying the locations of files (which will probably be different for someone else or on a different computer). Moreover, RStudio remembers which files you were editing for each project, so if you are working on multiple projects, it’s very easy to switch between them. We recommend creating a project now for the book (and/or for any projects you are working on), and always switching to a project when you open RStudio\n\n\n\nOn the right side of the RStudio workspace you will find two additional windows. In the top right pane there are two or more tabs: environment and history, and depending on additional packages you may have installed there may be some more. In environment you can manage your workspace (the set of elements you need to deploy for data analysis) and have a list of the objects you have uploaded to it. You may also import datasets with this tool. In the history tab you have an inventory of code executions, which you can save to a file, or move directly to console or to an R document.\nNote that in the environment you can save and load your “workspace” (all data in the computer memory). However, relying on this functionality is often not a good idea: it will only save the state of your current session, whereas you most likely will want to save your R syntax file and/or your data instead. If you have your raw input data (e.g., as a csv file, see Chapter 5) and your analysis script, you can always reproduce what you have been doing. If you only have a snapshot of your workspace, you know the state in which you arrived, but cannot necessarily reproduce (or change) how you got there.\nIn the bottom right pane there are five additional useful tabs. In files you can explore your computer and manage all the files you may use for the project, including importing datasets. In plots, help and viewer, you can visualize the outputs, figures, documentation and general outcomes, respectively, that you have executed in your script. Finally, the tab for packages will be of great utility since it will let you install or update packages from CRAN or even from a file saved on your computer with a friendly interface.\n\n\n1.4.2 Installing Python and Jupyter Notebook\nPython is an object-orientated programming language and it is probably the favorite language of computational and data scientists in all disciplines around the world. There are different releases of Python, but the biggest difference used to be between Python 2 and Python 3. Fortunately, you will probably never need to install or use Python 2, and in fact, since January 2020 it is no longer supported. Thus, you can just use any recent Python 3 version for this book. When browsing through questions on online fora such as Stackoverflow or reading other people’s code on Github (we will talk about that in Chapter 4), you still may come across legacy code in Python 2. Such code usually does not run directly in a Python 3 interpreter, but in most cases, only minor adaptions are necessary to make it work.\nWe will install and run Python and Jupyter Notebook using a terminal or command line interface. This is a tool that is installed on all computers that allows you to enter commands to the computer directly. First, create a project folder for this book using the File Explorer (Windows) or Finder (MacOS). Then, on Windows you can shift + Right click that folder and select “Open command Window here”. On MacOS, after navigating to the folder you just created, you click on “Finder” in the menu at the top of the screen, then on “Services”, then on “New Terminal at Folder.” In both cases, this should open a new window (usually black or gray) that allows you to type commands.\nNote that on most computers, Python is already installed by default. You can check this by typing the following command in your terminal:\npython3 --version\nOn some versions of Windows, you may need to use py instead of python3:\npy --version\nIn either case, the output of this command should be something like Python 3.8.5. If python --version also returns this version, you are free to use either command (but on older systems python can still refer to Python 2, so make sure that you are using Python 3 for this book!).\nIf Python is not installed on your system, go to www.python.org/downloads/windows/ or www.python.org/downloads/mac-osx/ and download and install the latest stable release (which at the time of writing is 3.9.0). 5 After installing it, open a terminal again and run the command above to verify that it is installed correctly.\nIncluded in any recent Python install is pip, the program that you will use for installing Python packages. You can check that pip is installed correctly by typing the following command on your terminal:\npip3 --version\nWhich should report something like pip 20.0.2 from ... (python 3.8). Again, if pip reports the same version you can also use it instead of pip3. On some systems pip3 will not work, so use pip in that case (but make sure to check that it points to Python 3).\nInstalling Jupyter Notebook. Next, we will install Jupyter Notebook, which you can use to run all the examples in this book and is a great environment for developing Python data analysis scripts. Jupyter Notebooks (in IDE JupyterLab if you installed that), are run as a web application that allows you to create documents that contain code and inline text fragments. One of the nicest things about the Jupyter Notebook is that the code is inserted in fields (so-called “cells”) that you can run one by one, getting its respective output, which when added to the narrative text, will make your script more clean and reproducible. You can also add formatted text blocks (using a simple formatting language called Markdown) to explain to the reader what you are doing. In Section 4.3, we will address notebooks again as a good practice for a computational scientist.\nYou can install Jupyter notebook directly using pip using the following command (executed in a terminal):\npip3 install jupyter-notebook\nNow, you can run Jupyter by executing the following command on the terminal:\njupyter notebook\nThis will print some useful information, including the URL at which you can access the notebook. However, it should also directly open this in a browser (e.g. Chrome) so you can directly start working. In your browser you should see the Jupyter main screen similar to the middle window in Figure 1.2. Create a new notebook by clicking on the New button in the top right and selecting Python 3. This should open a window similar to the bottom window in Figure 1.2.\n\n\n\nFigure 1.2: Jupyter Notebook.\n\n\nIn Jupyter, code is entered into cells. First, type print(\"Hello World\") into the empty cell next to the In [ ]: prompt. Then, click the Run button or press control+Enter. This should execute your command and display the text \"Hello World\" in the output area right below the input cell. Note that you can create more cells using the plus icon or with the insert menu. You can also set the cell type via the Cell menu: select code for analysis scripts (which is the default), or Markdown for text fragments, which can be used to explain the code and/or interpret the results."
  },
  {
    "objectID": "chapter01.html#sec-installingpackages",
    "href": "chapter01.html#sec-installingpackages",
    "title": "1  Introduction",
    "section": "1.5 Installing Third-Party Packages",
    "text": "1.5 Installing Third-Party Packages\nThe print function used above is automatically included when you start R or Python. Many functions, however, are included in separate packages (also known as libraries or modules), which are generally collections of commands for a certain task or activity.\nAlthough both R and Python come pre-installed with many useful packages, one of the great things of both languages is that they have a very active community that continuously develops, improves, and publishes new packages. Throughout this book, we will be using such third-party packages for a variety of tasks, from data wrangling and visualization to text analysis. For example, we will use the R package tidyverse and the Python packages pandas for data wrangling.\nTo install these packages on your computer, run the following commands: (Note: if you are using Anaconda, replace pip3 install with conda install)\n\nInstalling a package from JupyterInstalling a package in R\n\n\n\n!pip3 install pandas\n# (On some systems, !pip install pandas)\n\n\n\n\ninstall.packages(\"tidyverse\")\n\n\n\n\nThese commands will automatically fetch the package from the right repository6 and install them on your computer. This can take a while, especially for large packages such as tidyverse. Fortunately, this only needs to be done once. Every time you use a package, you also need to activate it using the import (Python) or library (R) command.\nIn general, whenever you get an error No module named 'pandas' (Python) or there is no package called ‘tidyverse’, you can just install the package with that name using the code listed above. If you get an error such as name 'pandas' is not defined (Python) or object 'ggplot' not found (R), it is quite possible you forgot to activate the package that includes that function.\n\n\n\n\n\n\nPackages used in each chapter\n\n\n\n\n\nSome packages, like the tidyverse (R) and pandas (Python) packages for data handling are used in almost every chapter. Many chapters also introduce specific packages such as igraph/networkx for network analysis in ?sec-chap-network. To make it easy to keep track of the packages needed for each chapter, every chapter that includes code in this book starts with a note like this that gives an overview of the main packages introduced in that chapter. It also includes the code needed to install these packages, which of course is only needed if you didn’t install these packages before. Note again that if you are using Anaconda for Python, you should replace !pip3 install with !conda install in that code. On some systems, you may need to use !pip install instead of !pip3 install.\nThese notes also includes a code block to import all the packages used for that chapter, which you need to run every time you use examples from that chapter.\n\n\n\n\n\n\n\nBoumans, Jelle W., and Damian Trilling. 2016. “Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars.” Digital Journalism 4 (1): 8–23. https://doi.org/10.1080/21670811.2015.1096598.\n\n\nBreiman, Leo. 2001. “Statistical modeling: The two cultures.” Statistical Science 16 (3): 199–215. https://doi.org/10.1214/ss/1009213726.\n\n\nBryman, Alan. 2012. Social Research Methods. 4th edition. New York, NY: Oxford University Press.\n\n\nCioffi-Revilla, Claudio. 2014. Introduction to Computational Social Science: Principles and Applications. London, UK: Springer.\n\n\nGonzález-Bailón, Sandra. 2017. Decoding the social world: Data science and the unintended consequences of communication. Cambridge, MA: MIT.\n\n\nGrimmer, J., and B. M. Stewart. 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.” Political Analysis 21 (3): 267–97. https://doi.org/10.1093/pan/mps028.\n\n\nKitchin, Rob. 2014a. “Big Data, new epistemologies and paradigm shifts.” Big Data & Society 1 (1): 1–12. https://doi.org/10.1177/2053951714528481.\n\n\n———. 2014b. The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences. Sage.\n\n\nMargolin, Drew B. 2019. “Computational Contributions: A Symbiotic Approach to Integrating Big, Observational Data Studies into the Communication Field.” Communication Methods and Measures 13 (4): 229–47.\n\n\nMayer-Schönberger, Viktor, and Kenneth Cukier. 2013. Big Data: A Revolution That Will Transform How We Live, Work, and Think. New York, NY: Houghton Mifflin Harcourt.\n\n\nRieder, Bernhard. 2017. “Scrutinizing an Algorithmic Technique: The Bayes Classifier as Interested Reading of Reality.” Information Communication and Society 20 (1): 100–117. https://doi.org/10.1080/1369118X.2016.1181195.\n\n\nSalganik, Matthew. 2019. Bit by Bit: Social Research in the Digital Age. Princeton University Press.\n\n\nScharkow, Michael. 2017. “Content Analysis, Automatic.” In The International Encyclopedia of Communication Research Methods, edited by Jörg Matthes, Christine S. Davis, and Robert F. Potter, 1–14. Hoboken, NJ: Wiley. https://doi.org/10.1002/9781118901731.iecrm0043.\n\n\nTrilling, Damian. 2017. “Big Data, Analysis of.” In The International Encyclopedia of Communication Research Methods, 1–20. Hoboken, NJ, USA: John Wiley & Sons, Inc. https://doi.org/10.1002/9781118901731.iecrm0014.\n\n\nWaldherr, Annie. 2014. “Emergence of News Waves: A Social Simulation Approach.” Journal of Communication 64 (5): 852–73. https://doi.org/10.1111/jcom.12117.\n\n\nWettstein, Martin. 2020. “Simulating hidden dynamics : Introducing Agent-Based Models as a tool for linkage analysis.” Computational Communication Research 2 (1): 1–33. https://doi.org/10.5117/CCR2020.1.001.WETT."
  },
  {
    "objectID": "chapter02.html#sec-funtweets",
    "href": "chapter02.html#sec-funtweets",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.1 Fun With Tweets",
    "text": "2.1 Fun With Tweets\nThe goal of this chapter is to showcase how you can use R or Python to quickly and easily run some impressive analyses of real world data. For this purpose, we will be using a dataset of tweets about the COVID pandemic that is engulfing much of the world at the time this book is written. Of course, tweets are probably only representative for what is said on Twitter, but the data are (semi-)public and rich, containing text, location, and network characteristics. This makes them ideal for exploring the many ways in which we can analyze and visualize information with Python and R.\nExample 2.1 shows how you can read this dataset into memory using a single command. Note that this does not retrieve the tweets from Twitter itself, but rather downloads our cached version of the tweets. In Chapter ?sec-chap-scraping we will show how you can download tweets and location data yourself, but to make sure we can get down to business immediately we will start from this cached version.\n\n\n\n\n\n\n\nExample 2.1 Retrieving cached tweets about COVID\n\nPython codeR code\n\n\n\ntw = pd.read_csv(\"https://cssbook.net/d/covid.csv\")\ntw.head()\n\n             status_id  ... reply_to_screen_name\n0  1309535775109926912  ...                  NaN\n1  1309626010221129729  ...                  NaN\n2  1309578234007257088  ...                  NaN\n3  1309557875296083969  ...                  NaN\n4  1309643186827132929  ...                  NaN\n\n[5 rows x 8 columns]\n\n\n\n\n\ntw = read_csv(\"https://cssbook.net/d/covid.csv\")\nhead(tw)\n\n# A tibble: 6 × 8\n  status_id created_at          screen_name  lang  locat…¹ text  retwe…² reply…³\n      <dbl> <dttm>              <chr>        <chr> <chr>   <chr>   <dbl> <chr>  \n1   1.31e18 2020-09-25 16:50:33 ghulamabbas… en    Lahore… \"Sec…    1203 <NA>   \n2   1.31e18 2020-09-25 22:49:07 GeoRebekah   en    Florid… \"On …    1146 <NA>   \n3   1.31e18 2020-09-25 19:39:16 AlexBerenson en    New Yo… \"Upd…     988 <NA>   \n4   1.31e18 2020-09-25 18:18:22 AlexBerenson en    New Yo… \"No …     953 <NA>   \n5   1.31e18 2020-09-25 23:57:22 B52Malmet    en    New Yo… \"Dr.…     946 <NA>   \n6   1.31e18 2020-09-26 08:28:51 iingwen      en    Taipei… \"It’…     436 <NA>   \n# … with abbreviated variable names ¹​location, ²​retweet_count,\n#   ³​reply_to_screen_name\n\n\n\n\n\n\n\n\n\nAs you can see, the dataset contains almost 10000 tweets, listing their sender, their location and language, the text, the number of retweets, and whether it was a reply (retweet). You can read the start of the three most retweeted messages, which contain one (political) tweet from India and two seemingly political and factual tweets from the United States.\nMy first bar plot. Before diving into the textual, network, and geographic data in the dataset, let’s first make a simple visualization of the date on which the tweets were posted. Example 2.2 does this in two steps: first, the number of tweets per hour is counted with an aggregation command. Next, a bar plot is made of this calculated value with some options to make it look relatively clean and professional. If you want to play around with this, you can for example try to plot the number of tweets per language, or create a line plot instead of a bar plot. For more information on visualization, please see Chapter 7. See Chapter 6 for an in-depth explanation of the aggregation command.\n\n\n\n\n\n\n\nExample 2.2 Barplot of tweets over time\n\nPython codeR code\n\n\n\ntw.index = pd.DatetimeIndex(tw[\"created_at\"])\ntw[\"status_id\"].groupby(pd.Grouper(freq=\"H\")).count().plot(kind=\"bar\")\n# (note the use of \\ to split a long line)\n\n\n\n\n\n\n\ntweets_per_hour = tw %>% \n  mutate(hour=round_date(created_at, \"hour\")) %>% \n  group_by(hour) %>% summarize(n=n()) \nggplot(tweets_per_hour, aes(x=hour, y=n)) + \n  geom_col() + theme_classic() + \n  xlab(\"Time\") + ylab(\"# of tweets\") +\n  ggtitle(\"Number of COVID tweets over time\")"
  },
  {
    "objectID": "chapter02.html#sec-funtext",
    "href": "chapter02.html#sec-funtext",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.2 Fun With Textual Data",
    "text": "2.2 Fun With Textual Data\nCorpus Analysis. Next, we can analyze which hashtags are most frequently used in this dataset. Example 2.3 does this by creating a document-term matrix using the package quanteda (in R) or by manually counting the words using a defaultdict (in Python). The code shows a number of steps that are made to create the final results, each of which represent researcher choices about which data to keep and which to discard as noise. In this case, we select English tweets, convert text to lower case, remove stop words, and keep only words that start with #, while dropping words starting with #corona and #covid. To play around with this example, see if you can adjust the code to e.g. include all words or only at-mentions instead of the hashtags and make a different selection of tweets, for example Spanish language tweets or only popular (retweeted) tweets. Please see Chapter ?sec-chap-dtm if you want to learn more about corpus analysis, and see Chapter 6 for more information on how to select subsets of your data.\n\n\n\n\n\n\n\nExample 2.3 My First Tag Cloud\n\nPython codeR code\n\n\n\nfreq = defaultdict(int)\nfor tweet in tw[\"text\"]:\n    for tag in re.findall(\"#\\w+\", tweet.lower()):\n        if not re.search(\"#covid|#corona\", tag):\n            freq[tag] += 1\nwc = WordCloud().generate_from_frequencies(freq)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\n\n\n\ndtm_tags = filter(tw, lang==\"en\") %>% \n  corpus() %>% tokens() %>% \n  dfm(tolower = T) %>% \n  dfm_select(pattern = \"#*\") %>% \n  dfm_remove(c(\"#corona*\", \"#covid*\")) \ntextplot_wordcloud(dtm_tags, max_words=100)\n\n\n\n\n\n\n\n\n\n\n\nTopic Model. Where a word cloud (or tag cloud) shows which words occur most frequently, a topic model analysis shows which words co-occur in the same documents. Using the most common topic modeling algorithm, Latent Dirichlet Allocation or LDA, Example 2.4 explores the tweets by automatically clustering the tags selected earlier into 10 topics. Topic modeling is non-deterministic – if you run it again you can get slightly different topics, and topics are swapped around randomly as the topic numbers have no special meaning. By setting the computer’s random seed you can ensure that if you run it again you get the same results. As you can see, some topics seem easily interpretable (such as topic 2 about social distancing, and topic 8 on health care), it is always recommended that you inspect the clustered documents and edge cases in addition to the top words (or tags) as shown here. You can play around with this example by using a different selection of words (modifying the code in Example 2.3) or changing the number of topics. You can also change (or remove) the random seed and see how running the same model multiple times will give different results. See ?sec-unsupervised for more information about fitting, interpreting, and validating topic models.\n\n\n\n\n\n\n\nExample 2.4 Topic Model of the COVID tags\n\nPython codeR code\n\n\n\ntags = [\n    [tag.lower() for tag in re.findall(\"#\\w+\", tweet)] for tweet in tw[\"text\"]\n]\nvoca = corpora.Dictionary(tags)\ncorpus = [voca.doc2bow(doc) for doc in tags]\nm = models.LdaModel(\n    corpus, num_topics=10, id2word=voca, distributed=False, random_state=123\n)\nfor topic, words in m.print_topics(num_words=3):\n    print(f\"{topic}: {words}\")\n\n0: 0.030*\"#coviduk\" + 0.019*\"#covid\" + 0.010*\"#lockdown2\"\n1: 0.132*\"#covid\" + 0.016*\"#salud\" + 0.012*\"#blm\"\n2: 0.105*\"#covid\" + 0.030*\"#vaccine\" + 0.029*\"#lockdown\"\n3: 0.151*\"#covid\" + 0.112*\"#covid19\" + 0.067*\"#coronavirus\"\n4: 0.664*\"#covid\" + 0.010*\"#covidー19\" + 0.005*\"#covidiots\"\n5: 0.023*\"#covid\" + 0.022*\"#mentalhealth\" + 0.017*\"#love\"\n6: 0.197*\"#covid\" + 0.032*\"#coronavirus\" + 0.030*\"#pandemic\"\n7: 0.157*\"#covid\" + 0.032*\"#trump\" + 0.031*\"#florida\"\n8: 0.051*\"#covid\" + 0.039*\"#healthcare\" + 0.033*\"#rss\"\n9: 0.205*\"#covid\" + 0.079*\"#coronavirus\" + 0.033*\"#covid19\"\n\n\n\n\n\nset.seed(1)\nm = convert(dtm_tags, to=\"topicmodel\") %>% \n  LDA(10, method=\"Gibbs\")\nterms(m, 5)\n\n     Topic 1       Topic 2             Topic 3            Topic 4         \n[1,] \"#vintage\"    \"#socialdistancing\" \"#florida\"         \"#maga\"         \n[2,] \"#vote\"       \"#staysafe\"         \"#economy\"         \"#business\"     \n[3,] \"#wfh\"        \"#lockdown\"         \"#sandiego\"        \"#climatechange\"\n[4,] \"#news\"       \"#love\"             \"#fridaythoughts\"  \"#\"             \n[5,] \"#remotework\" \"#stayhome\"         \"#rip_indianmedia\" \"#masks\"        \n     Topic 5      Topic 6    Topic 7         Topic 8       Topic 9        \n[1,] \"#sarscov2\"  \"#vaccine\" \"#pandemic\"     \"#health\"     \"#india\"       \n[2,] \"#china\"     \"#cdc\"     \"#virus\"        \"#healthcare\" \"#rss\"         \n[3,] \"#canada\"    \"#who\"     \"#masks\"        \"#medicine\"   \"#islamophobia\"\n[4,] \"#education\" \"#nhs\"     \"#pence\"        \"#doctor\"     \"#gandhi\"      \n[5,] \"#delhi\"     \"#podcast\" \"#publichealth\" \"#wellness\"   \"#nehru\"       \n     Topic 10\n[1,] \"#trump\"\n[2,] \"#usa\"  \n[3,] \"#biden\"\n[4,] \"#uk\"   \n[5,] \"#ue\""
  },
  {
    "objectID": "chapter02.html#sec-fungeo",
    "href": "chapter02.html#sec-fungeo",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.3 Fun With Visualizing Geographic Information",
    "text": "2.3 Fun With Visualizing Geographic Information\nFor the final set of examples, we will use the location information contained in the Twitter data. This information is based on what Twitter users enter into their profile, and as such it is incomplete and noisy with many users giving a nonsensical location such as `Ethereally here’ or not filling in any location at all. However, if we assume that most users that do enter a proper location (such as Lahore or Florida in the top tweets displayed above), we can use it to map where most tweets are coming from.\nThe first step in this analysis is to resolve a name such as `Lahore, Pakistan’ to its geographical coordinates (in this case, about 31 degrees north and 74 degrees east). This is called geocoding, and both Google maps and Open Street Maps can be used to perform this automatically. As with the tweets themselves, we will use a cached version of the geocoding results here so we can proceed directly. Please see https://cssbook.net/datasets for the code that was used to create this file so you can play around with it as well.\nExample 2.5 shows how this data can be used to create a map of Twitter activity. First, the cached user data is retrieved, showing the correct location for Lahore but also illustrating the noisiness of the data with the location “Un peu partout”. Next, this data is joined to the Twitter data, so the coordinates are filled in where known. Finally, we plot this information on a map, showing tweets with more retweets as larger dots. See Chapter 7 for more information on visualization.\n\n\n\n\n\n\n\nExample 2.5 Location of COVID tweets\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/covid_users.csv\"\nusers = pd.read_csv(url)\ntw2 = tw.merge(users, on=\"screen_name\", how=\"left\")\nworld = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\ngdf = gpd.GeoDataFrame(tw2, geometry=gpd.points_from_xy(tw2.long, tw2.lat))\nax = world.plot(color=\"white\", edgecolor=\"black\", figsize=(10, 10))\ngdf.plot(ax=ax, color=\"red\", alpha=0.2, markersize=tw[\"retweet_count\"])\nplt.show()\n\n\n\n\n\n\n\nurl = \"https://cssbook.net/d/covid_users.csv\"\nusers = read_csv(url)\ntw2 = left_join(tw, users)\nggplot(mapping=aes(x=long, y=lat)) +\n  geom_polygon(aes(group=group), \n    data=map_data(\"world\"), \n    fill=\"lightgray\", colour = \"white\") +\n  geom_point(aes(size=retweet_count, \n                 alpha=retweet_count), \n             data=tw2, color=\"red\") + \n  theme_void() + theme(aspect.ratio=1) + \n  guides(alpha=FALSE, size=FALSE) + \n  ggtitle(\"Location of COVID tweets\", \n          \"Size indicates number of retweets\")\n\n\n\n\n\n\n\n\n\n\n\nCombining textual and structured information. Since we know the location of a subset of our tweet’s users, we can differentiate between e.g. American, European, and Asian tweets. Example 2.6 creates a very rough identification of North American tweets, and uses that to compute the relative frequency of words in those tweets compared to the rest. Not surprisingly, those tweets are much more about American politics, locations, and institutions. The other tweets talk about UK politics but also use a variety of names to refer to the pandemic. To play around with this, see if you can isolate e.g. Asian or South American tweets, or compare Spanish tweets from different locations.\n\n\n\n\n\n\n\nExample 2.6 Corpus comparison: North American tweets vs. the rest\n\nPython codeR code\n\n\n\nnltk.download(\"stopwords\")\ncn = gdf.query(\"lang=='en'&(long<-60 & lat>25)\")\ncn = Counter(cn[\"text\"].str.cat().lower().split())\ncr = gdf.query(\"lang=='en' & (long>-60 | lat<25)\")\ncr = Counter(cr[\"text\"].str.cat().lower().split())\nfor k in stopwords.words(\"english\"):\n    del cn[k]\n    del cr[k]\nkey = sh.ProportionShift(type2freq_1=cn, type2freq_2=cr)\nkey.get_shift_graph().plot()\n\n\n\n\n\n\n\ndfm = tw2 %>% mutate(northamerica=ifelse(\n    long < -60 & lat > 25,\"N. America\",\"Rest\"))%>%\n  filter(lang==\"en\") %>% \n  corpus(docid_field=\"status_id\") %>% \n  tokens(remove_punct=T) %>%\n  tokens_group(northamerica) %>%\n  dfm(tolower=T) %>% \n  dfm_remove(stopwords(\"en\")) %>%\n  dfm_select(min_nchar=4)\nkey = textstat_keyness(dfm, target=\"N. America\")\ntextplot_keyness(key, margin=0.2) +\n  ggtitle(\"Words preferred by North Americans\", \n          \"(Only English-language tweets)\") + \n  theme_void()"
  },
  {
    "objectID": "chapter02.html#sec-funnet",
    "href": "chapter02.html#sec-funnet",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.4 Fun With Networks",
    "text": "2.4 Fun With Networks\nTwitter, of course, is a social network as well as a microblogging service: users are connected to other users because they follow each other and retweet and like each others’ tweets. Using the reply_to_screen_name column, we can inspect the retweet network contained in the COVID tweet dataset. Example 2.7 first uses the data summarization commands from tidyverse(R) and pandas(Python) to create a data frame of connections or edges listing how often each user retweets each other user. The second code block shows how the igraph (R) and networkx (Python) packages are used to convert this edge list into a graph. From this graph, we select only the largest connected component and use a clustering algorithm to analyze which nodes (users) form cohesive subnetworks. Finally, a number of options are used to set the color and size of the edges, nodes, and labels, and the resulting network is plotted. As you can see, the central node is Donald Trump, who is retweeted by a large number of users, some of which are then retweeted by other users. You can play around with different settings for the plot options, or try to filter e.g. only tweets from a certain language. You could also easily compute social network metrics such as centrality on this network, and/or export the network for further analysis in specialized social network analysis software. See Chapter ?sec-chap-network for more information on network analysis, and Chapter 6 for the summarization commands used to create the edge list.\n\n\n\n\n\n\n\nExample 2.7 Retweet nework in the COVID tweets.\n\nPython codeR code\n\n\n\nedges = tw2[[\"screen_name\", \"reply_to_screen_name\"]]\nedges = edges.dropna().rename(\n    {\"screen_name\": \"from\", \"reply_to_screen_name\": \"to\"}, axis=\"columns\"\n)\nedges.groupby([\"from\", \"to\"]).size().head()\n\nfrom            to            \n007Vincentxxx   ilfattovideo      1\n06CotedUsure    ArianeWalter      1\n1Million4Covid  1Million4Covid    3\n                JustinTrudeau     1\n1ctboy1         LegionPost13      1\ndtype: int64\n\n\n\n\n\nedges = tw2 %>% \n  select(from=screen_name, \n         to=reply_to_screen_name) %>% \n  filter(to != \"\") %>%\n  group_by(to, from) %>% \n  summarize(n=n())\nhead(edges)\n\n# A tibble: 6 × 3\n# Groups:   to [6]\n  to             from                n\n  <chr>          <chr>           <int>\n1 _FutureIsUs    _FutureIsUs         1\n2 _JaylaS_       AfronerdRadio       1\n3 _LoveMTB_      ExpatriateNl        1\n4 _nogueiraneto  ideobisium          1\n5 _NotFakeNews_  panich52            1\n6 _vikasupadhyay SHADABMOHAMMAD7     4\n\n\n\n\n\n\nPython codeR code\n\n\n\ng1 = nx.Graph()\ng1.add_edges_from(edges.to_numpy())\nlargest = max(nx.connected_components(g1), key=len)\ng2 = g1.subgraph(largest)\n\npos = nx.spring_layout(g2)\nplt.figure(figsize=(20, 20))\naxes_info = plt.axis(\"off\")\nsizes = [s * 1e4 for s in nx.centrality.degree_centrality(g2).values()]\nnx.draw_networkx_nodes(g2, pos, node_size=sizes)\nedge_info = nx.draw_networkx_labels(g2, pos)\nnx.draw_networkx_edges(g2, pos)\nplt.show()\n\n\n\n\n\n\n\n# create igraph and select largest component\ng = graph_from_data_frame(edges)\ncomponents <- decompose.graph(g)\nlargest = which.max(sapply(components, gsize))\ng2 = components[[largest]]\n# Color nodes by cluster\nclusters = cluster_spinglass(g2)\nV(g2)$color = clusters$membership\nV(g2)$frame.color = V(g2)$color\n# Set node (user) and edge (arrow) size\nV(g2)$size = degree(g2)^.5\nV(g2)$label.cex = V(g2)$size/3\nV(g2)$label = ifelse(degree(g2)<=1,\"\",V(g2)$name) \nE(g2)$width = E(g2)$n\nE(g2)$arrow.size= E(g2)$width/10\nplot(g2)\n\n\n\n\n\n\n\n\n\n\n\nGeographic networks. In the final example of this chapter, we will combine the geographic and network information to show which regions of the world interact with each other. For this, in Example 2.8 we join the user information to the edges data frame created above twice: once for the sender, once for the replied-to user. Then, we adapt the earlier code for plotting the map by adding a line for each node in the network. As you can see, users in the main regions (US, EU, India) mostly interact with each other, with almost all regions also interacting with the US.\n\n\n\n\n\n\n\nExample 2.8 Reply Network of Tweets\n\nPython codeR code\n\n\n\nu = users.drop([\"location\"], axis=1)\nuf = u.rename(\n    {\"screen_name\": \"from\", \"lat\": \"lat_from\", \"long\": \"long_from\"}, axis=1\n)\nut = u.rename({\"screen_name\": \"to\", \"lat\": \"lat_to\", \"long\": \"long_to\"}, axis=1)\nedges = edges.merge(uf).merge(ut).query(\"long_to!=long_from & lat_to!=lat_from\")\n\nworld = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\ng_to = gpd.GeoDataFrame(\n    edges.copy(), geometry=gpd.points_from_xy(edges.long_to, edges.lat_to)\n)\ng_from = gpd.GeoDataFrame(\n    edges.copy(), geometry=gpd.points_from_xy(edges.long_from, edges.lat_from)\n)\n\nax = world.plot(color=\"white\", edgecolor=\"black\", figsize=(10, 10))\ng_from.plot(ax=ax, color=\"red\", alpha=0.2)\ng_to.plot(ax=ax, color=\"blue\", alpha=0.2)\n\ne = g_from.join(g_to, lsuffix=\"_from\", rsuffix=\"_to\")\ne = e[[\"geometry_from\", \"geometry_to\"]]\npx = lambda point: point.x\npy = lambda point: point.y\n\n# WVA: This code no longer works but gives\n#      UnsupportedOperationException: getX called on empty Point\n# x_values = list(zip(e[\"geometry_from\"].map(px),\n#                    e[\"geometry_to\"].map(px)))\n# y_values = list(zip(e[\"geometry_from\"].map(py),\n#                    e[\"geometry_to\"].map(py)))\n# plt.plot(x_values, y_values, linewidth = 1,\n#    linestyle = \"-\", color = \"green\", alpha=.3)\n# plt.show()\n\n\n\n\nedges2 = edges %>% \n  inner_join(users, by=c(\"from\"=\"screen_name\"))%>%\n  inner_join(users, by=c(\"to\"=\"screen_name\"), \n             suffix=c(\"\", \".to\")) %>% \n  filter(lat != lat.to | long != long.to )\nggplot(mapping=aes(x = long, y = lat)) +\n  geom_polygon(aes(group=group),map_data(\"world\"),\n    fill=\"lightgray\", colour = \"white\") +\n  geom_point(aes(size=retweet_count, \n  alpha=retweet_count), data=tw2, color=\"red\")+\n  geom_curve(aes(xend=long.to,yend=lat.to,size=n),\n             edges2, curvature=.1, alpha=.5) +\n  theme_void() + guides(alpha=FALSE, size=FALSE) +\n  ggtitle(\"Retweet network of COVID tweets\", \n  \"Bubble size indicates total no. of retweets\")"
  },
  {
    "objectID": "chapter03.html#sec-datatypes",
    "href": "chapter03.html#sec-datatypes",
    "title": "3  Programming concepts for data analysis",
    "section": "3.1 About Objects and Data Types",
    "text": "3.1 About Objects and Data Types\nNow that you have seen what R and Python can do in Chapter 2, it is time to take a small step back and learn more about how it all actually works under the hood.\nIn both languages, you write a script or program containing the commands for the computer. But before we get to some real programming and exciting data analyses, we need to understand how data can be represented and stored.\nNo matter whether you use R or Python, both store your data in memory as objects. Each of these objects has a name, and you create them by assigning a value to a name. For example, the command x=10 creates a new object[^1], named x, and stores the value 10 in it. This object is now stored in memory and can be used in later commands. Objects can be simple values such as the number 10, but they can also be pieces of text, whole data frames (tables), or analysis results. We call this distinction the type or class of an object.\n\n\n\n\n\n\nObjects, pointers, and variables.\n\n\n\n\n\nIn programming, a distinction is often made between an object (such as the number 10) and the variable in which it is stored (such as x). The latter is also called a “pointer”. However, this distinction is not very relevant for most of our purposes. Moreover, in statistics, the word variable often refers to a column of data, rather than to the name of, for instance, the object containing the whole data frame (or table). For that reason, we will use the word object to refer to both the actual object or value and its name. (If you want some extra food for thought and want to challenge your brain a bit, try to see the relationship between the idea of a pointer and the discussion about mutable and immutable objects below.)\n\n\n\nLet us create an object that we call a (an arbitrary name, you can use whatever you want), assign the value 100 to it, and use the class function (R) or type function (Python) to check what kind of object we created (Example 3.1). As you can see, R reports the type of the number as “numeric”, while Python reports it as “int”, short for integer or whole number. Although they use different names, both languages offer very similar data types. Table 3.1 provides an overview of some common basic data types.\n\n\n\n\n\n\n\nExample 3.1 Determining the type of an object\n\nPython codeR code\n\n\n\na = 100\nprint(type(a))\n\n<class 'int'>\n\n\n\n\n\na = 100\nprint(class(a))\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\n\n\n\nTable 3.1: Most used basic data types in Python and R\n\n\nPython\n\nR\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nName\nExample\nName\nExample\n\n\n\nint\n1\ninteger\n1L\nwhole numbers\n\n\nfloat\n1.3\nnumeric\n1.3\nnumbers with decimals\n\n\nstr\n\"Spam\", 'ham'\ncharacter\n\"Spam\", 'ham'\ntextual data\n\n\nbool\nTrue, False\nlogical\nTRUE, FALSE\nthe truth values\n\n\n\n\nLet us have a closer look at the code in Example 3.1 above. The first line is a command to create the object a and store its value 100; and the second is illustrative and will give you the class of the created object, in this case “numeric”. Notice that we are using two native functions of R, print and class, and including a as an argument of class, and the very same class(a) as an argument of print. The only difference between R and Python, here, is that the relevant Python function is called type instead of class.\nOnce created, you can now perform multiple operations with a and other values or new variables as shown in Example 3.2. For example, you could transform a by multiplying a by 2, create a new variable b of value 50 and then create another new object c with the result of a + b.\n\n\n\n\n\n\n\nExample 3.2 Some simple operations\n\nPython codeR code\n\n\n\na = 100\na = a * 2  # equivalent to (shorter) a*=2\nb = 50\nc = a + b\nprint(a, b, c)\n\n200 50 250\n\n\n\n\n\na = 100\na = a*2\nb = 50\nc = a + b\nprint(a)\n\n[1] 200\n\nprint(b)\n\n[1] 50\n\nprint(c)\n\n[1] 250\n\n\n\n\n\n\n\n\n\n\n3.1.1 Storing Single Values: Integers, Floating-Point Numbers, Booleans\nWhen working with numbers, we distinguish between integers (whole numbers) and floating point numbers (numbers with a decimal point, called “numeric” in R). Both Python and R automatically determine the data type when creating an object, but differ in their default behavior when storing a number that can be represented as an int: R will store it as a float anyway and you need to force it to do otherwise, for Python it is the other way round (Example 3.3). We can also convert between types later on, even though converting a float to an int might not be too good an idea, as you truncate your data.\nSo why not just always use a float? First, floating point operations usually take more time than integer operations. Second, because floating point numbers are stored as a combination of a coefficient and an exponent (to the base of 2), many decimal fractions can only approximately be stored as a floating point number. Except for specific domains (such as finance), these inaccuracies are often not of much practical importance. But it explains why calculating 6*6/10 in Python returns 3.6, while 6*0.6 or 6*(6/10) returns 3.5999999999999996. Therefore, if a value can logically only be a whole number (anything that is countable, in fact), it makes sense to restrict it to an integer.\nWe also have a data type that is even more restricted and can take only two values: true or false. It is called “logical” (R) or “bool” (Python). Just notice that boolean values are case sensitive: while in R you must capitalize the whole value (TRUE, FALSE), in Python we only capitalize the first letter: True, False. As you can see in Example 3.3, such an object behaves exactly as an integer that is only allowed to be 0 or 1, and it can easily be converted to an integer.\n\n\n\n\n\n\n\nExample 3.3 Floating point numbers, integers, and boolean values.\n\nPython codeR code\n\n\n\nd = 20\nprint(type(d))\n# forcing python to treat 20 as a float\n\n<class 'int'>\n\nd2 = 20.0\nprint(type(d2))\n\n<class 'float'>\n\ne = int(20.7)\nprint(type(e))\n\n<class 'int'>\n\nprint(e)\n\n20\n\nf = True\nprint(type(f))\n\n<class 'bool'>\n\nprint(int(f))\n\n1\n\nprint(int(False))\n\n0\n\n\n\n\n\nd = 20\nprint(class(d))\n\n[1] \"numeric\"\n\n# forcing R to treat 20 as an int\nd2 = 20L\nprint(class(d2))\n\n[1] \"integer\"\n\ne = as.integer(20.7)\nprint(class(e))\n\n[1] \"integer\"\n\nprint(e)\n\n[1] 20\n\nf = TRUE\nprint(class(f))\n\n[1] \"logical\"\n\nprint(as.integer(f))\n\n[1] 1\n\nprint(as.integer(FALSE))\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 Storing Text\nAs a computational analyst of communication you will usually work with text objects or strings of characters. Commonly simply known as “strings”, such text objects are also referred to as “character vector objects” in R. Every time you want to analyze a social-media message, or any other text, you will be dealing with such strings.\n\n\n\n\n\n\n\nExample 3.4 Strings and bytes.\n\nPython codeR code\n\n\n\ntext1 = \"This is a text\"\nprint(f\"Type of text1: {type(text1)}\")\n\nType of text1: <class 'str'>\n\ntext2 = \"Using 'single' and \\\"double\\\" quotes\"\ntext3 = 'Using \"single\" and \"double\" quotes'\nprint(f\"Are text2 and text3 equal?{text2==text3}\")\n\nAre text2 and text3 equal?False\n\n\n\n\n\ntext1 = \"This is a text\"\nglue(\"Class of text1: {class(text1)}\")\n\nClass of text1: character\n\ntext2 = \"Using 'single' and \\\"double\\\" quotes\"\ntext3 = 'Using \\'single\\' and \"double\" quotes'\nglue(\"Are text2 and text3 equal? {text2==text3}\")\n\nAre text2 and text3 equal? TRUE\n\n\n\n\n\n\nPython codeR code\n\n\n\nsomebytes = text1.encode(\"utf-8\")\nprint(type(somebytes))\n\n<class 'bytes'>\n\nprint(somebytes)\n\nb'This is a text'\n\n\n\n\n\nsomebytes= charToRaw(text1)\nprint(class(somebytes))\n\n[1] \"raw\"\n\nprint(somebytes)\n\n [1] 54 68 69 73 20 69 73 20 61 20 74 65 78 74\n\n\n\n\n\n\n\n\n\nAs you see in Example 3.4, you can create a string by enclosing text in quotation marks. You can use either double or single quotation marks, but you need to use the same mark to begin and end the string. This can be useful if you want to use quotation marks within a string, then you can use the other type to denote the beginning and end of the string. If you need to use a single quotation mark within a single-quoted string, you can escape the quotation mark by prepending it with a backslash (\\'), and similarly for double-quoted strings. To include an actual backslash in a text, you also escape it with a backslash, so you end up with a double backslash (\\\\).\nThe Python example also shows a concept introduced in Python 3.6: the f-string. These are strings that are prefixed with the letter f and are formatted strings. This means that these strings will automatically insert a value where curly brackets indicate that you wish to do so. This means that you can write: print(f\"The value of i is {i}\") in order to print “The value of i is 5” (given that i equals 5). In R, the glue package allows you to use an f-string-like syntax as well: glue(\"The value of i is \\{i\\}\").\nAlthough this will be explained in more detail in Section 5.2.2 9.1, it is good to introduce how computers store text in memory or files. It is not too difficult to imagine how a computer internally handles integers: after all, even though the number may be displayed as a decimal number to us, it can be trivially converted and stored as a binary number (effectively, a series of zeros and ones) — we do not have to care about that. But when we think about text, it is not immediately obvious how a string should be stored as a sequence of zeros and ones, especially given the huge variety of writing systems used for different languages.\nIndeed, there are several ways of how textual characters can be stored as bytes, which are called encodings. The process of moving from bytes (numbers) to characters is called decoding, and the reverse process is called encoding. Ideally, this is not something you should need to think of, and indeed strings (or character vectors) already represent decoded text. This means that often when you read from or write data to a file, you need to specify the encoding (usually UTF-8). However, both Python and R also allow you to work with the raw data (e.g. before decoding) in the form of bytes (Python) or raw (R) data, which is sometimes necessary if there are encoding problems. This is shown briefly in the bottom part of var4. Note that while R shows the underlying hexadecimal byte values of the raw data (so 54 is T, 68 is h and so on) and Python displays the bytes as text characters, in both cases the underlying data type is the same: raw (non-decoded) bytes.\n\n\n3.1.3 Combining Multiple Values: Lists, Vectors, And Friends\nUntil now, we have focused on the basic, initial data types or “vector objects”, as they are called in R. Often, however, we want to group a number of these objects. For example, we do not want to manually create thousands of objects called tweet0001, tweet0002, …, tweet9999 – we’d rather have one list called tweets that contains all of them. You will encounter several names for such combined data structures: lists, vectors, arrays, series, and more. The core idea is always the same: we take multiple objects (be it numbers, strings, or anything else) and then create one object that combines all of them (Example 3.5).\n\n\n\n\n\n\n\nExample 3.5 Collections arrays (such as vectors in R or lists in Python) can contain multiple values\n\nPython codeR code\n\n\n\nscores = [8, 8, 7, 6, 9, 4, 9, 2, 8, 5]\nprint(type(scores))\n\n<class 'list'>\n\ncountries = [\"Netherlands\", \"Germany\", \"Spain\"]\nprint(type(countries))\n\n<class 'list'>\n\n\n\n\n\nscores = c(8, 8, 7, 6, 9, 4, 9, 2, 8, 5)\nprint(class(scores))\n\n[1] \"numeric\"\n\ncountries = c(\"Netherlands\", \"Germany\", \"Spain\")\nprint(class(countries))\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\nAs you see, we now have one name (such as scores) to refer to all of the scores. The Python object in Example 3.5 is called a list, the R object a vector. There are more such combined data types, which have slightly different properties that can be important to know about: first, whether you can mix different types (say, integers and strings); second, what happens if you change the array. We will discuss both points below and show how this relates to different specific types of arrays in Python and R which you can choose from. But first, we will show how to work with them.\nOperations on vectors and lists. One of the most basic operations you can perform on all types of one-dimensional arrays is indexing. It lets you locate any given element or group of elements within a vector using its or their positions. The first item of a vector in R is called 1, the second 2, and so on; in Python, we begin counting with 0. You can retrieve a specific element from a vector or list by simply putting the index between square brackets [] (Example 3.6).\n\n\n\n\n\n\n\nExample 3.6 Slicing vectors and converting data types\n\nPython codeR code\n\n\n\nscores = [\"8\", \"8\", \"7\", \"6\", \"9\", \"4\", \"9\", \"2\", \"8\", \"5\"]\n\nprint(scores[4])\n\n9\n\nprint([scores[0], scores[9]])\n\n['8', '5']\n\nprint(scores[0:4])\n\n# Convert the first 4 scores into numbers\n# Note the use of a list comprehension [.. for ..]\n# This will be explained in the section on loops\n\n['8', '8', '7', '6']\n\nscores_new = [int(e) for e in scores[1:4]]\nprint(type(scores_new))\n\n<class 'list'>\n\nprint(scores_new)\n\n[8, 7, 6]\n\n\n\n\n\nscores=c(\"8\",\"8\",\"7\",\"6\",\"9\",\"4\",\"9\",\"2\",\"8\",\"5\")\n\nscores[5]\n\n[1] \"9\"\n\nscores[c(1, 10)]\n\n[1] \"8\" \"5\"\n\nscores[1:4]\n\n[1] \"8\" \"8\" \"7\" \"6\"\n\n# Convert the first 4 scores into numbers\nscores_new = as.numeric(scores[1:4])\nclass(scores_new)\n\n[1] \"numeric\"\n\nscores_new\n\n[1] 8 8 7 6\n\n\n\n\n\n\n\n\n\nIn the first case, we asked for the score of the 5th student (“9”); in the second we asked for the 1st and 10th position (“8” “5”); and finally for all the elements between the 1st and 4th position (“8” “8” “7” “6”). We can directly indicate a range by using a :. After the colon, we provide the index of the last element (in R), while Python stops just before the index.1 If we want to pass multiple single index values instead of a range in R, we need to create a vector of these indices by using c() (Example 3.6). Take a moment to compare the different ways of indexing between Python and R in Example 3.6!\nIndexing is very useful to access elements and also to create new objects from a part of another one. The last line of our example shows how to create a new array with just the first four entries of scores and store them all as numbers. To do so, we use slicing to get the first four scores and then either change its class using the function as.numeric (in R) or convert the elements to integers one-by-one (Python) (Example 3.6).\n\n\n\n\n\n\n\nExample 3.7 Some more operations on one-dimensional arrays\n\nPython codeR code\n\n\n\n# Appending a new value to a list:\nscores.append(7)\n\n# Create a new list instead of overwriting:\nscores4 = scores + [7]\n\n# Removing an entry:\ndel scores[-10]\n\n# Creating a list containing various ranges\nlist(range(1, 21))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n\nlist(range(-5, 6))\n\n# A range of fractions: 0, 0.2, 0.4, ... 1.0\n# Because range only handles integers, we first\n#   make a range of 0, 2, etc, and divide by 10\n\n[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n\nmy_sequence = [e / 10 for e in range(0, 11, 2)]\n\n\n\n\n# appending a new value to a vector\nscores = c(scores, 7)\n\n# Create a new list instead of overwriting:\nscores4 = c(scores, 7)\n\n# removing an entry from a vector\nscores = scores[-10]\n\n# Creating a vector containing various ranges\nrange1 = 1:20\nrange2 = -5:5\n\n# A range of fractions: 0, 0.2, 0.4, ... 1.0 \nmy_sequence = seq(0,1, by=0.2)\n\n\n\n\n\n\n\n\nWe can do many other things like adding or removing values, or creating a vector from scratch by using a function (Example 3.7). For instance, rather than just typing a large number of values by hand, we often might wish to create a vector from an operator or a function, without typing each value. Using the operator : (R) or the functions seq (R) or range (Python), we can create numeric vectors with a range of numbers.\nCan we mix different types?. There is a reason that the basic data types (numeric, character, etc.) we described above are called “vector objects” in R: The vector is a very important structure in R and consists of these objects. A vector can be easily created with the c function and can only combine elements of the same type (numeric, integer, complex, character, logical, raw). Because the data types within a vector correspond to only one class, when we create a vector with for example numeric data, the class function will display “numeric” and not “vector”.\nIf we try to create a vector with two different data types, R will force some elements to be transformed, so that all elements belong to the same class. For example, if you re-build the vector of scores with a new student who has been graded with the letter b instead of a number (Example 3.8), your vector will become a character vector. If you print it, you will see that the values are now displayed surrounded by \".\n\n\n\n\n\n\n\nExample 3.8 R enforces that all elements of a vector have the same data type ## R code\n\nscores2 = c(8, 8, 7, 6, 9, 4, 9, 2, 8, 5, \"b\")\nprint(class(scores2))\n\n[1] \"character\"\n\nprint(scores2)\n\n [1] \"8\" \"8\" \"7\" \"6\" \"9\" \"4\" \"9\" \"2\" \"8\" \"5\" \"b\"\n\n\n\n\n\n\nIn contrast to a vector, a list is much less restricted: a list does not care whether you mix numbers and text. In Python, such lists are the most common type for creating a one-dimensional array. Because they can contain very different objects, running the type function on them does not return anything about the objects inside the list, but simply states that we are dealing with a list (Example 3.5). In fact, lists can even contain other lists, or any other object for that matter.\nIn R you can also use lists, even though they are much less popular in R than they are in Python, because vectors are better if all objects are of the same type. R lists are created in a similar way as vectors, except that we have to add the word list before declaring the values. Let us build a list with four different kinds of elements, a numeric object, a character object, a square root function (sqrt), and a numeric vector (Example 3.9). In fact, you can use any of the elements in the list through indexing – even the function sqrt that you stored in there to get the square root of 16!\n\n\n\n\n\n\n\nExample 3.9 Lists can store very different objects of multiple data types and even functions\n\nPython codeR code\n\n\n\nmy_list = [33, \"Twitter\", np.sqrt, [1, 2, 3, 4]]\nprint(type(my_list))\n\n# this resolves to sqrt(16):\n\n<class 'list'>\n\nprint(my_list[2](16))\n\n4.0\n\n\n\n\n\nmy_list = list(33, \"Twitter\", sqrt, c(1,2,3,4))\nclass(my_list)\n\n[1] \"list\"\n\n# this resolves to sqrt(16):\nmy_list[[3]](16)\n\n[1] 4\n\n\n\n\n\n\n\n\n\nPython users often like the fact that lists give a lot of flexibility, as they happily accept entries of very different types. But also Python users sometimes may want a stricter structure like R’s vector. This may be especially interesting for high-performance calculations, and therefore, such a structure is available from the numpy (which stands for Numbers in Python) package: the numpy array. This will be discussed in more detail when we deal with data frames in Chapter 5.\n\n\n\n\n\n\nObject references and mutable objects.\n\n\n\n\n\nA subtle difference between Python and R is how they deal with copying objects. Suppose we define \\(x\\) containing the numbers \\(1,2,3\\) (x=[1,2,3] in Python or x=c(1,2,3) in R) and then define an object \\(y\\) to equal \\(x\\) (y=x). In R, both objects are kept separate, so changing \\(x\\) does not affect \\(y\\), which is probably what you expect. In Python, however, we now have two variables (names) that both point to or reference the same object, and if we change \\(x\\) we also change \\(y\\) and vice versa, which can be quite unexpected. Note that if you really want to copy an object in Python, you can run x.copy(). See Example 3.10 for an example.\nNote that this is only important for mutable objects, that is, objects that can be changed. For example, lists in Python and R and vectors in R are mutable because you can replace or append members. Strings and numbers, on the other hand, are immutable: you cannot change a number or string, a statement such as x=x*2 creates a new object containing the value of x*2 and stores it under the name x.\n\n\n\n\n\n\n\n\n\n\nExample 3.10 The (unexpected) behavior of mutable objects\n\nPython codeR code\n\n\n\nx = [1, 2, 3]\ny = x\ny[0] = 99\nprint(x)\n\n[99, 2, 3]\n\n\n\n\n\nx = c(1,2,3)\ny = x\ny[1] = 99\nprint(x)\n\n[1] 1 2 3\n\n\n\n\n\n\n\n\n\nSets and Tuples. The vector (R) and list (Python) are the most frequently used collections for storing multiple objects. In Python there are two more collection types you are likely to encounter. First, tuples are very similar to lists, but they cannot be changed after creating them (they are immutable). You can create a tuple by replacing the square brackets by regular parentheses: x=(1,2,3).\nSecond, in Python there is an object type called a set. A set is a mutable collection of unique elements (you cannot repeat a value) with no order. As it is not properly ordered, you cannot run any indexing or slicing operation on it. Although R does not have an explicit set type, it does have functions for the various set operations, the most useful of which is probably the function unique which removes all duplicate values in a vector. Example 3.11 shows a number of set operations in Python and R, which can be very useful, e.g. finding all elements that occur in two lists.\n\n\n\n\n\n\n\nExample 3.11 Sets\n\nPython codeR code\n\n\n\na = {3, 4, 5}\nmy_list = [3, 2, 3, 2, 1]\nb = set(my_list)\nprint(f\"Set a: {a}; b: {b}\")\n\nSet a: {3, 4, 5}; b: {1, 2, 3}\n\nprint(f\"intersect:  a & b = {a & b}\")\n\nintersect:  a & b = {3}\n\nprint(f\"union:      a | b = {a | b}\")\n\nunion:      a | b = {1, 2, 3, 4, 5}\n\nprint(f\"difference: a - b = {a - b}\")\n\ndifference: a - b = {4, 5}\n\n\n\n\n\na = c(3, 4, 5)\nmy_vector = c(3, 2, 3, 2, 1)\nb = unique(my_vector)\nprint(b)\n\n[1] 3 2 1\n\nprint(intersect(a,b))\n\n[1] 3\n\nprint(union(a,b))\n\n[1] 3 4 5 2 1\n\nprint(setdiff(a,b))\n\n[1] 4 5\n\n\n\n\n\n\n\n\n\n\n\n3.1.4 Dictionaries\nPython dictionaries are a very powerful and versatile data type. Dictionaries contain unordered2 and mutable collections of objects that contain certain information in another object. Python generates this data type in the form of {key : value} pairs in order to map any object by its key and not by its relative position in the collection. Unlike in a list, in which you index with an integer denoting the position in a list, you can index a dictionary using the key. This is the case shown in Example 3.12, in which we want to get the values of the object “positive” in the dictionary sentiments and of the object “A” in the dictionary grades. You will find dictionaries very useful in your journey as a computational scientist or practitioner, since they are flexible ways to store and retrieve structured information. We can create them using the curly brackets {} and including each key-value pair as an element of the collection (Example 3.12).\nIn R, the closest you can get to a Python dictionary is to use lists with named elements. This allows you to assign and retrieve values by key, however the key is restricted to names, while in Python most objects can be used as keys. You create a named list with d = list(name=value) and access individual elements with either d$name or d[[\"name\"]].\n\n\n\n\n\n\n\nExample 3.12 Key-value pairs in Python dictionaries and R named lists\n\nPython codeR code\n\n\n\nsentiments = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\nprint(type(sentiments))\n\n<class 'dict'>\n\nprint(\"Sentiment for positive:\", sentiments[\"positive\"])\n\nSentiment for positive: 1\n\ngrades = {}\ngrades[\"A\"] = 4\ngrades[\"B\"] = 3\ngrades[\"C\"] = 2\ngrades[\"D\"] = 1\n\nprint(f\"Grade for A: {grades['A']}\")\n\nGrade for A: 4\n\nprint(grades)\n\n{'A': 4, 'B': 3, 'C': 2, 'D': 1}\n\n\n\n\n\nsentiments = list(positive=1, neutral=0, \n                  negative=-1)\nprint(class(sentiments))\n\n[1] \"list\"\n\nprint(glue(\"Sentiment for positive: \",\n           sentiments$positive))\n\nSentiment for positive: 1\n\ngrades =  list()\ngrades$A = 4\ngrades$B = 3\ngrades$C = 2\ngrades$D = 1\n# Note: grades[[\"A\"]] is equivalent to grades$A\nprint(glue(\"Grade for A: {grades[['A']]}\"))\n\nGrade for A: 4\n\nprint(glue(\"Grade for A: {grades$A}\"))\n\nGrade for A: 4\n\nprint(grades)\n\n$A\n[1] 4\n\n$B\n[1] 3\n\n$C\n[1] 2\n\n$D\n[1] 1\n\n\n\n\n\n\n\n\n\nA good analogy for a dictionary is a telephone book (imagine a paper one, but it actually often holds true for digital phone books as well): the names are the keys, and the associated phone numbers the values. If you know someone’s name (the key), it is very easy to look up the corresponding values: even in a phone book of thousands of pages, it takes you maybe 10 or 20 seconds to look up the name (key). But if you know someone’s phone number (the value) instead and want to look up the name, that’s very inefficient: you need to read the whole phone book until you find the number.\nJust as the elements of a list can be of any type, and you can have lists of lists, you can also nest dictionaries to get dicts of dicts. Think of our phone book example: rather than storing just a phone number as value, we could store another dict with the keys “office phone”, “mobile phone”, etc. This is very often done, and you will come across many examples dealing with such data structures. You have one restriction, though: the keys in a dictionary (as opposed to the values) are not allowed to be mutable. After all, imagine that you could use a list as a key in a dictionary, and if at the same time, some other pointer to that very same list could just change it, this would lead to a quite confusing situation.\n\n\n3.1.5 From One to More Dimensions: Matrices and \\(n\\)-Dimensional Arrays\nMatrices are two-dimensional rectangular datasets that include values in rows and columns. This is the kind of data you will have to deal with in many analyses shown in this book, such as those related to machine learning. Often, we can generalize to higher dimensions.\n\n\n\n\n\n\n\nExample 3.13 Working with two- or \\(n\\)-dimensional arrays\n\nPython codeR code\n\n\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(matrix)\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\narray2d = np.array(matrix)\nprint(array2d)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\n\n\n\nmy_matrix = matrix(c(0, 0, 1, 1, 0, 1), \n    nrow = 2, ncol = 3, byrow = TRUE)\nprint(dim(my_matrix))\n\n[1] 2 3\n\nprint(my_matrix)\n\n     [,1] [,2] [,3]\n[1,]    0    0    1\n[2,]    1    0    1\n\nmy_matrix2 = matrix(c(0, 0, 1, 1, 0, 1), \n    nrow = 2, ncol = 3, byrow = FALSE)\nprint(my_matrix2)\n\n     [,1] [,2] [,3]\n[1,]    0    1    0\n[2,]    0    1    1\n\n\n\n\n\n\n\n\n\nIn Python, the easiest representation is to simply construct a list of lists. This is, in fact, often done, but has the disadvantage that there are no easy ways to get, for instance, the dimensions (the shape) of the table, or to print it in a neat(er) format. To get all that, one can transform the list of lists into an array, a datastructure provided by the package numpy (see Chapter 5 for more details).\nTo create a matrix in R, you have to use the function matrix and create a vector of values with the indication of how many rows and columns will be on it. We also have to tell R if the order of the values is determined by the row or not. In Example 3.13, we create two matrices in which we vary the byrow argument to be TRUE and FALSE, respectively, to illustrate how it changes the values of the matrix, even when the shape (\\(2 \\times3\\)) remains identical. As you may imagine, we can operate with matrices, such as adding up two of them.\n\n\n3.1.6 Making Life Easier: Data Frames\nSo far, we have discussed the general built-in collections that you find in most programming languages such as the list and array. However, in data science and statistics you are very likely to encounter a specific collection type that we haven’t discussed yet: the Data frame. Data frames are discussed in detail in Chapter 5, but for completeness we will also introduce them briefly here.\nData frames are user-friendly data structures that look very much like what you find in SPSS, Stata, or Excel. They will help you in a wide range of statistical analysis. A data frame is a tabular data object that includes rows (usually the instances or cases) and columns (the variables). In a three-column data frame, the first variable can be numeric, the second character and the third logical, but the important thing is that each variable is a vector and that all these vectors must be of the same length. We create data frames from scratch using the data.frame() function. Let’s generate a simple data frame of three instances (each case is an author of this book) and three variables of the types numeric (age), character (country where they obtained their master degree) and logic (living abroad, whether they currently live outside the country in which they were born) (Example 3.14). Notice that you have the label of the variables at the top of each column and that it creates an automatic numbering for indexing the rows.\n\n\n\n\n\n\n\nExample 3.14 Creating a simple data frame\n\nPython codeR code\n\n\n\nauthors = pd.DataFrame(\n    {\n        \"age\": [38, 36, 39],\n        \"countries\": [\"Netherlands\", \"Germany\", \"Spain\"],\n        \"living_abroad\": [False, True, True],\n    }\n)\nprint(authors)\n\n   age    countries  living_abroad\n0   38  Netherlands          False\n1   36      Germany           True\n2   39        Spain           True\n\n\n\n\n\nauthors = data.frame(age = c(38, 36, 39), \n  countries = c(\"Netherlands\",\"Germany\",\"Spain\"), \n  living_abroad= c(FALSE, TRUE, TRUE))\nprint(authors)\n\n  age   countries living_abroad\n1  38 Netherlands         FALSE\n2  36     Germany          TRUE\n3  39       Spain          TRUE"
  },
  {
    "objectID": "chapter03.html#sec-controlstructures",
    "href": "chapter03.html#sec-controlstructures",
    "title": "3  Programming concepts for data analysis",
    "section": "3.2 Simple Control Structures: Loops and Conditions",
    "text": "3.2 Simple Control Structures: Loops and Conditions\n\n\n\n\n\n\nControl structures in Python and R.\n\n\n\n\n\nThis section and the next explain the working of control structures such as loops, conditions, and functions. These exist (and are very useful) in both Python and R. In R, however, you do not need them as much because most functions can work on whole columns in one go, while in Python you often run things on each row of a column and sometimes do not use data frames at all. Thus, if you are primarily interested in using R you could consider skipping the remainder of this chapter for now and returning later when you are ready to learn more. If you are learning Python, we strongly recommend continuing with this chapter, as control structures are used in many of the examples in the book.\n\n\n\nHaving a clear understanding of objects and data types is a first step towards comprehending how object-orientated languages such as R and Python work, but now we need to get some literacy in writing code and interacting with the computer and the objects we created. Learning a programming language is just like learning any new language. Imagine you want to speak Italian or you want to learn how to play the piano. The first thing will be to learn some words or musical notes, and to get familiarized with some examples or basic structures – just as we did in Chapter 2. In the case of Italian or the piano, you would then have to learn some grammar: how to form sentences, how play some chords; or, more generally, how to reproduce patterns. And this is exactly how we now move on to acquiring computational literacy: by learning some rules to make the computer do exactly what you want.\nRemember that you can interact with R and Python directly on their consoles just by typing any given command. However, when you begin to use several of these commands and combine them you will need to put all these instructions into a script that you can then run partially or entirely. Recall Section 1.4, where we showed how IDEs such as RStudio (and Pycharm) offer both a console for directly typing single commands and a larger window for writing longer scripts.\nBoth R and Python are interpreted languages (as opposed to compiled languages), which means that interacting with them is very straightforward: You provide your computer with some statements (directly or from a script), and your computer reacts. We call a sequence of these statements a computer program. When we created objects by writing, for instance, a = 100, we already dealt with a very basic statement, the assignment statement. But of course the statements can be more complex.\nIn particular, we may want to say more about how and when statements need to be executed. Maybe we want to repeat the calculation of a value for each item on a list, or maybe we want to do this only if some condition is fulfilled.\nBoth R and Python have such loops and conditional statements, which will make your coding journey much easier and with more sophisticated results because you can control the way your statements are executed. By controlling the flow of instructions you can deal with a lot of challenges in computer programming such as iterating over unlimited cases or executing part of your code as a function of new inputs.\nIn your script, you usually indicate such loops and conditions visually by using indentation. Logical empty spaces – two in R and four in Python – depict blocks and sub-blocks on your code structure. As you will see in the next section, in R, using indentation is optional, and curly brackets will indicate the beginning ({) and end (}) of a code block; whereas in Python, indentation is mandatory and tells your interpreter where the block starts and ends.\n\n3.2.1 Loops\nLoops can be used to repeat a block of statements. They are executed once, indefinitely, or until a certain condition is reached. This means that you can operate over a set of objects as many times as you want just by giving one instruction. The most common types of loops are for, while, and repeat (do-while), but we will be mostly concerned with so-called for-loops. Imagine you have a list of headlines as an object and you want a simple script to print the length of each message. Of course you can go headline by headline using indexing, but you will get bored or will not have enough time if you have thousands of cases. Thus, the idea is to operate a loop in the list so you can get all the results, from the first until the last element, with just one instruction. The syntax of the for-loop is:\n\nPython codeR code\n\n\n\nfor val in sequence:\n    statement1\n    statement2\n    statement3\n\n\n\n\nfor (val in sequence) {\n    statement1 \n    statement2 \n    statement3\n}\n\n\n\n\nAs Example 3.15 illustrates, every time you find yourself repeating something, for instance printing each element from a list, you can get the same results easier by iterating or looping over the elements of the list, in this case. Notice that you get the same results, but with the loop you can automate your operation writing few lines of code. As we will stress in this book, a good practice in coding is to be efficient and harmonious in the amount of code we write, which is another justification for using loops.\n\n\n\n\n\n\n\nExample 3.15 For-loops let you repeat operations.\n\nPython codeR code\n\n\n\nheadlines = [\n    \"US condemns terrorist attacks\",\n    \"New elections forces UK to go back to the UE\",\n    \"Venezuelan president is dismissed\",\n]\n# Manually counting each element\nprint(\"manual results:\")\n\nmanual results:\n\nprint(len(headlines[0]))\n\n29\n\nprint(len(headlines[1]))\n\n44\n\nprint(len(headlines[2]))\n# and the second is using a for-loop\n\n33\n\nprint(\"for-loop results:\")\n\nfor-loop results:\n\nfor x in headlines:\n    print(len(x))\n\n29\n44\n33\n\n\n\n\n\nheadlines = list(\"US condemns terrorist attacks\", \n  \"New elections forces UK to go back to the UE\",\n  \"Venezuelan president is dismissed\")\n# Manually counting each element\nprint(\"manual results:  \")\n\n[1] \"manual results:  \"\n\nprint(nchar(headlines[1]))\n\n[1] 29\n\nprint(nchar(headlines[2]))\n\n[1] 44\n\nprint(nchar(headlines[3]))\n\n[1] 33\n\n# Using a for-loop\nprint(\"for-loop results:\")\n\n[1] \"for-loop results:\"\n\nfor (x in headlines){\n  print(nchar(x))\n}\n\n[1] 29\n[1] 44\n[1] 33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t repeat yourself!\n\n\n\n\n\nYou may be used to copy-pasting syntax and slightly changing it when working with some statistics program: you run an analysis and then you want to repeat the same analysis with different datasets or different specifications. But this is error-prone and hard to maintain, as it involves a lot of extra work if you want to change something. In many cases where you find yourself pasting multiple versions of your code, you would probably be better using a for-loop instead.\n\n\n\nAnother way to iterate in Python is using list comprehensions (not available natively in R), which are a stylish way to create list of elements automatically even with conditional clauses. This is the syntax:\nnewlist  = [expression for item in list if conditional]\nIn Example 3.16 we provide a simple example (without any conditional clause) that creates a list with the number of characters of each headline. As this example illustrates, list comprehensions allow you to essentially write a whole for-loop in one line. Therefore, list comprehensions are very popular in Python.\n\n\n\n\n\n\n\nExample 3.16 List comprehensions are very popular in Python ## Python code\n\nlen_headlines = [len(x) for x in headlines]\nprint(len_headlines)\n\n# Note: the \"list comprehension\" above is\n#   equivalent to the more verbose code below:\n\n[29, 44, 33]\n\nlen_headlines = []\nfor x in headlines:\n    len_headlines.append(len(x))\nprint(len_headlines)\n\n[29, 44, 33]\n\n\n\n\n\n\n\n\n3.2.2 Conditional Statements\nConditional statements will allow you to control the flow and order of the commands you give the computer. This means you can tell the computer to do this or that, depending on a given circumstance. These statements use logic operators to test if your condition is met (True) or not (False) and execute an instruction accordingly. Both in R and Python, we use the clauses if, else if (elif in Python), and else to write the syntax of the conditional statements. Let’s begin showing you the basic structure of the conditional statement:\n\nPython codeR code\n\n\n\nif condition:\n    statement1\nelif other_condition:\n    statement2\nelse:\n    statement3\n\n\n\n\nif (condition) {\n    statement1\n} else if (other_condition) {\n    statement2\n} else {\n    statement3\n}\n\n\n\n\nSuppose you want to print the headlines of Example 3.15 only if the text is less than 40 characters long. To do this, we can include the conditional statement in the loop, executing the body only if the condition is met (Example 3.17)\n\n\n\n\n\n\n\nExample 3.17 A simple conditional control structure\n\nPython codeR code\n\n\n\nfor x in headlines:\n    if len(x) < 40:\n        print(x)\n\nUS condemns terrorist attacks\nVenezuelan president is dismissed\n\n\n\n\n\nfor (x in headlines){\n  if (nchar(x)<40) {\n    print(x)}\n  }\n\n[1] \"US condemns terrorist attacks\"\n[1] \"Venezuelan president is dismissed\"\n\n\n\n\n\n\n\n\n\nWe could also make it a bit more complicated: first check whether the length is smaller than 40, then check whether it is exactly 44 (elif / else if), and finally specify what to do if none of the conditions was met (else).\nIn Example 3.18, we will print the headline if it is shorter than 40 characters, print the string “What a coincidence!” if it is exactly 44 characters, and print “Too Low” in all other cases. Notice that we have included the clause elif in the structure (in R it is noted else if). elif is a combination of else and if: if the previous condition is not satisfied, this condition is checked and the corresponding code block (or else block) is executed. This avoids having to nest the second if within the else, but otherwise the reasoning behind the control flow statements remains the same.\n\n\n\n\n\n\n\nExample 3.18 A more complex conditional control structure\n\nPython codeR code\n\n\n\nfor x in headlines:\n    if len(x) < 30:\n        print(x)\n    elif len(x) == 44:\n        print(\"What a coincidence!\")\n    else:\n        print(\"Too low\")\n\nUS condemns terrorist attacks\nWhat a coincidence!\nToo low\n\n\n\n\n\nfor (x in headlines) {\n  if (nchar(x)<30) {\n    print(x)\n  } else if (nchar(x)==44) {\n      print(\"What a coincidence!\")\n  } else {\n      print(\"Too low\")\n  }\n}\n\n[1] \"US condemns terrorist attacks\"\n[1] \"What a coincidence!\"\n[1] \"Too low\""
  },
  {
    "objectID": "chapter03.html#sec-functions",
    "href": "chapter03.html#sec-functions",
    "title": "3  Programming concepts for data analysis",
    "section": "3.3 Functions and Methods",
    "text": "3.3 Functions and Methods\nFunctions and methods are fundamental concepts in writing code in object-orientated programming. Both are objects that we use to store a set of statements and operations that we can use later without having to write the whole syntax again. This makes our code simpler and more powerful.\nWe have already used some built-in functions, such as length and class (R) and len and type (Python) to get the length of an object and the class to which it belongs. But, as you will learn in this chapter, you can also write your own functions. In essence, a function takes some input (the arguments supplied between brackets) and returns some output. Methods and functions are very similar concepts. The difference between them is that the functions are defined independently from the object, while methods are created based on a class, meaning that they are associated with an object. For example, in Python, each string has an associated method lower, so that writing 'HELLO'.lower() will return ‘hello’. In R, in contrast, one uses a function, tolower('HELLO'). For now, it is not really important to know why some things are implemented as a method and some are implemented as a function; it is partly an arbitrary choice that the developers made, and to fully understand it, you need to dive into the concept of classes, which is beyond the scope of this book.\n\n\n\n\n\n\nTab completion.\n\n\n\n\n\nBecause methods are associated with an object, you have a very useful trick at your disposal to find out which methods (and other properties of an object) there are: TAB completion. In Jupyter, just type the name of an object followed by a dot (e.g., a.<TAB> in case you have an object called a) and hit the TAB key. This will open a drop-down menu to choose from.\n\n\n\nWe will illustrate how to create simple functions in R and Python, so you will have a better understanding of how they work. Imagine you want to create two functions: one that computes the 60% of any given number and another that estimates this percentage only if the given argument is above the threshold of 5. The general structure of a function in R and Python is:\n\nPython codeR code\n\n\n\ndef f(par1, par2=0):\n    statements\n    return return_value\n\nresult = f(arg1, arg2)\nresult = f(par1=arg1, par2=arg2)\nresult = f(arg1, par2=arg2)\nresult = f(arg1)\n\n\n\n\nf = function(par1, par2=0) {\n   statements \n   return_value\n}\nresult = f(arg1, arg2)\nresult = f(par1=arg1, par2=arg2)\nresult = f(arg1, par2=arg2)\nresult = f(arg1)\n\n\n\n\nIn both cases, this defines a function called f, with two arguments, arg_1 and arg_2. When you call the function, you specify the values for these parameters (the arguments) between brackets after the function name. You can then store the result of the function as an object as normal.\nAs you can see in the syntax above, you have some choices when specifying the arguments. First, you can specify them by name or by position. If you include the name (f(param1=arg1)) you explicitly bind that argument to that parameter. If you don’t include the name (f(arg1, arg2)) the first argument matches the first parameter and so on. Note that you can mix and match these choices, specifying some parameters by name and others by position.\nSecond, some functions have optional parameters, for which they provide a default value. In this case, par2 is optional, with default value 0. This means that if you don’t specify the parameter it will use the default value instead. Usually, the mandatory parameters are the main objects used by the function to do its work, while the optional parameters are additional options or settings. It is recommended to generally specify these options by name when you call a function, as that increases the readability of the code. Whether to specify the mandatory arguments by name depends on the function: if it’s obvious what the argument does, you can specify it by position, but if in doubt it’s often better to specify them by name.\nFinally, note that in Python you explicitly indicate the result value of the function with return value. In R, the value of the last expression is automatically returned, although you can also explicitly call return(value).\nExample 3.19 shows how to write our function and how to use it.\n\n\n\n\n\n\n\nExample 3.19 Writing functions\n\nPython codeR code\n\n\n\n# The first function just computes 60% of the value\ndef perc_60(x):\n    return x * 0.6\n\nprint(perc_60(10))\n\n6.0\n\nprint(perc_60(4))\n\n# The second function only computes 60% it the\n#  value is bigger than 5\n\n2.4\n\ndef perc_60_cond(x):\n    if x > 5:\n        return x * 0.6\n    else:\n        return x\n\nprint(perc_60_cond(10))\n\n6.0\n\nprint(perc_60_cond(4))\n\n4\n\n\n\n\n\n#The first function just computes 60% of the value\nperc_60 = function(x) x*0.6\n\nprint(perc_60(10))\n\n[1] 6\n\nprint(perc_60(4))\n\n[1] 2.4\n\n# The second function only computes 60% it the\n#  value is bigger than 5\nperc_60_cond = function(x) {\n  if (x>5) {\n    return(x*0.6)\n  } else {\n    return(x)\n  }\n}\nprint(perc_60_cond(10))\n\n[1] 6\n\nprint(perc_60_cond(4))\n\n[1] 4\n\n\n\n\n\n\n\n\n\nThe power of functions, though, lies in scenarios where they are used repeatedly. Imagine that you have a list of 5 (or 5 million!) scores and you wish to apply the function perc_60_cond to all the scores at once using a loop. This costs you only two extra lines of code (Example 3.20).\n\n\n\n\n\n\n\nExample 3.20 Functions are particular useful when used repeatedly\n\nPython codeR code\n\n\n\n# Apply the function in a for-loop\nscores = [3, 4, 5, 7]\nfor x in scores:\n    print(perc_60_cond(x))\n\n3\n4\n5\n4.2\n\n\n\n\n\n# Apply the function in a for-loop\nscores = list(3,4,5,6,7)\nfor (x in scores) {\n  print(perc_60_cond(x))\n}\n\n[1] 3\n[1] 4\n[1] 5\n[1] 3.6\n[1] 4.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nA specific type of Python function that you may come across at some point (for instance, in Section ?sec-crawling) is the generator. Think of a function that returns a list of multiple values. Often, you do not need all values at once: you may only need the next value at a time. This is especially interesting when calculating the whole list would take a lot of time or a lot of memory. Rather than waiting for all values to be calculated, you can immediately begin processing the first value before the next arrives; or you can work with data so large that it doesn’t all fit into your memory at the same time. You recognize a generator by the yield keyword instead of a return keyword (Example 3.21)\n\n\n\n\n\n\n\n\n\n\nExample 3.21 Generators behave like lists in that you can iterate (loop) over them, but each element is only calculated when it is needed. Hence, they do not have a length. ## Python code\n\nmylist = [35, 2, 464, 4]\n\ndef square1(somelist):\n    listofsquares = []\n    for i in somelist:\n        listofsquares.append(i**2)\n    return listofsquares\n\nmysquares = square1(mylist)\nprint(\n    f\"\"\"As a list:\ntype: {type(mysquares)}\nrepresentation: {mysquares}\nentries: {[x for x in mysquares]}\nsecond pass: {[x for x in mysquares]}\nlength: {len(mysquares)}\"\"\"\n)\n\nAs a list:\ntype: <class 'list'>\nrepresentation: [1225, 4, 215296, 16]\nentries: [1225, 4, 215296, 16]\nsecond pass: [1225, 4, 215296, 16]\nlength: 4\n\ndef square2(somelist):\n    for i in somelist:\n        yield i**2\n\nmysquares = square2(mylist)\nprint(\n    f\"\"\"As a generator:\ntype: {type(mysquares)}\nrepresentation: {mysquares}\nentries: {[x for x in mysquares]}\nsecond pass: {[x for x in mysquares]}\"\"\"\n)\n# This throws an error (generators have no length)\n# print(f\"length: {len(mysquares)}\")\n\nAs a generator:\ntype: <class 'generator'>\nrepresentation: <generator object square2 at 0x7f686aee19a0>\nentries: [1225, 4, 215296, 16]\nsecond pass: []\n\n\n\n\n\n\nSo far you have taken your first steps as a programmer, but there are many more advanced things to learn that are beyond the scope of this book. You can find a lot of literature, online documentation and even wonderful Youtube tutorials to keep learning. We can recommend the books by Crawley (2012) and VanderPlas (2016) to have more insights into R and Python, respectively. In the next chapter, we will go deeper into the world of code in order to learn how and why you should re-use existing code, what to do if you get stuck during your programming journey and what are the best practices when coding. [^1]: In both R and Python, the equals sign (=) can be used to assign values. In R, however, the traditional way of doing this is using an arrow (<-). In this book we will use the equals sign for assignment in both languages, but remember that for R, x=10 and x<-10 are essentially the same.\n\n\n\n\nCrawley, Michael J. 2012. The r Book. 2nd Edition. Wiley.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly."
  },
  {
    "objectID": "chapter04.html#sec-code",
    "href": "chapter04.html#sec-code",
    "title": "4  How to write code",
    "section": "4.1 Re-using Code: How Not to Re-Invent the Wheel",
    "text": "4.1 Re-using Code: How Not to Re-Invent the Wheel\nJust as in any human language, programming languages also consist of a vocabulary, syntax rules, and expressions. Using the proper words and grammar, you can build from scratch any idea your imagination allows. That’s a wonderful thing! But, let’s be honest: the language itself, the expressions, ideas, and all the abstract constructs seldom come originally from you. And in fact, that’s great as well: otherwise, you’d have to deeply think of every element before talking and expressing any thought. Instead, you use pre-existing rules, ideas, perceptions, and many different narratives to create your own messages to interact with the world. It’s the same with coding: you never start from scratch.\nOf course you can code anything you want from the very beginning, even just using 0’s and 1’s! When reading through the previous chapters, maybe you even started to think that complex operations will be exhausting and will take a really long time. After all, from the basic operations we did to a useful statistical model seems like a long way to go.\nLuckily, this is not the case. There is almost no project in which computational scientists, data analysts, or developers do not re-use earlier code in order to achieve their goals more quickly and efficiently. The more common a task is, the greater the chance that you do not have to re-invent the wheel. Of course, you have to give credit where credit is due, but it is not uncommon to paste code snippets from others into your own code and adapt them. This is especially true for standard operations, for which there are only so many ways to achieve the desired result.\nThere are different ways to re-use earlier code. One is to copy and adapt raw lines of code written by someone else or by yourself in the past. In fact, there are many online repositories such as GitHub or BitBucket that contain many programs and well-documented code examples (see Section 4.3). When conducting computational analyses, you will spend a significant part of your time in such repositories trying to understand what others have done and figuring out how you can use it in your own work. Of course, make sure that the license of the code allows you to use it in the way you want. Also, give credit where credit is due: at the very least, place a comment with a link in your code to indicate what inspired you.\nAnother way is to build or import functions that summarize many lines of code into a simpler command, as we explained in Section 3.3. Functions are indeed powerful ways of reusing code, since you do not have to write the same code over and over again if you need it in multiple places. Packages are probably the most elegant approach to recycle the work done by other colleagues. In Section 1.4 you already learned how to install a package, and you probably noticed how easy it is to bring many pre-built functionalities onto your workspace. You can also write and publish your own package in the future to help your colleagues to write less code and to be more efficient in their daily job (see also Section ?sec-publishingsource)!\nMany questions can arise here: what to re-use? When to use a function written by someone else instead of writing the code yourself? Which scripts and sources are trustworthy? Which is the best package to choose? How many packages should we use within the same project? Should we care about package versions? And must we know every package that is released in our field? There are of course multiple answers to these questions and it will be probably a matter of practice how to obtain the most appropriate ones. In general, we can say that one premise is to re-use and share code as much as you can. This idea is limited by constraints of quality, availability, parsimony, updates, and expertise. In other words, when recycling code we should think of the reputation of the source, the difficulty of accessing it, the risk of having an excessive and messy number of inputs, the need to share the last developments with your colleagues, and the fact that you will never be able to know everything.\nLet’s take an example. Imagine you want to compute the Levenshtein distance between two strings. That’s a pretty straightforward metric that answers the question: “How many edits (removing/changing/inserting characters) do I need to apply to transform string1 into string2?” It can be used for plagiarism detection, but may be interesting for us to determine, for instance, whether a newspaper copied some content from somewhere else, even if small changes have been applied. You could now try to write some code to calculate that (and we are sure you could do that if you invested some time in it!), but it is such a common problem that it has been solved multiple times before. You could, for instance, look up some functions that are known to solve the problem and copy-paste them into your code. You can find a large number of different implementations for both Python and R here: en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance. You can then choose and copy-paste the one which is most appropriate for you. One that is very fast, because you want to compare a huge set of strings? One that is easy to understand? One that uses only a few lines of code to not distract the reader? Alternatively, if you look for available packages for Python and R, you see that there are multiple packages that you can install with install.packages (R) or pip (Python) and then import. If you go for that route, you don’t need to care about the internal workings and can “abstract away” and outsource the problem – on the other hand, the users of your code now have one more dependency to install before they can use your code.\nIn the case of package selection, we understand it can be quite overwhelming, with so many different packages from different contributors. In fact, sometimes the same task, such as topic modeling, can be done using multiple different packages. So, how to find and choose the best package? Besides resources like this book, the most important guide is probably the community around you: using packages that a lot of other people also use means that the package is probably well maintained and documented, and that there is a community to ask for help if needed. Since all packages on Pypi and CRAN are free to download and install, however, you can also shop around and see what the various packages do. When comparing different packages, it is always good to check their documentation and their GitHub page: packages that are well documented and that are updated frequently are often a good choice.\nFor example, the authors of this book had several intensive discussions of which packages to mention and use in the proposed exercises, an issue that became complex given the variety of topics addressed in this book. In the case of text analysis, a library such as NLTK for Python was incredibly popular among computational analysts until a few years ago becoming a package of reference in the field, but it has – at least for some applications – been overpassed by friendly and sophisticated new packages for natural language processing like SpaCy. So, which should we have included in this book? The one which is well-known (with excellent documentation by the way) and still used by thousands of practitioners and students around the world, or the one which is penetrating the market because of its easiness and advantages? Moreover, when choosing the second option, are we sure a more trendy package is going to be stable in time or is it going to be superseded by a different one in just few months?\nThere isn’t the one golden way of how to re-use code and packages, but this dynamic scenario also depicts an exciting and provocative field that forces us to keep ourselves up to date."
  },
  {
    "objectID": "chapter04.html#sec-errors",
    "href": "chapter04.html#sec-errors",
    "title": "4  How to write code",
    "section": "4.2 Understanding Errors and Getting Help",
    "text": "4.2 Understanding Errors and Getting Help\nEven though re-using code makes writing programs easier and less error-prone, every programmer makes mistakes. Programming can be a frustrating endeavor, and you will encounter error messages, bugs, and problems that you don’t know how to fix. This section shows how error messages are useful rather than scary and lists the main error messages encountered in the beginning. It explains how to search for help within the R/Python documentation, how to use online resources, and how to formulate questions to your instructor or community so you get a useful answer.\nIf you tried out some of the concepts in Chapter 3, you have probably already come across some typical or basic errors in programming. Maybe you tried to call a function from a library that you forgot to load before, or maybe you tried to multiply a string with a float. There are thousands of errors that you will encounter, and there is no exhaustive list of them, so you won’t find a complete structured catalogue to solve your problems when coding. This might seem a rough road for any scientist but in fact you will get used to finding the answers by different means.\n\n4.2.1 Error Messages\nThere are two common strategies to avoid getting stuck and move on with your task: one is to understand the type of error you are getting, and the other is to know where to go to obtain valuable help. We would add a third one: be patient and do not despair!\nBoth R and Python produce warning or error messages when something is wrong in your code. Beginning computational researchers may sometimes feel afraid, confused, or even frustrated when they get such a painful message (we have all felt this) and some then would become so anxious that they don’t pay enough attention to the text of the error message thinking it will not be helpful to solve the problem and blaming themselves for not being a perfect programmer. But the more you code, the more you realize that getting these error messages is just part of the routine and that it is very useful to carefully read the warning instead of skipping it.\nIn most cases, the error message in your console will tell you exactly where the problem is: a specific line or operation within your code. With this information in many cases you will quickly identify what the problem is about and you will know how to solve it. One of the most common causes for errors is just very silly typos!\nNext to the location (the line number), the error message will also tell you more about the problem. For example, when trying to multiply the float object a by the string object b you will get “Error in a * b : non-numeric argument to binary operator” in R or “TypeError: can’t multiply sequence by non-int of type ‘float’” in Python. As intimidating as this language may sound in the first place, if you re-read it, you will realize that it, in fact, explains exactly what went wrong. This helps you to understand what you did wrong and enable you to fix it.\nIf you get a warning error that you don’t understand or get an incorrect result in your code you have three options to get more information: use the help commands to know more about any object or function (help(object) in both R and Python); read the documentation of base R, base Python or of any individual package (there are plenty of them online!); and look at the wonderful community of worldwide coders, read what they have discussed so far or even pose a question to challenge their minds.\nLet’s consider this third option. Imagine you read the text of an error message and you feel you don’t understand it. It may be because the wording is too complex or because it just gives an “error code” (i.e. “error 401 - Unauthorized” when trying to connect to the Twitter API). If your first thought is to try searching for it in Google, then this is completely correct: it might take you to code documentation, or better to an online discussion in sites such as Stack Overflow, which is a useful question and answer website for coders (see Figure 4.1). It is very likely that some colleagues have already posed a question about the meaning of that error and others have already provided an answer to what it means and especially help with how to fix it.\n\n\n\nFigure 4.1: A online discussion in Stack Overflow about a warning message\n\n\nDepending on the complexity and novelty of your problem you might find a helpful answer in a few minutes or it might take you hours. Never get desperate if you visit many discussions without understanding everything directly: you may have to come back to some of them after reading all. Moreover, some answers will include the exact code you need (ready for copy-and-paste), code to be adapted (i.e. changing the name of your variables) or in pseudocode (informal description of the code). In all of the cases you will be the responsible for making sense of the huge (and sometimes messy) amount of sources you will come across.\nIt is of course possible that you don’t get what you need in previous discussions. In that case you will be able to create your own question and wait for someone to reply. If you decide to do this, take some advice into account. First, be sure that the answer is not elsewhere within the same website (a first answer could be just a link to a previous post!). Second, don’t worry that your question is silly or too basic: you will find in the community all kinds of coders, from those who are taking their first steps to those who are very advanced. Third, be clear, specific, and focus on what you need to solve. This is probably the most important advice since it is necessary that other coders understand what you need in a few words (not philosophical discussions or previous elaborated rationales) so they can decide to spend some minutes of their time and help you. It is a very common practice that you copy in the questions the warning message or the code you are having trouble with because peers can even fix it themselves and give the solution right away. Do not worry if your post receives a lot of replies after getting what you needed. This thread might also help others in the future!\n\n\n4.2.2 Debugging Strategies\nIt’s not always straightforward to understand what is going wrong. Maybe your script does not even produce an error message, but just produces some unexpected result.\nOf course, every program is different and there is not one way to solve every issue, but there are some simple strategies that help you debugging your code. The underlying core principle is to better understand what exactly is happening.\n\nPrint more. For example, if you have a for-loop, just add a print statement to the loop that prints the current value that is processed, or some other information that helps you understanding what data exactly are processed, or what intermediate result is achieved at which step. There are more advanced tools for keeping track of values, such as the so-called debuggers in advanced IDEs or the logging module in Python, but a couple of extra print functions can serve the same purpose.\n\nKeep track of which code blocks have been executed how often. For instance, maybe you have some if statement, but the condition is simply never True, so that the whole code block is never executed. You can create an integer with value 0 at the beginning of the code, and then increment it by one within the code block. If you print it afterwards, you know how often the block has been visited.\nCut it down. Remove (comment out) everything that is not strictly necessary and see whether you can make a simplified version of your code run, before you extend it.\nAdd consistency checks. For instance, if from a theoretical point of view, two lists need to have the same length, check it with the length function; similarly, if you know that an object must have a specific value (e.g., because you know the result), check this assumption.\n\n\nFinally, when you know that some typical errors may arise and you don’t want your script to stop or crash, you can add an exception in your code. Suppose for example that you are building a function to connect to an API (see Section ?sec-apis). There might be many reasons for getting an error, such as an Internet connection problem, a server issue, or a missing document. You might decide to skip the error and continue the next lines or you could even give more detailed instructions of what to do (i.e. wait five minutes and try again). The inclusion of these exceptions are in fact a good practice and will help your code to be more robust and stable.\nLet’s make Example 3.17 from the previous chapter more robust so that it does not fail if an invalid headline is passed. For instance, in Python, the object None has no defined length; and in R, it is illegal to calculate the number of characters in a factor. It is a good idea to think about how you want to deal with this: either you want your script to just fail (and clean up the data), or you may want to deal with the error in some way. Especially if you have little control over the input data and/or if the process you are dealing with takes a long time, you may want to handle these errors rather than having your script fail. In Example 4.1, we show how to use such a try/except construction: you indicate which code block you want to try (e.g., run as normal); and in the next block, you indicate what should happen if that results in an error.\nNote that using try … except statements like this is fairly common in Python code, in R it is not needed as frequently. In many cases where a Python function like int raises an exception if the input cannot be converted to an integer, the R function as.numeric just returns a missing value. Thus, in R you normally only encounter these statements when using external resources, for example when using an API or scraping a web page. See Chapter ?sec-chap-scraping for more details on these topics.\n\n\n\n\n\n\n\nExample 4.1 Error handling.\n\nPython codeR code\n\n\n\nheadlines = (\n    \"US condemns terrorist  attacks\",\n    None,\n    \"Venezuelan president is dismissed\",\n)\n\nfor x in headlines:\n    try:\n        # Getting len of None will raise an error\n        if len(x) < 40:\n            print(x)\n    except:\n        print(f\"{x} is not a valid headline\")\n\nUS condemns terrorist  attacks\nNone is not a valid headline\nVenezuelan president is dismissed\n\n\n\n\n\nheadlines = list(\"US condemns terrorist attacks\", \n  NA, \"Venezuelan president is dismissed\")\n\nfor (x in headlines){\n  tryCatch(\n      # Getting nchar of NA will raise an error\n      if (nchar(x)<40) print(x),\n      error=function(error_condition) {\n        print(paste(x, \"is not a valid headline\"))\n    }\n  )\n}\n\n[1] \"US condemns terrorist attacks\"\n[1] \"NA is not a valid headline\"\n[1] \"Venezuelan president is dismissed\""
  },
  {
    "objectID": "chapter04.html#sec-practices",
    "href": "chapter04.html#sec-practices",
    "title": "4  How to write code",
    "section": "4.3 Best Practice: Beautiful Code, GitHub, and Notebooks",
    "text": "4.3 Best Practice: Beautiful Code, GitHub, and Notebooks\nThis section gives a brief explanation of “computational hygiene”: how to structure your code so you can understand it later, the importance of naming and documentation, the use of versioning and online repositories such as GitHub, and the use of literate programming (such as through the use of RMarkdown or Jupyter notebooks) to explain, share, and publish code.\nCoding is more than learning the basic rules and creating a message. If you want to use code to communicate ideas and to work with peers you have to take care of many content and shape details in order to guarantee the comprehension and reproducibility of the scripts. It even applies to the code you write for “private use” because it is highly likely that you will forget your original thoughts from one day to another, or that you later realize that you need to share it with someone else to ask for help. Thus instead of writing personal, hidden and illegible code without adopting any social conventions, you should dedicate some extra effort to make your scripts easy and ready to share.\nThe first step of the computational hygiene is within the code itself. Every time you create an object, a variable, or a function, you have to take many apparently unimportant decisions such as giving a name, separating words, lines, or blocks, and including comments. These decisions are personal, but should mostly depend on social conventions in order to be useful. As you may imagine, there are many of these conventions for general programming and specially for specific languages. To mention just a few, you can find an “official” style guide for Python1 or Google’s R style guide2. Some of these guides are extensive (they cover every detail) and some are more general or abstract. You do not have to see them as a “bible” that needs to be strictly adhered to in each and every situation, but they offer very good guidance for best practice. In fact, even when you find them useful it is true that you will probably learn more of these practices from reading good examples, and especially from interacting with a specific community and its rules.\nWe will mention some general guidelines that apply for both R and Python. If it is the first time you are venturing into the world of code you will find this advice useful, but if you are a more advanced learner you will probably get more specific knowledge in the more detailed sources for each language and community.\nIn the case of naming, we encourage you to use meaningful names or standard abbreviations to objects, using lower-case or mixed-case (remember both Python and R are case-sensitive!), avoiding special characters and operators (such as &, @ or %), and not exceeding 32 characters. You normally begin with a letter3 (an upper-case when defining a class), followed by other letters or numbers, and using underscores to separate the words if necessary (i.e. data_2020 or Filter_Text). Some suggest that variable names should be nouns and function names should be verbs, which seems logical if you think of the nature of these objects.\nWhen writing code, please also take into consideration white space and indentations, because you should use them to give proper structure to the code by creating the block statements. In the case of R, also pay attention to the use of curly braces: the convention is that the opening curly brace begins after some code and is always followed by a new line; and the closing curly brace is in its own line except if there are more instructions in the block. Do not write very long lines (more than 80 characters) to help your code fit the screen and avoid lateral scrolling. Good separation of words, lines and blocks will make your script more readable!\nNow, if you want to make your code highly understandable and shareable, you have to include documentation. This is probably a very basic dimension of coding but unfortunately some authors forget to take the few minutes it takes to describe what their script does (and why), making your journey more difficult. An essential good practice in coding is to include enough information to clarify your code when it is not clear by itself. You can do this in different ways (even by writing a separate codebook), but the most natural and straightforward manner is to include some comments in the code. These comments should be included both at the beginning of the script to give an overview or introduction to the code, and within the script (in independent lines or at the end of a line) to give specific orientations. In many cases, you will need to read your code later (for example when you need to revise an article or analysis), and a short time spent documenting your code will save you a lot of time later.\nR and Python use the hash sign \\# to create these comments. The comment will always begin after the hash. If the first character in your line is a \\# all the text included will be considered as a comment; but if you have already written some code in a line and include a \\# after the code, the initial code will be executed and you will always see the comment by its side. You will normally combine these two ways of documenting your script. As a rule of thumb, insert a comment if the code itself is not obvious, and explain the choices and intentions of the code. So, if a line says df = df - 1, a comment like Decrease df by one is not very useful (as that is obvious from the code), but a comment like Remove one degree of freedom since we estimated the mean does help, as it makes it clear why we are subtracting one from the df object.\nAdditionally, Python and R encourage the use of so-called docstrings: In Python, place a string surrounded by triple quotation marks at the start of a function; in R, place a comment #' right above the function. 4 In this documentation, you can explain what the function does and what parameters it requires. The nice thing is that if properly used, docstrings are automatically displayed in help functions and automatically generated documentation.\nAnother way to make your code more beautiful and, crucially, easier to re-use by others and yourself is to make your code as generic as possible. For instance, imagine you need to calculate the sum of the length of two texts, “Good morning!” and “Goodbye!”. You could just write x = 13 + 8. But what if the strings change in the future? And how to remember what 13 + 8 was supposed to mean? Instead of using such hardcoded values, you can therefore write it better as x = len(\"Good morning!\") + len(\"Goodbye\") (for R, replace len by nchar). But the strings themselves are still hardcoded, so you can create these strings and assign them the names s1 and s2 first, and then just calculate x = len(s1) + len(s2). In practice, these types of generalization often involve the uses of functions (Section 3.3) and loops (Section ?sec-loops). So, don’t use hard-coded values or “magic numbers”: circumference=6.28*r is much less clear than PI=3.14; circumference=2*PI*r.\nMoreover, you must be aware that your code is dynamic and it will normally evolve over time. For example, you may have different files (.py or .R) containing different versions of your script, though this is normally inefficient and chaotic. In order to have a more powerful control of versions and to track changes, coders usually use online repositories to host their scripts for private use and especially to share them. And there are many of these sites, but we believe that GitHub5 is the most well-known and is preferred by data scientists (Figure 4.2 shows the repository we used to write this book).\n\n\n\nFigure 4.2: The online repository GitHub.\n\n\nOnce you upload (or commit and push) your code to GitHub, you can access it from anywhere, and will be able to track the historical changes, which in practice will allow you to have multiple versions in the very same place. You will decide if you make the code public or keep it private, and who to invite to edit the repository. When working collaboratively you it will feel like editing a wiki of code, while having a webpage for your project and a network of friends (followers), will be similar to social media. You can work locally or even from a web interface, and then synchronize the changes. When you allow colleagues to download (or clone) your repository you are then making a good contribution to the community of developers and you can also monitor your impact. In addition to code, you can also upload other kinds of files, including notebooks, and organize them in folders, just as you have on your own computer.\nOne extended good practice when sharing code is the use of literate programming, which is an elegant, practical, and pedagogic way to document and execute the base code. We have already mentioned in this section the importance of including documentation within your code (i.e. using the \\# sign and docstrings), but you also have the opportunity to extend this documentation (with formatted texts, images and even equations!) and put everything together to present in a logical structure everything necessary to understand the code and to run the executable lines step by step.\nThere are different approaches to implement this literate programming in web and local environments, but the standard in R and Python is the use of notebooks. In a notebook you can alternate a text processor with an executable cell to place formatted text between blocks of code. By doing this you can include complete documentation of your scripts, and even more important you can execute each cell one step at a time (loading the results in memory while the notebook is open). This last point allows you avoid the risk of executing the whole script at once, and also gives you more control of the intermediate outputs produced in your code. Once you get used to notebooks, you will probably never write code for data analysis in a basic editor again!\nThe usual tool in R is the R Markdown Notebook, and in Python the Jupyter Notebook (see figure 4.3), but in practice you can also deploy Python in Markdown and R in Jupyter. Both tools can help you with similar tasks to organize your script, though their internal technical procedures are quite different. We have chosen Jupyter to develop the examples in this book because it is a web-based interactive tool. Moreover, there are several services such as Google Colab6 (Figure 4.4), that allow you to remotely run these notebooks online without installing anything on your computer, making the code highly reproducible.\n\n\n\nFigure 4.3: Markdown (left) and Jupyter (right) Notebooks\n\n\nSo far you have seen many of the possibilities that the world of code offers you from a technical and collaboration perspective. We will come back to ethical and normative considerations throughout the book, in particular in Section ?sec-ethicallegalpractical and Section ?sec-ethics.\n\n\n\nFigure 4.4: Jupyter notebook in Google Colab"
  },
  {
    "objectID": "chapter05.html#sec-dataframes",
    "href": "chapter05.html#sec-dataframes",
    "title": "5  From file to data frame and back",
    "section": "5.1 Why and When Do We Use Data Frames?",
    "text": "5.1 Why and When Do We Use Data Frames?\nIn Section 3.1, we introduced basic data types: strings (which contain text), integers (which contain whole numbers, or numbers without anything “behind the dot”), floats (floating point numbers; numbers with decimals), and bools (boolean values, True or False). We also learned that a series of multiple values (e.g., multiple integers, multiple strings) can be stored in what we call a vector (R) or a list (Python).\nIn most social-scientific applications, however, we do not deal with isolated series of values. We rather want to link multiple values to each other. One way to achieve this is by the use of dictionaries (see Section 3.1). Such data structures are really useful for nested data: For example, if we do not want to only store people’s ages, but also their addresses, we could store a dict within a dict.\nIn fact, as we will see later in this chapter, much of the data used by computational social scientists comes in such a format. For instance, data about an online product can contain many reviews which in turn have various pieces of information on the review author.\nBut ultimately, for many social-scientific analyses, a tabular data format is preferred. We are used to thinking of observations (cases) as rows with columns containing information or measurements about these observations (e.g., age, gender, days per week of newspaper reading, …). It also simplifies how we can run many statistical analyses later on.\nWe could simply construct a list of lists to achieve such a tabular data format. In fact, this list-of-lists technique is often used to store tabular data or matrices, and you will probably encounter it in some examples in this book or elsewhere. The list-of-lists approach is very low-level, though: if we wanted, for instance, to insert a column or a row at a specific place, writing the code to do so could be cumbersome. There are also no things like column headers, and no consistency checks: nothing would warn us if one row actually contained more “columns” than another, which should not be the case in a rectangular table.\nTo make our lives easier, we can therefore use a data structure called a data frame. Data frames can be generated from list-of-list structures, from dictionaries, and many others. One way of doing this is shown in Example 5.1, but very often, you’d rather read data from a file or an online resource directly into a data frame (see Section 5.2).\n\n\n\n\n\n\n\nExample 5.1 Creating a data frame from other data structures\n\nPython codeR code\n\n\n\n# Create two lists that will be columns\nlist1 = [\"Anna\", \"Peter\", \"Sarah\", \"Kees\"]\nlist2 = [40, 33, 40, 77]\n\n# or we could have a list of lists instead\nmytable = [[\"Anna\", 40], [\"Peter\", 33], [\"Sarah\", 40], [\"Kees\", 77]]\n\n# Convert an array to a dataframe\ndf = pd.DataFrame(mytable)\n\n# Or create the data frame directly from vectors\ndf2 = pd.DataFrame.from_records(zip(list1, list2))\n\n# No. of rows, no. of columns, and shape\nprint(f\"{len(df)} rows x {len(df.columns)} cols\")\n\n4 rows x 2 cols\n\nprint(f\"Its shape is {df.shape}\")\n\nIts shape is (4, 2)\n\nprint(\"Element-wise equality of df and df2:\")\n\nElement-wise equality of df and df2:\n\nprint(df == df2)\n\n      0     1\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n\n\n\n\n\n# Create two vectors that will be columns\nvector1 <- c(\"Anna\",\"Peter\",\"Sarah\",\"Kees\")\nvector2 <- c(40,33,40,77)\n\n# Create an array of four rows and two columns\nmyarray <- array(c(vector1,vector2), dim=c(4,2))\n\n# Convert an array to a dataframe\ndf1=data.frame(myarray)\n\n# Or create the data frame directly from vectors\ndf2=data.frame(vector1, vector2)\n\n# No. of rows, no. of columns, and dimension\nprint(glue(\"{ncol(df1)} rows x {nrow(df1)} cols\"))\n\n2 rows x 4 cols\n\nprint(dim(df1))\n\n[1] 4 2\n\nprint(\"Element-wise equality of df1 and df2:\")\n\n[1] \"Element-wise equality of df1 and df2:\"\n\nprint(df1 == df2)\n\n       X1   X2\n[1,] TRUE TRUE\n[2,] TRUE TRUE\n[3,] TRUE TRUE\n[4,] TRUE TRUE\n\n\n\n\n\n\n\n\n\nIn this book, we use data frames a lot, because they are very convenient for handling tabular data, and because they provide a lot of useful functionalities, instead of requiring us to re-invent the wheel all the time. In the next section, we will discuss some of them.\nOf course, there are some situations when data frames are not a good choice to organize your data: - Your data is one-dimensional. Think, for example, of resources like a list of stopwords, or a list of texts without any meta-information. - Your data do not have a tabular structure. Think, for example, of deeply nested data, network data or of very messy data. - Your data are so large that you cannot (or do not want to) load it into memory. For instance, if you want to process the text of all articles on Wikipedia, you probably want to process them one-by-one instead of loading all articles at the same time.\nTherefore, you will come across (and we will introduce you to) examples in which we do not use data frames to organize our data. But in most cases we will, because they make our life easier: once we have constructed our data frame, we have a range of handy functions at our disposal that allow us to select rows or columns, add new rows or columns, apply functions to them, and so on. We will discuss these in Chapter 6.\nBut how do we – toy examples like those in Example 5.1 aside – get data into and out of data frames?"
  },
  {
    "objectID": "chapter05.html#sec-reading",
    "href": "chapter05.html#sec-reading",
    "title": "5  From file to data frame and back",
    "section": "5.2 Reading and Saving Data",
    "text": "5.2 Reading and Saving Data\n\n5.2.1 The Role of Files\nIn statistical software like SPSS or Stata, or in all typical office applications for that matter, you open a file, do some work on it, and then save the changes to the same file once you are done. You basically “work on that file”.\nThat’s not how your typical workflow in R or Python looks. Here, you work on one or multiple data frames (or some other data structures). That means that you might start by reading the contents of some file into a data frame, but once that is done, there is no link between the data frame and that file any more. Once your work is done, you can save your data frame to a file, of course, but it is a good practice not to overwrite your input file, so that you can always go back to where you started. A typical workflow would look like this:\n\nRead raw data from file myrawdata.csv into data frame df - Do some operations and analyses on df - Save df to file myfinaldata.csv Note that the last step is not even necessary, but may be handy if running the script takes very long, or if you want to re-distribute the resulting file.\n\nThe format in which we read files into a data frame and the format to which we save our final data frame by no means needs to be identical. We can, for example, read data created by someone else in Stata’s proprietary .dta format into a data frame and later save it to a .csv table.\nWhile we sometimes do not have the choice in which format we get our input data, we have a range of options regarding our output data. We usually prefer formats that are open and interoperable for this, which ensures that they can be used by as many people as possible, and that they are not tied to any specific (proprietary) software tool which might not be available to everyone and can be discontinued in the future.\nThe most common file formats that are relevant to us are listed in Table 5.1. txt files are particularly useful for long texts (think of one file containing one newspaper article or even a whole book), but they are bad for storing associated meta data. csv files are the default choice for tabular data, and json files allow us to store nested data in a dictionary-like format.\nFor the sake of completeness, we also listed the native Python and R formats pickle, RDS, and RDA. Because of their lack of interoperability, they are not very suitable for long-term storage or for sharing data, but they can have a place in a workflow as an intermediate step to solve the issue that none of the other formats are able to store all properties of a data frame (e.g., the csv file cannot store whether a given column in an R data frame is to be understood as containing strings such as “man”, “woman”, “non-binary” or a factor with the three levels man, woman, non-binary). If it is important to store an object (such as a data frame) exactly as-it-is, we can use these formats. One of the rare instances where we use these formats is in Example ?exm-reuse, where we store machine learning models for later reuse.\n\n\nTable 5.1: Basics of data frame handling\n\n\n\nUsed for?\nopen\ninteroperable?\n\n\n\n\ntxt\nplain text\nyes\nyes\n\n\ncsv\ntabular data\nyes\nyes\n\n\njson\nnested data, key-value pairs\nyes\nyes\n\n\npickle\nPython objects\nyes\nno\n\n\nRDS/RDA\nR objects\nyes\nno\n\n\n\n\n\n\n5.2.2 Encodings and Dialects\nPlain txt files, csv files, and json files are all files that are based on text. Unlike binary file formats, you can read them in any text editor. Try it yourself to understand what is going on under the hood.\nDownload a csv file (such as cssbook.net/d/gun-polls.csv) and open it in a text editor of your choice. Some people swear that their preferred editor is the best (google to learn about the vi versus emacs war for some entertainment), but if you have no strong feeling, then Notepad++, Atom, or Sublime may be good choices that you may want to look into.\nAs you will see (Figure 5.1), a csv file internally just looks like a bunch of text in which each line represents a row and in which the columns are separated by a comma (hence the name comma separated values (csv)). Looking at the data in a text editor is a very good way to find out what happens if reading your files into a data frame does not work as expected – which can happen more frequently than you would expect.\nMostly due to historical reasons, not every text based file (which, as we have seen, includes csv files) is internally stored in the same way. For a long time, it was common to encode in such a way that one character mapped to one byte. That was easy from a programming perspective (after all, the \\(n\\)th character of a text can be directly read from and written to the \\(n\\)th byte of a file) and was also storage-efficient. But given that a byte consists of 8 bits, that means that there are only 256 possible characters. All letters in the alphabet in uppercase, again in lowercase, numbers, punctuation, some control characters – and you are out of characters. Due to this limitation, there were different encodings or codepages for different languages that told a program which value should be interpreted as which character.\nWe all know the phenomenon of garbled special characters, like German umlauts or Scandinavian characters like ø, å, or œ being displayed as something completely different. This happens when files are read with a different encoding than the encoding that was used for creating them.\nIn principle, this issue has been solved due to the advent of Unicode. Unicode allows all characters from all scripts to be handled, including emoticons, Korean and Chinese characters, and so on. The most popular encoding for Unicode characters is called UTF-8, and it has been around for decades.\nTo avoid any data loss, it is advisable to make sure that your whole workflow uses UTF-8 files. Most modern applications support UTF-8, even though some still by default use a different encoding (e.g., “Windows-1252”) to store data. As Figure 5.1 illustrates, you can use a text editor to find out what encoding your data has, and many editors also offer an option to change the encoding. However, you cannot recover what has been lost (e.g., if at one point you saved your data with an encoding that only allows 256 different characters, it follows logically that you cannot recover that information).\n\n\n\nFigure 5.1: A csv file opened in a text editor, illustrating that the columns are separated by commas, and showing the encoding and the line endings.\n\n\nAs we will show in the practical code examples below, you can also force Python and R to use a specific encoding, which can come in handy if your data arrives in a legacy encoding.\nRelated to the different encodings a file can have, but less problematic, are different conventions of how a line ending is denoted. Windows-based programs have been using a Carriage Return followed by a Line Feed (denoted as \\r\\n), very old versions of MacOS used a Carriage Return only (\\r), and newer versions of MacOS as well as Linux use a Line Feed only (n). In our field, the Linux (or Unix) style line endings have become most dominant, and Python 3 even automatically converts Windows style line endings to Unix style line endings when reading a file – even on Windows itself.\nA third difference is the use of so-called byte-order markers (BOM). In essence, a BOM is an additional byte added to the beginning of a text file to indicate that it is a UTF-encoded file and to indicate in which order the bytes are to be read (the so-called endianness). While informative, this can cause trouble if your program does not expect that byte to be there. In that case, you might either want to remove it or explicitly specify the encoding as such. For instance, you can add an argument such as encoding=\"UTF-8\" or encoding=\"UTF-8bom\" to the open (Python) or scan (R) command.\nIn short, the most standard form in which you probably want to encode your data is in UTF-8 with Linux-style line endings without the use of a byte-order marker.\nIn the case of reading and writing csv files, we thus need to know the encoding, and potentially also the line ending conventions and the presence of a byte-order marker. However, there are also some additional variations that we need to consider. There is no single definition of what a csv file needs to look like, and there are multiple dialects that are widely used. They mainly differ in two aspects: the delimiter that is chosen, and the quoting and/or escaping of values.\nFirst, even though csv stands for comma separated values, one could use other characters instead of a comma to separate the columns. In fact, because many countries use a comma instead of a dot as a decimal separator ($10.30 versus 10,30€), in these countries a semicolon (;) is used instead of a comma as the column delimiter. To avoid any possible confusion, others use a tab character (t) to separate columns. Sometimes, these files are then called a tab-separated file, and instead of .csv, they may have a file extension such as .tsv, .tab, or even .txt. However, this does not change the way how you can read them – but what you need to know is whether your columns are separated by ,, ;, or t.\nSecond, there may be different ways to deal with strings as values in a csv file. For instance, it may be that a specific value contains the same character that is also used as a delimiter. These cases are usually resolved by either putting all strings into quotes, putting only strings that contain such ambiguities in quotes, or by prepending the ambiguous character with a specific escape character. Most likely, all of this is just handled automatically under the hood, but in case of problems, you might want to look into this and check out the documentation of the packages you are using on how to specify which strategy is to be used.\nLet’s get practical and try out reading and writing files into a data frame (Example 5.2).\n\n\n\n\n\n\n\nExample 5.2 Reading files into a data frame\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/media.csv\"\n# Directly read a csv file from internet\ndf = pd.read_csv(url)\n\n# We can also explicitly specify delimiter etc.\ndf = pd.read_csv(url, delimiter=\",\")\n# Note: use help(pd.read_csv) to see all options\n\n# Save dataframe to a csv:\ndf.to_csv(\"mynewcsvfile.csv\")\n\n\n\n\nurl = \"https://cssbook.net/d/media.csv\"\n# Directly read a csv file from internet\ndf = read_csv(url)\n\n# We can also explicitly specify delimiter etc.\ndf = read_delim(url, delim = \",\")\n# Note: use ?read_csv to see all options\n\n# Save dataframe to a csv:\nwrite_csv(df,\"mynewcsvfile.csv\")\n\n\n\n\n\n\n\n\nOf course, we can read more than just csv files. In the Python example, you can use tabcompletion to get an overview of all file formats Python supports: type pd.read and then press the TAB key to get a list of all supported files. For instance, you could pd.read_excel('test.xlsx'), df3 = pd.read_stata('test.dta'), or df4 = pd.read_json('test.json') Similarly, for R, you can hit TAB after typing haven:: to get an overview over functions such as read_spss.\n\n\n5.2.3 File handling beyond data frames\nData frames are a very useful data structure for organizing and analyzing data, and will occur in many examples in this book. However, not all things that we might want to read from a file needs to go into a data frame. Imagine if we have a list of words that we later want to remove from some texts (so-called stopwords, see Chapter 9). We could make a list (or vector) of such words directly in our code. But if we have more than a couple of such words, it is easier and more readable to keep them in an external file. We could create a file stopwords.txt in a text editor with one of such words per line:\nand\nor\na\nan\nIf you do not wish to create this list yourself, you could also download one from cssbook.net/d/stopwords.txt and save it in the same directory as your Python or R script.\nThen, you can read this file into a vector or list (see Example 5.3).\n\n\n\n\n\n\n\nExample 5.3 Reading files without data frames\n\nPython codeR code\n\n\n\n# Define stopword list in the code itself\nstopwords = [\"and\", \"or\", \"a\", \"an\", \"the\"]\n\n# Better idea: Download stopwords file and read it\nurl = \"https://cssbook.net/d/stopwords.txt\"\nurllib.request.urlretrieve(url, \"stopwords.txt\")\n\n('stopwords.txt', <http.client.HTTPMessage object at 0x7efdcdac6ce0>)\n\nwith open(\"stopwords.txt\") as f:\n    stopwords = [w.strip() for w in f]\nstopwords\n\n['and', 'or', 'a', 'an', 'the']\n\n\n\n\n\n# Define stopword list in the code itself \nstopwords = c(\"and\", \"or\", \"a\", \"an\", \"the\")\n\n# Better idea: Download stopwords file and read it\nurl = \"https://cssbook.net/d/stopwords.txt\"\ndownload.file(url, \"stopwords.txt\")\nstopwords =  scan(\"stopwords.txt\", what=\"string\")\nstopwords\n\n[1] \"and\" \"or\"  \"a\"   \"an\"  \"the\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.4 More examples for reading from and writing to files.\n\nPython codeR code\n\n\n\n# Modify the stopword list and save it:\nstopwords += [\"somenewstopword\", \"andanotherone\"]\nwith open(\"newstopwords.txt\", mode=\"w\") as f:\n    f.writelines(stopwords)\n\n# Use json to read/write dictionaries\nsomedict = {\"label\": \"Report\", \"entries\": [1, 2, 3, 4]}\n\nwith open(\"test.json\", mode=\"w\") as f:\n    json.dump(somedict, f)\n\nwith open(\"test.json\", mode=\"r\") as f:\n    d = json.load(f)\nprint(d)\n\n{'label': 'Report', 'entries': [1, 2, 3, 4]}\n\n\n\n\n\n# Modify the stopword list and save it:\nstopwords = c(stopwords, \n              \"somenewstopword\", \"andanotherone\")\nfileConn<-file(\"newstopwords.txt\")\nwriteLines(stopwords, fileConn)\nclose(fileConn)\n\n# Use json to read/write named lists\nsomedict = list(label=\"Report\",\n               entries=c(1,2,3,4))\n\nwrite_json(somedict, \"/tmp/x.json\", auto_unbox=T)\n\nd=read_json(\"/tmp/x.json\", simplifyVector = T)\nprint(d)\n\n$label\n[1] \"Report\"\n\n$entries\n[1] 1 2 3 4\n\n\n\n\n\n\n\n\n\nExample 5.4 provides you with some more elaborate code examples that allows us to dig a bit deeper into the general way of handling files.\nIn the Python example, we can open a file and assign a handle to it that allows us to refer to it (the name of the handle is arbitrary, let’s just call it f here). Then, we can use a for loop to iterate over all lines in the file and add it to a list.\nThe mode = 'r' specifies that we want to read from the file. mode = 'w' would open the file for writing, create it if necessary, and immediately deletes all content that may have been in there if the file already existed (!). Note that the .strip() is necessary to remove the line ending itself, and also any possible whitespace at the beginning or end of a line. If we want to save our stopwords, we can do this in a similar way: we first open the file (this time, for writing), and then use the file handle’s methods to write to it. We are not limited to plain text files, here. For instance, we can use the same approach to read json files into a Python dict or to store a Python dict into a json file.\nWe could also combine this with a for loop that goes over all files in a dictionary. Imagine we have a folder full of positive movie reviews, and another one full of negative movie reviews that we want to use to train a machine learning classifier (see Section ?sec-supervised). Let’s further assume that all these reviews are saved as .txt files. We can iterate over all of them, as shown in Example ?exm-reviewdata. If you want to read text files into a data frame in R, the readtext package may be interesting for you."
  },
  {
    "objectID": "chapter05.html#sec-gathering",
    "href": "chapter05.html#sec-gathering",
    "title": "5  From file to data frame and back",
    "section": "5.3 Data from online sources",
    "text": "5.3 Data from online sources\nMany data that are interesting to those analyzing communication are nowadays gathered online. In Chapter ?sec-chap-scraping, you will learn how to use APIs to retrieve data from web services, and how to write your own web scraper to automatically download large numbers of web pages and extract relevant information. For instance, you might want to retrieve customer reviews from a website or articles from news sites.\nIn this section, however, we will focus on how to re-use existing datasets that others have made available online. For instance, the open science movement has led to more and more datasets being shared openly using repositories such as Dataverse, Figshare, or others. Re-using existing data can be very good for several reasons: first, to confirm (or not) the conclusions drawn by others; second, to avoid wasting resources by re-collecting very similar or even identical data all over again; and third, because gathering a large, high-quality dataset might just not be feasible with your means. This is especially true when you need annotated (i.e., hand-coded) data for supervised machine learning purposes (Chapter 8).\nWe can distinguish between two types of existing online datasets: datasets that are inherently interesting, and so-called toy datasets.\nToy datasets may include made-up data, but often, they contain real data. However, they are not analyzed to gain scientific insights (any more), as they may be too small, outdated, or already analyzed all-over again. These provide a great way, though, to learn and explore new techniques: after all, the results and the characteristics of the data are already known. Hence, such toy datasets are often even included in R and Python packages. Some of them are really well-known in teaching (e.g., the iris dataset containing measurements of some flowers; or the titanic dataset containing statistics on survival rates of passengers on the Titanic; MINIST for image classification; or the MPG dataset on car fuel consumption). Many of these are included in packages like scikit-learn, seaborn, or ggplot2– and you can have a look at their documentation.\nFor instance, the 20 Newsgroups dataset contains \\(18846\\) posts from newsgroups plus the groups where they were posted (Example 5.5). This can be an interesting resource for practicing with natural language processing, unsupervised, and supervised machine learning. Other interesting resource are collections of political speeches, such as the state-of-the-union speeches from the US, which are available in multiple packages (Example 5.6). Other interesting datasets with large collections of textual data may be the Financial News dataset compiled by Chen (2017) or the political news dataset compiled by Horne et al. (2018).\n\n\n\n\n\n\n\nExample 5.5 In Python, scikit-learn has a convenience function to automatically download the 20 newsgroup dataset and automatically clean it up. In R, you can download the raw version (there are multiple copies floating around on the internet) and perform the cleaning yourself.\n\nPython codeR code\n\n\n\n# Note: use fetch_20newsgroups? for more options\nd = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"))\ndf = pd.DataFrame(zip(d[\"data\"], d[\"target_names\"]))\ndf.head()\n\n                                                   0                         1\n0  I was wondering if anyone out there could enli...               alt.atheism\n1  A fair number of brave souls who upgraded thei...             comp.graphics\n2  well folks, my mac plus finally gave up the gh...   comp.os.ms-windows.misc\n3  \\nDo you have Weitek's address/phone number?  ...  comp.sys.ibm.pc.hardware\n4  From article <C5owCB.n3p@world.std.com>, by to...     comp.sys.mac.hardware\n\n\n\n\n\nurl = \"https://cssbook.net/d/20_newsgroups.csv\"\nd = read_csv(url)\nhead(d)\n\n# A tibble: 6 × 3\n   ...1 target text                                                             \n  <dbl>  <dbl> <chr>                                                            \n1     0      9 \"From: cubbie@garnet.berkeley.edu (                             …\n2     1      4 \"From: gnelson@pion.rutgers.edu (Gregory Nelson) Subject: Thanks…\n3     2     11 \"From: crypt-comments@math.ncsu.edu Subject: Cryptography FAQ 10…\n4     3      4 \"From:  () Subject: Re: Quadra SCSI Problems??? Organization: Ap…\n5     4      0 \"From: keith@cco.caltech.edu (Keith Allan Schneider) Subject: Re…\n6     5      4 \"From: taihou@chromium.iss.nus.sg (Tng Tai Hou) Subject: ADB and…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.6 A collection of US state-of-the-union speeches is available in multiple packages in various forms.\n\nPython codeR code\n\n\n\n# Note: download is only needed once...\nnltk.download(\"state_union\")\n\nTrue\n\n[nltk_data] Downloading package state_union to /home/wva/nltk_data...\n[nltk_data]   Package state_union is already up-to-date!\n\nsentences = state_union.sents()\nprint(f\"There are {len(sentences)} sentences.\")\n\nThere are 17930 sentences.\n\n\n\n\n\nspeeches = sotu_meta\n# show only first 50 characters\nspeeches %>% \n    mutate(text = substr(sotu_text,0,50)) %>%\n    head()\n\n  X         president year years_active       party sotu_type\n1 1 George Washington 1790    1789-1793 Nonpartisan    speech\n2 2 George Washington 1790    1789-1793 Nonpartisan    speech\n3 3 George Washington 1791    1789-1793 Nonpartisan    speech\n4 4 George Washington 1792    1789-1793 Nonpartisan    speech\n5 5 George Washington 1793    1793-1797 Nonpartisan    speech\n6 6 George Washington 1794    1793-1797 Nonpartisan    speech\n                                                  text\n1   Fellow-Citizens of the Senate and House of Represe\n2 \\n\\n Fellow-Citizens of the Senate and House of Repr\n3 \\n\\n Fellow-Citizens of the Senate and House of Repr\n4   Fellow-Citizens of the Senate and House of Represe\n5 \\n\\n Fellow-Citizens of the Senate and House of Repr\n6 \\n\\n Fellow-Citizens of the Senate and House of Repr\n\n\n\n\n\n\n\n\n\nThere are also some more generic resources that you may want to consider for finding more datasets to play around with. On datasetsearch.research.google.com, you can search for datasets of all kinds, both really interesting ones and toy datasets. Another great research is kaggle.com, a site that hosts data science competitions.\n\n\n\n\nChen, Lizi. 2017. “News-Processed-Dataset.” https://doi.org/10.6084/m9.figshare.5296357.v1.\n\n\nHorne, Benjamin D., William Dron, Sara Khedr, and Sibel Adali. 2018. “Sampling the News Producers: A Large News and Feature Data Set for the Study of the Complex Media Landscape.” In 12th International AAAI Conference on Web and Social Media (ICWSM), 518–27. Icwsm. http://arxiv.org/abs/1803.10124."
  },
  {
    "objectID": "chapter06.html#filtering-selecting-and-renaming",
    "href": "chapter06.html#filtering-selecting-and-renaming",
    "title": "6  Data Wrangling",
    "section": "6.1 Filtering, Selecting, and Renaming",
    "text": "6.1 Filtering, Selecting, and Renaming\nSelecting and renaming columns. A first clean up step we often want to do is removing unnecessary columns and renaming columns with unclear or overly long names. In particular, it is often convenient to rename columns that contain spaces or non-standard characters, so it is easier to refer to them later.\nSelecting rows. As a next step, we can decide to filter certain rows. For example, we might want to use only a subset of the data, or we might want to remove certain rows because they are incomplete or incorrect.\nAs an example, FiveThirtyEight published a quiz about American public opinion about guns, and were nice enough to also publish the underlying data1. Example 6.1 gives an example of loading and cleaning this dataset, starting with the function read_csv (included in both tidyverse and pandas) to load the data directly from the Internet. This dataset contains one poll result per row, with a Question column indicating which question was asked, and the columns listing how many Americans (adults or registered voters) were in favor of that measure, in total and for Republicans and Democrats. Next, the columns Republican and Democratic Support are renamed to shorten the names and remove the space. Then, the URL column is dropped using the tidyverse function select in R or the pandas function drop in Python. Notice that the result of these operations is assigned to the same object d. This means that the original d is overwritten.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn R, the tidyverse function select is quite versatile. You can specify multiple columns using select(d, column1, column2) or by specifying a range of columns: select(d, column1:column3). Both commands keep only the specified columns. As in the example, you can also specify a negative selection with the minus sign: select(d, -column1) drops column1, keeping all other columns. Finally, you can rename columns in the select command as well: select(d, column1=col1, column2) renames col to column1, keeps that column and column2, and drops all other columns.\n\n\n\nWe then filter the dataset to list only the polls on whether teachers should be armed (you can understand this is close to our heart). This is done by comparing the value of the Question column to the value 'arm-teachers'. This comparison is done with a double equal sign (==). In both Python and R, a single equals sign is used for assignment, and a double equal sign is used for comparison. A final thing to notice is that while in R we used the dplyr function (filter) to filter out rows, in Python we index the data frame using square brackets on the pandas DataFrame attribute loc(ation): d.loc[].\nNote that we chose to assign the result of this filtering to d2, so after this operation we have the original full dataset d as well as the subset d2 at our disposal. In general, it is your choice whether you overwrite the data by assigning to the same object, or create a copy by assigning to a new name2. If you will later need to work with a different subset, it is smart to keep the original so you can subset it again later. On the other hand, if all your analyses will be on the subset, you might as well overwrite the original. We can always re-download it from the internet (or reload it from our harddisk) if it turns out we needed the original anyway.\n\n\n\n\n\n\n\nExample 6.1 Filtering\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/guns-polls.csv\"\nd = pd.read_csv(url)\nd = d.rename(columns={\"Republican Support\": \"rep\", \"Democratic Support\": \"dem\"})\nd = d.drop(columns=\"URL\")\n# alternatively, we can write:\n# d.drop(columns=\"URL\", inplace=True)\nd2 = d.loc[d.Question == \"arm-teachers\"]\nd2\n\n        Question    Start      End  ... Support rep  dem\n7   arm-teachers  2/23/18  2/25/18  ...      41  69   20\n8   arm-teachers  2/20/18  2/23/18  ...      44  68   20\n9   arm-teachers  2/27/18  2/28/18  ...      43  71   24\n10  arm-teachers  2/27/18  2/28/18  ...      41  68   18\n11  arm-teachers   3/3/18   3/5/18  ...      40  77   10\n12  arm-teachers  2/26/18  2/28/18  ...      43  80   11\n\n[6 rows x 8 columns]\n\n\n\n\n\nurl=\"https://cssbook.net/d/guns-polls.csv\"\nd = read_csv(url)\nd = rename(d, rep=`Republican Support`, \n           dem=`Democratic Support`)\nd = select(d, -URL)\n\nd2 = filter(d, Question == \"arm-teachers\")\nd2\n\n# A tibble: 6 × 8\n  Question     Start   End     Pollster        Population    Support   rep   dem\n  <chr>        <chr>   <chr>   <chr>           <chr>           <dbl> <dbl> <dbl>\n1 arm-teachers 2/23/18 2/25/18 YouGov/Huffpost Registered V…      41    69    20\n2 arm-teachers 2/20/18 2/23/18 CBS News        Adults             44    68    20\n3 arm-teachers 2/27/18 2/28/18 Rasmussen       Adults             43    71    24\n4 arm-teachers 2/27/18 2/28/18 NPR/Ipsos       Adults             41    68    18\n5 arm-teachers 3/3/18  3/5/18  Quinnipiac      Registered V…      40    77    10\n6 arm-teachers 2/26/18 2/28/18 SurveyMonkey    Registered V…      43    80    11"
  },
  {
    "objectID": "chapter06.html#sec-calculate",
    "href": "chapter06.html#sec-calculate",
    "title": "6  Data Wrangling",
    "section": "6.2 Calculating Values",
    "text": "6.2 Calculating Values\nVery often, we need to calculate values for new columns or change the content of existing columns. For example, we might wish to calculate the difference between two columns, or we may need to clean a column by correcting clerical errors or converting between data types.\nIn these steps, the general pattern is that a column is assigned a new value based on a calculation that generally involves other columns. In both R and Python, there are two general ways to accomplish this. First, you can simply assign to an existing or new column, using the column selection notation discussed in Section 3.1: df[\"column\"] = ... in Python, or df$column = ... in R.\nBoth Python and R also offer a function that allows multiple columns to be changed, returning a new copy of the data frame rather than changing the original data frame. In R, this is done using the tidyverse function mutate, which is the recommended way to compute values. The Python equivalent, pandas function assign, is used more rarely as it does not offer many advantages over direct assignment.\nIn either case, you can use arithmetic: e.g. rep - dem to compute the difference between these columns. This works directly in R mutate, but in Python or in R direct assignment you also need to specify the name of the data frame. In Python, this would be d[\"rep\"] - d[\"dem\"] 3, while in R this is d$rep - d$dem.\nIn many cases, however, you want to use various functions to perform tasks like cleaning and data conversion (see Section 3.3 for a detailed explanation of built-in and custom functions). For example, to convert a column to numeric you would use the base R function as.numeric in R or the pandas function to_numeric in Python. Both functions take a column as argument and convert it to a numeric column.\nAlmost all R functions work on whole columns like that. In Python, however, many functions work on individual values rather than columns. To apply a function on each element of a column col, you can use df.col.apply(my_function) (where df and col are the names of your data frame and column). In contast, Pandas columns have multiple useful methods that – because they are methods of that column – apply to the whole column4. For example, the method df.col.fillna replaces missing values in the column col, and df.col.str.replace conducts a find and replace. Unlike functions that expect individual values rather than columns as an input, there is no need to explicitly apply such a method. As always, you can use tab completion (pressing the TAB key after writing df.col.) to get a menu that includes all available methods.\n\n\n\n\n\n\n\nExample 6.2 Mutate\n\nPython codeR code\n\n\n\n# version of the guns polls with some errors\nurl = \"https://cssbook.net/d/guns-polls-dirty.csv\"\nd2 = pd.read_csv(url)\n\n# Option 1: clean with direct assignment\n# Note that when creating a new column,\n# you have to use df[\"col\"] rather than df.col\nd2[\"rep2\"] = d2.rep.str.replace(\"[^0-9\\\\.]\", \"\")\nd2[\"rep2\"] = pd.to_numeric(d2.rep2)\nd2[\"Support2\"] = d2.Support.fillna(d.Support.mean())\n\n# Alternatively, clean with .assign\n# Note the need to use an anonymous function\n# (lambda) to chain calculations\ncleaned = d2.assign(\n    rep2=d2.rep.str.replace(\"[^0-9\\\\.]\", \"\"),\n    rep3=lambda d2: pd.to_numeric(d2.rep2),\n    Support2=d2.Support.fillna(d2.Support.mean()),\n)\n\n# Finally, you can create your own function\ndef clean_num(x):\n    x = re.sub(\"[^0-9\\\\.]\", \"\", x)\n    return int(x)\n\ncleaned[\"rep3\"] = cleaned.rep.apply(clean_num)\ncleaned.head()\n\n       Question    Start      End         Pollster  ... dem  rep2 Support2  rep3\n0  arm-teachers  2/23/18  2/25/18  YouGov/Huffpost  ...  20    69     41.0    69\n1  arm-teachers  2/20/18  2/23/18         CBS News  ...  20    68     41.6    68\n2  arm-teachers  2/27/18  2/28/18        Rasmussen  ...  24    71     43.0    71\n3  arm-teachers  2/27/18  2/28/18        NPR/Ipsos  ...  18    68     41.0    68\n4  arm-teachers   3/3/18   3/5/18       Quinnipiac  ...  10    77     40.0    77\n\n[5 rows x 11 columns]\n\n\n\n\n\n# version of the guns polls with some errors\nurl=\"https://cssbook.net/d/guns-polls-dirty.csv\"\nd2 = read_csv(url)\n\n# Option 1: clean with direct assignment. \n# Note the need to specify d2$ everywhere\nd2$rep2=str_replace_all(d2$rep, \"[^0-9\\\\.]\", \"\")\nd2$rep2 = as.numeric(d2$rep2)\nd2$Support2 = replace_na(d2$Support, \n                        mean(d2$Support, na.rm=T))\n\n# Alternative, clean with mutate\n# No need to specify d2$, \n# and we can assign to a new or existing object\ncleaned = mutate(d2, \n    rep2 = str_replace_all(rep, \"[^0-9\\\\.]\", \"\"),\n    rep2 = as.numeric(rep2),\n    Support2 = replace_na(Support, \n        mean(Support, na.rm=TRUE)))\n\n# Finally, you can create your own function\nclean_num = function(x) {\n    x = str_replace_all(x, \"[^0-9\\\\.]\", \"\")\n    as.numeric(x)\n}\ncleaned = mutate(cleaned, rep3 = clean_num(rep))\nhead(cleaned)\n\n# A tibble: 6 × 11\n  Question   Start End   Polls…¹ Popul…² Support   rep   dem  rep2 Suppo…³  rep3\n  <chr>      <chr> <chr> <chr>   <chr>     <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl>\n1 arm-teach… 2/23… 2/25… YouGov… Regist…      41    69    20    69    41      69\n2 arm-teach… 2/20… 2/23… CBS Ne… Adults       NA    68    20    68    41.6    68\n3 arm-teach… 2/27… 2/28… Rasmus… Adults       43    71    24    71    43      71\n4 arm-teach… 2/27… 2/28… NPR/Ip… Adults       41    68    18    68    41      68\n5 arm-teach… 3/3/… 3/5/… Quinni… Regist…      40    77    10    77    40      77\n6 arm-teach… 2/26… 2/28… Survey… Regist…      43    80    11    80    43      80\n# … with abbreviated variable names ¹​Pollster, ²​Population, ³​Support2\n\n\n\n\n\n\n\n\n\nTo illustrate some of the many possibilities, Example 6.2 has code for cleaning a version of the gun polls in which we intentionally introduced two problems: we added some typos to the rep column and introduced a missing value in the Support column. To clean this, we perform three steps: First, we remove all non-numeric characters using a regular expression (see Section 9.2 for more information on text handling and regular expressions). Next, we need to explicitly convert the resulting column into a numeric column so we can later use it in calculations. Finally, we replace the missing value by the column mean (of course, it is doubtful that that is the best strategy for imputing missing values here, we do it mainly to show how one can deal with missing values technically. You will find some more discussion about missing values in Section 7.1).\nThe cleaning process is actually performed twice: lines 5-10 use direct assignment, while lines 12-19 use the mutate/assign function. Finally, lines 21-27 show how you can define and apply a custom function to combine the first two cleaning steps. This can be quite useful if you use the same cleaning steps in multiple places, since it reduces the repetition of code and hence the possibility of introducing bugs or inconsistencies.\nNote that all these versions work fine and produce the same result. In the end, it is up to the researcher to determine which feels most natural given the circumstances. As noted above, in R we would generally prefer mutate over direct assignment, mostly because it fits nicely into the tidyverse workflow and you do not need to repeat the data frame name. In Python, we would generally prefer the direct assignment, unless a copy of the data with the changes made is convenient, in which case assign can be more useful."
  },
  {
    "objectID": "chapter06.html#sec-grouping",
    "href": "chapter06.html#sec-grouping",
    "title": "6  Data Wrangling",
    "section": "6.3 Grouping and Aggregating",
    "text": "6.3 Grouping and Aggregating\nThe functions we used to change the data above operated on individual rows. Sometimes, however, we wish to compute summary statistics of groups of rows. This essentially shifts the unit of analysis to a higher level of abstraction. For example, we could compute per-school statistics from a data file containing information per student; or we could compute the average number of mentions of a politician per day from data file containing information per articles (each date might have multiple articles and each article multiple mentions to politicians!).\nIn data analysis, this is called aggregation. In both Python and R, it consists of two steps: First, you define which rows are grouped together to form a new unit by specifying which column identifies these groups. In the previous examples, this would be the school name or the date of each article. It is also possible to group by multiple columns, for example to compute the average per day per news source.\nThe next step is to specify one or more summary (or aggregation) functions to be computed over the desired value columns. These functions compute a summary value, like the mean, sum, or standard deviation, over all the values belonging to each group. In the example, to compute average test scores per school we would apply the average (or mean) function to the test score value column. In general, you can use multiple functions (e.g. mean and variance) and multiple columns (e.g. mean test score and mean parental income).\nThe resulting dataset is reduced both in rows and in columns. Each row now represents a group of previuos cases (e.g. school or date), and the columns are now only the grouping columns and the computed summary scores.\nExample 6.3 shows the code in R and Python to define groups and compute summary values. First, we group by poll question; and for each question, we compute the average and standard deviation. The syntax is a little different for R and Python, but the idea is the same: first we create a new variable groups that stores the grouping information, and then we create the aggregate statistics. In this example, we do not store the result of the computation, but print it on the screen. To store the results, simply assign it to a new object as normal.\n\n\n\n\n\n\n\nExample 6.3 Aggregation. Note that in the Python example, we can specify often-used functions such as “mean” simply as a string, but instead, we could also pass functions directly, such as numpy’s np.mean\n\nPython codeR code\n\n\n\ngroups = d.groupby(\"Question\")\ngroups.agg({\"Support\": [\"mean\", \"std\"]})\n\n                               Support          \n                                  mean       std\nQuestion                                        \nage-21                       75.857143  6.011893\narm-teachers                 42.000000  1.549193\nbackground-checks            87.428571  7.322503\nban-assault-weapons          61.750000  6.440285\nban-high-capacity-magazines  67.285714  3.860669\nmental-health-own-gun        85.833333  5.455884\nrepeal-2nd-amendment         10.000000       NaN\nstricter-gun-laws            66.454545  5.145165\n\n\n\n\n\ngroups = group_by(d, Question)\nsummarize(groups, m=mean(Support), sd=sd(Support))\n\n# A tibble: 8 × 3\n  Question                        m    sd\n  <chr>                       <dbl> <dbl>\n1 age-21                       75.9  6.01\n2 arm-teachers                 42    1.55\n3 background-checks            87.4  7.32\n4 ban-assault-weapons          61.8  6.44\n5 ban-high-capacity-magazines  67.3  3.86\n6 mental-health-own-gun        85.8  5.46\n7 repeal-2nd-amendment         10   NA   \n8 stricter-gun-laws            66.5  5.15\n\n\n\n\n\n\n\n\n\nIn R, you use the dplyr function group_by to define the groups, and then call the function summarize to compute summary values by specifying name=function(value).\nIn Python, the grouping step is quite similar. In the summarization step, however, you specify which summaries to compute in a dictionary5. The keys of the dictionary list the value columns to compute summaries of, and the values contain the summary functions to apply, so 'value': function or 'value': [list of functions].\n\n6.3.1 Combining Multiple Operations\nIn the examples above, each line of code (often called a statement) contained a single operation, generally a call to a function or method (see Section 3.3). The general shape of each line in R was data = function(data, arguments), that is, the data is provided as the first argument to the function. In Python, we often used methods that “belong to” objects such as data frames or columns. Here, we therefore specify the object itself followed by a period and its method that is to be called, i.e. object = object.method(arguments).\nAlthough there is nothing wrong with limiting each line to a single operation, both languages allow multiple operations to be chained together. Especially for grouping and summarizing, it can make sense to link these operations together as they can be thought of as a single “data wrangling” step.\nIn Python, this can be achieved by adding the second .method() directly to the end of the first statement. Essentially, this calls the second method on the result of the first method: data = data.method1(arguments).method2(arguments). In R, the data needs, of course, to be included in the function arguments. But we can also chain these function calls. This is done using the pipe operator (%>%) from the (cutely named) magrittr package. The pipe operator inserts the result of the first function as the first argument of the second function. More technically, f1(d) %>% f2() is equivalent to f2(f1(d)). This can be used to chain multiple commands together, e.g. data = data %>% function1(arguments) %>% function2(arguments).\n\n\n\n\n\n\n\nExample 6.4 Combining multiple functions or methods. The result is identical to\n\nPython codeR code\n\n\n\nd.groupby(\"Question\").agg({\"Support\": [\"mean\", \"std\"]})\n\n                               Support          \n                                  mean       std\nQuestion                                        \nage-21                       75.857143  6.011893\narm-teachers                 42.000000  1.549193\nbackground-checks            87.428571  7.322503\nban-assault-weapons          61.750000  6.440285\nban-high-capacity-magazines  67.285714  3.860669\nmental-health-own-gun        85.833333  5.455884\nrepeal-2nd-amendment         10.000000       NaN\nstricter-gun-laws            66.454545  5.145165\n\n\n\n\n\nd %>% group_by(Question) %>% \n  summarize(m=mean(Support), sd=sd(Support))\n\n# A tibble: 8 × 3\n  Question                        m    sd\n  <chr>                       <dbl> <dbl>\n1 age-21                       75.9  6.01\n2 arm-teachers                 42    1.55\n3 background-checks            87.4  7.32\n4 ban-assault-weapons          61.8  6.44\n5 ban-high-capacity-magazines  67.3  3.86\n6 mental-health-own-gun        85.8  5.46\n7 repeal-2nd-amendment         10   NA   \n8 stricter-gun-laws            66.5  5.15\n\n\n\n\n\n\n\n\n\nExample 6.4 shows the same operation as in Example 6.3, but chained into a single statement.\n\n\n6.3.2 Adding Summary Values\nRather than reducing a data frame to contain only the group-level information, it is sometimes desirable to add the summary values to the original data. For example, if we add the average score per school to the student-level data, we can then determine whether individual students outperform the school average.\nOf course, the summary scores are the same for all rows in the same group: all students in the same school have the same school average. So, these values will be repeated for these rows, essentially mixing individual and group level variables in the same data frame.\n\n\n\n\n\n\n\nExample 6.5 Adding summary values to individual cases\n\nPython codeR code\n\n\n\n# Note the use of ( ) to split a long line\nd[\"mean\"] = d.groupby(\"Question\")[\"Support\"].transform(\"mean\")\nd[\"deviation\"] = d[\"Support\"] - d[\"mean\"]\nd.head()\n\n  Question    Start      End  ... dem       mean  deviation\n0   age-21  2/20/18  2/23/18  ...  86  75.857143  -3.857143\n1   age-21  2/27/18  2/28/18  ...  92  75.857143   6.142857\n2   age-21   3/1/18   3/4/18  ...  76  75.857143  -8.857143\n3   age-21  2/22/18  2/26/18  ...  92  75.857143   8.142857\n4   age-21   3/3/18   3/5/18  ...  93  75.857143   2.142857\n\n[5 rows x 10 columns]\n\n\n\n\n\nd = d %>% group_by(Question) %>% \n  mutate(mean = mean(Support), \n         deviation=Support - mean)\nhead(d)\n\n# A tibble: 6 × 10\n# Groups:   Question [1]\n  Question Start   End     Pollster    Popul…¹ Support   rep   dem  mean devia…²\n  <chr>    <chr>   <chr>   <chr>       <chr>     <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 age-21   2/20/18 2/23/18 CNN/SSRS    Regist…      72    61    86  75.9   -3.86\n2 age-21   2/27/18 2/28/18 NPR/Ipsos   Adults       82    72    92  75.9    6.14\n3 age-21   3/1/18  3/4/18  Rasmussen   Adults       67    59    76  75.9   -8.86\n4 age-21   2/22/18 2/26/18 Harris Int… Regist…      84    77    92  75.9    8.14\n5 age-21   3/3/18  3/5/18  Quinnipiac  Regist…      78    63    93  75.9    2.14\n6 age-21   3/4/18  3/6/18  YouGov      Regist…      72    65    80  75.9   -3.86\n# … with abbreviated variable names ¹​Population, ²​deviation\n\n\n\n\n\n\n\n\n\nExample 6.5 shows how this can be achieved in Python and R, computing the mean support per question and then calculating how each poll deviates from this mean.\nIn R, the code is very similar to Example 6.4 above, simply replacing the dplyr function summarize by the function mutate discussed above. In this function you can mix summary functions and regular functions, as shown in the example: first the mean per group is calculated, followed by the deviation of this mean.\nThe Python code also uses the same syntax used for computing new columns. The first line selects the Support column on the grouped dataset, and then calls the pandas method transform on that column to compute the mean per group, adding it as a new column by assigning it to the column name. The second line uses the regular assignment syntax to create the deviation based on the support and calculated mean."
  },
  {
    "objectID": "chapter06.html#sec-join",
    "href": "chapter06.html#sec-join",
    "title": "6  Data Wrangling",
    "section": "6.4 Merging Data",
    "text": "6.4 Merging Data\nIn many cases, we need to combine data from different sources or data files. For example, we might have election poll results in one file and socio-economic data per area in another. To test whether we can explain variance in poll results from factors such as education level, we would need to combine the poll results with the economic data. This process is often called merging or joining data.\n\n6.4.1 Equal Units of Analysis\n\n\n\n\n\n\n\nExample 6.6 Private and Public Capital data (source: Piketty 2014).\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/private_capital.csv\"\nprivate = pd.read_csv(url)\nprivate.tail()\n\n    Year  U.S.  Japan  Germany  France  U.K.  Italy  Canada  Australia  Spain\n36  2006  4.88   5.83     3.78    5.34  5.19   6.37    3.88       5.32   7.69\n37  2007  4.94   5.79     3.79    5.53  5.23   6.42    4.02       5.55   7.92\n38  2008  4.36   5.87     3.90    5.53  4.91   6.61    3.83       5.44   7.86\n39  2009  4.06   6.19     4.15    5.63  5.04   6.91    4.13       5.04   7.89\n40  2010  4.10   6.01     4.12    5.75  5.22   6.76    4.16       5.18   7.55\n\n\n\n\n\nurl=\"https://cssbook.net/d/private_capital.csv\"\nprivate = read_csv(url)\ntail(private)\n\n# A tibble: 6 × 10\n   Year  U.S. Japan Germany France  U.K. Italy Canada Australia Spain\n  <dbl> <dbl> <dbl>   <dbl>  <dbl> <dbl> <dbl>  <dbl>     <dbl> <dbl>\n1  2005  4.7   5.74    3.84   5     4.99  6.24   3.73      5.22  7.24\n2  2006  4.88  5.83    3.78   5.34  5.19  6.37   3.88      5.32  7.69\n3  2007  4.94  5.79    3.79   5.53  5.23  6.42   4.02      5.55  7.92\n4  2008  4.36  5.87    3.9    5.53  4.91  6.61   3.83      5.44  7.86\n5  2009  4.06  6.19    4.15   5.63  5.04  6.91   4.13      5.04  7.89\n6  2010  4.1   6.01    4.12   5.75  5.22  6.76   4.16      5.18  7.55\n\n\n\n\n\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = pd.read_csv(url)\npublic.tail()\n\n    Year  U.S.  Japan  Germany  France  U.K.  Italy  Canada  Australia  Spain\n36  2006  0.51   0.36     0.02    0.37  0.32  -0.54   -0.10       0.69   0.20\n37  2007  0.54   0.38     0.06    0.46  0.32  -0.52   -0.03       0.69   0.26\n38  2008  0.49   0.34     0.08    0.43  0.28  -0.52    0.00       0.71   0.25\n39  2009  0.36   0.24     0.07    0.35  0.19  -0.65   -0.02       0.71   0.14\n40  2010  0.21   0.14     0.04    0.31  0.06  -0.68   -0.04       0.67   0.05\n\n\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = read_csv(url)\ntail(public)\n\n# A tibble: 6 × 10\n   Year  U.S. Japan Germany France  U.K. Italy Canada Australia Spain\n  <dbl> <dbl> <dbl>   <dbl>  <dbl> <dbl> <dbl>  <dbl>     <dbl> <dbl>\n1  2005  0.48  0.34    0.04   0.28  0.32 -0.56  -0.16      0.67  0.13\n2  2006  0.51  0.36    0.02   0.37  0.32 -0.54  -0.1       0.69  0.2 \n3  2007  0.54  0.38    0.06   0.46  0.32 -0.52  -0.03      0.69  0.26\n4  2008  0.49  0.34    0.08   0.43  0.28 -0.52   0         0.71  0.25\n5  2009  0.36  0.24    0.07   0.35  0.19 -0.65  -0.02      0.71  0.14\n6  2010  0.21  0.14    0.04   0.31  0.06 -0.68  -0.04      0.67  0.05\n\n\n\n\n\n\n\n\n\nThe easiest joins are when both datasets have the same unit of analysis, i.e. the rows represent the same units. For example, consider the data on public and private capital ownership published by Piketty (2017) alongside his landmark book Capital in the 21st Century. As shown in Example 6.6, he released separate files for public and private capital ownership. If we wished to analyze the relationship between these (for example to recreate Figure 3.6 on page 128 of that book), we first need to combine them into a single data frame.\nTo combine these data frames, we use the pandas data frame method merge in Python or the dplyr method full_join in R. Both methods join the data frames on one or more key columns. The key column(s) identify the units in both data frames, so in this case the Year column. Often, the key column is some sort of identifier, like a respondent or location ID. The resulting data frame will contain the shared key column(s), and all other columns from both joined data frames.\nIn both Python and R, all columns that occur in both data frames are by default assumed to be the key columns. In many cases, this is the desired behavior as both data frames may contain e.g. a Year or RepondentID column. Sometimes, however, this is not the case. Possibly, the key column is called differently in both data frames, e.g. respID in one and Respondent in the other. It is also possible that the two frames contain columns with the same name, but which contain actual data that should not be used as a key. For example, in the Piketty data shown above the key column is called Year in both frames, but they also share the columns for the countries which are data columns.\nIn these cases, it is possible to explicitly specify which columns to join on (using the on= (Python) / by= (R) argument). However, we would generally recommend preprocessing the data first and select and/or rename columns such that the only shared columns are the key columns. The reason for that is that if columns in different data frames mean the same thing (i.e. respID and Respondent), they should generally have the same name to avoid confusion. In the case of “accidentally” shared column names, such as the country names in the current example, it is also better to rename them so it is obvious which is which in the resulting dataset: if shared columns are not used in the join, by default they get “.x” and “.y” (R) or “_x” and “_y” (Python) appended to their name, which is not very meaningful. Even if the key column is the only shared column, however, it can still be good to explicitly select that column to make it clear to the reader (or for yourself in the future) what is happening.\n\n\n\n\n\n\n\nExample 6.7 Merging private and public data for France.\n\nPython codeR code\n\n\n\nprivate_fr = private[[\"Year\", \"France\"]].rename(\n    columns={\"France\": \"fr_private\"}\n)\npublic_fr = public[[\"Year\", \"France\"]].rename(columns={\"France\": \"fr_public\"})\ncapital_fr = pd.merge(private_fr, public_fr)\n# Data for Figure 3.6 (Piketty, 2014, p 128)\ncapital_fr.head()\n\n   Year  fr_private  fr_public\n0  1970        3.10       0.41\n1  1971        3.04       0.43\n2  1972        3.07       0.45\n3  1973        3.05       0.46\n4  1974        3.03       0.48\n\n\n\n\n\nprivate_fr = private %>% \n    select(Year, fr_private=France)\npublic_fr = public %>% \n    select(Year, fr_public=France)\ncapital_fr = full_join(private_fr, public_fr)\n# Data for Figure 3.6 (Piketty, 2014, p 128)\nhead(capital_fr)\n\n# A tibble: 6 × 3\n   Year fr_private fr_public\n  <dbl>      <dbl>     <dbl>\n1  1970       3.1       0.41\n2  1971       3.04      0.43\n3  1972       3.07      0.45\n4  1973       3.05      0.46\n5  1974       3.03      0.48\n6  1975       3.17      0.53\n\n\n\n\n\n\nPython codeR code\n\n\n\n# Are private and public capital correlated?\nr, p = scipy.stats.pearsonr(capital_fr.fr_private, capital_fr.fr_public)\nprint(f\"Pearson correlation: rho={r:.2},p={p:.3}\")\n\nPearson correlation: rho=-0.32,p=0.0404\n\n\n\n\n\n# Are private and public capital correlated?\ncor.test(capital_fr$fr_private, \n         capital_fr$fr_public)\n\n\n    Pearson's product-moment correlation\n\ndata:  capital_fr$fr_private and capital_fr$fr_public\nt = -2.1204, df = 39, p-value = 0.04039\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.57252488 -0.01537337\nsample estimates:\n       cor \n-0.3215032 \n\n\n\n\n\n\n\n\n\nThis is shown in Example 6.7. The first two lines select only the Year and France columns, and rename the France column to indicate whether it is the private or public data. Line 3 does the actual join, with and without the explicit selection of key column, respectively. This is then used to compute the correlation between private and public capital, which shows that there is a weak but (just) significant negative correlation (\\(\\rho=-.32, p=.04\\)) 6.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNext to merge, /textitPandas data frames also have a method called join. It is a simplified version for joining on indices (i.e., the row labels). If you have two data frames in which corresponding rows have the same row number, you can simply write df1.join(df2). In short: both methods do the same, but merge provides more options, and join is easier if you want to join on the indices.\n\n\n\n\n\n6.4.2 Inner and Outer Joins\nIn the example above, both datasets had exactly one entry for each unit (year), making it the most straightforward case. If either (or both) of the datasets have missing units, however, you need to specify how to deal with this.\nTable 6.1 list the four possible ways of joining, keeping all rows (outer join), only rows present in both (inner join), or all rows from one of the sets and matching rows from the other (left or right join). Left and right here literally refer to the order in which you type the data frame names. Figure 6.1 and Table 6.1 give an overview. In all cases except inner joins, this can create units where information from one of the datasets is missing. This will be lead to missing values (NA/NaN) being inserted in the columns of the datasets with missing units.\n\n\n\nFigure 6.1: The solid area indicates whether the cases in the resulting datasets need to appear in one, both, or any of the datasets.\n\n\n\n\nTable 6.1: Different types of joins between datasets d1 and d2\n\n\n\n\n\n\n\n\nType\nDescription\nR\nPython\n\n\n\n\nOuter\nAll units from both sets\nfull_join(d1,d2)\nd1.merge(d2, how='outer')\n\n\nInner\nOnly units that are in both sets\ninner_join(d1,d2)\nd1.merge(d2, how='inner')\n\n\nLeft\nAll units from left-hand set\nleft_join(d1,d2)\nd1.merge(d2, how='left')\n\n\nRight\nAll units from right-hand set\nright_join(d1,d2)\nd1.merge(d2, how='right')\n\n\n\n\nIn most cases, you will either use inner join or left join. Inner join is useful when information should be complete, or where you are only interested in units with information in both datasets. In general, when joining sets with the same units, it is smart to check the number of rows before and after the operation. If it decreases, this shows that there are units where information is missing in either set. If it increases, it shows that apparently the sets are not at the same level of analysis, or there are duplicate units in the data. In either case, an unexpected change in the number of rows is a good indicator that something is wrong.\nLeft joins are useful when you are adding extra information to a “primary” dataset. For example, you might have your main survey results in a dataset, to which you want to add metadata or extra information about your respondents. If this data is not available for all respondents, you can use a left join to add the information where it is available, and simply leave the other respondents with missing values.\nA similar use case is when you have a list of news items, and a separate list of items that were coded or found with some search term. Using a left join will let you keep all news items, and add the coding where it is available. Especially if items that had zero hits of a search term are excluded from the search results, you might use a left join followed by a calculation to replace missing values by zeros to indicate that the counts for items aren’t actually missing, but were zero.\nOf course, you could also use a right join to achieve the same effect. It is more natural, however, to work from your primary dataset and add the secondary data, so you will generally use left joins rather than right joins.\nOuter (or full) joins can be useful when you are adding information from e.g. multiple survey waves, and you want to include any respondent that answered any of the waves. Of course, you will have to carefully think about how to deal with the resulting missing values in the substantive analysis.\n\n\n6.4.3 Nested Data\nThe sections above discuss merging two datasets at the same level of analysis, i.e. with rows representing the same units (respondents, items, years) in both sets. It is also possible, however, to join a more aggregate (high level) set with a more detailed dataset. For example, you might have respondents that are part of a school or organizational unit. It can be desirable to join the respondent level information with the school level information, for example to then explore differences between schools or do multilevel modeling.\nFor this use the same commands as for equal joins. In the resulting merged dataset, information from the group level will be duplicated for all individuals in that group.\nFor example, take the two datasets shown in Example 6.8. The results dataset shows how many votes each US 2016 presidential primary candidate received in each county: Bernie Sanders got 544 votes in Autauga County in the US state of Alabama, which was 18.2% of all votes cast in the Democratic primary. Conversely, the counties dataset shows a large number of facts about these counties, such as population, change in population, gender and education distribution, etc.\n\n\n\n\n\n\n\nExample 6.8 2016 Primary results and county-level metadata. Note that to avoid duplicate output, we display the counties data in the Python example and the results data in the R example\n\nPython codeR code\n\n\n\nr = \"https://cssbook.net/d/2016_primary_results.csv\"\nresults = pd.read_csv(r)\nresults.head()\n\n     state state_abbrev   county  ...        candidate votes fraction_votes\n0  Alabama           AL  Autauga  ...   Bernie Sanders   544          0.182\n1  Alabama           AL  Autauga  ...  Hillary Clinton  2387          0.800\n2  Alabama           AL  Baldwin  ...   Bernie Sanders  2694          0.329\n3  Alabama           AL  Baldwin  ...  Hillary Clinton  5290          0.647\n4  Alabama           AL  Barbour  ...   Bernie Sanders   222          0.078\n\n[5 rows x 8 columns]\n\n\n\n\n\nr=\"https://cssbook.net/d/2016_primary_results.csv\"\nresults = read_csv(r) \nhead(results)\n\n# A tibble: 6 × 8\n  state   state_abbrev county   fips party    candidate       votes fraction_v…¹\n  <chr>   <chr>        <chr>   <dbl> <chr>    <chr>           <dbl>        <dbl>\n1 Alabama AL           Autauga  1001 Democrat Bernie Sanders    544        0.182\n2 Alabama AL           Autauga  1001 Democrat Hillary Clinton  2387        0.8  \n3 Alabama AL           Baldwin  1003 Democrat Bernie Sanders   2694        0.329\n4 Alabama AL           Baldwin  1003 Democrat Hillary Clinton  5290        0.647\n5 Alabama AL           Barbour  1005 Democrat Bernie Sanders    222        0.078\n6 Alabama AL           Barbour  1005 Democrat Hillary Clinton  2567        0.906\n# … with abbreviated variable name ¹​fraction_votes\n\n\n\n\n\n\nPython codeR code\n\n\n\nc = \"https://cssbook.net/d/2016_primary_county.csv\"\ncounties = pd.read_csv(c)\ncounties.head()\n\n   fips       area_name state_abbrev  ...  Building_permits   Land_area  Pop_density\n0     0   United States          NaN  ...           1046363  3531905.43         87.4\n1  1000         Alabama          NaN  ...             13369    50645.33         94.4\n2  1001  Autauga County           AL  ...               131      594.44         91.8\n3  1003  Baldwin County           AL  ...              1384     1589.78        114.6\n4  1005  Barbour County           AL  ...                 8      884.88         31.0\n\n[5 rows x 54 columns]\n\n\n\n\n\nc=\"https://cssbook.net/d/2016_primary_county.csv\"\ncounties = read_csv(c)\nhead(counties)\n\n# A tibble: 6 × 54\n   fips area_n…¹ state…² Pop_2…³ Pop_2…⁴ Pop_c…⁵ Pop_2…⁶ Age_u…⁷ Age_u…⁸ Age_o…⁹\n  <dbl> <chr>    <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     0 United … <NA>     3.19e8  3.09e8     3.3  3.09e8     6.2    23.1    14.5\n2  1000 Alabama  <NA>     4.85e6  4.78e6     1.4  4.78e6     6.1    22.8    15.3\n3  1001 Autauga… AL       5.54e4  5.46e4     1.5  5.46e4     6      25.2    13.8\n4  1003 Baldwin… AL       2.00e5  1.82e5     9.8  1.82e5     5.6    22.2    18.7\n5  1005 Barbour… AL       2.69e4  2.75e4    -2.1  2.75e4     5.7    21.2    16.5\n6  1007 Bibb Co… AL       2.25e4  2.29e4    -1.8  2.29e4     5.3    21      14.8\n# … with 44 more variables: Sex_female_pct <dbl>, Race_white_pct <dbl>,\n#   Race_black_pct <dbl>, Race_native_pct <dbl>, Race_asian_pct <dbl>,\n#   Race_island_pct <dbl>, Race_mixed_pct <dbl>, Race_hispanic_pct <dbl>,\n#   Race_white_not_hispanic_pct <dbl>, Pop_same_house_pct <dbl>,\n#   Pop_foreign_born_pct <dbl>, Pop_nonenglish_home_pct <dbl>,\n#   Pop_hs_grad_pct <dbl>, Pop_college_grad_pct <dbl>,\n#   Pop_veterans_count <dbl>, Pop_avg_commute_mins <dbl>, …\n\n\n\n\n\n\n\n\n\nSuppose we hypothesize that Hillary Clinton would do relatively well in areas with more black voters. We would then need to combine the county level data about ethnic composition with the county \\(\\times\\) candidate level data on vote outcomes.\nThis is achieved in Example 6.9 in two steps. First, both datasets are cleaned to only contain the relevant data: for the results dataset only the Democrat rows are kept, and only the fips (county code), candidate, votes, and fraction columns. For the counties dataset, all rows are kept but only the county code, name, and Race_white_pct columns are kept.\n\n\n\n\n\n\n\nExample 6.9 Joining data at the result and the county level\n\nPython codeR code\n\n\n\nc = counties[[\"fips\", \"area_name\", \"Race_black_pct\"]]\n\nr = results.loc[results.candidate == \"Hillary Clinton\"]\nr = r[[\"fips\", \"votes\", \"fraction_votes\"]]\nr = r.merge(c)\nr.head()\n\n     fips  votes  fraction_votes       area_name  Race_black_pct\n0  1001.0   2387           0.800  Autauga County            18.7\n1  1003.0   5290           0.647  Baldwin County             9.6\n2  1005.0   2567           0.906  Barbour County            47.6\n3  1007.0    942           0.755     Bibb County            22.1\n4  1009.0    564           0.551   Blount County             1.8\n\n\n\n\n\nc = counties %>% \n  select(\"fips\", \"area_name\", \"Race_black_pct\")\nr = results %>% \n  filter(candidate == \"Hillary Clinton\") %>% \n  select(fips, votes, fraction_votes)\nr = inner_join(r, c)\ncor.test(r$Race_black_pct, r$fraction_votes)\n\n\n    Pearson's product-moment correlation\n\ndata:  r$Race_black_pct and r$fraction_votes\nt = 50.944, df = 2806, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6734586 0.7119165\nsample estimates:\n      cor \n0.6931806 \n\n\n\n\n\n\n\n\n\nIn the next step, both sets are joined using an inner join from the results dataset. Note that we could also have used a left join here, but with an inner join it will be immediately obvious if county level data is missing, as the number of rows will then decrease. In fact, in this case the number of rows does decrease, because some results do not have corresponding county data. As a puzzle, can you use the dataset filtering commands discussed above to find out which results these are?\nNote also that the county level data contains units that are not used, particularly the national and state level statistics. These, and the results that do not correspond to counties, are automatically filtered out by using an inner join.\nFinally, we can create a scatter plot or correlation analysis of the relation between ethnic composition and electoral success (see how to create the scatter plot in Section 7.2). In this case, it turns out that Hillary Clinton did indeed do much better in counties with a high percentage of black residents. Note that we cannot take this to mean there is a direct causal relation, there could be any number of underlying factors, including the date of the election which is very important in primary races. Statistically, since observations within a state are not independent, we should really control for the state-level vote here. For example, we could use a partial correlation, but we would still be violating the independence assumption of the errors, so it would be better to take a more sophisticated (e.g. multilevel) modeling approach. This, however, is well beyond the scope of this chapter."
  },
  {
    "objectID": "chapter06.html#sec-pivot",
    "href": "chapter06.html#sec-pivot",
    "title": "6  Data Wrangling",
    "section": "6.5 Reshaping Data: Wide To Long And Long To Wide",
    "text": "6.5 Reshaping Data: Wide To Long And Long To Wide\nData that you find or create does not always have the shape that you need it to be for your analysis. In many cases, for further data wrangling or for analyses you want each observation to be in its own row. However, many data sources list multiple observations in columns. For example, data from panel surveys asking the same question every week will often have one row per respondent, and one column for each weekly measurement. For a time-series analysis, however, each row should be a single measurement, i.e. the unit of analysis is a respondent per week.\nGenerally, data with multiple observations of the same unit is called wide data (as there are many columns), while a dataset with one row for each observation is called long data (as there are many rows). In most cases, long data is easiest to work with, and in fact in tidyverse jargon such data is called tidy data.\nAs a first relatively simple example, consider the datasets containing public and private capital. This data is “wide” in the sense that the measurements for the different countries are contained in the columns. To make this data “long” we would have to create rows for each country–year combination. This will make it much easier to do further data wrangling or analysis, as you can now e.g. directly merge the datasets and compute the pooled correlation between these variables. In fact, when we merged these datasets earlier in Example 6.10, we selected only the measurements for France, essentially turning it into long data.\n\n\n\n\n\n\n\nExample 6.10 Converting wide to long data to facilitate merging and visualizing.\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/private_capital.csv\"\nprivate = pd.read_csv(url)\nprivate = private.melt(id_vars=\"Year\", var_name=\"country\", value_name=\"capital\")\nprivate.head()\n\n   Year country  capital\n0  1970    U.S.     3.42\n1  1971    U.S.     3.41\n2  1972    U.S.     3.49\n3  1973    U.S.     3.39\n4  1974    U.S.     3.21\n\n\n\n\n\nurl = \"https://cssbook.net/d/private_capital.csv\"\nprivate = read_csv(url)\nprivate = private %>% pivot_longer(cols = -Year,\n    names_to=\"country\", values_to=\"capital\")\nhead(private)\n\n# A tibble: 6 × 3\n   Year country capital\n  <dbl> <chr>     <dbl>\n1  1970 U.S.       3.42\n2  1970 Japan      2.99\n3  1970 Germany    2.25\n4  1970 France     3.1 \n5  1970 U.K.       3.06\n6  1970 Italy      2.39\n\n\n\n\n\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = pd.read_csv(url)\npublic = public.melt(id_vars=\"Year\", var_name=\"country\", value_name=\"capital\")\nd = pd.concat([private.assign(type=\"private\"), public.assign(type=\"public\")])\n\ncountries = {\"France\", \"U.K.\", \"Germany\"}\nd = d.loc[d.country.isin(countries)]\nd.reset_index(inplace=True)\nplt = sns.lineplot(data=d, x=\"Year\", y=\"capital\", hue=\"country\", style=\"type\")\nplt.set(ylabel=\"Capital (% of national income\")\nplt.set_title(\n    \"Capital in Europe, 1970 - 2010\" \"\\nPartial reproduction of Piketty fig 4.4\"\n)\n\n\n\n\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = read_csv(url) %>% pivot_longer(-Year,\n    names_to=\"country\", values_to=\"capital\")\n\nd = bind_rows(\n    private %>% add_column(type=\"private\"),\n    public %>% add_column(type=\"public\"))\ncountries = c(\"Germany\", \"France\", \"U.K.\")\nd %>% filter(country %in% countries) %>% \n  ggplot(aes(x=Year, y=capital, \n             color=country, lty=type)) + \n  geom_line()+ \n  ylab(\"Capital (% of national income)\") +\n  guides(colour=guide_legend(\"Country\"), \n         linetype=guide_legend(\"Capital\")) + \n  theme_classic() + \n  ggtitle(\"Capital in Europe, 1970 - 2010\", \n    \"Partial reproduction of Piketty fig 4.4\")\n\n\n\n\n\n\n\n\n\n\n\nExample 6.10 shows how you can “pivot” the capital data to long format using pivot_longer (R) and melt (Pandas). The second part of this example then goes on to do this for both datasets, merge them, and partially reproduce Figure 4.4 from Piketty (2017)."
  },
  {
    "objectID": "chapter06.html#restructuring-messy-data",
    "href": "chapter06.html#restructuring-messy-data",
    "title": "6  Data Wrangling",
    "section": "6.6 Restructuring Messy Data",
    "text": "6.6 Restructuring Messy Data\nAs a final example, we will look at the data on income and wage shares from Piketty (supplemental tables S8.1 and S8.2). We want to visualize the income and wage share going to the top 1% earners in France and the US. Figure 6.2 shows a screen shot of this data in Libre Office, with the US data having a similar shape. For the previous examples, we used a clean csv version of this data, but now we will tackle the additional challenge of dealing with the Excel file including extra header rows and column names aimed at human consumption rather than easy computing.\n\n\n\nFigure 6.2: Data on top incomes as provided in Piketty (2014; digital appendix).\n\n\nIn order to perform our visualization, we want a dataset containing a single measurement column (percentage share), and a row for each year–country–type combination, i.e. one row for wage inequality in 1910 in the US. One of the most important skills in computational social science (and data-driven analysis in general) is understanding which series of generally small steps are needed to go from one data format to the other. Although there is not a fixed set of steps that are always needed, the steps to get from the raw data visualized in Figure 6.2 to a “tidy” dataset are fairly typical:\n\nInput: read the data into data frames. In this case, reading from an Excel sheet and skipping the extra header rows\n\nReshape: pivoting the data into long format\nNormalize: normalize names, value types, etc. In this case, also separate a header like “Top 1% income share” into income type (income, wage) and percentile (10%, 1%, etc)\nFilter: filter for the desired data\nAnalyze: create the visualization\n\n\nFortunately, these steps have been discussed before: reading csv data in Section 5.2; pivot to long data in Section 6.5; add a column in Section 6.2; joining data in Section 6.4; and visualizing in Section 7.2.\nExample 6.11 shows how to perform these steps for the US case. First, we use the readxl (R) and xlrd (Python) to read a sheet from an Excel file into a data frame, manually specifying the number of header and footer rows to skip. Then, we pivot the columns into a long format. In step 3, we split the header into two columns using separate (R) and split (Python). Finally, steps 4 and 5 take the desired subset and create a line plot.\nThe missing step, splitting a header into two columns, is done using separate (R) and split (Python).\n\n\n\n\n\n\n\nExample 6.11 Dealing with ``messy’’ data.\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/Chapitre8.xls\"\n# 1 Input: Read the data into a data frame\nd = pd.read_excel(url, sheet_name=\"TS8.2\", skiprows=4, skipfooter=3)\n\nd = d.rename(columns={\"Unnamed: 0\": \"year\"})\n\n# 2 Reshape: Pivoting to long, dropping missing\nd = d.melt(value_name=\"share\", id_vars=\"year\")\n\n# 3 Normalize\ncols = [\"_top\", \"percentile\", \"type\", \"_share\", \"capital_gains\"]\nd[cols] = d.variable.str.split(n=4, expand=True)\nd = d.drop(columns=[\"variable\", \"_top\", \"_share\"])\nd[\"capital_gains\"] = d[\"capital_gains\"].notna()\nd.head()\n\n   year  share percentile    type  capital_gains\n0  1900  0.405        10%  income          False\n1  1901    NaN        10%  income          False\n2  1902    NaN        10%  income          False\n3  1903    NaN        10%  income          False\n4  1904    NaN        10%  income          False\n\n\n\n\n\n#1 Input: Read the data into a data frame\nurl=\"https://cssbook.net/d/Chapitre8.xls\"\ndest = tempfile(fileext=\".xls\")\ndownload.file(url, dest)\nd = read_excel(dest,sheet=\"TS8.2\",skip=4)\nd = d%>% rename(\"year\"=1)\n\n#2 Reshape: Pivoting to long, dropping missing\nd = d%>%pivot_longer(-year, values_to=\"share\")%>% \n        na.omit()\n\n#3 Normalize\ncols = c(NA,\"percent\",\"type\",NA,\"capital_gains\")\nd = d %>% separate(name, into=cols,\n   sep=\" \", extra=\"merge\", fill=\"right\") %>% \n  mutate(year=as.numeric(year), \n         capital_gains=!is.na(capital_gains))\nhead(d)\n\n# A tibble: 6 × 5\n   year percent type   capital_gains  share\n  <dbl> <chr>   <chr>  <lgl>          <dbl>\n1  1900 10%     income FALSE         0.405 \n2  1900 10%     income TRUE          0.403 \n3  1910 10%     income FALSE         0.406 \n4  1910 10%-5%  income FALSE         0.0989\n5  1910 5%-1%   income FALSE         0.129 \n6  1910 1%      income FALSE         0.178 \n\n\n\n\n\n\nPython codeR code\n\n\n\n# 4 Filter for the desired data\nsubset = d[\n    (d.year >= 1910) & (d.percentile == \"1%\") & (d.capital_gains == False)\n]\n\n# 5 Analyze and/or visualize\nplt = sns.lineplot(data=subset, hue=\"type\", x=\"year\", y=\"share\")\nplt.set(xlabel=\"Year\", ylabel=\"Share of income going to top-1%\")\n\n\n\n\n\n\n\n#4 Filter for the desired data\nsubset = d %>% filter(year >=1910, \n                      percent==\"1%\", \n                      capital_gains==F)\n\n#5 Analyze and/or visualization\nggplot(subset, aes(x=year, y=share, color=type)) +\n  geom_line() + xlab(\"Year\") + \nylab(\"Share of income going to top-1%\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPiketty, Thomas. 2017. Capital in the Twenty-First Century. Cambridge, MA: Harvard University Press."
  },
  {
    "objectID": "chapter07.html#sec-simpleeda",
    "href": "chapter07.html#sec-simpleeda",
    "title": "7  Exploratory data analysis",
    "section": "7.1 Simple Exploratory Data Analysis",
    "text": "7.1 Simple Exploratory Data Analysis\nNow that you are familiar with data structures (Chapter 5) and data wrangling (Chapter 6) you are probably eager to get some real insights into your data beyond the basic techniques we briefly introduced in Chapter 2.\nAs we outlined in Chapter 1, the computational analysis of communication can be bottom-up or top-down, inductive or deductive. Just as in traditional research methods Bryman (2012), sometimes, an inductive bottom-up approach is a goal in itself: after all, explorative analyses are invaluable for generating hypotheses that can be tested in follow-up research. But even when you are conducting a deductive, hypothesis-testing study, it is a good idea to start by describing your dataset using the tools of exploratory data analysis to get a better picture of your data. In fact, we could even go as far as saying that obtaining details like frequency tables, cross-tabulations, and summary statistics (mean, median, mode, etc.) is always necessary, even if your research questions or hypotheses require further complex analysis. For the computational analysis of communication, a significant amount of time may actually be invested at this stage.\nExploratory data analysis (EDA), as originally conceived by Tukey (1977), can be a very powerful framework to prepare and evaluate data, as well as to understand its properties and generate insights at any stage of your research. It is mandatory to do some EDA before any sophisticated analysis to know if the data is clean enough, if there are missing values and outliers, and how the distributions are shaped. Furthermore, before making any multivariate or inferential analysis we might want to know the specific frequencies for each variable, their measures of central tendency, their dispersion, and so on. We might also want to integrate frequencies of different variables into a single table to have an initial picture of their interrelations.\nTo illustrate how to do this in R and Python, we will use existing representative survey data to analyze how support for migrants or refugees in Europe changes over time and differs per country. The Eurobarometer (freely available at the Leibniz Institute for the Social Sciences – GESIS) has contained these specific questions since 2015. We might pose questions about the variation of a single variable or also describe the covariation of different variables to find patterns in our data. In this section, we will compute basic statistics to answer these questions and in the next section we will visualize them by plotting within and between variable behaviors of a selected group of features of the Eurobarometer conducted in November 2017 to 33193 Europeans.\nFor most of the EDA we will use tidyverse in R and pandas as well as numpy and scipy in Python (Example 7.1). After loading a clean version of the survey data1 stored in a csv file (using the tidyverse function read_csv in R and the pandas function read_csv in R), checking the dimensions of our data frame (33193 x 17), we probably want to get a global picture of each of our variables by getting a frequency table. This table shows the frequency of different outcomes for every case in a distribution. This means that we can know how many cases we have for each number or category in the distribution of every variable, which is useful in order to have an initial understanding of our data.\n\n\n\n\n\n\npandas versus pure numpy/scipy\n\n\n\n\n\nIn this book, we use pandas data frames a lot: they make our lives easier compared to native data types (Section 3.1), and they already integrate a lot of functionality of underlying math and statistics packages such as numpy and scipy. However, you do not have to force your data into a data frame if a different structure makes more sense in your script. numpy and scipy will happily calculate mean, media, skewness, and kurtosis of the values in a list, or the correlation between two lists. It’s up to you.\n\n\n\n\n\n\n\n\n\n\nExample 7.1 Load data from Eurobarometer survey and select some variables\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/eurobarom_nov_2017.csv\"\nd2 = pd.read_csv(url)\nprint(\"Shape of my filtered data =\", d2.shape)\n\nShape of my filtered data = (33193, 17)\n\nprint(d2.columns)\n\nIndex(['survey', 'uniqid', 'date', 'country', 'marital_status', 'educational',\n       'gender', 'age', 'occupation', 'type_community',\n       'household_composition', 'support_refugees', 'support_migrants',\n       'date_n', 'support_refugees_n', 'support_migrants_n', 'educational_n'],\n      dtype='object')\n\n\n\n\n\nurl=\"https://cssbook.net/d/eurobarom_nov_2017.csv\"\nd2= read_csv(url, col_names = TRUE)\nglue(\"{nrow(d2)} row x {ncol(d2)} columns\")\n\n33193 row x 17 columns\n\ncolnames(d2)\n\n [1] \"survey\"                \"uniqid\"                \"date\"                 \n [4] \"country\"               \"marital_status\"        \"educational\"          \n [7] \"gender\"                \"age\"                   \"occupation\"           \n[10] \"type_community\"        \"household_composition\" \"support_refugees\"     \n[13] \"support_migrants\"      \"date_n\"                \"support_refugees_n\"   \n[16] \"support_migrants_n\"    \"educational_n\"        \n\n\n\n\n\n\n\n\n\nLet us first get the distribution of the categorical variable gender by creating tables that include absolute and relative frequencies. The frequency tables (using the dplyr functions group_by and summarize in R, and pandas function value_counts in Python) reveals that 17716 (53.38%) women and 15477 (46.63%) men answered this survey (Example 7.2). We can do the same with the level of support of refugees [support_refugees] (To what extent do you agree or disagree with the following statement: our country should help refugees) and obtain that 4957 (14.93%) persons totally agreed with this statement, 12695 (38.25%) tended to agree, 5931 (16.24%) tended to disagree and 3574 (10.77%) totally disagreed.\n\n\n\n\n\n\n\nExample 7.2 Absolute and relative frequencies of support of refugees and gender.\n\nPython codeR code\n\n\n\nprint(d2[\"gender\"].value_counts())\n\nWoman    17716\nMan      15477\nName: gender, dtype: int64\n\nprint(d2[\"gender\"].value_counts(normalize=True))\n\nWoman    0.533727\nMan      0.466273\nName: gender, dtype: float64\n\n\n\n\n\nd2 %>%\n  group_by(gender) %>%\n  summarise(frequency = n()) %>%\n  mutate(rel_freq = frequency / sum(frequency))   \n\n# A tibble: 2 × 3\n  gender frequency rel_freq\n  <chr>      <int>    <dbl>\n1 Man        15477    0.466\n2 Woman      17716    0.534\n\n\n\n\n\n\nPython codeR code\n\n\n\nprint(d2[\"support_refugees\"].value_counts())\n\nTend to agree       12695\nTend to disagree     5391\nTotally agree        4957\nTotally disagree     3574\nName: support_refugees, dtype: int64\n\nprint(d2[\"support_refugees\"].value_counts(normalize=True, dropna=False))\n\nTend to agree       0.382460\nNaN                 0.198114\nTend to disagree    0.162414\nTotally agree       0.149339\nTotally disagree    0.107673\nName: support_refugees, dtype: float64\n\n\n\n\n\nd2 %>%\n  group_by(support_refugees) %>%\n  summarise(frequency = n()) %>%\n  mutate(rel_freq = frequency / sum(frequency)) \n\n# A tibble: 5 × 3\n  support_refugees frequency rel_freq\n  <chr>                <int>    <dbl>\n1 Tend to agree        12695    0.382\n2 Tend to disagree      5391    0.162\n3 Totally agree         4957    0.149\n4 Totally disagree      3574    0.108\n5 <NA>                  6576    0.198\n\n\n\n\n\n\n\n\n\nBefore diving any further into any between variables analysis, you might have noticed that there might be some missing values in the data. These values represent an important amount of data in many real social and communication analysis (just remember that you cannot be forced to answer every question in a telephone or face-to-face survey!). From a statistical point of view, we can have many approaches to address missing values: For example, we can drop either the rows or columns that contain any of them, or we can impute the missing values by predicting them based on their relation with other variables – as we did in Section 6.2 by replacing the missing values with the column mean. It goes beyond the scope of this chapter to explain all the imputation methods (and, in fact, mean imputation has some serious drawbacks when used in subsequent analysis), but at least we need to know how to identify the missing values in our data and how to drop the cases that contain them from our dataset.\nIn the case of the variable support_refugees we can count its missing data (6576 cases) with base R function is.na and the pandas method isna2. Then we may decide to drop all the records that contain these values in our dataset using the tidyr function drop_na in R and the Pandas function dropna in Python3 (Example 7.3). By doing this we get a cleaner dataset and can continue with a more sophisticated EDA with cross-tabulation and summary statistics for the group of cases.\n\n\n\n\n\n\n\nExample 7.3 Drop missing values\n\nPython codeR code\n\n\n\nn_miss = d2[\"support_refugees\"].isna().sum()\nprint(f\"# of missing values: {n_miss}\")\n\n# of missing values: 6576\n\nd2 = d2.dropna()\nprint(f\"Shape after dropping NAs: {d2.shape}\")\n\nShape after dropping NAs: (23448, 17)\n\n\n\n\n\nn_miss = sum(is.na(d2$support_refugees))\nprint(glue(\"# of missing values: {n_miss}\"))\n\n# of missing values: 6576\n\nd2 = d2 %>% drop_na()\nprint(glue(\"Rows after dropping NAs: {nrow(d2)}\"))\n\nRows after dropping NAs: 23448\n\n\n\n\n\n\n\n\n\nNow let us cross tabulate the gender and support_refugees to have an initial idea of what the relationship between these two variables might be. With this purpose we create a contingency table or cross-tabulation to get the frequencies in each combination of categories (using dplyr functions group_by, summarize and spread in R, and the pandas function crosstab in Python; example 7.4). From this table you can easily see that 2178 women totally supported helping refugees and 1524 men totally did not. Furthermore, other interesting questions about our data might now arise if we compute summary statistics for a group of cases (using again dplyr functions group_by, summarize and spread, and base mean in R; and pandas function groupby and base mean in Python). For example, you might wonder what the average ages of the women were that totally supported (52.42) or not (53.2) to help refugees. This approach will open a huge amount of possible analysis by grouping variables and estimating different statistics beyond the mean, such as count, sum, median, mode, minimum or maximum, among others.\n\n\n\n\n\n\n\nExample 7.4 Cross tabulation of support of refugees and gender, and summary statistics\n\nPython codeR code\n\n\n\nprint(\"Crosstab gender and support_refugees:\")\n\nCrosstab gender and support_refugees:\n\nprint(pd.crosstab(d2[\"support_refugees\"], d2[\"gender\"]))\n\ngender             Man  Woman\nsupport_refugees             \nTend to agree     5067   5931\nTend to disagree  2176   2692\nTotally agree     2118   2178\nTotally disagree  1524   1762\n\nprint(\"Summary statistics for group of cases:\")\n\nSummary statistics for group of cases:\n\nprint(d2.groupby([\"support_refugees\", \"gender\"])[\"age\"].mean())\n\nsupport_refugees  gender\nTend to agree     Man       54.073022\n                  Woman     53.373799\nTend to disagree  Man       52.819853\n                  Woman     52.656761\nTotally agree     Man       53.738905\n                  Woman     52.421947\nTotally disagree  Man       52.368110\n                  Woman     53.203746\nName: age, dtype: float64\n\n\n\n\n\nprint(\"Crosstab gender and support_refugees:\")\n\n[1] \"Crosstab gender and support_refugees:\"\n\nd2 %>%\n  group_by(gender, support_refugees)%>%\n  summarise(n=n())%>%\n  pivot_wider(values_from=\"n\",names_from=\"gender\")\n\n# A tibble: 4 × 3\n  support_refugees   Man Woman\n  <chr>            <int> <int>\n1 Tend to agree     5067  5931\n2 Tend to disagree  2176  2692\n3 Totally agree     2118  2178\n4 Totally disagree  1524  1762\n\nprint(\"Summary statistics for group of cases:\")\n\n[1] \"Summary statistics for group of cases:\"\n\nd2 %>%\n  group_by(support_refugees, gender)%>%\n  summarise(mean_age=mean(age, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   support_refugees [4]\n  support_refugees gender mean_age\n  <chr>            <chr>     <dbl>\n1 Tend to agree    Man        54.1\n2 Tend to agree    Woman      53.4\n3 Tend to disagree Man        52.8\n4 Tend to disagree Woman      52.7\n5 Totally agree    Man        53.7\n6 Totally agree    Woman      52.4\n7 Totally disagree Man        52.4\n8 Totally disagree Woman      53.2"
  },
  {
    "objectID": "chapter07.html#sec-visualization",
    "href": "chapter07.html#sec-visualization",
    "title": "7  Exploratory data analysis",
    "section": "7.2 Visualizing Data",
    "text": "7.2 Visualizing Data\nData visualization is a powerful technique for both understanding data yourself and communicating the story of your data to others. Based on ggplot2 in R and matplotlib and seaborn in Python, this section covers histograms, line and bar graphs, scatterplots and heatmaps. It touches on combining multiple graphs, communicating uncertainty with boxplots and ribbons, and plotting geospatial data. In fact, visualizing data is an important stage in both EDA and advanced analytics, and we can use graphs to obtain important insights into our data. For example, if we want to visualize the age and the support for refugees of European citizens, we can plot a histogram and a bar graph, respectively.\n\n\n\n\n\n\nR: GGPlot syntax\n\n\n\n\n\nOne of the nicest features of using R for data exploration is the ggplot2 package for data visualization. This is a package that brings a unified method for visualizing with generally good defaults but that can be customized in every way if desired. The syntax, however, can look a little strange at first. Let’s consider the command from Example 7.5:\nggplot (data=d2) + geom_bar(mapping=aes(x= support_refugees), fill=\"blue\")\nWhat you can see here is that every ggplot is composed of multiple sub-commands that are added together with the plus sign. At a minimum, every ggplot needs two sub-commands: ggplot, which initiates the plot and can be seen as an empty canvas, and one or more geom commands which add geometries to the plot, such as bars, lines, or points. Moreover, each geometry needs a data source, and an aesthetic mapping which tells ggplot how to map columns in the data (in this case the support_refugees column) to graphical (aesthetic) elements of the plot, in this case the \\(x\\) position of each bar. Graphical elements can also be set to a constant value rather than mapped to a column, in which case the argument is placed outside the aes function, as in the fill=\"blue\" above.\nEach aesthetic mapping is assigned a scale. This scale is initialized with a sensible default which depends on the data type. For example, the color of the lines in Example 7.9 are mapped to the group column. Since that is a nominal value (character column), ggplot automatically assigns colors to each group, in this case blue and red. In Example 7.15, on the other hand, the fill color is mapped to the score column, which is numerical (interval) data, to which ggplot by default assigns a color range of white to blue.\nAlmost every aspect of ggplot can be customized by adding more subcommands. For example, you can specify the title and axis labels by adding + labs(title=\"Title\", x=\"Axis Label\") to the plot, and you can completely alter the look of the graph by applying a theme. For example, the ggthemes package defines an Economist theme, so by simply adding + theme_economist() to your plot you get the characteristic layout of plots from that magazine. You can also customize the way scales are mapped using the various scale_variable_mapping functions. For example, Example 7.19 uses scale_fill_viridis_c(option = \"B\") to use the viridis scale for the fill aesthetic, specifying that scale B should be used. Similar commands can be used to e.g. change the colors of color ranges, the size of points, etc.\nBecause all geometries start with geom_, all scales start with scale_, all themes start with theme_, etc., you can use the RStudio autocompletion to browse through the complete list of options: simply type geom_, press tab or control+space, and you get a list of the options with a short description, and you can press F1 to get help on each option. The help for every geometry also lists all aesthetic elements that can or must be supplied.\nBesides the built-in help, there are a number of great (online) resources to learn more. Specifically, we recommend the book Data Visualization: A practical introduction by Kieran Healy4. Another great resource is the R Graph Gallery5, which has an enormous list of possible visualizations, all with R code included and most of them based on ggplot. Finally, we recommend the Data-to-Viz6 website, which allows you to explore a number of graph types depending on your data, lists the do’s and don’ts for each graph, and links to the Graph Gallery for concrete examples.\n\n\n\n\n7.2.1 Plotting Frequencies and Distributions\nIn the case of nominal data, the most straightforward way to visualize them is to simply count the frequency of value and then plot them as a bar chart. For instance, when we depict the support to help refugees (Example 7.5) you can quickly get that the option “tend to agree” is the most frequently voiced answer.\n\n\n\n\n\n\n\nExample 7.5 Barplot of support for refugees\n\nPython codeR code\n\n\n\nd2[\"support_refugees\"].value_counts().plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\n\n\nggplot(data=d2) +\n  geom_bar(mapping = aes(x= support_refugees))\n\n\n\n\n\n\n\n\n\n\n\nIf we have continuous variables, however, having such a bar chart would lead to too many bars: we may lose oversight (and creating the graph may be resource-intensive). Instead, we want to group the data into bins, such as age groups. Hence, a histogram is used to examine the distribution of a continuous variable (ggplot2 function geom_histogram in R and pandas function hist in Python) and a bar graph to inspect the distribution of a categorical one (ggplot2 function geom_bar() in R and matplotlib function plot in Python). In Example 7.6 you can easily see the shape of the distribution of the variable age, with many values close to the average and a slightly bigger tail to the right (not that far from the normal distribution!).\n\n\n\n\n\n\n\nExample 7.6 Histogram of Age\n\nPython codeR code\n\n\n\nd2.hist(column=\"age\", bins=15)\nplt.show()\n\n\n\n\n\n\n\nggplot(data=d2) +\n  geom_histogram(mapping = aes(x= age), bins = 15)\n\n\n\n\n\n\n\n\n\n\n\nAnother way to show distributions is using bloxplots, which are powerful representations of the distribution of our variables through the use of quartiles that are marked with the 25th, 50th (median) and 75th percentiles of any given variable. By examining the lower and upper levels of two or more distributions you can compare their variability and even detect possible outliers. You can generate multiple boxplots to compare the ages of the surveyed citizens by country and quickly see that in terms of age the distributions of Spain and Greece are quite similar, but we can identify some differences between Croatia and the Netherlands. In R we use the base function geom_boxplot, while in Python we use the seaborn function boxplot.\n\n\n\n\n\n\n\nExample 7.7 Bloxplots of age by country\n\nPython codeR code\n\n\n\nd2 = d2.sort_values(by=\"country\")\nplt.figure(figsize=(8, 8))\nsns.boxplot(x=\"age\", y=\"country\", data=d2)\nplt.show()\n\n\n\n\n\n\n\nggplot(d2, aes(y=fct_rev(country), x=age))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Plotting Relationships\nAfter having inspected distributions of single variables, you may want to check how two variables are related. We are going to discuss two ways of doing so: plotting data over time, and scatterplots to illustrate the relationship between two continuous variables.\nThe Eurobarometer collects data for 15 days (in the example from November 5 to 19, 2017) and you may wonder if the level of support to refugees or even to general migrants changes over the time. This is actually a simple time series and you can use a line graph to represent it. Firstly you must use a numerical variable for the level of support (support_refugees_n, which ranges from 1 to 4, 4 being the maximum support) and group it by day in order to get the average for each day. In the case of R, you can plot the two series using the base function plot, or you can use the ggplot2 function geom_line. In the case of Python you can use the matplotlib function plot or the seaborn function lineplot. To start, Example 7.8 shows how to create a graph for the average support for refugees by day.\n\n\n\n\n\n\n\nExample 7.8 Line graph of average support for refugees by day\n\nPython codeR code\n\n\n\nsupport_refugees = d2.groupby([\"date_n\"])[\"support_refugees_n\"].mean()\nsupport_refugees = support_refugees.to_frame()\n\nplt.plot(support_refugees.index, support_refugees[\"support_refugees_n\"])\nplt.xlabel(\"Day\")\nplt.ylabel(\"Support for refugees\")\nplt.show()\n\n\n\n\n\n\n\nsupport_refugees = d2 %>%\n  group_by(date_n) %>%\n  summarise(support=mean(support_refugees_n, \n                         na.rm = TRUE))\nggplot(support_refugees,aes(x=date_n, y=support))+\n  geom_line() + \n  xlab(\"Day\") + \n  ylab(\"Support for refugees\")\n\n\n\n\n\n\n\n\n\n\n\nTo also plot the support for migrants, you can combine multiple subgraphs in a single plot, giving the reader a broader and more comparative perspective (Example 7.9). In R, the geom_line also takes a color aesthetic, but this requires the data to be in long format. So, we first reshape the data and also change the factor labels to get a better legend (see Section 6.5). In Python, you can plot the two lines as separate figures and add the pyplot function show to display an integrated figure.\n\n\n\n\n\n\n\nExample 7.9 Plotting multiple lines in one graph\n\nPython codeR code\n\n\n\n# Combine data\nsupport_combined = d2.groupby([\"date_n\"]).agg(\n    refugees=(\"support_refugees_n\", \"mean\"),\n    migrants=(\"support_migrants_n\", \"mean\"),\n)\n\n# plot\nsns.lineplot(x=\"date_n\", y=\"refugees\", data=support_combined, color=\"blue\")\nsns.lineplot(x=\"date_n\", y=\"migrants\", data=support_combined, color=\"red\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Level of support\")\nplt.title(\"Support of refugees and migrants\")\nplt.show()\n\n\n\n\n\n\n\n# Combine data\nsupport_combined = d2 %>% group_by(date_n) %>%\n summarise(\n  refugees=mean(support_refugees_n, na.rm = TRUE),\n  migrants=mean(support_migrants_n, na.rm = TRUE))\n\n# Pivot to long format and plot \nsupport_long = support_combined %>% \n  pivot_longer(-date_n, names_to=\"group\", \n               values_to=\"support\")\nggplot(support_long, \n       aes(x=date_n, y=support, colour=group)) +\n  geom_line(size = 1.5) +\n  labs(title=\"Support for refugees and migrants\", \n       x=\"Day\", y=\"Level of Support\") \n\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can create multiple subplots, one for each group that you want to show (Example 7.10). In ggplot (R), you can use the facet_grid function to automatically create subplots that each show one of the groups. In the case of Python you can use the matplotlib function subplots that allows you to configure multiple plots in a single one.\n\n\n\n\n\n\n\nExample 7.10 Creating subfigures)\n\nPython codeR code\n\n\n\nf, axes = plt.subplots(2, 1)\nsns.lineplot(x=\"date_n\", y=\"refugees\", data=support_combined, ax=axes[0])\nsns.lineplot(x=\"date_n\", y=\"migrants\", data=support_combined, ax=axes[1])\n\nsns.lineplot(x=\"date_n\", y=\"support_refugees_n\", data=d2, ci=0, ax=axes[0])\nsns.lineplot(x=\"date_n\", y=\"support_migrants_n\", data=d2, ci=0, ax=axes[1])\nplt.show()\n\n\n\n\n\n\n\nggplot(support_long, aes(x=date_n, y=support)) +  \n  geom_line() + facet_grid(rows=vars(group)) +\n  xlab(\"Day\") + ylab(\"Support\")\n\n\n\n\n\n\n\n\n\n\n\nNow if you want to explore the possible correlation between the average support for refugees (mean_support_refugees_by_day) and the average support to migrants by year (mean_support_migrants_by_day), you might need a scatterplot, which is a better way to visualize the type and strength of this relationship scatter.\n\n\n\n\n\n\n\nExample 7.11 Scatterplot of average support for refugees and migrants by year\n\nPython codeR code\n\n\n\nsns.scatterplot(data=support_combined, x=\"refugees\", y=\"migrants\")\n\n\n\n\n\n\n\nggplot(support_combined, \n       aes(x=refugees, y=migrants))+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nA scatterplot uses dots to depict the values of two variables in a Cartesian plane (with coordinates for the axes \\(x\\) and \\(y\\)). You can easily plot this figure in R using the ggplot2 function geom_point (and geom_smooth to display a regression line!), or in Python using seaborn function scatterplot (lmplot to include the regression line as shown in Example 7.12).\n\n\n\n\n\n\n\nExample 7.12 Scatterplot with regression line\n\nPython codeR code\n\n\n\nsns.lmplot(data=support_combined, x=\"refugees\", y=\"migrants\")\n\n\n\nplt.show()\n\n\n\n\n\n\n\nggplot(support_combined,\n       aes(x=refugees, y= migrants))+\n  geom_point()+\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\n\n\n\nLooking at the dispersion of points in the provided example you can infer that there might be a positive correlation between the two variables, or in other words, the more the average support to refugees the more the average support to migrants over time.\nWe can check and measure the existence of this correlation by computing the Pearson correlation coefficient or Pearson’s r, which is the most well-known correlation function. As you probably remember from your statistics class, a correlation refers to a relationship between two continuous variables and is usually applied to measure linear relationships (although there also exist nonlinear correlation coefficients, such as Spearman’s \\(\\rho\\)). Specifically, Pearson’s \\(r\\) measures the linear correlation between two variables (X and Y) producing a value between \\(-1\\) and \\(+1\\), where 0 depicts the absence of correlation and values near to 1 a strong correlation. The signs (\\(+\\) or \\(-\\)) represent the direction of the relationship (being positive if two variables variate in the same direction, and negative if they vary in the opposite direction). The correlation coefficient is usually represented with r or the Greek letter \\(\\rho\\) and mathematically expressed as:\nYou can estimate this correlation coefficient with the pandas function corr in Python and the base R function cor in R. As shown in Example 7.13 the two variables plotted above are highly correlated with a coefficient of 0.95.\n\n\n\n\n\n\n\nExample 7.13 Pearson correlation coefficient\n\nPython codeR code\n\n\n\nprint(\n    support_combined[\"refugees\"].corr(\n        support_combined[\"migrants\"], method=\"pearson\"\n    )\n)\n\n0.9541243084907629\n\n\n\n\n\ncor.test(support_combined$refugees, \n         support_combined$migrants, \n         method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  support_combined$refugees and support_combined$migrants\nt = 9.0133, df = 8, p-value = 1.833e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8127522 0.9893855\nsample estimates:\n      cor \n0.9541243 \n\n\n\n\n\n\n\n\n\nAnother useful representation is the heatmap. This figure can help you plot a continuous variable using a color scale and shows its relation with another two variables. This means that you represent your data as colors, which might be useful for understanding patterns. For example, we may wonder what the level of support for refugees is given the nationality and the gender of the individuals. For this visualization, it is necessary to create a proper data frame (Example 7.14) to plot the heatmap, in which each number of your continuous variable *_refugees_n* is included in a table where each axis (x= gender, y=country) represents the categorical variables. This pivoted table stored in an object called pivot_data can be generated using some of the already explained commands.\n\n\n\n\n\n\n\nExample 7.14 Create a data frame to plot the heatmap\n\nPython codeR code\n\n\n\npivot_data = pd.pivot_table(\n    d2, values=\"support_refugees_n\", index=[\"country\"], columns=\"gender\"\n)\n\n\n\n\npivot_data= d2 %>% \n  select(gender, country, support_refugees_n) %>%\n  group_by(country, gender) %>%\n  summarise(score = mean(support_refugees_n))\n\n\n\n\n\n\n\n\nIn the first resulting figure proposed in Example 7.15, the lighter the blue the greater the support in each combination of country \\(\\times\\) gender. You can see that level of support is similar in countries such as Slovenia or Spain, and is different in the Czech Republic or Austria. It also seems that women have a higher level of support. For this default heatmap we can use the ggplot2 function geom_tile in R and seaborn function heatmap in Python. To personalize the scale colors (e.g. if we want a scale of blues) we can use the ggplot2 function scale_fill_gradient in R or the parameter cmap of the seaborn function heatmap in Python.\n\n\n\n\n\n\n\nExample 7.15 Heatmap of country gender and support for refugees\n\nPython codeR code\n\n\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_data, cmap=\"Blues\", cbar_kws={\"label\": \"support_refugees_n\"})\nplt.show()\n\n\n\n\n\n\n\nggplot(pivot_data, aes(x = gender, \n    y = fct_rev(country), fill = score)) + \n  geom_tile()+\n  scale_fill_gradient2(low=\"white\", high=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\nAs you will notice, one of the goals of EDA is exploring the variance of our variables, which includes some uncertainty about their behavior. We will introduce you to two basic plots to visually communicate this uncertainty. Firstly, ribbons and area plots can help us to clearly identify a predefined interval of a variable in order to interpret its variance over some cases. Let us mark this interval in 0.15 points in the above-mentioned plots of the average support to refugees or migrants by day, and we can see that the lines tend to converge more on the very last day and are more separated by day four. This simple representation can be conducted in R using the ggplot2 function geom_ribbon and in Python using the parameter ci of the seaborn function lineplot.\n\n\n\n\n\n\n\nExample 7.16 Add ribbons to the graph lines of support to refugees and migrants\n\nPython codeR code\n\n\n\nsns.lineplot(\n    x=\"date_n\",\n    y=\"support_refugees_n\",\n    data=d2,\n    color=\"blue\",\n    ci=100,\n    label=\"Refugees\",\n)\nsns.lineplot(\n    x=\"date_n\",\n    y=\"support_migrants_n\",\n    data=d2,\n    color=\"red\",\n    ci=100,\n    label=\"Migrants\",\n)\nplt.xlabel(\"Day\")\nplt.ylabel(\"Level of support\")\nplt.title(\"Support for refugees and migrants\")\nplt.show()\n\n\n\n\n\n\n\nggplot(support_long, \n       aes(x=date_n, y=support, color=group)) + \n  geom_line(size=1.5) + \n  geom_ribbon(aes(fill=group, ymin=support-0.15,\n                  ymax=support+0.15),\n              alpha=.1, lty=0) +\n  ggtitle(\"Support for refugees and migrants\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Plotting Geospatial Data\nPlotting geospatial data is a more powerful tool to compare countries or other regions. Maps are very easy to understand and can have greater impact to all kinds of readers, which make them a useful representation for a wide range of studies that any computational analyst has to deal with. Geospatial data is based on the specific location of any country, region, city or geographical area, marked by its coordinates, latitude and longitude, that can later build points and polygon areas. The coordinates are normally mandatory to plot any data on a map, but are not always provided in our raw data. In those cases, we must join the geographical information we have (i.e. the name of a country) with its coordinates in order to have an accurate data frame for plotting geospatial data. Some libraries in R and Python might directly read and interpret different kinds of geospatial information by recognizing strings such as “France” or “Paris”, but in the end they will be converted into coordinates.\nUsing the very same data as our example, we might want to plot in a map the level of support to European refugees by country. Firstly, we should create a data frame with the average level of support to refugees by country (supports_country). Secondly, we must install an existing library that provides you with accurate geospatial information. In the case of R, we recommend the package maps which contains the function map_data that helps you generate an object with geospatial information of specific areas, countries or regions, that can be easily read and plotted by ggplot2. Even if not explained in this book, we also recommend ggmap in R (Kahle and Wickham, 2013). When working with Python we recommend geopandas that works very well with pandas and matplotlib (it will also need some additional packages such as descartes).\nIn Example 7.17 we illustrate how to plot a world map (from existing geographical information). We then save a partial map into the object some_eu_maps containing the European countries that participated in the survey. After we merge supports_country and some_eu_maps (by region) and get a complete data frame called support_map with coordinates for each country (Example 7.18). Finally, we plot it using the ggplot2 function geom_polygon in R and the geopandas method plot in Python (Example 7.19). Voilà: a nice and comprehensible representation of our data with a scale of colors!\n\n\n\n\n\n\n\nExample 7.17 Simple world map\n\nPython codeR code\n\n\n\nsupports_country = (\n    d2.groupby([\"country\"])[\"support_refugees_n\"]\n    .mean()\n    .to_frame()\n    .reset_index()\n)\n\n# Load a world map and plot it\nwmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\nwmap = wmap.rename(columns={\"name\": \"country\"})\nwmap.plot()\n\n\n\n\n\n\n\nsupports_country = d2 %>%\n  group_by(country) %>%\n  summarise(m=mean(support_refugees_n,na.rm=TRUE))\n\n#Load a world map and plot it\nwmap = map_data(\"world\")\nggplot(wmap, aes(x=long,y=lat,group=group)) +\n  geom_polygon(fill=\"lightgray\", colour = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.18 Select EU countries and joint the map with Eurobarometer data\n\nPython codeR code\n\n\n\ncountries = [\n    \"Portugal\",\n    \"Spain\",\n    \"France\",\n    \"Germany\",\n    \"Austria\",\n    \"Belgium\",\n    \"Netherlands\",\n    \"Ireland\",\n    \"Denmark\",\n    \"Poland\",\n    \"UK\",\n    \"Latvia\",\n    \"Cyprus\",\n    \"Croatia\",\n    \"Slovenia\",\n    \"Hungary\",\n    \"Slovakia\",\n    \"Czech republic\",\n    \"Greece\",\n    \"Finland\",\n    \"Italy\",\n    \"Luxemburg\",\n    \"Sweden\",\n    \"Sweden\",\n    \"Bulgaria\",\n    \"Estonia\",\n    \"Lithuania\",\n    \"Malta\",\n    \"Romania\",\n]\nm = wmap.loc[wmap[\"country\"].isin(countries)]\nm = pd.merge(supports_country, m, on=\"country\")\n\n\n\n\ncountries = c(\n  \"Portugal\", \"Spain\", \"France\", \"Germany\",\n  \"Austria\", \"Belgium\", \"Netherlands\", \"Ireland\",\n  \"Denmark\", \"Poland\", \"UK\", \"Latvia\", \"Cyprus\",\n  \"Croatia\", \"Slovenia\", \"Hungary\", \"Slovakia\",\n  \"Czech republic\", \"Greece\", \"Finland\", \"Italy\",\n  \"Luxemburg\", \"Sweden\", \"Sweden\", \"Bulgaria\", \n  \"Estonia\", \"Lithuania\", \"Malta\", \"Romania\")\nm = wmap %>% rename(country=region) %>% \n  filter(country %in% countries) %>%\n  left_join(supports_country, by=\"country\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.19 Map of Europe with the average level of support for refugees by country\n\nPython codeR code\n\n\n\nm = gpd.GeoDataFrame(m, geometry=m[\"geometry\"])\nm.plot(\n    column=\"support_refugees_n\",\n    legend=True,\n    cmap=\"OrRd\",\n    legend_kwds={\"label\": \"Level of suppport\"},\n).set_title(\"Support of refugees by country\")\n\n\n\n\n\n\n\nggplot(m, aes(long, lat, group=group))+\n  geom_polygon(aes(fill = m), color=\"white\")+\n  scale_fill_viridis_c(option=\"B\")+\n  labs(title=\"Support for refugees by country\", \n       fill=\"Level of support\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.4 Other Possibilities\nThere are many other ways of visualizing data. For EDA we have covered in this chapter only some of the most used techniques but they might be still limited for your future work. There are many books that cover data visualization in detail, such as Tufte (2006), Cairo (2019), and Kirk (2016). There are also many online resources, such as the Python Graph Gallery 7 and the R Graph Gallery 8, which introduce you to other useful plot types. These sites include code examples, many using the ggplot, matplotlib and seaborn packages introduced here, but also using other packages such as bokeh or plotly for interactive plots."
  },
  {
    "objectID": "chapter07.html#sec-clustering",
    "href": "chapter07.html#sec-clustering",
    "title": "7  Exploratory data analysis",
    "section": "7.3 Clustering and Dimensionality Reduction",
    "text": "7.3 Clustering and Dimensionality Reduction\nSo far, we have reviewed traditional statistical exploratory and visualization techniques that any social scientist should be able to apply. A more computational next step in your EDA workflow is using machine learning (ML) to let your computer “learn” about our data and in turn give more initial insights. ML is a branch of artificial intelligence that uses algorithms to interact with data and obtain some patterns or rules that characterize that data. We normally distinguish between supervised machine learning (SML) and unsupervised machine learning (UML). In Chapter 8, we will come back to this distinction. For now, it may suffice to say that the main characteristic of unsupervised methods is that we do not have any measurement available for a dependent variable, label, or categorization, which we want to predict. Instead, we want to identify patterns in the data without knowing in advance what these may look like. In this, unsupervised machine learning is very much of a inductive, bottom-up technique (see Chapter 1 and Boumans and Trilling (2016)).\nIn this chapter, we will focus on UML as a means of finding groups and latent dimensions in our data, which can also help to reduce our number of variables. Specifically, we will use base R and Python’s scikit-learn to conduct \\(k\\)-means clustering, hierarchical clustering, and principal component analysis (PCA) as well as the closely related singular value decomposition (SVD).\nIn data mining, we use clustering as a UML technique that aims to find the relationship between a set of descriptive variables. By doing cluster analysis we can identify underlying groups in our data that we will call clusters. Imagine we want to explore how European countries can be grouped based on their average support to refugees/migrants, age and educational level. We might create some a priori groups (such as southern versus northern countries), but cluster analysis would be a great method to let the data “talk” and then create the most appropriate groups for this specific case. As in all UML, the groups will come unlabeled and the computational analyst will be in charge of finding an appropriate and meaningful label for each cluster to better communicate the results.\n\n7.3.1 \\(k\\)-means Clustering\n\\(k\\)-means is a very frequently used algorithm to perform cluster analysis. Its main advantage is that, compared to the hierarchical clustering methods we will discuss later, it is very fast and does not consume many resources. This makes it especially useful for larger datasets.\n\\(k\\)-means cluster analysis is a method that takes any number of observations (cases) and groups them into a given number of clusters based on the proximity of each observation to the mean of the formed cluster (centroid). Mathematically, we measure this proximity as the distance of any given point to its cluster center, and can be expressed as\nwhere \\(\\|x_n - \\mu_k\\|\\) is the distance between the data point \\(x_n\\) and the center of the cluster \\(\\mu_k\\).\nInstead of taking the mean, some variations of this algorithm take the median (\\(k\\)-medians) or a representative observation, also called medoid (\\(k\\)-medoids or partitioning around medoids, PAM) as a way to optimize the initial method.\nBecause \\(k\\)-means clustering calculates distances between cases, these distances need to be meaningful – which is only the cases if the scales on which the variables are measured are comparable. If all your variables are measured on the same (continuous) scale with the same endpoints, you may be fine. In most cases, you need to normalize your data by transforming them into, for instance, \\(z\\)-scores9, or a scale from 0 to 1.\nHence, the first thing we do in our example, is to prepare a proper dataset with only continuous variables, scaling the data (for comparability) and avoiding missing values (drop or impute). In Example 7.20, we will use the variables support to refugees (support_refugees_n), support to migrants (support_migrants_n), age (age) and educational level (number of years of education) (educational_n) and will create a data frame d3 with the mean of all these variables for each country (each observation will be a country). \\(k\\)-means requires us to specify the number of clusters, \\(k\\), in advance. This is a tricky question, and (besides arbitrarily deciding \\(k\\)!), you essentially need to re-estimate your model multiple times with different \\(k\\)s.\nThe simplest method to obtain the optimal number of clusters is to estimate the variability within the groups for different runs. This means that we must run \\(k\\)-means for different number of clusters (e.g. 1 to 15 clusters) and then choose the number of clusters that decreases the variability maintaining the highest number of clusters. When you generate and plot a vector with the variability, or more technically, the within-cluster sum of squares (WSS) obtained after each execution, it is easy to identify the optimal number: just look at the bend (knee or elbow) and you will find the point where it decreases the most and then get the optimal number of clusters (three clusters in our example).\n\n\n\n\n\n\n\nExample 7.20 Getting the optimal number of clusters\n\nPython codeR code\n\n\n\n# Average variables by country and scale\nd3 = d2.groupby([\"country\"])[\n    [\"support_refugees_n\", \"support_migrants_n\", \"age\", \"educational_n\"]\n].mean()\n\nscaler = StandardScaler()\nd3_s = scaler.fit_transform(d3)\n\n# Store sum of squares for 1..15 clusters\nwss = []\nfor i in range(1, 15):\n    km_out = KMeans(n_clusters=i, n_init=20)\n    km_out.fit(d3_s)\n    wss.append(km_out.inertia_)\n\nKMeans(n_clusters=14, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=14, n_init=20)\n\nplt.plot(range(1, 15), wss, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Within groups sum of squares\")\nplt.show()\n\n\n\n\n\n\n\n# Average variables by country and scale\nd3_s = d2%>%\n  group_by(country)%>%\n  summarise(\n    m_refugees=mean(support_refugees_n, na.rm=T), \n    m_migrants=mean(support_migrants_n, na.rm=T),\n    m_age=mean(age, na.rm=T),\n    m_edu=mean(educational_n, na.rm=T)) %>%\n  column_to_rownames(var=\"country\") %>%\n  scale()\n# Store sum of squares for 1..15 clusters\nwss = list()\nfor (i in 1:15) {\n  km.out = kmeans(d3_s, centers=i, nstart=25)\n  wss[[i]] = tibble(k=i, ss=km.out$tot.withinss)\n}\nwss = bind_rows(wss)\nggplot(wss, aes(x=k, y=ss)) + \n  geom_line() + geom_point() + \n  xlab(\"Number of Clusters\") + \n  ylab(\"Within groups sum of squares\")\n\n\n\n\n\n\n\n\n\n\n\nNow we can estimate our final model (Example 7.21). We generate 25 initial random centroids (the algorithm will choose the one that optimizes the cost). The default of this parameter is 1, but it is recommended to set it with a higher number (i.e. 20 to 50) to guarantee the maximum benefit of the method. The base R function kmeans and scikit-learn function KMeans in Python will produce the clustering. You can observe the mean (scaled) for each variable in each cluster, as well as the corresponding cluster for each observation.\n\n\n\n\n\n\n\nExample 7.21 Using Kmeans to group countries based on the average support of refugees and migrants, age, and educational level\n\nPython codeR code\n\n\n\n# Compute k-means with k = 3\nkm_res = KMeans(n_clusters=3, n_init=25).fit(d3_s)\nprint(km_res)\n\nKMeans(n_clusters=3, n_init=25)\n\nprint(\"K-means cluste sizes:\", np.bincount(km_res.labels_[km_res.labels_ >= 0]))\n\nK-means cluste sizes: [13  3 12]\n\nprint(f\"Cluster means: {km_res.cluster_centers_}\")\n\nCluster means: [[-0.89000978 -0.82574663 -0.3892184  -0.21560025]\n [ 1.2101425   1.01720791  1.78536032  2.49604445]\n [ 0.66164163  0.64025687 -0.02468681 -0.39044418]]\n\nprint(\"Clustering vector:\")\n\nClustering vector:\n\nprint(np.column_stack((d3.index, km_res.labels_)))\n\n[['Austria' 2]\n ['Belgium' 2]\n ['Bulgaria' 0]\n ['Croatia' 0]\n ['Cyprus' 2]\n ['Czech republic' 0]\n ['Denmark' 1]\n ['Estonia' 0]\n ['Finland' 1]\n ['France' 2]\n ['Germany' 2]\n ['Greece' 0]\n ['Hungary' 0]\n ['Ireland' 2]\n ['Italy' 0]\n ['Latvia' 0]\n ['Lithuania' 0]\n ['Luxemburg' 2]\n ['Malta' 2]\n ['Netherlands' 2]\n ['Poland' 0]\n ['Portugal' 2]\n ['Romania' 0]\n ['Slovakia' 0]\n ['Slovenia' 0]\n ['Spain' 2]\n ['Sweden' 1]\n ['UK' 2]]\n\nprint(\"Within cluster sum of squares:\")\n\nWithin cluster sum of squares:\n\nprint(km_res.inertia_)\n\n42.50488485460036\n\n\n\n\n\nset.seed(123)\nkm.res = kmeans(d3_s, 3, nstart=25)\nprint(km.res)\n\nK-means clustering with 3 clusters of sizes 13, 3, 12\n\nCluster means:\n  m_refugees m_migrants       m_age      m_edu\n1 -0.8739722 -0.8108671 -0.38220489 -0.2117152\n2  1.1883363  0.9988783  1.75318903  2.4510670\n3  0.6497192  0.6287198 -0.02424197 -0.3834086\n\nClustering vector:\n       Austria        Belgium       Bulgaria        Croatia         Cyprus \n             3              3              1              1              3 \nCzech republic        Denmark        Estonia        Finland         France \n             1              2              1              2              3 \n       Germany         Greece        Hungary        Ireland          Italy \n             3              1              1              3              1 \n        Latvia      Lithuania      Luxemburg          Malta    Netherlands \n             1              1              3              3              3 \n        Poland       Portugal        Romania       Slovakia       Slovenia \n             1              3              1              1              1 \n         Spain         Sweden             UK \n             3              2              3 \n\nWithin cluster sum of squares by cluster:\n[1] 20.485188  2.984011 17.517654\n (between_SS / total_SS =  62.0 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\n\n\n\nUsing the function fviz_cluster of the library factoextra in R, or the pyplot function scatter in Python, you can get a visualization of the clusters. In Example 7.22 you can clearly identify that the clusters correspond to Nordic countries (more support to foreigners, more education and age), Central and Southern European countries (middle support, lower education and age), and Eastern European countries (less support, lower education and age)10 .\n\n\n\n\n\n\n\nExample 7.22 Visualization of clusters\n\nPython codeR code\n\n\n\nfor cluster in range(km_res.n_clusters):\n    plt.scatter(\n        d3_s[km_res.labels_ == cluster, 0], d3_s[km_res.labels_ == cluster, 1]\n    )\nplt.scatter(\n    km_res.cluster_centers_[:, 0],\n    km_res.cluster_centers_[:, 1],\n    s=250,\n    marker=\"*\",\n)\nplt.legend(scatterpoints=1)\nplt.show()\n\n\n\n\n\n\n\nfviz_cluster(km.res, d3_s, ellipse.type=\"norm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.2 Hierarchical Clustering\nAnother method to conduct a cluster analysis is hierarchical clustering, which builds a hierarchy of clusters that we can visualize in a dendogram. This algorithm has two versions: a bottom-up approach (observations begin in their own clusters), also called agglomerative, and a top-down approach (all observations begin in one cluster), also called divisive. We will follow the bottom-up approach in this chapter and when you look at the dendogram you will realize how this strategy repeatedly combines the two nearest clusters at the bottom into a larger one in the top. The distance between clusters is initially estimated for every pair of observation points and then put every point in its own cluster in order to get the closest pair of points and iteratively compute the distance between each new cluster and the previous ones. This is the internal rule of the algorithm and we must choose a specific linkage method (complete, single, average or centroid, or Ward’s linkage). Ward’s linkage is a good default choice: it minimizes the variance of the clusters being merged. In doing so, it tends to produce roughly evenly sized clusters and is less sensitive to noise and outliers than some of the other methods. In Example 7.23 we will use the function hcut of the package factoextra in R and scikit-learn function AgglomerativeClustering in Python, to compute the hierarchical clustering.\nA big advantage of hierarchical clustering is that, once estimated, you can freely choose the number of clusters in which to group your cases without re-estimating the model. If you decide, for instance, to use four instead of three clusters, then the cases in one of your three clusters are divided into two subgroups. With \\(k\\)-means, in contrast, a three-cluster solution can be completely different from a four-cluster solution. However, this comes at a big cost: hierarchical clustering requires a lot more computing resources and may therefore not be feasible for large datasets.\n\n\n\n\n\n\n\nExample 7.23 Using hierarchical clustering to group countries based on the average support of refugees and migrants, age and educational level\n\nPython codeR code\n\n\n\nhc_res = AgglomerativeClustering(affinity=\"euclidean\", linkage=\"complete\")\nhc_res.fit_predict(d3_s)\n\narray([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 0])\n\nprint(hc_res)\n\nAgglomerativeClustering(affinity='euclidean', linkage='complete')\n\n\n\n\n\nhc.res <- hcut(d3_s, hc_method=\"complete\")\nsummary(hc.res)\n\n            Length Class  Mode     \nmerge        54    -none- numeric  \nheight       27    -none- numeric  \norder        28    -none- numeric  \nlabels       28    -none- character\nmethod        1    -none- character\ncall          3    -none- call     \ndist.method   1    -none- character\ncluster      28    -none- numeric  \nnbclust       1    -none- numeric  \nsilinfo       3    -none- list     \nsize          2    -none- numeric  \ndata        112    -none- numeric  \n\n\n\n\n\n\n\n\n\nWe can then plot the dendogram with base R function plot and scipy (module cluster.hierarchy) function dendogram in Python. The summary of the initial model suggest two clusters (size=2) but by looking at the dendogram you can choose the number of clusters you want to work with by choosing a height (for example four to get three clusters).\n\n\n\n\n\n\n\nExample 7.24 Dendogram to visualize the hierarchical clustering\n\nPython codeR code\n\n\n\ndendrogram = sch.dendrogram(\n    sch.linkage(d3_s, method=\"complete\"),\n    labels=list(d3.index),\n    leaf_rotation=90,\n)\n\n\n\n\nplot(hc.res, cex=0.5)\n\n\n\n\n\n\n\n\n\n\n\nIf you re-run the hierarchical clustering for three clusters (Example 7.25) and visualize it (Example 7.26) you will get a graph similar to the one produced by \\(k\\)-means.\n\n\n\n\n\n\n\nExample 7.25 Re-run hierarchical clustering with three clusters\n\nPython codeR code\n\n\n\nhc_res = AgglomerativeClustering(\n    n_clusters=3, affinity=\"euclidean\", linkage=\"ward\"\n)\nhc_res.fit_predict(d3_s)\n\narray([0, 0, 2, 0, 0, 2, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0,\n       0, 2, 0, 0, 1, 0])\n\nprint(hc_res)\n\nAgglomerativeClustering(affinity='euclidean', n_clusters=3)\n\n\n\n\n\nhc.res = hcut(d3_s, k=3, hc_method=\"complete\") \nsummary(hc.res)\n\n            Length Class  Mode     \nmerge        54    -none- numeric  \nheight       27    -none- numeric  \norder        28    -none- numeric  \nlabels       28    -none- character\nmethod        1    -none- character\ncall          3    -none- call     \ndist.method   1    -none- character\ncluster      28    -none- numeric  \nnbclust       1    -none- numeric  \nsilinfo       3    -none- list     \nsize          3    -none- numeric  \ndata        112    -none- numeric  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.26 Re-run hierarchical clustering with three clusters\n\nPython codeR code\n\n\n\nfor cluster in range(hc_res.n_clusters):\n    plt.scatter(\n        d3_s[hc_res.labels_ == cluster, 0], d3_s[hc_res.labels_ == cluster, 1]\n    )\nplt.legend(scatterpoints=1)\nplt.show()\n\n\n\n\n\n\n\nfviz_cluster(hc.res, d3_s, ellipse.type=\"convex\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.3 Principal Component Analysis and Singular Value Decomposition\nCluster analyses are in principle used to group similar cases. Sometimes, we want to group similar variables instead. A well-known method for this is principal component analysis (PCA)11. This unsupervised method is useful to reduce the dimensionality of your data by creating new uncorrelated variables or components that describe the original dataset. PCA uses linear transformations to create principal components that are ordered by the level of explained variance (the first component will catch the largest variance). We will get as many principal components as number of variables we have in the dataset, but when we look at the cumulative variance we can easily select only few of these components to explain most of the variance and thus work with a smaller and summarized data frame that might be more convenient for many tasks (i.e. those that require avoiding multicollinearity or just need to be more computationally efficient). By simplifying the complexity of our data we can have a first understanding of how our variables are related and also of how our observations might be grouped. All components have specific loadings for each original variable, which can tell you how the old variables are represented in the new components. This statistical technique is especially useful in EDA when working with high dimensional datasets but it can be used in many other situations.\nThe mathematics behind PCA can be relatively easy to understand. However, for the sake of simplicity, we will just say that in order to obtain the principal components the algorithm firstly has to compute the mean of each variable and then compute the covariance matrix of the data. This matrix contains the covariance between the elements of a vector and the output will be a square matrix with an identical number of rows and columns, corresponding to the total number of dimensions of the original dataset. Specifically, we can calculate the covariance matrix of the variables X and y with the formula:\nSecondly, using the covariance matrix the algorithm computes the eigenvectors and their corresponding eigenvalues, and then drop the eigenvectors with the lowest eigenvalues. With this reduced matrix it transforms the original values to the new subspace in order to obtain the principal components that will synthesize the original dataset.\nLet us now conduct a PCA over the Eurobarometer data. In Example 7.27 we will re-use the sub-data frame d3 containing the means of 4 variables (support to refugees, support to migrants, age and educational level) for each of the 30 European countries. The question is can we have a new data frame containing less than 4 variables but that explains most of the variance, or in other words, that represents our original dataset well enough, but with fewer dimensions? As long as our features are measured on different scales, it is normally suggested to center (to mean 0) and scale (to standard deviation 1) the data. You may also know this transformation as “calculating \\(z\\)-scores”. We can perform the PCA in R using the base function prcomp and in Python using the function PCA of the module decomposition of scikit-learn.\n\n\n\n\n\n\n\nExample 7.27 Principal component analysis (PCA) of a data frame with 30 records and 4 variables\n\nPython codeR code\n\n\n\npca_m = PCA()\npca = pca_m.fit(d3_s)\npca_n = PCA()\npca = pca_n.fit_transform(d3_s)\npca_df = pd.DataFrame(data=pca, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"])\npca_df.index = d3.index\nprint(pca_df.head())\n\n               PC1       PC2       PC3       PC4\ncountry                                         \nAustria  -0.103285 -1.220018 -0.535673  0.066888\nBelgium  -0.029355 -0.084707  0.051515  0.227609\nBulgaria -1.660518  0.949533 -0.480337 -0.151837\nCroatia  -1.267502 -0.819093 -0.920657  0.843682\nCyprus    0.060590 -0.195928  0.573670  0.812519\n\npca_df_2 = pd.DataFrame(\n    data=pca_n.components_.T, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"]\n)\npca_df_2.index = d3.columns\nprint(pca_df_2)\n\n                         PC1       PC2       PC3       PC4\nsupport_refugees_n  0.573292 -0.369010  0.139859  0.718058\nsupport_migrants_n  0.513586 -0.533140 -0.094283 -0.665659\nage                 0.445117  0.558601  0.670994 -0.199005\neducational_n       0.457642  0.517261 -0.722023  0.041073\n\n\n\n\n\npca = prcomp(d3_s, scale=TRUE)\nhead(pca$x)\n\n                       PC1         PC2         PC3         PC4\nAustria         0.10142430  1.19803416 -0.52602091  0.06568251\nBelgium         0.02882642  0.08318037  0.05058673  0.22350741\nBulgaria        1.63059623 -0.93242257 -0.47168110 -0.14910143\nCroatia         1.24466229  0.80433304 -0.90406722  0.82847927\nCyprus         -0.05949790  0.19239739  0.56333279  0.79787757\nCzech republic  2.17979559 -0.66348027 -0.74991773 -0.08459459\n\npca$rotation\n\n                  PC1        PC2         PC3         PC4\nm_refugees -0.5732924  0.3690096  0.13985877  0.71805799\nm_migrants -0.5135857  0.5331396 -0.09428317 -0.66565948\nm_age      -0.4451170 -0.5586008  0.67099390 -0.19900549\nm_edu      -0.4576422 -0.5172613 -0.72202312  0.04107309\n\n\n\n\n\n\n\n\n\nThe generated object with the PCA contains different elements (in R sdev, rotation, center, scale and x) or attributes in Python (components_, explained_variance_, explained_variance_ratio, singular_values_, mean_, n_components_, n_features_, n_samples_, and noise_variance_). In the resulting object we can see the values of four principal components of each country, and the values of the loadings, technically called eigenvalues, for the variables in each principal component. In our example we can see that support for refugees and migrants are more represented on PC1, while age and educational level are more represented on PC2. If we plot the first two principal components using base function biplot in R and the library bioinfokit in Python (Example 7.28), we can clearly see how the variables are associated with either PC1 or with PC2 (we might also want to plot any pair of the four components!). But we can also get a picture of how countries are grouped based only in these two new variables.\n\n\n\n\n\n\n\nExample 7.28 Plot PC1 and PC2\n\nPython codeR code\n\n\n\nvar1 = round(pca_n.explained_variance_ratio_[0], 2)\nvar2 = round(pca_n.explained_variance_ratio_[1], 2)\nbioinfokit.visuz.cluster.biplot(\n    cscore=pca,\n    loadings=pca_n.components_,\n    labels=pca_df_2.index.values,\n    var1=var1,\n    var2=var2,\n    show=True,\n)\n\n\n\n\n\n\n\nbiplot(x = pca, scale = 0, cex = 0.6, \n       col = c(\"blue4\", \"brown3\"))\n\n\n\n\n\n\n\n\n\n\n\nSo far we are not sure how many components are enough to accurately represent our data, so we need to know how much variance (which is the square of the standard deviation) is explained by each component. We can get the values (Example 7.29) and plot the proportion of explained variance (Example 7.30). We get that the first component explains 57.85% of the variance, the second 27.97%, the third 10.34% and the fourth just 3.83%.\n\n\n\n\n\n\n\nExample 7.29 Proportion of variance explained\n\nPython codeR code\n\n\n\nprint(\"Proportion of variance explained:\")\n\nProportion of variance explained:\n\nprint(pca_n.explained_variance_ratio_)\n\n[0.57848569 0.27974794 0.10344996 0.03831642]\n\n\n\n\n\nprint(\"Proportion of variance explained:\")\n\n[1] \"Proportion of variance explained:\"\n\nprop_var = tibble(pc=1:4,\n    var=pca$sdev^2 / sum(pca$sdev^2))\nprop_var\n\n# A tibble: 4 × 2\n     pc    var\n  <int>  <dbl>\n1     1 0.578 \n2     2 0.280 \n3     3 0.103 \n4     4 0.0383\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.30 Plot of the proportion of variance explained\n\nPython codeR code\n\n\n\nplt.bar([1, 2, 3, 4], pca_n.explained_variance_ratio_)\nplt.ylabel(\"Proportion of variance explained\")\nplt.xlabel(\"Principal component\")\nplt.xticks([1, 2, 3, 4])\nplt.show()\n\n\n\n\n\n\n\nggplot(prop_var, aes(x=pc, y=var)) +\n  geom_col() +\n  scale_y_continuous(limits = c(0,1)) +\n  xlab(\"Principal component\") + \n  ylab(\"Proportion of variance explained\")\n\n\n\n\n\n\n\n\n\n\n\nWhen we estimate (Example 7.31) and plot (Example 7.32) the cumulative explained variance it is easy to identify that with just the two first components we explain 88.82% of the variance. It might now seem a good deal to reduce our dataset from four to two variables, or let’s say half of the data, but retaining most of the original information.\n\n\n\n\n\n\n\nExample 7.31 Cumulative explained variance\n\nPython codeR code\n\n\n\ncvar = np.cumsum(pca_n.explained_variance_ratio_)\ncvar\n\narray([0.57848569, 0.85823362, 0.96168358, 1.        ])\n\n\n\n\n\ncvar = cumsum(prop_var)\ncvar\n\n# A tibble: 4 × 2\n     pc   var\n  <int> <dbl>\n1     1 0.578\n2     3 0.858\n3     6 0.962\n4    10 1    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.32 Plot of the cumulative explained variance\n\nPython codeR code\n\n\n\nplt.plot(cvar)\nplt.xlabel(\"number of components\")\nplt.xticks(np.arange(len(cvar)), np.arange(1, len(cvar) + 1))\nplt.ylabel(\"cumulative explained variance\")\nplt.show()\n\n\n\n\n\n\n\nggplot(cvar, aes(x=pc, y=var)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  xlab(\"Principal component\") +\n  ylab(\"Cumulative explained variance\")\n\n\n\n\n\n\n\n\n\n\n\nAnd what if we want to use this PCA and deploy a clustering (as explained above) with just these two new variables instead of the four original ones? Just repeat the \\(k\\)-means procedure but now using a new smaller data frame selecting PC1 and PC2 from the PCA. After estimating the optimal number of clusters (three again!) we can compute and visualize the clusters, and get a very similar picture to the one obtained in the previous examples, with little differences such as the change of cluster of the Netherlands (more similar now to the Nordic countries!). This last exercise is a good example of how to combine different techniques in EDA.\n\n\n\n\n\n\n\nExample 7.33 Combining PCA to reduce dimensionality and \\(k\\)-means to group countries\n\nPython codeR code\n\n\n\n# Generate a new dataset with first components\nd5 = pca[:, 0:2]\nd5[0:5]\n\n# Get optimal number of clusters\n\narray([[-0.10328545, -1.22001827],\n       [-0.02935539, -0.08470675],\n       [-1.66051792,  0.94953266],\n       [-1.26750204, -0.81909268],\n       [ 0.06058969, -0.19592792]])\n\nwss = []\nfor i in range(1, 15):\n    km_out = KMeans(n_clusters=i, n_init=20)\n    km_out.fit(d5)\n    wss.append(km_out.inertia_)\n\n# Plot sum of squares vs. number of clusters\n\nKMeans(n_clusters=14, n_init=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=14, n_init=20)\n\nplt.plot(range(1, 15), wss, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Within groups sum of squares\")\nplt.show()\n\n# Compute again with k = 3 and visualize\n\n\n\nkm_res_5 = KMeans(n_clusters=3, n_init=25).fit(d5)\nfor cluster in range(km_res_5.n_clusters):\n    plt.scatter(\n        d3_s[km_res_5.labels_ == cluster, 0],\n        d3_s[km_res_5.labels_ == cluster, 1],\n    )\nplt.scatter(\n    km_res_5.cluster_centers_[:, 0],\n    km_res_5.cluster_centers_[:, 1],\n    s=250,\n    marker=\"*\",\n)\nplt.legend(scatterpoints=1)\nplt.show()\n\n\n\n\n\n\n\n#Generate a new dataset with first components\nd5 = pca$x[, c(\"PC1\", \"PC2\")]\nhead(d5)\n\n                       PC1         PC2\nAustria         0.10142430  1.19803416\nBelgium         0.02882642  0.08318037\nBulgaria        1.63059623 -0.93242257\nCroatia         1.24466229  0.80433304\nCyprus         -0.05949790  0.19239739\nCzech republic  2.17979559 -0.66348027\n\n#Get optimal number of clusters\nwss = list()\nfor (i in 1:15) {\n  km.out = kmeans(d5, centers = i, nstart = 20)\n  wss[[i]] = tibble(k=i, ss=km.out$tot.withinss)\n}\nwss = bind_rows(wss)\n\n# Plot sum of squares vs. number of clusters\nggplot(wss, aes(x=k, y=ss)) + geom_line() + \n     xlab(\"Number of Clusters\") + \n     ylab(\"Within groups sum of squares\")\n\n\n\n# Compute again with k = 3 and visualize\nset.seed(123)\nkm.res_5 <- kmeans(d5, 3, nstart = 25)\nfviz_cluster(km.res_5, d5, ellipse.type = \"norm\")\n\n\n\n\n\n\n\n\n\n\n\nWhen your dataset gets bigger, though, you may actually not use PCA but the very much related singular value decomposition, SVD. They are closely interrelated, and in fact SVD can be used “under the hood” to estimate a PCA. While PCA is taught in a lot of classical textbooks for statistics in the social sciences, SVD is usually not. Yet, it has a great advantage: in the way that it is implemented in scikit-learn, it does not require to store the (dense) covariance matrix in memory (see the feature box in ?sec-workflow for more information on sparse versus dense matrices). This means that once your dataset grows bigger than typical survey datasets, a PCA maybe quickly become impossible to estimate, whereas the SVD can still be estimated without much resource required. Therefore, especially when you are working with textual data, you will see that SVD is used instead of PCA. For all practical purposes, the way that you can use and interpret the results stays the same.\n\n\n\n\nBoumans, Jelle W., and Damian Trilling. 2016. “Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars.” Digital Journalism 4 (1): 8–23. https://doi.org/10.1080/21670811.2015.1096598.\n\n\nBryman, Alan. 2012. Social Research Methods. 4th edition. New York, NY: Oxford University Press.\n\n\nCairo, Alberto. 2019. How Charts Lie. WW Norton & Company.\n\n\nKirk, Andy. 2016. Data Visualisation: A Handbook for Data Driven Design. London, UK: SAGE.\n\n\nTufte, Edward R. 2006. Beautiful Evidence. Vol. 1. Graphics Press Cheshire, CT.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. Vol. 2. Reading, Mass."
  },
  {
    "objectID": "chapter08.html#sec-prediction",
    "href": "chapter08.html#sec-prediction",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.1 Statistical Modeling and Prediction",
    "text": "8.1 Statistical Modeling and Prediction\nMachine learning, many people joke, is nothing other than a fancy name for statistics. And, in fact, there is some truth to this: if you say “logistic regression”, this will sound familiar to both statisticians and machine learning practitioners. Hence, it does not make much sense to distinguish between statistics on the one hand and machine learning on the other hand. Still, there are some differences between traditional statistical approaches that you may have learned about in your statistics classes and the machine learning approach, even if some of the same mathematical tools are used. One may say that the focus is a different one, and the objective we want to achieve may differ.\nLet us illustrate this with an example. media.csv1 contains a few columns from survey data on how many days per week respondents turn to different media types (radio, newspaper, tv and Internet) in order to follow the news2. It also contains their age (in years), their gender (coded as female = 0, male = 1), and their education (on a 5-point scale).\nA straightforward question to ask is how far the sociodemographic characteristics of the respondents explain their media use. Social scientists would typically approach this question by running a regression analysis. Such an analysis tells us how some independent variables \\(x_1, x_2, \\ldots, x_n\\) can explain \\(y\\). In an ordinary least square regression (OLS), we would estimate \\(y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\\).\nIn a typical social-science paper, we would then interpret the coefficients that we estimated, and say something like: when \\(x_1\\) increases by one unit, \\(y\\) increases by \\(\\beta_1\\). We sometimes call this “the effect of \\(x_1\\) on \\(y\\)” (even though, of course, it depends on the study design whether the relationship can really be interpreted as a causal effect). Additionally, we might look at the explained variance \\(R^2\\), to assess how well the model fits our data. In Example 8.1 we use this regression approach to model the relationship of age and gender over the number of days per week a person reads a newspaper. We fit the linear model using the stats function lm in R and the statsmodels function ols (imported from the module statsmodels.formula.api) in Python.\n\n\n\n\n\n\n\nExample 8.1 Obtaining a model through estimating an OLS regression\n\nPython codeR code\n\n\n\ndf = pd.read_csv(\"https://cssbook.net/d/media.csv\")\nmod = smf.ols(formula=\"newspaper ~ age + gender\", data=df).fit()\n# mod.summary() would give a lot more info,\n# but we only care about the coefficients:\nmod.params\n\nIntercept   -0.089560\nage          0.067620\ngender       0.176665\ndtype: float64\n\n\n\n\n\ndf = read.csv(\"https://cssbook.net/d/media.csv\")\nmod = lm(formula = \"newspaper ~ age + gender\",\n         data = df)\n# summary(mod) would give a lot more info, \n# but we only care about the coefficients:\nmod\n\n\nCall:\nlm(formula = \"newspaper ~ age + gender\", data = df)\n\nCoefficients:\n(Intercept)          age       gender  \n   -0.08956      0.06762      0.17666  \n\n\n\n\n\n\n\n\n\nMost traditional social-scientific analyses stop after reporting and interpreting the coefficients of age (\\(\\beta = 0.0676\\)) and gender (\\(\\beta = -0.0896\\)), as well as their standard errors, confidence intervals, p-values, and the total explained variance (19%). But we can go a step further. Given that we have already estimated our regression equation, why not use it to do some prediction?\nWe have just estimated that\nBy just filling in the values for a 20 year old man, or a 40 year old woman, we can easily calculate the expected number of days such a person reads the newspaper per week, even if no such person exists in the original dataset.\nWe learn that\nThis was easy to do by hand, but of course, we could do this automatically for a large and essentially unlimited number of cases. This could be as simple as shown in Example 8.2.\n\n\n\n\n\n\n\nExample 8.2 Using the OLS model we estimated before to predict the dependent variable for new data where the dependent variable is unknown.\n\nPython codeR code\n\n\n\nnewdata = pd.DataFrame([{\"gender\": 1, \"age\": 20}, {\"gender\": 0, \"age\": 40}])\nmod.predict(newdata)\n\n0    1.439508\n1    2.615248\ndtype: float64\n\n\n\n\n\ngender = c(1,0)\nage = c(20,40)\nnewdata = data.frame(age, gender)\npredict(mod, newdata)\n\n       1        2 \n1.439508 2.615248 \n\n\n\n\n\n\n\n\n\nIn doing so, we shift our attention from the interpretation of coefficients to the prediction of the dependent variable for new, unknown cases. We do not care about the actual values of the coefficients, we just need them for our prediction. In fact, in many machine learning models, we will have so many of them that we do not even bother to report them.\nAs you see, this implies that we proceed in two steps: first, we use some data to estimate our model. Second, we use that model to make predictions.\nWe used an OLS regression for our first example, because it is very straightforward to interpret and most of our readers will be familiar with it. However, a model can take the form of any function, as long as it takes some characteristics (or “features”) of the cases (in this case, people) as input and returns a prediction.\nUsing such a simple OLS regression approach for prediction, as we did in our example, can come with a couple of problems, though. One problem is that in some cases, such predictions do not make much sense. For instance, even though we know that the output should be something between 0 and 7 (as that is the number of days in a week), our model will happily predict that once a man reaches the age of 105 (rare, but not impossible), he will read a newspaper on 7.185 out of 7 days. Similarly, a one year old girl will even have a negative amount of newspaper reading. A second problem relates to the models’ inherent assumptions. For instance, in our example it is quite an assumption to make that the relationships between these variables are linear –- we will therefore discuss multiple models that do not make such assumptions later in this chapter. And, finally, in many cases, we are actually not interested in getting an accurate prediction of a continuous number (a regression task), but rather in predicting a category. We may want to predict whether a tweet goes viral or not, whether a user comment is likely to contain offensive language or not, whether an article is more likely to be about politics, sports, economy, or lifestyle. In machine learning terms, these tasks are known as classification.\nIn the next section, we will outline key terms and concepts in machine learning. After that, we will discuss specific models that you can use for different use applications."
  },
  {
    "objectID": "chapter08.html#sec-principles",
    "href": "chapter08.html#sec-principles",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.2 Concepts and Principles",
    "text": "8.2 Concepts and Principles\nThe goal of Supervised Machine Learning can be summarized in one sentence: estimate a model based on some data, and then use the model to predict the expected outcome for some new cases, for which we do not know the outcome yet. This is exactly what we have done in the introductory example in Section 8.1.\nBut when do we need it?\nIn short, in any scenario where the following two preconditions are fulfilled. First, we have a large dataset (say, \\(100000\\) headlines) for which we want to predict to which class they belong to (say, whether they are clickbait or not). Second, for a random subset of the data (say, \\(2000\\) of the headlines), we already know the class. For example because we have manually coded (“annotated”) them.\nBefore we start using SML, though, we first need to have a common terminology. At the risk of oversimplifying matters, Table 8.1 provides a rough guideline of how some typical machine learning terms translate to statistical terms that you may be familiar with.\n\n\nTable 8.1: Some common machine learning terms explained\n\n\n\n\n\n\nmachine learning lingo\nstatistics lingo\n\n\n\n\nfeature\nindependent variable\n\n\nlabel\ndependent variable\n\n\nlabeled dataset\ndataset with both independent and dependent variables\n\n\nto train a model\nto estimate\n\n\nclassifier (classification)\nmodel to predict nominal outcomes\n\n\nto annotate\nto (manually) code (content analysis)\n\n\n\n\nLet us explain them more in detail by walking through a typical SML workflow.\nBefore we start, we need to get a labeled dataset. It may be given to us, or we may need to create it ourselves. For instance, often we can draw a random sample of our data and use techniques of manual content analysis (e.g., Riffe et al. 2019) to annotate (i.e., to manually code) the data. You can download an example for this process (annotating the topic of news articles) from dx.doi.org/10.6084/m9.figshare.7314896.v1 (Vermeer 2018).\nIt is hard to give a rule of thumb for how much labeled data you need. It depends heavily on the type of data you have (for instance, if it is a binary as opposed to a multi-class classification problem), and on how evenly distributed (class balance) they are (after all, having \\(10000\\) annotated headlines doesn’t help you if \\(9990\\) are not clickbait and only \\(10\\) are). These reservations notwithstanding, it is fair to say that typical sizes in our field are (very roughly) speaking often in the order of \\(1000\\) to \\(10000\\) when classifying longer texts (see Burscher et al. 2014), even though researchers studying less rich data sometimes annotate larger datasets (e.g., \\(60000\\) social media messages in Vermeer et al. 2019).\nOnce we have established that this labeled dataset is available and have ensured that it is of good quality, we randomly split it into two datasets: a training dataset and a test dataset.3 We will use the first one to train our model, and the second to test how well our model performs. Common ratios range from 50:50 to 80:20; and especially if the size of your labeled dataset is rather limited, you may want to have a slightly larger training dataset at the expense of a slightly smaller test dataset.\nIn Example 8.3, we prepare the dataset we already used in Section 8.1 for classification by creating a dichotomous variable (the label) and splitting it into a training and a test dataset. We use y_train to denote the training labels and X_train to denote the feature matrix of the training dataset; y_test and X_test is the corresponding test dataset. We set a so-called random-state seed to make sure that the random splitting will be the same when re-running the code. We can easily split these datasets using the rsample function initial_split in R and the sklearn function train_test_split in Python.\n\n\n\n\n\n\n\nExample 8.3 Preparing a dataset for supervised machine learning\n\nPython codeR code\n\n\n\ndf = pd.read_csv(\"https://cssbook.net/d/media.csv\")\n\ndf[\"uses-internet\"] = (df[\"internet\"] > 0).replace(\n    {True: \"user\", False: \"non-user\"}\n)\ndf.dropna(inplace=True)\nprint(\"How many people used online news at all?\")\n\nHow many people used online news at all?\n\nprint(df[\"uses-internet\"].value_counts())\n\nuser        1262\nnon-user     803\nName: uses-internet, dtype: int64\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df[[\"age\", \"education\", \"gender\"]],\n    df[\"uses-internet\"],\n    test_size=0.2,\n    random_state=42,\n)\nprint(f\"We have {len(X_train)} training and \" f\"{len(X_test)} test cases.\")\n\nWe have 1652 training and 413 test cases.\n\n\n\n\n\ndf = read.csv(\"https://cssbook.net/d/media.csv\")\ndf = na.omit(df %>% mutate(\n    usesinternet=recode(internet, \n            .default=\"user\", `0`=\"non-user\")))\n\nset.seed(42)\ndf$usesinternet = as.factor(df$usesinternet)\nprint(\"How many people used online news at all?\")\n\n[1] \"How many people used online news at all?\"\n\nprint(table(df$usesinternet))\n\n\nnon-user     user \n     803     1262 \n\nsplit = initial_split(df, prop = .8)\ntraindata = training(split)\ntestdata  = testing(split)\n\nX_train = select(traindata, \n                 c(\"age\", \"gender\", \"education\"))\ny_train = traindata$usesinternet\nX_test = select(testdata, \n                c(\"age\", \"gender\", \"education\"))\ny_test = testdata$usesinternet\n\nglue(\"We have {nrow(X_train)} training and {nrow(X_test)} test cases.\")\n\nWe have 1652 training and 413 test cases.\n\n\n\n\n\n\n\n\n\nWe now can train our classifier (i.e., estimate our model using the training dataset contained in the objects X_train and y_train). This can be as straightforward as estimating a logistic regression equation (we will discuss different classifiers in Section 8.3). It may be that we first need to create new independent variables, so-called features, a step known as feature engineering, for example by transforming existing variables, combining them, or by converting text to numerical word frequencies. Example 8.4 shows how easy it is to train a classifier using the Naïve Bayes algorithm with packages caret/naivebayes in R and sklearn in Python (this approach will be better explained in Section 8.3.1).\n\n\n\n\n\n\n\nExample 8.4 A simple Naïve Bayes classifier\n\nPython codeR code\n\n\n\nmyclassifier = GaussianNB()\nmyclassifier.fit(X_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\ny_pred = myclassifier.predict(X_test)\n\n\n\n\nmyclassifier = train(x = X_train, y = y_train, \n                     method = \"naive_bayes\")\ny_pred = predict(myclassifier, newdata = X_test)\n\n\n\n\n\n\n\n\nBut before we can actually use this classifier to do some useful work, we need to test how capable it is to predict the correct labels, given a set of features. One might think that we could just feed it the same input data (i.e., the same features) again and see whether the predicted labels match the actual labels of the test dataset. In fact, we could do that. But this test would not be strict enough: after all, the classifier has been trained on exactly these data, and therefore one would expect it to perform pretty well. In particular, it may be that the classifier is very good in predicting its own training data, but fails at predicting other data, because it overgeneralizes some idiosyncrasy in the data, a phenomenon known as overfitting (see Figure 8.1).\n\n\n\nFigure 8.1: Underfitting and overfitting. Example adapted from https://scikit-learn.org/stable/auto _ examples/model _ selection/plot _ underfitting _ overfitting.html\n\n\nInstead, we use the features of the test dataset (stored in the objects X_test and y_test) as input for our classifier, and evaluate how far the predicted labels match the actual labels. Remember: the classifier has at no point in time seen the actual labels. Therefore, we can in fact calculate how often the prediction is right.4\n\n\n\n\n\n\n\nExample 8.5 Calculating precision and recall\n\nPython codeR code\n\n\n\nprint(\"Confusion matrix:\")\n\nConfusion matrix:\n\nprint(confusion_matrix(y_test, y_pred))\n\n[[ 55 106]\n [ 40 212]]\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n    non-user       0.58      0.34      0.43       161\n        user       0.67      0.84      0.74       252\n\n    accuracy                           0.65       413\n   macro avg       0.62      0.59      0.59       413\nweighted avg       0.63      0.65      0.62       413\n\n\n\n\n\nprint(confusionMatrix(y_pred, y_test))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction non-user user\n  non-user       62   53\n  user           99  199\n                                          \n               Accuracy : 0.632           \n                 95% CI : (0.5834, 0.6786)\n    No Information Rate : 0.6102          \n    P-Value [Acc > NIR] : 0.1958408       \n                                          \n                  Kappa : 0.1843          \n                                          \n Mcnemar's Test P-Value : 0.0002623       \n                                          \n            Sensitivity : 0.3851          \n            Specificity : 0.7897          \n         Pos Pred Value : 0.5391          \n         Neg Pred Value : 0.6678          \n             Prevalence : 0.3898          \n         Detection Rate : 0.1501          \n   Detection Prevalence : 0.2785          \n      Balanced Accuracy : 0.5874          \n                                          \n       'Positive' Class : non-user        \n                                          \n\nprint(\"Confusion matrix:\")\n\n[1] \"Confusion matrix:\"\n\nconfmat = table(testdata$usesinternet, y_pred)\nprint(confmat)\n\n          y_pred\n           non-user user\n  non-user       62   99\n  user           53  199\n\nprint(\"Precision for predicting True internet\")\n\n[1] \"Precision for predicting True internet\"\n\nprint(\"users and non-internet-users:\")\n\n[1] \"users and non-internet-users:\"\n\nprecision = diag(confmat) / colSums(confmat)\nprint(precision)\n\n non-user      user \n0.5391304 0.6677852 \n\nprint(\"Recall for predicting True internet\")\n\n[1] \"Recall for predicting True internet\"\n\nprint(\"users and non-internet-users:\")\n\n[1] \"users and non-internet-users:\"\n\nrecall = (diag(confmat) / rowSums(confmat))\nprint(recall)\n\n non-user      user \n0.3850932 0.7896825 \n\n\n\n\n\n\n\n\n\nAs shown in Example 8.5, we can create a confusion matrix (generated with caret function confusionMatrix in R and sklearn function confusion_matrix in Python), and then estimate two measures: precision and recall (using base R calculations in R and sklearn function classification_report in Python). In a binary classification, the confusion matrix is a useful table in which each column usually represents the number of cases in a predicted class, and each row the number of cases in the real or actual class. With this matrix (see Figure 8.2) we can then estimate the number of true positives (TP) (correct prediction), false positives (FP) (incorrect prediction), true negatives (TN) (correct prediction) and false negatives (FN) (incorrect prediction).\n\n\n\nFigure 8.2: Visual representation of a confusion matrix.\n\n\nFor a better understanding of these concepts, imagine that we build a sentiment classifier, that predicts – based on the text of a movie review – whether it is a positive review or a negative review. Let us assume that the goal of training this classifier is to build an app that recommends only good movies to the user. There are two things that we want to achieve: we want to find as many positive films as possible (recall), but we also want that the selection we found only contains positive films (precision).\nPrecision is calculated as \\(\\frac{\\rm{TP}}{\\rm{TP}+\\rm{FP}}\\), where TP are true positives and FP are false positives. For example, if our classifier retrieves 200 articles that it classifies as positive films, but only 150 of them indeed are positive films, then the precision is \\(\\frac{150}{150+50} = \\frac{150}{200} = 0.75\\).\nRecall is calculated as \\(\\frac{\\rm{TP}}{\\rm{TP}+\\rm{FN}}\\), where TP are true positives and FN are false negatives. If we know that the classifier from the previous paragraph missed 20 positive films, then the recall is \\(\\frac{150}{150+20} = \\frac{150}{170}= 0.88\\).\nIn other words: recall measures how many of the cases we wanted to find we actually found. Precision measures how much of what we have found is actually correct.\nOften, we have to make a trade-off between precision and recall. For example, just retrieving every film would give us a recall of 1.0 (after all, we didn’t miss a single positive film). But on the other hand, we retrieved all the negative films as well, so precision will be extremely low. It can depend on the task at hand whether precision or recall is more important. In Section 8.5, we discuss this trade-off in detail, as well as other metrics such as accuracy, \\(F_1\\)-score or the area under the curve (AUC)."
  },
  {
    "objectID": "chapter08.html#sec-nb2dnn",
    "href": "chapter08.html#sec-nb2dnn",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.3 Classical Machine Learning: From Naïve Bayes to Neural Networks",
    "text": "8.3 Classical Machine Learning: From Naïve Bayes to Neural Networks\nTo do supervised machine learning, we can use several models, all of which have different advantages and disadvantages, and are more useful for some use cases than for others. We limit ourselves to the most common ones in this chapter. The website of scikit-learn (www.scikit-learn.org) gives a good overview of more alternatives.\n\n8.3.1 Naïve Bayes\nThe Naïve Bayes classifier is a very simple classifier that is often used as a “baseline”. Before estimating more complicated and resource-intensive models, it is a good idea to estimate a simpler model first, to assess how much better the other model actually is. Sometimes, the simple model might even be just fine.\nThe Naïve Bayes classifier allows you to predict a binary outcome, such as: “Is this message spam or not?”, “Is this article about politics or not?”, “Will this go viral or not?”. It, in fact, also allows you to do the same with more than one category, and both the Python and the R implementation will happily let you train a Naïve Bayes classifier on nominal data, such as whether an article is about politics, sports, the economy, or something different.\nFor the sake of simplicity, we will discuss a binary example, though.\nAs its name suggests, a Naïve Bayes classifier is based on Bayes’ theorem, and it is “naïve”. It may sound a bit weird to call a model “naïve”, but what it actually means is not so much that it is stupid, but that it makes very far-reaching assumptions about the data (hence, it is naïve). Specifically, it assumes that all features are independent from each other. Of course, that is hardly ever the case – for instance, in a survey data set, while age and gender indeed are generally independent from each other, this is not the case for education, political interest, media use, and so on. And in textual data, whether a word \\(W_1\\) is used is not independent from the use of word \\(W_2\\) – after all, both are not randomly drawn from a dictionary, but depend on the topic of the text (and other things). Astonishingly, even though these assumptions are regularly violated, the Naïve Bayes classifier works reasonably well in practice.\nThe Bayes part of the Naïve Bayes classifier comes from the fact that it uses Bayes’ formula, $ P(\\rm{label})$.\nIf we fill this in our formula, we get:\nRemember that all we need to do to calculate this formula is: (1) count how many cases we have in total; (2) count how many cases have our label; (3) count how many cases in (1) have feature \\(x\\); (4) count how many cases in (2) have feature \\(x\\). As you can imagine, doing this does not take much time to do, which is what makes the Naïve Bayes classifier such a fast and efficient choice. This may in particular be true if you have a lot of features (i.e., high-dimensional data).\nCounting whether a feature is present or not, of course, is only possible for binary data. We could for example simply check whether a given word is present in a text or not. But what if our features are continuous data, such as the number of times the word is present? We could dichotomize it, but that would discard information. So, what we do instead, is that we estimate P\\((x_i)\\) using a distribution, for example a Gaussian, Bernoulli, or multinomial distribution. The core idea, though, stays the same.\nOur examples in Section 8.2 illustrate how to train a Naïve Bayes classifier. We first create the labels (whether someone uses online news at all or not), split our data into a training and a test dataset (here, we use 80% for training and 20% for testing) (Example 8.3), then fit (train) a classifier (Example 8.4), before we assess how well it predicts our training data (Example 8.5).\nIn Section 8.5, we discuss in more detail how to evaluate different classifiers, but let’s have a sneak preview at the most used measures of how well our classifier performs. The confusion matrix from Example 8.5 tells us how many users were indeed classified as users (55), and how many (wrongly) as non-users (106).5 That doesn’t look very good; but on the other hand, 212 of the non-users were correctly classified as such, and only 40 were not.\nMore formally, we can express this using precision and recall. When we are interested in finding true users, we get a precision of \\(\\frac{212}{212+106} = 0.67\\) and a recall of \\(\\frac{212}{212+40} = 0.84\\). However, if we want to know how good we are in identifying those who do not use online news, we do – as we saw in the confusion matrix – considerably worse: precision and recall are 0.58 and 0.34, respectively.\n\n\n8.3.2 Logistic Regression\nRegression analysis does not make as strong an assumption about the independence of features as the Naïve Bayes classifier does. Sure, we have been warned about the dangers of multicollinearity in statistics classes, but correlation between features (for which multicollinearity is a fancy term) affects the coefficients and their \\(p\\)-values, but not the predictions of the model as a whole. To put it differently, in regression models, we do not estimate the probability of a label given a feature, independent of all the other features, but are able to “control for” their influence. In theory, this should make our models better, and also in practice, this regularly is the case. However, ultimately, it is an empirical question which model performs best.\nWhile we started this chapter with an example of an OLS regression to estimate a continuous outcome (well, by approximation, as for “days per week” not all values make sense), we will now use a regression approach to predict nominal outcomes, just as in the Naïve Bayes example. The type of regression analysis to use for this is called logistic regression.\nIn a normal OLS regression, we estimate\nBut this gives us a continuous outcome, which we do not want. In a logistic regression, we therefore use the sigmoid function to map this continuous outcome to a value between 0 and 1. The sigmoid function is defined as \\(sigmoid(x) = \\frac{1}{1 + e^{-x}}\\) and depicted in Figure 8.3.\n\n\n\nFigure 8.3: The sigmoid function.\n\n\nCombining these formulas gives us:\nWait, you might say. Isn’t \\(P\\) still continuous, even though it is now bounded between 0 and 1? Yes, it is. Therefore, after having estimated the model, we use a threshold value (typically, 0.5, but we will discuss in Section 8.5.1 how to select different ones) to predict the label. If \\(P>0.5\\), we predict that the case is spam/about politics/will go viral, if not, we predict it’s not. A nice side effect of this is that we still can use the probabilities in case we are interested in them, for example to figure out for which cases we are more confident in our prediction.\nJust as with the Naïve Bayes classifier, also for logistic regression classifiers, Python and R will happily allow us to estimate models with multiple nominal outcomes instead of a binary outcome. In Example 8.6 we fit the logistic regression using the caret method logreg in R and the sklearn (module linear_model) function LogisticRegression in Python.\nAnd, of course, you actually can do OLS regression (or more advanced regression models) if you want to estimate a continuous outcome.\n\n\n\n\n\n\n\nExample 8.6 A simple logistic regression classifier\n\nPython codeR code\n\n\n\nmyclassifier = LogisticRegression(solver=\"lbfgs\")\nmyclassifier.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\ny_pred = myclassifier.predict(X_test)\n\n\n\n\nmyclassifier = train(x = X_train, y = y_train,\n    method = \"glm\",family = \"binomial\")\ny_pred = predict(myclassifier, newdata = X_test)\n\n\n\n\n\n\n\n\n\n\n8.3.3 Support Vector Machines\nSupport Vector Machines (SVM) are another very popular and versatile approach to supervised machine learning. In fact, they are quite similar to logistic regression, but try to optimize a different function. In technical terms, SVM minimizes hinge loss instead of logistic loss.\nWhat does that mean to us? When estimating logistic regressions, we are interested in estimating probabilities, while when training a SVM, we are interested in finding a plane (more specifically, a hyperplane) that best separates the data points of the two classes (e.g., spam versus non-spam messages) that we want to distinguish. This also means that a SVM does not give you probabilities associated with your prediction, but just the label. But usually, that’s all that you want anyway.\nWithout going into mathematical detail here (for that, a good source would be Kelleher et al., 2015), we can say that finding the widest separating margin that we can achieve constructing a plane in a graphical space (SVM) versus optimizing a log-likelihood function (logistic regression) results in a model that is less sensitive to outliers, and tends to be more balanced.\nThere are a lot of graphical visualizations available, for example in the notebooks supplementing VanderPlas (2016) 6. For now, it may suffice to imagine the two-dimensional case: we construct a line that separates two groups of dots with the broadest possible margin. The dots that the margin of this line just touches are called the “support vectors”, hence the name.\nYou could imagine that sometimes we may want to be a bit lenient about the margins. If we have thousands of data points, then maybe it is okay if one or two of these data points are, in fact, within the margin of the separating line (or hyperplane). We can control this with a parameter called \\(C\\): For very high values, this is not allowed, but the lower the value, the “softer” the margin is. In Section 8.5.3, we will show an approach to find the optimal value.\nA big advantage of SVMs is that they can be extended to non-linearly separable classes. Using a so-called kernel function or kernel trick, we can transform our data so that the dataset becomes linearly separable. Choices include but are not limited to multinomial kernels, the radial basis function (RBF), or Gaussian kernels. If we, for example, have two concentric rings of data points (like a donut), then we cannot find a straight line separating them. However, a RBF kernel can transfer them into a linearly separable space. The aforementioned online visualizations can be very instructive here.\nExample 8.7 shows how we implement standard SVM to our data using the caret method svmLinear3 in R and the sklearn (module svm) function SVC in Python. You can see in the code that feature data is standardized or normalized (with \\(m = 0\\) and \\(\\rm{std} = 1\\)) before model training in order to have all the features measured at the same scale, as required by SMV.\n\n\n\n\n\n\n\nExample 8.7 A simple Support Vector Machine classifier\n\nPython codeR code\n\n\n\n# !!! We normalize our features to have M=0 and\n# SD=1. This is necessary as our features are not\n# measured on the same scale, which SVM requires.\n# Alternatively, rescale to [0:1] or [-1:1]\n\nscaler = preprocessing.StandardScaler().fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmyclassifier = SVC(gamma=\"scale\")\nmyclassifier.fit(X_train_scaled, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC()\n\ny_pred = myclassifier.predict(X_test_scaled)\n\n\n\n\n# !!! We normalize our features to have M=0 and \n# SD=1. This is necessary as our features are not \n# measured on the same scale, which SVM requires.\n# Alternatively, rescale to [0:1] or [-1:1]\n\nmyclassifier = train(x = X_train, y = y_train, \n    preProcess = c(\"center\", \"scale\"), \n                     method = \"svmLinear3\")\ny_pred = predict(myclassifier, newdata = X_test)\n\n\n\n\n\n\n\n\n\n\n8.3.4 Decision Trees and Random Forests\nIn the models we have discussed so far, we were essentially modeling linear relationships. If the value of a feature is twice as high, its influence on the outcome will be twice as high as well. Sure, we can (and do, as in the case of the sigmoid function or the SVM kernel trick) apply some transformations, but we have not really considered yet how we can model situations in which, for instance, we care about whether the value of a feature is above (or below) a specific threshold. For instance, if we have a set of social media messages and want to model the medium from which they most likely come, then its length is very important information. If it is longer than 280 characters (or, historically, 140), then we can be very sure it is not from Twitter, even though the reverse is not necessarily true. But it does not matter at all whether it is 290 or \\(10000\\) characters long.\nEntering this variable into a logistic regression, thus, would not be a smart idea. We could, of course, dichotomize it, but that would only partly solve the problem, as its effect can still be overridden by other variables. In this example, we know how to dichotomize it based on our prior knowledge about the number of characters in a tweet, but this does not necessarily need to be the case; it might be something we need to estimate.\nA step-wise decision, in which we first check one feature (the length), before checking another feature, can be modeled as a decision tree. Figure 8.4 depicts a (hypothetical) decision tree with three leaves.\n\n\n\nFigure 8.4: A simple decision tree.\n\n\nFaced with the challenge to predict whether a social media message is a tweet or a Facebook post, we could predict ‘Facebook post’ if its length is greater than 280 characters. If not, we check whether it includes hashtags, and if so, we predict ‘tweet’, otherwise, ‘Facebook post’. Of course, this simplistic model will be wrong at some times, because not all tweets have hashtags, and some Facebook posts actually do include hashtags.\nWhile we constructed this hypothetical decision tree by hand, usually, we are more interested in learning such non-linear relationships from the data. This means that we do not have to determine the cutoff point ourselves, but also that we do not determine the order in which we check multiple variables by hand.\nDecision trees have two nice properties. First, they are very easy to explain. In fact, a figure like Figure 8.4 is understandable for non-experts, which can be important in scenarios where for accountability reasons, the decision of a classifier must be as transparent as possible. Second, they allow us to approximate almost all non-linear relationships (be it not necessarily very accurately).\nHowever, this comes at large costs. Formulating a model as a series of yes/no questions, as you can imagine, inherently loses a lot of nuance. More importantly, in such a tree, you cannot “move up” again. In other words, if you make a wrong decision early on in the tree (i.e., close to its root node), you cannot correct it later. This rigidity makes decision trees also prone to overfitting: they may fit the training data very well, but may not generalize well enough to slightly different (test) data.\nBecause of these drawbacks, decision trees are seldom used in real-life classification tasks. Instead, one uses an ensemble model: so-called random forests. Drawing random samples from the data, we estimate multiple decision trees – hence, a forest. To arrive at a final prediction, we can then let the trees “vote” on which label we should predict. This procedure is called “majority voting”, but there are also other methods available. For example, scikit-learn in Python by default uses a method called probabilistic prediction, which takes into account probability values instead of simple votes.\nIn Example 8.8 we create a random forest classifier with 100 trees using the caret method rf in R and the sklearn (module ensamble) function RandomForestClassifier in Python.\n\n\n\n\n\n\n\nExample 8.8 A simple Random Forest classifier\n\nPython codeR code\n\n\n\nmyclassifier = RandomForestClassifier(n_estimators=100)\nmyclassifier.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\ny_pred = myclassifier.predict(X_test)\n\n\n\n\nmyclassifier = train(x = X_train, y = y_train, \n                     method = \"rf\")\n\nnote: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n\ny_pred = predict(myclassifier, newdata = X_test)\n\n\n\n\n\n\n\n\nBecause random forests alleviate the problems of decision trees, but keep the advantage of being able to model non-linear relationships, they are frequently used when we expect such relationships (or have no idea about what the relationship looks like). Also, random forests may be a good choice if you have very different types of features (some nominal, some continuous, etc.) in your model. The same holds true if you have a lot (really a lot) of features: methods like SVM would require constructing large matrices in memory, which random forests do not. But if the relationships between your features and your labels are actually (approximately) linear, then you are probably better off with one of the other models we discussed.\n\n\n8.3.5 Neural Networks\nInspired by the neurons in the brains of humans (and other animals), neural networks consist of connections between neurons that are activated if the total input is above a certain threshold.\nFigure 8.5 shows the simplest type of neural network, sometimes called a perceptron. This neural network consists only of a series of input neurons (representing the features or independent variables) which are directly connected with the output neuron or neurons (representing the output class(es)). Each of the connections between neurons has a weight, which can be positive or negative. For each output neuron, the weighted sum of inputs is calculated and a function is applied to determine the result. An example output function is the sigmoid function (Figure 8.3) which transforms the output to a value between zero and one, in which case the resulting model is essentially a form of logistic regression.\n\n\n\nFigure 8.5: Schematic representation of a typical classical machine learning model.\n\n\nIf we consider a neural network for sentiment analysis of tweets, the input neurons could be the frequencies of words such as “great” or “terrible”, and we would assume that the weight of the first would be positive while the second would be negative. Such a network cannot take combinations into account, however: the result of “not great” will simply be the addition of the results of “not” and “great”.\nTo overcome this limitation, it is possible to add a hidden layer of latent variables between the input and output layer, such as shown in Figure 8.6. This allows for combinations of neurons, with for example both “not” and “great” loading onto a hidden neuron, which can then override the direct effect of “great”. An algorithm called backpropagation can be used to iteratively approximate the optimal values for the model. This algorithm starts from a random state and optimizes the second layer while keeping the first constant, then optimizing the first layer, and repeating until it converges.\nAlthough such hidden layers, which can easily contain thousands of neurons, are hard to interpret substantively, they can substantially improve the performance of the model. In fact, the Universal Approximation theorem states that every decision function can be approximated to infinite precision with a single (but possibly very large) hidden layer (Goldberg 2017). Of course, since training data is always limited there is a practical limit to how deep or wide the network can be, but this shows the big difference that a hidden layer can make in the range of regularities that can be “captured” in the model.\n\n\n\nFigure 8.6: A neural network."
  },
  {
    "objectID": "chapter08.html#sec-deeplearning",
    "href": "chapter08.html#sec-deeplearning",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.4 Deep Learning",
    "text": "8.4 Deep Learning\nIn Section 8.3.5, we introduced neural networks with hidden layers and the backpropagation algorithm to fit them, both of which date back to at least the 1970’s. In the past decade, however, the Artificial Intelligence community has been transformed by the introduction of deep learning, where deep refers to a large amount of hidden layers between the input and output layers. Many of the recent advances in AI, from self-driving cars to automatic translation and voice assistants, are made possible by the application of deep learning techniques to the enormous amounts of digital data now becoming available.\nAn extensive treatment of deep learning is beyond the scope of this book (we recommend Géron (2019) instead). However, in this section we will give you a brief introduction that should help you understand deep learning at a conceptual level, and in ?sec-chap-dtm and ?sec-chap-image we will explain how these techniques can be applied to text analysis and visual analysis, respectively.\nIn principle, there is no clear demarcation between a “classical” neural network with hidden layers and a “deep” neural network. There are three properties, however, that distinguish deep learning and explain why it is so successful: scale, structure, and feature learning.\nScale. First, and perhaps most importantly, deep learning models are many orders of magnitude larger and more complex than the models trained in earlier decades. This has been made possible by the confluence of unprecedented amounts of digital training data and increased computer processing power. Partly, this has been enabled by the use of graphical processing units (GPUs), hardware designed for rendering the three-dimensional worlds used in games, but that can also be used very efficiently for the computations needed to train neural networks (and mine bitcoins, but that’s another story).\nStructure. Most classical neural networks have only “fully connected” hidden layers with forward propagation, meaning that each neuron in one layer is connected to each neuron in the next layer. In deep learning, many specific architectures (some of which will be discussed below) are used to process information in certain ways, limiting the number of parameters that need to be estimated.\nFeature Learning. In all models described so far with the exception of neural networks with hidden layers, there was a direct relationship between the input features and the output class. This meant that it is important to make sure that the required information the model needs to distinguish the classes is directly encoded in the input features. In the example used earlier, if “not” and “good” are separate features, a single-layer network (or a Naïve Bayes model) cannot learn that these words together have a different meaning than the addition of their separate meanings. However, similar to regression analysis, where you can create an interaction term or squared term to model a non-linear relationship, the researcher can create input features for e.g. word pairs, for example including bigrams (word pairs) such as “not_good”. In fact, engineering the right features was the main way in which a researcher could improve model performance. In deep learning, however, this feature learning step is generally included in the model itself, with subsequent layers encoding different aspects of the raw data.\nThe properties of scale, structure, and feature learning are intertwined in deep learning: the much larger networks enable structures with beautiful names such as “recurrent networks”, “convolutional layers” or “long short-term memory”, which are used to encode specific relationships and dependencies between features. In this book, we will focus on convolutional networks as our only example of deep learning, mostly because these networks are widely used in both text and image analysis. Hopefully, this will give you insight into the general idea behind deep learning, and you can learn about this and other models in more detail in the specialized resources cited above.\n\n8.4.1 Convolutional Neural Networks\nOne challenge in many machine learning problems is a mismatch between the level of measurement of the output and the input. For example, we normally want to assign a single code such as sentiment or topic to a document or image. The raw input, however, is at the word or pixel level. In classical machine learning, this is generally solved by summarizing the input at the higher level of abstraction, for example by using the total frequency of each word per document as input feature. The problem is, however, that this summarization process removes a lot information that could be useful to the machine learning model, for example combinations of words (“not good”) or their ordering (“John voted for Mary” versus “Mary voted for John”), unless the researcher engineers features such as word pairs to add this information.\nConvolutional Neural Networks are one way in which deep learning can overcome this limitation. Essentially, the model internalizes the feature learning as a first part or “layer” of the model, using a specialized network to summarize the raw input values into document (or image) level features.\n\n\n\nFigure 8.7: Simplified example of a Convolutional Network applied to text analysis.\n\n\nFigure 8.7 shows a highly simplified example of this for text analysis of a sentence fragment “Would not recommend”. The left hand side shows how each word is encoded as a binary vector (e.g. 010 for “not”, and 001 for “recommend”). In the second column, a shifting window concatenates these values for word pairs (so 010001 for “not recommend”). Next, a feature map layer detects interesting features in these concatenated values, for example a feature for a negated positive term that has positive weights for negators in the first half and for positive words in the second. These features are then pooled together to create document-level features, for example by taking the maximum value per feature, which means that a feature is present in a document if it is present in any of the word windows in the document. Finally, these document-level features are then used in a regular (dense) neural network which is connected to the output value, e.g. the document sentiment. Since the convolutional layer is now connected with the output class, the feature maps can be automatically learned using the backpropagation algorithm explained above. This means that the model can find the features in the word windows that are most helpful in predicting the document class, bringing the feature learning into the modeling process.\nOf course, this is a highly simplified example, but it shows how local dependencies can be detected automatically using the convolutional network, as long as the interesting features are found within the specified word window. Other architectures, such as the Long Short Term Memory, can also be used to find non-local dependencies, but a full discussion of different architectures is well beyond the scope of this book. ?sec-chap-dtm will give a more detailed example of deep learning for text analysis, where an embedding layer is combined with a convolutional network to build a sentiment analysis model. Similarly, ?sec-chap-image will show how a similar technique can be used to extract features from small areas of images which are then used in automatic image classification. This involves creating a two-dimensional window over pixels rather than a unidimensional window over words, and often multiple convolutional layers are chained to detect features in increasingly large areas of the image. The underlying technique of convolutional networks, however, is the same in both cases."
  },
  {
    "objectID": "chapter08.html#sec-validation",
    "href": "chapter08.html#sec-validation",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.5 Validation and Best Practices",
    "text": "8.5 Validation and Best Practices\n\n8.5.1 Finding a Balance Between Precision and Recall\nIn the previous sections, we have learned how to fit different models: Naïve Bayes, logistic regressions, support vector machines, and random forests. We have also had a first look at confusion matrices, precision, and recall.\nBut how do we find the best model? “Best”, here, should be read as “best for our purposes” – some models may be bad, and some may be good, but which one is really the best may depend on what matters most for us: do we care more about precision or about recall? Are all classes equally important to us? And of course, other factors, such as explainability or computational costs, may factor into our decision.\nBut in any event, we need to decide which metrics to focus on. We can then either manually inspect them and look, for instance, which model has the highest accuracy, or the best balance of precision and recall, or a recall higher than some threshold you are willing to accept.\nIf we build a classifier to distinguish spam messages from legitimate messages, we could ask the following questions: - Precision.Which percentage of what our classifier predicts to be spam really is spam? - Recall.What percentage of all spam messages has our classifier found? - Accuracy.In which percentage of all cases was our classifier right?\nWe furthermore have: - \\(F_1\\)-score.The harmonic mean of precision and recall: \\(F_1 = 2  \\cdot \\frac{\\rm precision \\cdot recall}{\\rm precision + recall}\\) - AUC.The AUC (Area under Curve) is the area under the curve that one gets when plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. A perfect model will receive a value of 1.0, while random guessing between two equally probable classes will result in a value of 0.5 - Micro- and macro-average.Especially when we have more than two classes, we can calculate the average of measures such as precision, recall, or \\(F_1\\)-score. We can do so based on the separately calculated measures (macro), or based on the underlying values (TP, FP, etc.) (micro), which has different implications in the interpretation – especially if the classes have very different sizes.\nSo, which one to choose? If we really do not want to be annoyed by any spam in our inbox, we need a high recall (we want to find all spam messages). If, instead, we want to be sure that we do not accidentally throw away legitimate messages, we need a high precision (we want to be sure that all spam really is spam).\nMaybe you say: well, I want both! You could look at the accuracy, a very straightforward to interpret measure. However, if you get many more legitimate messages than spam (or the other way round), this measure can be misleading: after all, even if your classifier finds almost none of the spam messages (it has a recall close to zero), you still get a very high accuracy, simply because there are so many legitimate messages. In other words, the accuracy is not a good measure when working with highly unbalanced classes. Often, it is therefore a better idea to look at the harmonic mean of precision and recall, the \\(F_1\\)-score, if you want to find a model that gives you a good compromise between precision and recall.\nIn fact, we can even fine-tune our models in such a way that they are geared towards either a better precision or a better recall. As an example, let us take a logistic regression model. It predicts a class label (such as “spam” versus “legitimate”), but it can also return the assigned probabilities. For a specific message, we can thus say that we estimate its probability of being spam as, say, 0.65. Unless we specify otherwise, everything above 0.5 will then be judged to be spam, everything below as legitimate. But we could specify a different cutoff point: we could, for instance, decide to classify everything above 0.7 as spam. This would give us a more conservative spam filter, with probably a higher precision at the expense of a lower recall.\n\n\n\nFigure 8.8: A (pretty good) ROC curve.\n\n\nWe can visualize this with a so-called ROC (receiver operator characteristic), a plot in which we plot true positives against false positives at different thresholds (Figure 8.8). A good model extends until close to the upper left corner, and hence has a large area under the curve (AUC). If we choose a threshold at the left end of the curve, we get few false positives (good!), but also few true positives (bad!), if we go too far to the right, we get the other extreme. So, how can we find the best spot?\nOne approach is to print a table with three columns: the false positive rate, the true positive rate, and the threshold value. You then decide which FPR–TPR combination is most appealing to you, and use the corresponding threshold value. Alternatively, you can find the threshold value with the maximum distance between TPR and FPR, an approach also known as Yoden’s J (Example 8.9). Plotting the ROC curve can also help interpreting which TPR/FPR combination is most promising (i.e., closest to the upper left corner).\n\n\n\n\n\n\n\nExample 8.9 Choosing a different cutoff point for predictions with logistic regression. In this case, we make a trade-off and maximize the difference between false positive rate and true positive rate to improve the precision for the second category at the expense of precision for the first category\n\nPython codeR code\n\n\n\nmyclassifier = LogisticRegression(solver=\"lbfgs\")\nmyclassifier.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\nprint(\"With default cutoff point (.5):\")\n\nWith default cutoff point (.5):\n\ny_pred = myclassifier.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n    non-user       0.58      0.37      0.45       161\n        user       0.67      0.83      0.74       252\n\n    accuracy                           0.65       413\n   macro avg       0.63      0.60      0.60       413\nweighted avg       0.64      0.65      0.63       413\n\nprint(confusion_matrix(y_test, y_pred))\n\n[[ 59 102]\n [ 42 210]]\n\n\n\n\n\nm = glm(usesinternet ~ age + gender + education, \n        data=traindata, family=\"binomial\")\ny_pred = predict(m, newdata = testdata,\n                 type = \"response\")\npred_default = as.factor(ifelse(y_pred>0.5, \n                            \"user\", \"non-user\"))\n\nprint(\"Confusion matrix, default threshold (0.5)\")\n\n[1] \"Confusion matrix, default threshold (0.5)\"\n\nconfmat = table(y_test, pred_default)\nprint(confmat)\n\n          pred_default\ny_test     non-user user\n  non-user       59  102\n  user           51  201\n\nprint(\"Recall for predicting True internet\nusers and non-internet-users:\")\n\n[1] \"Recall for predicting True internet\\nusers and non-internet-users:\"\n\nprint(diag(confmat) / rowSums(confmat))\n\n non-user      user \n0.3664596 0.7976190 \n\nprint(\"Precision for predicting True internet\nusers and non-internet-users:\")\n\n[1] \"Precision for predicting True internet\\nusers and non-internet-users:\"\n\nprint(diag(confmat) / colSums(confmat))\n\n non-user      user \n0.5363636 0.6633663 \n\n\n\n\n\n\nPython codeR code\n\n\n\n# get all predicted probabilities and ROC curve\npredprobs = myclassifier.predict_log_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, predprobs[:, 1], pos_label=\"user\")\n\n# determine the cutoff point\noptimal_threshold = thresholds[np.argmax(tpr - fpr)]\n\nprint(\n    \"With the optimal probability threshold is\"\n    f\"{optimal_threshold}, which is equivalent to\"\n    f\"a cutoff of {np.exp(optimal_threshold)},\"\n    \"we get:\"\n)\n\nWith the optimal probability threshold is-0.38805646013068085, which is equivalent toa cutoff of 0.6783740410958308,we get:\n\ny_pred_alt = np.where(predprobs[:, 1] > optimal_threshold, \"user\", \"non-user\")\nprint(classification_report(y_test, y_pred_alt))\n\n              precision    recall  f1-score   support\n\n    non-user       0.50      0.80      0.61       161\n        user       0.79      0.49      0.61       252\n\n    accuracy                           0.61       413\n   macro avg       0.64      0.64      0.61       413\nweighted avg       0.68      0.61      0.61       413\n\nprint(confusion_matrix(y_test, y_pred_alt))\n\n[[128  33]\n [128 124]]\n\n\n\n\n\nroc_ = roc(testdata$usesinternet ~ y_pred)\nopt = roc_$thresholds[which.max(\n    roc_$sensitivities + roc_$specificities)]\n\nprint(glue(\"Confusion matrix with optimal\",\n           \"threshold ({opt}):\"))\n\nConfusion matrix with optimalthreshold (0.619297246226928):\n\npred_opt = ifelse(y_pred>opt, \"user\", \"non-user\")\nconfmat = table(y_test, pred_opt)\nprint(confmat)\n\n          pred_opt\ny_test     non-user user\n  non-user      113   48\n  user          103  149\n\nprint(\"Recall for predicting True internet\")\n\n[1] \"Recall for predicting True internet\"\n\nprint(\"users and non-internet-users:\")\n\n[1] \"users and non-internet-users:\"\n\nprint(diag(confmat) / rowSums(confmat))\n\n non-user      user \n0.7018634 0.5912698 \n\nprint(\"Precision for predicting True internet\")\n\n[1] \"Precision for predicting True internet\"\n\nprint(\"users and non-internet-users:\")\n\n[1] \"users and non-internet-users:\"\n\nprint(diag(confmat) / colSums(confmat))\n\n non-user      user \n0.5231481 0.7563452 \n\n\n\n\n\nNote: Python and R have slightly different outcomes because the underlying implementation is different, but for this example that may be ignored.\n\n\n\n\n\n\n\n\n\n\n\nExample 8.10 The ROC curve of a (not very impressive) classifier and its area under the curve (AUC)\n\nPython codeR code\n\n\n\nplt.figure(figsize=(5, 5))\nplt.title(\"Receiver Operating Characteristic\")\nplt.plot(fpr, tpr, \"b\", label=f\"AUC = {auc(fpr,tpr):0.2f}\")\nplt.legend(loc=\"lower right\")\nplt.plot([0, 1], [0, 1], \"r--\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel(\"True Positive Rate\")\nplt.show()\n\n\n\n\n\n\n\nroc_ = roc(testdata$usesinternet ~ y_pred, plot=T,\n    print.auc=T, print.thres=\"best\",\n    print.thres.pattern=\"Best threshold: %1.2f\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.5.2 Train, Validate, Test\nBy now, we have established which measures we can use to decide which model to use. For all of them, we have assumed that we split our labeled dataset into two: a training dataset and a test dataset. The logic behind it was simple: if we calculate precision and recall on the training data itself, our assessment would be too optimistic – after all, our models have been trained on exactly these data, so predicting the label isn’t too hard. Assessing the models on a different dataset, the test dataset, instead, gives us an assessment of what precision and recall look like if the labels haven’t been seen earlier – which is exactly what we want to know.\nUnfortunately, if we calculate precision and recall (or any other metric) for multiple models on the same test dataset, and use these results to determine which metric to use, we can run into a problem: we may avoid overfitting of our model on the training data, but we now risk overfitting it on the test data! After all, we could tweak our models until they fit our test data perfectly, even if this makes the predictions for other cases worse.\nOne way to avoid this is to split the original data into three datasets instead of two: a training dataset, a validation dataset, and a test dataset. We train multiple model configurations on the training dataset and calculate the metrics of interest for all of them on the validation dataset. Once we have decided on a final model, we calculate its performance (once) on the test dataset, to get an unbiased estimate of its performance.\n\n\n8.5.3 Cross-validation and Grid Search\nIn an ideal world, we would have a huge labeled dataset and would not need to worry about the decreasing size of our training dataset as we set aside our validation and test datasets.\nUnfortunately, our labeled datasets in the real world have a limited size, and setting aside too many cases can be problematic. Especially if you are already on a tight budget, setting aside not only a test dataset, but also a validation dataset of meaningful size may lead to critically small training datasets. While we have addressed the problem of overfitting, this could lead to underfitting: we may have removed the only examples of some specific feature combination, for instance.\nA common approach to address this issue is \\(k\\)-fold cross-validation. To do this, we split our training data into \\(k\\) partitions, known as folds. We then estimate our model \\(k\\) times, and each time leave one of the folds aside for validation. Hence, every fold is exactly one time the validation dataset, and exactly \\(k-1\\) times part of the training data. We then simply average the results of our \\(k\\) values for the evaluation metric we are interested in.\nIf our classifier generalizes well, we would expect that our metric of interest (e.g., the accuracy, or the \\(F_1\\)-score, …) is very similar in all folds. Example 8.11 performs a cross-validation based on the logistic regression classifier we built above. We see that the standard deviation is really low, indicating that there are almost no changes between the runs, which is great.\nRunning the same cross-validation on our random forest, instead, would produce not only worse (lower) means, but also worse (higher) standard deviations, even though also here, there are no dramatic changes between the runs.\n\n\n\n\n\n\n\nExample 8.11 Crossvalidation\n\nPython codeR code\n\n\n\nmyclassifier = LogisticRegression(solver=\"lbfgs\")\nacc = cross_val_score(\n    estimator=myclassifier, X=X_train, y=y_train, scoring=\"accuracy\", cv=5\n)\nprint(acc)\n\n[0.64652568 0.64048338 0.62727273 0.64242424 0.63636364]\n\nprint(f\"M={acc.mean():.2f}, SD={acc.std():.3f}\")\n\nM=0.64, SD=0.007\n\n\n\n\n\nmyclassifier = train(x = X_train, y = y_train,\n    method = \"glm\", family=\"binomial\",\n    metric=\"Accuracy\", trControl = trainControl(\n     method = \"cv\", number = 5, \n     returnResamp =\"all\", savePredictions=TRUE),)\nprint(myclassifier$resample)\n\n   Accuracy     Kappa parameter Resample\n1 0.6646526 0.2564808      none    Fold1\n2 0.6616314 0.2441998      none    Fold2\n3 0.6606061 0.2057079      none    Fold3\n4 0.6575758 0.2099241      none    Fold4\n5 0.6333333 0.1670491      none    Fold5\n\nprint(myclassifier$results)\n\n  parameter  Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.6555598 0.2166724 0.01267959 0.03525159\n\n\n\n\n\n\n\n\n\nVery often, cross-validation is used when we want to compare many different model specifications, for example to find optimal hyperparameters. Hyperparameters are parameters of the model that are not estimated from the data. These depend on the model, but could for example be the estimation method to use, the number of times a bootstrap should be repeated, etc. Very good examples are the hyperparameters of support vector machines (see above): it is hard to know how soft our margins should be (the \\(C\\)), and we may also be unsure about the right kernel (Example 8.13), or in the case of a polynomial kernel, how many degrees we want to consider.\nUsing the help function (e.g., RandomForestClassifier? in Python), you can look up which hyperparameters you can specify. For a random forest classifier, for instance, this includes the number of estimators in the model, the criterion, and whether or not to use bootstrapping. Example 8.12, Example 8.13, and Example 8.14 illustrate how you can automatically assess which values you should choose.\nNote that in R, not all parameters are “tunable” using standard caret. Therefore, an exact replication of the grid searches in Example 8.12 and Example 8.13 would requires either manual comparisons or writing a so-called caret extension.\n\n\n\n\n\n\n\nExample 8.12 A simple gridsearch in Python ## Python code\n\nf1scorer = make_scorer(f1_score, pos_label=\"user\")\n\n\nmyclassifier = RandomForestClassifier()\n\ngrid = {\n    \"n_estimators\": [10, 50, 100, 200],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"bootstrap\": [True, False],\n}\nsearch = GridSearchCV(\n    estimator=myclassifier, param_grid=grid, scoring=f1scorer, cv=5\n)\nsearch.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=RandomForestClassifier(),\n             param_grid={'bootstrap': [True, False],\n                         'criterion': ['gini', 'entropy'],\n                         'n_estimators': [10, 50, 100, 200]},\n             scoring=make_scorer(f1_score, pos_label=user))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestClassifier(),\n             param_grid={'bootstrap': [True, False],\n                         'criterion': ['gini', 'entropy'],\n                         'n_estimators': [10, 50, 100, 200]},\n             scoring=make_scorer(f1_score, pos_label=user))estimator: RandomForestClassifierRandomForestClassifier()RandomForestClassifierRandomForestClassifier()\n\nprint(search.best_params_)\n\n{'bootstrap': True, 'criterion': 'entropy', 'n_estimators': 100}\n\nprint(classification_report(y_test, search.predict(X_test)))\n\n              precision    recall  f1-score   support\n\n    non-user       0.43      0.38      0.40       161\n        user       0.63      0.68      0.66       252\n\n    accuracy                           0.56       413\n   macro avg       0.53      0.53      0.53       413\nweighted avg       0.55      0.56      0.56       413\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 8.13 A gridsearch in Python using multiple CPUs ## Python code\n\nmyclassifier = SVC(gamma=\"scale\")\n\ngrid = {\"C\": [100, 1e4], \"kernel\": [\"linear\", \"rbf\", \"poly\"], \"degree\": [3, 4]}\n\nsearch = GridSearchCV(\n    estimator=myclassifier,\n    param_grid=grid,\n    scoring=f1scorer,\n    cv=5,\n    n_jobs=-1,  # use all cpus\n    verbose=10,\n)\nsearch.fit(X_train_scaled, y_train)\n\nGridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n             param_grid={'C': [100, 10000.0], 'degree': [3, 4],\n                         'kernel': ['linear', 'rbf', 'poly']},\n             scoring=make_scorer(f1_score, pos_label=user), verbose=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n             param_grid={'C': [100, 10000.0], 'degree': [3, 4],\n                         'kernel': ['linear', 'rbf', 'poly']},\n             scoring=make_scorer(f1_score, pos_label=user), verbose=10)estimator: SVCSVC()SVCSVC()\n\nprint(f\"Hyperparameters {search.best_params_} \" \"give the best performance:\")\n\nHyperparameters {'C': 100, 'degree': 3, 'kernel': 'poly'} give the best performance:\n\nprint(classification_report(y_test, search.predict(X_test_scaled)))\n\n              precision    recall  f1-score   support\n\n    non-user       0.58      0.04      0.08       161\n        user       0.62      0.98      0.76       252\n\n    accuracy                           0.62       413\n   macro avg       0.60      0.51      0.42       413\nweighted avg       0.60      0.62      0.49       413\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 8.14 A gridsearch in R. ## R code\n\n# Create the grid of parameters\ngrid = expand.grid(Loss=c(\"L1\",\"L2\"),\n                   cost=c(100,1000))\n\n# Train the model using our previously defined \n# parameters\ngridsearch = train(x = X_train, y = y_train,\n    preProcess = c(\"center\", \"scale\"), \n    method = \"svmLinear3\", \n    trControl = trainControl(method = \"cv\", \n            number = 5),\n    tuneGrid = grid)\ngridsearch\n\nL2 Regularized Support Vector Machine (dual) with Linear Kernel \n\n1652 samples\n   3 predictor\n   2 classes: 'non-user', 'user' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1322, 1322, 1321, 1321, 1322 \nResampling results across tuning parameters:\n\n  Loss  cost  Accuracy   Kappa    \n  L1     100  0.6458555  0.1994112\n  L1    1000  0.5587091  0.1483755\n  L2     100  0.6525185  0.2102270\n  L2    1000  0.6525185  0.2102270\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were cost = 100 and Loss = L2.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSupervised machine learning is one of the areas where you really see differences between Python and R. While in Python, virtually all you need is available via scikit-learn, in R, we often need to combine caret with various libraries providing the actual models. In contrast, all components we need for machine learning in Python are developed within one package, which leads to less friction. This is what you see in the gridsearch examples in this section. In scikit-learn, any hyperparameter can be part of the grid, but no hyperparameter has to be. Note that in R, in contrast, you cannot (at least, not easily) put any parameter of the model in the grid. Instead, you can look up the “tunable parameters” which must be present as part of the grid in the caret documentation. This means that an exact replication of the grid searches in Example 8.12 and Example 8.13 is not natively supported using caret and requires either manual testing or writing a so-called caret extension.\nWhile in the end, you can find a supervised machine learning solution for all your use cases in R as well, if supervised machine learning is at the core of your project, it may save you a lot of cursing to do this in Python. Hopefully, the package will provide a better solution for machine learning in R in the near future.\n\n\n\n\n\n\n\nBurscher, Björn, Daan Odijk, Rens Vliegenthart, Maarten de Rijke, and Claes H. de Vreese. 2014. “Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis.” Communication Methods and Measures 8 (3): 190–206. https://doi.org/10.1080/19312458.2014.937527.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.\n\n\nGoldberg, Yoav. 2017. Neural Network Models for Natural Language Processing. Morgan & Claypool.\n\n\nRiffe, Daniel, Stephen Lacy, Frederick Fico, and Brendan Watson. 2019. Analyzing Media Messages. Using Quantitative Content Analysis in Research. 4th edition. New York, NY: Routledge.\n\n\nTrilling, Damian. 2013. “Following the news: Patterns of online and offline news consumption.” {PhD} Theses, University of Amsterdam. https://hdl.handle.net/11245/1.394551.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly.\n\n\nVermeer, Susan A. M. 2018. “A supervised machine learning method to classify Dutch-language news items.” https://doi.org/10.6084/m9.figshare.7314896.v1.\n\n\nVermeer, Susan A. M., Theo Araujo, Stefan F. Bernritter, and Guda van Noort. 2019. “Seeing the wood for the trees: How machine learning can help firms in identifying relevant electronic word-of-mouth in social media.” International Journal of Research in Marketing 36 (3): 492–508. https://doi.org/10.1016/j.ijresmar.2019.01.010."
  },
  {
    "objectID": "chapter09.html#sec-unicode",
    "href": "chapter09.html#sec-unicode",
    "title": "9  Processing text",
    "section": "9.1 Text as a String of Characters",
    "text": "9.1 Text as a String of Characters\n\n\n\n\n\n\nImportant: Unicode and Encodings\n\n\n\n\n\nTechnically speaking, text is represented as bytes (numbers) rather than characters. The Unicode standard determines how these bytes should be interpreted or “decoded”. This chapter assumes that the bytes in a file are already “decoded” into characters (or Unicode code points), and we can just work with the characters. Especially if you are not working with English text, it is very important to make sure you understand Unicode and encodings and check that the texts you work with are decoded properly. Please see Section 5.2.2 for more information on how this works.\n\n\n\nWhen we think about text, we might think of sentences or words, but the computer only “thinks” about letters: text is represented internally as a string of characters. This is reflected of course in the type name, with R calling it a character vector and Python a string.\n\n\n\n\n\n\n\nExample 9.1 Internal representation and of single and multiple texts.\n\nPython codeR code\n\n\n\ntext = \"This is text.\"\nprint(f\"type(text): {type(text)}\")\n\ntype(text): <class 'str'>\n\nprint(f\"len(text): {len(text)}\")\n\nlen(text): 13\n\nprint(f\"text[0]: '{text[0]}'\")\n\ntext[0]: 'T'\n\nprint(f\"text[5:7]: '{text[5:7]}'\")\n\ntext[5:7]: 'is'\n\nprint(f\"text[-1]: '{text[-1]}'\")\n\ntext[-1]: '.'\n\nprint(f\"text[-4:]: '{text[-5:]}'\")\n\ntext[-4:]: 'text.'\n\n\n\n\n\ntext = \"This is text.\"\nglue(\"class(text): {class(text)}\")\n\nclass(text): character\n\nglue(\"length(text): {length(text)}\")\n\nlength(text): 1\n\nglue(\"text[1]: {text[1]}\")\n\ntext[1]: This is text.\n\nglue(\"str_length(text): {str_length(text)}\")\n\nstr_length(text): 13\n\nglue(\"str_sub(text, 6,7): {str_sub(text, 6,7)}\")\n\nstr_sub(text, 6,7): is\n\n\n\n\n\n\nPython codeR code\n\n\n\nwords = [\"These\", \"are\", \"words\"]\nprint(f\"type(words): {type(words)}\")\n\ntype(words): <class 'list'>\n\nprint(f\"len(words): {len(words)}\")\n\nlen(words): 3\n\nprint(f\"words[0]: '{words[0]}'\")\n\nwords[0]: 'These'\n\nprint(f\"words[1:3]: '{words[1:3]}'\")\n\nwords[1:3]: '['are', 'words']'\n\n\n\n\n\nwords = c(\"These\", \"are\", \"words\")\nglue(\"class(words): {class(words)}\")\n\nclass(words): character\n\nprint(\"length(words): {length(words)}\")\n\n[1] \"length(words): {length(words)}\"\n\nglue(\"words[1]: {words[1]}\")\n\nwords[1]: These\n\n# Note: use collapse to convert to single value\nwords_2_3 = str_c(words[2:3], collapse=\", \")\nglue(\"words[2:3]: {words_2_3}\")\n\nwords[2:3]: are, words\n\n\n\n\n\n\n\n\n\nAs a simple example, the figure at the top of Example 9.1 shows how the text “This is text.” is represented. This text is split into separate characters, with each character representing a letter (or space, punctuation, emoji, or Chinese character). These characters are indexed starting from the first one, with (as always) R counting from one, but Python counting from zero.\nIn Python, texts are represented as str (string) objects, in which we can directly address the individual characters by their position: text[0] is the first character of text, and so on. In R, however, texts (like all objects) represent columns (or vectors) rather than individual values. Thus, text[1] in R is the first text in a series of text. To access individual characters in a text, you have to use a function such as str_length and str_sub that will be discussed in more detail below. This also means that in Python, if you have a column (or list) of strings that you need to apply an operation to, you either need to use one of /textitPandas’ methods shown below or use a for loop or list comprehension to iterate over all the strings (see also section 3.2).\n\n9.1.1 Methods for Dealing With Text\n\n\n\n\n\n\nStringi, stringr, and base string operations in R\n\n\n\n\n\nAs is so often the case, R has multiple packages that partially replicate functionality for basic text handling. In this book we will mainly use the stringr package, which is part of tidyverse. This is not because that package is necessarily better or easier than the alternative stringi package or the built-in (base) methods. However, the methods are well-documented, clearly named, and consistent with other tidyverse functions, so for now it is easiest to stick to stringr. In particular, stringr is very similar to stringi (and in fact is partially based on it). So, to give one example, the function str_detect is more or less the same as stringi::str_detect and base::grepl.\n\n\n\nThe first thing to keep in mind is that once you load any text in R or Python, you usually store this content as a character or string object (you may also often use lists or dictionaries, but they will have strings inside them), which means that basic operations and conditions of this data type apply, such as indexing or slicing to access individual characters or substrings (see Section 3.1). In fact, base strings operations are very powerful to clean your text and eliminate a large amount of noise. Table 9.1 summarizes some useful operations on strings in R and Python that will help you in this stage.\n\n\n\n\n\nTable 9.1: Useful strings operations in R and Python to clean noise.\n\n\n\n\n\n\n\n\n\nString operation\nR (stringr)\nPython\nPandas\n\n\n\n\n\n\n(whole column)\n(single string)\n(whole column)\n\n\n\nCount characters in s\nstr_length(s)\nlen(s)\ns.str.len()\n\n\n\nExtract a substring\nstr_sub(s, n1, n2)\ns[n1:n2]\ns.str.slice(n1, n2)\n\n\n\nTest if s contains s2\nstr_detect(s, s2)*\ns2 in s\ns.str.match(s2)*\n\n\n\nStrip spaces\ntrimws(s)\ns.strip()\ns.str.strip()\n\n\n\nConvert to lowercase\ntolower(s)\ns.lower()\ns.str.lower()\n\n\n\nConvert to uppercase\ntoupper(s)\ns.upper()\ns.str.upper()\n\n\n\nFind s1 and replace by s2\nstr_replace(s, s1, s2)*\ns.replace(s1, s2)\ns.str.replace(s1, s2)*\n\n\n\n\n\n\nTable notes\n*) The R functions str_detect and str_replace and the Pandas function s.str.match and s.str.replace use regular expressions to define what to find and replace. See Section 9.2 below for more information.\n\nLet us apply some of these functions/methods to a simple Wikipedia text that contains HTML tags, or boilerplate, and upper/lower case letters. Using the stringr function str_replace_all in R and replace in Python we can do a find-and-replace and replace substrings by others (in our case, replace <b> with a space, for instance). To remove unnecessary double spaces we apply the str_squish function provided by stringr and in Python, we first chunk our string into a list of words by using the split string method, before we use the join method to join them again with now a single space. In the case of converting letters from upper to lower case, we use the base R function tolower and the string method lower in Python. Finally, the base R function trimws and the Python string method strip remove the white space from the beginning and end of the string. Example 9.2 shows how to conduct this cleaning process.\nWhile you can get quite far with these techniques, there are more advanced and flexible approaches possible. For instance, you probably do not want to list all possible HTML tags in separate replace methods or str_replace_all functions. In the next section, we therefore show how to use so-called regular expressions to formulate such generalizable patterns.\n\n\n\n\n\n\n\nExample 9.2 Some basic text cleaning approaches\n\nPython codeR code\n\n\n\ntext = \"\"\"   <b>Communication</b>    \n    (from Latin communicare, meaning to share) \"\"\"\n# remove tags:\ncleaned = text.replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n# normalize white space\ncleaned = \" \".join(cleaned.split())\n# lower case\ncleaned = cleaned.lower()\n# trim spaces from start and end\ncleaned = cleaned.strip()\n\nprint(cleaned)\n\ncommunication (from latin communicare, meaning to share)\n\n\n\n\n\ntext = \"    <b>Communication</b>    \n     (from Latin communicare, meaning to share)  \"\ncleaned = text %>% \n  # remove HTML tags:\n  str_replace_all(\"<b>\", \" \")  %>% \n  str_replace_all(\"</b>\", \" \")  %>% \n  # normalize white space \n  str_squish() %>%\n  # lower case\n  tolower()  %>% \n  # trim spaces at start and end\n  trimws()\n\nglue(cleaned)\n\ncommunication (from latin communicare, meaning to share)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo regex or not to regex in Python\n\n\n\n\n\nYou may wonder why we introduce basic string methods like replace or the split-then-join trick, if everything can be done with regular expressions anyway. There are a couple of reasons for still using these methods: first, they are easy and don’t have any dependencies. If you just want to replace a single thing, then you don’t need to import any additional module. Second, regular expressions are considerably slower than string methods – in most cases, you won’t notice, but if you do a lot of replacements (think in thousands per news article, for a million articles), then this may matter. Third, you can use the join trick also for other things like punctuation removal – in this case, by generating a list of all characters in a string called text provided they are no punctuation characters, and then joining them directly to each other: from string import punctuation; \"\".join([c for c in text if c not in punctuation])"
  },
  {
    "objectID": "chapter09.html#sec-regular",
    "href": "chapter09.html#sec-regular",
    "title": "9  Processing text",
    "section": "9.2 Regular Expressions",
    "text": "9.2 Regular Expressions\nA regular expression or regex is a powerful language to locate strings that conform to a given pattern. For instance, we can extract usernames or email-addresses from text, or normalize spelling variations and improve the cleaning methods covered in the previous section. Specifically, regular expressions are a sequence of characters that we can use to design a pattern and then use this pattern to find strings (identify or extract) and also replace those strings by new ones.\nRegular expressions look complicated, and in fact they take time to get used to initially. For example, a relatively simple (and not totally correct) expression to match an email address is [\\w\\.-]+@[\\w\\.-]+\\.\\w\\w+, which doesn’t look like anything at all unless you know what you are looking for. The good news is that regular expression syntax is the same in R and Python (and many other languages), so once you learn regular expressions you will have acquired a powerful and versatile tool for text processing.\nIn the next section, we will first review general expression syntax without reference to running them in Python or R. Subsequently, you will see how you can apply these expressions to inspect and clean texts in both languages.\n\n9.2.1 Regular Expression Syntax\nAt its core, regular expressions are patters for matching sequences of characters. In the simplest case, a regular letter just matches that letter, so the pattern “cat” matches the text “cat”. Next, there are various wildcards, or ways to match different letters. For example, the period (.) matches any character, so c.t matches both “cat” and “cot”. You can place multiple letters between square brackets to create a character class that matches all the specified letters, so c[au]t matches “cat” and “cut”, but not “cot”. There are also a number of pre-defined classes, such as \\w which matches “word characters” (letters, digits, and (curiously) underscores).\nFinally, for each character or group of characters you can specify how often it should occur. For example, a+ means one or more a’s while a? means zero or one a, so lo+l matches lol',lool’, etc., and lo?l matches lol' orll’. This raises the question, of course, of how to look for actual occurrences of a plus, question mark, or period. The solution is to escape these special symbols by placing a backslash (\\) before them: a\\+ matches the literal text “a+”, and \\\\w (with a double backslash) matches the literal text “”.\nNow, we can have another look at the example email address pattern given above. The first part, [\\w\\.-] creates a character class containing word characters, (literal) periods, and dashes. Thus, [\\w\\.-]+@[\\w\\.-]+ means one or more letters, digits, underscores, periods, or dashes, followed by an at sign, followed by one or more letters, digits, etc. Finally, the last part \\.\\w\\w+ means a literal period, a word character, and one or more word characters. In other words, we are looking for a name (possibly containing dashes or periods) before the at sign, followed by a domain, followed by a top level domain (like .com) of at least two characters.\nIn essence, thinking in terms of what you want to match and how often you want to match it is all there is to regular expressions. However, it will take some practice to get comfortable with turning something sensible (such as an email address) into a correct regular expression pattern. The next subsection will explain regular expression syntax in more detail, followed by an explanation of grouping, and in the final subsection we will see how to use these regular expressions in R and Python to do text cleaning.\n\n\n\n\n\nTable 9.2: Regular expression syntax\n\n\n\n\n\n\n\n\n\nFunction\nSyntax\nExample\nMatches\n\n\n\n\n\nSpecifier: What to match\n\n\n\n\n\n\nAll characters except for new lines\n.\nd.g\ndig,d!g\n\n\n\nWord characters*(letters, digits,_)\n\\w\nd\\wg\ndig,dog\n\n\n\nDigits*(0 to 9)\n\\d\n202\\d\n2020,2021\n\n\n\nWhitespace*(space, tab, newline)\n\\s\n\n\n\n\n\nNewline\n\\n\n\n\n\n\n\nBeginning of the string\n\\^\n\\^go\ngogo go\n\n\n\nEnding of the string\n\\$\ngo\\$\ngo gogo\n\n\n\nBeginning or end of word\n\\b\n\\bword\\b\na word!\n\n\n\nEither first or second option\n…|…\ncat|dog\ncat,dog\n\n\n\nQuantifier: How many to match\n\n\n\n\n\n\nZero or more\n*\nd.*g\ndg,drag,d = g\n\n\n\nZero or more (non-greedy)\n*?\nd.*?g\ndogg\n\n\n\nOne or more\n+\n\\d+%\n1%,200%\n\n\n\nOne or more (non-greedy)\n+?\n\\d+%\n200%\n\n\n\nZero or one\n?\ncolou?r\ncolor,colour\n\n\n\nExactly n times\n{n}\n\\d{4}\n1940,2020\n\n\n\nAt least n times\n{n,}\n\n\n\n\n\nBetween n and m times\n{n,m}\n\n\n\n\n\nOther constructs\n\n\n\n\n\n\nGroups\n(…)\n'(bla )+'\n'bla bla bla'\n\n\n\nSelection of characters\n[…]\nd[iuo]g\ndig,dug,dog\n\n\n\nRange of characters in selection\n[a-z]\n\n\n\n\n\nEverything except selection\n[\\^...]\n\n\n\n\n\nEscape special character\n\\\n3\\.14\n3.14\n\n\n\nUnicode character properties†\n\n\n\n\n\n\nLetters*\n\\p{LETTER}\n\nwords,単語\n\n\n\nPunctuation*\n\\p{PUNCTUATION}\n\n. , :\n\n\n\nQuotation marks*\n\\p{QUOTATION MARK}\n\n' ` \" «\n\n\n\nEmoji*\n\\p{EMOJI}\n\n😊\n\n\n\nSpecific scripts, e.g. Hangul*\n\\p{HANG}\n\n한글\n\n\n\n\n\n\nTable notes\n*) These selectors can be inverted by changing them into capital letters. Thus, \\W matches everything except word characters, and \\P\\{PUNCTUATION\\} matches everything except punctuation.\n†) See www.unicode.org/reports/tr44/#Property_Index for a full list of Unicode properties. Note that when using Python, these are only available if you use regex, which is a drop-in replacement for the more common re.\n\nIn Table 9.2 you will find an overview of the most important parts of regular expression syntax.1 The first part shows a number of common specifiers for determining what to match, e.g. letters, digits, etc., followed by the quantifiers available to determine how often something should be matched. These quantifiers always follow a specifier, i.e. you first say what you’re looking for, and then how many of those you need. Note that by default quantifiers are greedy, meaning they match as many characters as possible. For example, <.*> will match everything between angle brackets, but if you have something like <p>a paragraph</p> it will happily match everything from the first opening bracket to the last closing bracket. By appending a question mark (?) to the quantifier, it becomes non-greedy. so, <.*?> will match the individual <p> and </p> substrings.\nThe third section discusses other constructs. Groups are formed using parentheses () and are useful in at least three ways. First, by default a quantifier applies to the letter directly before it, so no+ matches “no”, “nooo”, etc. If you group a number of characters you can apply a quantifier to the group. So, that's( not)? good matches either “that’s not good” or “that’s good”. Second, when using a vertical bar (|) to have multiple options, you very often want to put them into a group so you can use it as part of a larger pattern. For example, a( great| fantastic)? victory matches either “a victory”, “a great victory”, or “a fantastic victory”. Third, as will be discussed below in Section 9.3, you can use groups to capture (extract) a specific part of a string, e.g. to get only the domain part of a web address.\nThe other important construct are character classes, formed using square brackets []. Within a character class, you can specify a number of different characters that you want to match, using a dash (-) to indicate a range. You can add as many characters as you want: [A-F0-9] matches digits and capital letters A through F. You can also invert this selection using an initial caret: [^a-z] matches everything except for lowercase Latin letters. Finally, you sometimes need to match a control character (e.g. +, ?, \\). Since those characters have a special meaning within a regular expressing, they cannot be used directly. The solution is to add a backslash (\\) behind them to escape them: . matches any character, but \\. matches an actual period. \\\\ matches an actual backslash.\n\n\n\n9.2.2 Example Patterns\nUsing the syntax explained in the previous section, we can now make patterns for common tasks in cleaning and analyzing text. Table 9.3 lists a number of regular expressions for common tasks such as finding dates or stripping HTML artifacts.\n\n\nTable 9.3: Regular expression syntax in Python and R\n\n\nGoal\nPattern\nExample\n\n\n\n\nUS Zip Code\n\\d{5}\n90210\n\n\nUS Phone number\n(\\d{3}-)?\\d{3}-\\d{4}\n202-456-1111,456-1111\n\n\nDutch Postcode\n\\d{4} ?[A-Za-z]{2}\n1015 GK\n\n\nISO Date\n\\d{4}-\\d{2}-\\d{2}\n2020-07-20\n\n\nGerman Date\n\\d{1,2}\\.\\d{1,2}\\.\\d{4}\n25.6.1988\n\n\nInternational phone number\n\\+(\\d[-]?){7,}\\d\n+1 555-1234567\n\n\nURL\nhttps?://\\S+\nhttps://example.com?a=b\n\n\nE-mail address\n[\\w\\.-]+@[\\w\\.-]+\\.\\w+\nme@example.com\n\n\nHTML tags\n</?\\w[^>]*>\n</html>\n\n\nHTML Character escapes\n&[^;]+;\n&nbsp;\n\n\n\n\nPlease note that most of these patterns do not correctly distinguish all edge cases (and hence may lead to false negatives and/or false positives) and are provided for educational purposes only.\nWe start with a number of relatively simple patterns for Zip codes and phone numbers. Starting with the simplest example, US Zip codes are simply five consecutive numbers. Next, a US phone number can be written down as three groups of numbers separated by parentheses, where the first group is made optional for local phone numbers using parentheses to group these numbers so the question mark applies to the whole group. Next, Dutch postal codes are simply four numbers followed by two letters, and we allow an optional space in between. Similarly simple, dates in ISO format are three groups of numbers separated by dashes. German dates follow a different order, use periods as separator, and allow for single-digit day and month numbers. Note that these patterns do not check for the validity of dates. A simple addition would be to restrict months to 01-12, e.g. using (0[1-9]|1[0-2]). However, in general validation is better left to specialized libraries, as properly validating the day number would require taking the month (and leap years) into account.\nA slightly more complicated pattern is the one given for international phone numbers. They always start with a plus sign and contain at least eight numbers, but can contain dashes and spaces depending on the country. So, after the literal + (which we need to escape since + is a control character), we look for seven or more numbers, optionally followed by a single dash or space, and end with a single number. This allows dashes and spaces at any position except the start and end, but does not allow for e.g. double dashes. It also makes sure that there are at least eight numbers regardless of how many dashes or spaces there are.\nThe final four examples are patterns for common notations found online. For URLs, we look for http:// or https:// and take everything until the next space or end of the string. For email addresses, we define a character class for letters, periods, or dashes and look for it before and after the at sign. Then, there needs to be at least one period and a top level domain containing only letters. Note that the dash within the character class does not need to be escaped because it is the final character in the class, so it cannot form a range. For HTML tags and character escapes, we anchor the start (< and &) and end (> and ;) and allow any characters except for the ending character in between using an inverted character class.\nNote that these example patterns would also match if the text is enclosed in a larger text. For example, the zip code pattern would happily match the first five numbers of a 10-digit number. If you want to check that an input value is a valid zip code (or email address, etc.), you probably want to check that it only contains that code by surrounding it with start-of-text and end-of-text markers: ^\\d{5}$. If you want to extract e.g. zip codes from a longer document, it is often useful to surround them with word boundary markers: \\b\\d{5}\\b.\nPlease note that many of those patterns are not necessarily fully complete and correct, especially the final patterns for online notations. For example, email addresses can contain plus signs in the first part, but not in the domain name, while domain names are not allowed to start with a dash – a completely correct regular expression to match email addresses is over 400 characters long! Even worse, complete HTML tags are probably not even possible to describe using regular expressions, because HTML tags frequently contain comments and nested escapes within attributes. For a better way to deal with analyzing HTML, please see Chapter ?sec-chap-scraping. In the end, patterns like these are fine for a (somewhat) noisy analysis of (often also somewhat noisy) source texts as long as you understand the limitations."
  },
  {
    "objectID": "chapter09.html#sec-regextract",
    "href": "chapter09.html#sec-regextract",
    "title": "9  Processing text",
    "section": "9.3 Using Regular Expressions in Python and R",
    "text": "9.3 Using Regular Expressions in Python and R\nNow that you hopefully have a firm grasp of the syntax of regular expressions, it is relatively easy to use these patterns in Python or R (or most other languages). Table 9.4 lists the commands for four of the most common use cases: identifying matching texts, removing and replacing all matching text, extracting matched groups, and splitting texts.\n\n\nTable 9.4: Regular expression syntax\n\n\n\n\n\n\n\n\nOperation\nR (stringr)\nPython\nPandas\n\n\n\n\n\n(whole column)\n(single string)\n(whole column)\n\n\nDoes pattern p occur in text t?\nstr_detect(t, p)\nre.search(p, t)\nt.str.contains(p)\n\n\nDoes text t start with pattern p?\nstr_detect(t, \"\\^p\")\nre.match(p, t)\nt.str.match(p)\n\n\nCount occurrences of p in t\nstr_count(t, \"\\^p\")\nre.match(p, t)\nt.str.count(p)\n\n\nRemove all occurences of p in t\nstr_remove_all(t, p)\nre.sub(p, \"\", t)\nt.str.replace(p, \"\")\n\n\nReplace p by r in text t\nstr_replace_all(t, p, r)\nre.sub(p, r, t)\nt.str.replace(p, r)\n\n\nExtract the first match of p in t\nstr_extract(t, p)\nre.search(p, t).group(1)\nt.str.extract(p)\n\n\nExtract all matches of p in t\nstr_extract_all(t, p)\nre.findall(p, t)\nt.str.extractall(p)\n\n\nSplit t on matches of p\nstr_split(t, p)\nre.split(p, t)\nt.str.split(p)\n\n\n\n\nNote: if using Unicode character properties (\\p), use the same functions in package regex instead of re\nFor R, we again use the functions from the stringr package. For Python, you can use either the re or regex package, which both support the same functions and syntax so you can just import one or the other. The re package is more common and significantly faster, but does not support Unicode character properties (\\p). We also list the corresponding commands for pandas, which are run on a whole column instead of a single text (but note that pandas does not support Unicode character properties.)\nFinally, a small but important note about escaping special characters by placing a backslash (\\) before them. The regular expression patterns are used within another language (in this case, Python or R), but these languages have their own special characters which are also escaped. In Python, you can create a raw string by putting a single r before the opening quotation mark: r\"\\d+\" creates the regular expression pattern \\d. From version 4.0 (released in spring 2020), R has a similar construct: r\"(\\d+)\". In R, the parentheses are part of the string delimiters, but you can use more parentheses within the string without a problem. The only thing you cannot include in a string is the closing sequence )\", but as you are also allowed to use square or curly brackets instead of parentheses and single instead of double quotes to delimit the raw string you can generally avoid this problem: to create the pattern \"(cat|dog)\" (i.e. cat or dog enclosed in quotation marks), you can use r\"{\"(cat|dog)\"}\" or r'(\"(cat|dog)\")' (or even more legible: r'{\"(cat|dog)\"}').\nUnfortunately, in earlier versions of R (and in any case if you don’t use raw strings), you need to escape special characters twice: first for the regular expression, and then for R. So, the pattern \\d becomes \"\\\\d\". To match a literal backslash you would use the pattern \\\\, which would then be represented in R as \"\\\\\\\\\"!\nExample 9.3 cleans the same text as Example 9.2 above, this time using regular expressions. First, it uses <[^>+]> to match all HTML tags: an angular opening bracket, followed by anything except for a closing angular bracket ([^>]), repeated one or more times (+), finally followed by a closing bracket. Next, it replaces one or more whitespace characters (\\s+) by a single space. Finally, it uses a vertical bar to select either space at the start of the string (^\\s+), or at the end (\\s+$), and removes it. As you can see, you can express a lot of patterns using regular expressions in this way, making for more generic (but sometimes less readable) clean-up code.\n\n\n\n\n\n\n\nExample 9.3 Using regular expressions to clean a text\n\nPython codeR code\n\n\n\ntext = \"\"\"   <b>Communication</b>    \n    (from Latin communicare, meaning to share) \"\"\"\n# remove tags:\ncleaned = re.sub(\"<[^>]+>\", \"\", text)\n# normalize white space\ncleaned = re.sub(\"\\s+\", \" \", cleaned)\n# trim spaces from start and end\ncleaned = re.sub(\"^\\s+|\\s+$\", \"\", cleaned)\ncleaned = cleaned.strip()\n\nprint(cleaned)\n\nCommunication (from Latin communicare, meaning to share)\n\n\n\n\n\ntext = \"    <b>Communication</b>    \n     (from Latin communicare, meaning to share)  \"\ncleaned = text %>% \n  # remove HTML tags:\n  str_replace_all(\"<[^>]+>\", \" \")  %>% \n  # normalize white space \n  str_replace_all(\"\\\\p{space}+\", \" \")  %>% \n  # trim spaces at start and end\n  str_remove_all(\"^\\\\s+|\\\\s+$\")\n\ncleaned\n\n[1] \"Communication (from Latin communicare, meaning to share)\"\n\n\n\n\n\n\n\n\n\nFinally, Example 9.4 shows how you can run the various commands on a whole column of text rather than on individual strings, using a small set of made-up tweets to showcase various operations. First, we determine whether a pattern occurs, in this case for detecting hashtags. This is very useful for e.g. subsetting a data frame to only rows that contain this pattern. Next, we count how many at-mentions are contained in the text, where we require that the character before the mention needs to be either whitespace or the start of the string (^), to exclude email addresses and other non-mentions that do contain at signs. Then, we extract the (first) url found in the text, if any, using the pattern discussed above. Finally, we extract the plain text of the tweet in two chained operations: first, we remove every word starting with an at-sign, hash, or http, removing everything up to the next whitespace character. Then, we replace everything that is not a letter by a single space.\n\n\n\n\n\n\n\nExample 9.4 Using regular expressions on a data frame\n\nPython codeR code\n\n\n\n\n\n\nurl = \"https://cssbook.net/d/example_tweets.csv\"\ntweets = pd.read_csv(url, index_col=\"id\")\n# identify tweets with hashtags\ntweets[\"tag\"] = tweets.text.str.contains(r\"#\\w+\")\n# How many at-mentions are there?\ntweets[\"at\"] = tweets.text.str.count(r\"(^|\\s)@\\w+\")\n# Extract first url\ntweets[\"url\"] = tweets.text.str.extract(r\"(https?://\\S+)\")\n# Remove urls, tags, and @-mentions\nexpr = r\"(^|\\s)(@|#|https?://)\\S+\"\ntweets[\"plain2\"] = tweets.text.str.replace(expr, \" \", regex=True).replace(\n    r\"\\W+\", \" \"\n)\ntweets\n\n                                       text  ...                   plain2\nid                                           ...                         \n1   RT: @john_doe https://example.com/ne...  ...  RT:   very interesting!\n2                      tweet with just text  ...     tweet with just text\n3   http://example.com/pandas #breaking ...  ...                         \n4               @me and @myself #selfietime  ...                    and  \n\n[4 rows x 5 columns]\n\n\n\n\n\nlibrary(tidyverse)\nurl=\"https://cssbook.net/d/example_tweets.csv\"\ntweets = read_csv(url)\ntweets = tweets %>% mutate(\n    # identify tweets with hashtags\n    has_tag=str_detect(text, \"#\\\\w+\"),\n    # How many at-mentions are there?\n    n_at = str_count(text, \"(^|\\\\s)@\\\\w+\"),\n    # Extract first url\n    url = str_extract(text, \"(https?://\\\\S+)\"),\n    # Remove at-mentions, tags, and urls\n    plain2 = str_replace_all(text, \n       \"(^|\\\\s)(@|#|https?://)\\\\S+\", \" \") %>% \n             str_replace_all(\"\\\\W+\", \" \")\n    )\ntweets\n\n# A tibble: 4 × 6\n     id text                                          has_tag  n_at url   plain2\n  <dbl> <chr>                                         <lgl>   <int> <chr> <chr> \n1     1 RT: @john_doe https://example.com/news very … FALSE       1 http… \"RT v…\n2     2 tweet with just text                          FALSE       0 <NA>  \"twee…\n3     3 http://example.com/pandas #breaking #mustread TRUE        0 http… \" \"   \n4     4 @me and @myself #selfietime                   TRUE        2 <NA>  \" and…\n\n\n\n\n\n\n\n\n\n\n9.3.1 Splitting and Joining Strings, and Extracting Multiple Matches\nSo far, the operations we used all took a single string object and returned a single value, either a cleaned version of the string or e.g. a boolean indicating whether there is a match. This is convenient when using data frames, as you can transform a single column into another column. There are three common operations, however, that complicate matters: you can split a string into multiple substrings, or extract multiple matches from a string, and you can join multiple matches together.\n\n\n\n\n\n\n\nExample 9.5 Splitting extracting and joining a single text\n\nPython codeR code\n\n\n\ntext = \"apples, pears, oranges\"\n# Three ways to achieve the same thing:\nitems = text.split(\", \")\nitems = regex.split(r\"\\p{PUNCTUATION}\\s*\", text)\nitems = regex.findall(r\"\\p{LETTER}+\", text)\nprint(f\"Split text into items: {items}\")\n\nSplit text into items: ['apples', 'pears', 'oranges']\n\njoined = \" & \".join(items)\nprint(joined)\n\napples & pears & oranges\n\n\n\n\n\ntext = \"apples, pears, oranges\"\nitems=strsplit(text,\", \", fixed=T)[[1]]\nitems=str_split(text,\"\\\\p{PUNCTUATION}\\\\s*\")[[1]]\nitems=str_extract_all(text,\"\\\\p{LETTER}+\")[[1]]\nprint(items)\n\n[1] \"apples\"  \"pears\"   \"oranges\"\n\njoined = str_c(items, collapse=\" & \")\nprint(joined)\n\n[1] \"apples & pears & oranges\"\n\n\n\n\n\n\n\n\n\nExample 9.5 shows the “easier” case of splitting up a single text and joining the result back together. We show three different ways to split: using a fixed pattern to split on (in this case, a comma plus space); using a regular expression (in this case, any punctuation followed by any space); and by matching the items we are interested in (letters) rather than the separator. Finally, we join these items together again using join (Python) and str_c (R).\nOne thing to note in the previous example is the use of the index [[1]] in R to select the first element in a list. This is needed because in R, splitting a text actually splits all the given texts, returning a list containing all the matches for each input text. If there is only a single input text, it still returns a list, so we select the first element of the list.\nIn many cases, however, you are not working on a single text but rather on a series of texts loaded into a data frame, from tweets to news articles and open survey questions. In the example above, we extracted only the first url from each tweet. If we want to extract e.g. all hash tags from each tweet, we cannot simply add a “tags” column, as there can be multiple tags in each tweet. Essentially, the problem is that the URLs per tweet are now nested in each row, creating a non-rectangular data structure.\nAlthough there are multiple ways of dealing with this, if you are working with data frames our advice is to normalize the data structure to a long format. In the example, that would mean that each tweet is now represented by multiple rows, namely one for each hash tag. Example 9.6 shows how this can be achieved in both R and Pandas. One thing to note is that in pandas, t.str.extractall automatically returns the desired long format, but it is essential that the index of the data frame actually contains the identifier (in this case, the tweet (status) id). t.str.split, however, returns a data frame with a column containing lists, similar to how both R functions return a list containing character vectors. We can normalize this to a long data frame using t.explode (pandas) and pivot_longer (R). After this, we can use all regular data frame operations, for example to join and summarize the data.\nA final thing to note is that while you normally use a function like mean to summarize the values in a group, you can also join strings together as a summarization. The only requirement for a summarization function is that it returns a single value for a group of values, which of course is exactly what joining a multiple string together does. This is shown in the final line of the example, where we split a tweet into words and then reconstruct the tweet from the individual words.\n\n\n\n\n\n\n\nExample 9.6 Applying split and extract _ all on text columns\n\nPython codeR code\n\n\n\ntags = tweets.text.str.extractall(\"(#\\\\w+)\")\ntags.merge(tweets, left_on=\"id\", right_on=\"id\")\n\n              0  ...   plain2\nid               ...         \n3     #breaking  ...         \n3     #mustread  ...         \n4   #selfietime  ...    and  \n\n[3 rows x 6 columns]\n\n\n\n\n\ntags = tweets %>% mutate(\n    tag=str_extract_all(tweets$text,\"(#\\\\w+)\"))%>%\n  select(id, tag)\ntags_long = tags  %>% unnest(tag)\nleft_join(tags_long, tweets)\n\n# A tibble: 3 × 7\n     id tag         text                              has_tag  n_at url   plain2\n  <dbl> <chr>       <chr>                             <lgl>   <int> <chr> <chr> \n1     3 #breaking   http://example.com/pandas #break… TRUE        0 http… \" \"   \n2     3 #mustread   http://example.com/pandas #break… TRUE        0 http… \" \"   \n3     4 #selfietime @me and @myself #selfietime       TRUE        2 <NA>  \" and…\n\n\n\n\n\n\nPython codeR code\n\n\n\nwords = tweets.text.str.split(\"\\\\W+\")\nwords_long = words.explode()\n\n\n\n\nwords = tweets %>% mutate(\n    word=str_split(tweets$text, \"\\\\W+\")) %>% \n  select(id, word)\nwords_long = words %>% unnest(word)\nhead(words_long)\n\n# A tibble: 6 × 2\n     id word    \n  <dbl> <chr>   \n1     1 RT      \n2     1 john_doe\n3     1 https   \n4     1 example \n5     1 com     \n6     1 news    \n\n\n\n\n\n\nPython codeR code\n\n\n\nwords_long.groupby(\"id\").agg(\"_\".join)\n\nid\n1    RT_john_doe_https_example_com_news_v...\n2                       tweet_with_just_text\n3    http_example_com_pandas_breaking_mus...\n4                  _me_and_myself_selfietime\nName: text, dtype: object\n\n\n\n\n\nwords_long %>% \n  group_by(id) %>% \n  summarize(joined=str_c(word, collapse=\"_\"))\n\n# A tibble: 4 × 2\n     id joined                                              \n  <dbl> <chr>                                               \n1     1 RT_john_doe_https_example_com_news_very_interesting_\n2     2 tweet_with_just_text                                \n3     3 http_example_com_pandas_breaking_mustread           \n4     4 _me_and_myself_selfietime"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Boumans, Jelle W., and Damian Trilling. 2016. “Taking stock of the toolkit: An overview of relevant\nautmated content analysis approaches and techniques for digital\njournalism scholars.” Digital Journalism 4 (1):\n8–23. https://doi.org/10.1080/21670811.2015.1096598.\n\n\nBreiman, Leo. 2001. “Statistical modeling:\nThe two cultures.” Statistical Science 16 (3):\n199–215. https://doi.org/10.1214/ss/1009213726.\n\n\nBryman, Alan. 2012. Social Research Methods. 4th edition. New\nYork, NY: Oxford University Press.\n\n\nBurscher, Björn, Daan Odijk, Rens Vliegenthart, Maarten de Rijke, and\nClaes H. de Vreese. 2014. “Teaching the\ncomputer to code frames in news: Comparing two supervised machine\nlearning approaches to frame analysis.” Communication\nMethods and Measures 8 (3): 190–206. https://doi.org/10.1080/19312458.2014.937527.\n\n\nCairo, Alberto. 2019. How Charts Lie. WW Norton & Company.\n\n\nChen, Lizi. 2017. “News-Processed-Dataset.” https://doi.org/10.6084/m9.figshare.5296357.v1.\n\n\nCioffi-Revilla, Claudio. 2014. Introduction to Computational Social\nScience: Principles and Applications. London, UK: Springer.\n\n\nCrawley, Michael J. 2012. The r Book. 2nd Edition. Wiley.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow: Concepts, Tools, and Techniques to Build\nIntelligent Systems. O’Reilly Media.\n\n\nGoldberg, Yoav. 2017. Neural Network Models for Natural Language\nProcessing. Morgan & Claypool.\n\n\nGonzález-Bailón, Sandra. 2017. Decoding the\nsocial world: Data science and the unintended consequences of\ncommunication. Cambridge, MA: MIT.\n\n\nGrimmer, J., and B. M. Stewart. 2013. “Text as Data:\nThe Promise and Pitfalls of Automatic Content Analysis\nMethods for Political Texts.” Political Analysis 21 (3):\n267–97. https://doi.org/10.1093/pan/mps028.\n\n\nHorne, Benjamin D., William Dron, Sara Khedr, and Sibel Adali. 2018.\n“Sampling the News Producers: A Large News\nand Feature Data Set for the Study of the Complex Media\nLandscape.” In 12th International AAAI Conference on\nWeb and Social Media (ICWSM), 518–27. Icwsm. http://arxiv.org/abs/1803.10124.\n\n\nKirk, Andy. 2016. Data Visualisation: A Handbook for Data Driven\nDesign. London, UK: SAGE.\n\n\nKitchin, Rob. 2014a. “Big Data, new\nepistemologies and paradigm shifts.” Big Data\n& Society 1 (1): 1–12. https://doi.org/10.1177/2053951714528481.\n\n\n———. 2014b. The Data Revolution: Big Data, Open Data, Data\nInfrastructures and Their Consequences. Sage.\n\n\nMargolin, Drew B. 2019. “Computational Contributions: A Symbiotic\nApproach to Integrating Big, Observational Data Studies into the\nCommunication Field.” Communication Methods and Measures\n13 (4): 229–47.\n\n\nMayer-Schönberger, Viktor, and Kenneth Cukier. 2013. Big Data: A\nRevolution That Will Transform How We Live, Work, and Think. New\nYork, NY: Houghton Mifflin Harcourt.\n\n\nPiketty, Thomas. 2017. Capital in the Twenty-First Century.\nCambridge, MA: Harvard University Press.\n\n\nRieder, Bernhard. 2017. “Scrutinizing an Algorithmic Technique:\nThe Bayes Classifier as Interested Reading of\nReality.” Information Communication and Society 20 (1):\n100–117. https://doi.org/10.1080/1369118X.2016.1181195.\n\n\nRiffe, Daniel, Stephen Lacy, Frederick Fico, and Brendan Watson. 2019.\nAnalyzing Media Messages. Using Quantitative Content\nAnalysis in Research. 4th edition. New York, NY: Routledge.\n\n\nSalganik, Matthew. 2019. Bit by Bit: Social Research in the Digital\nAge. Princeton University Press.\n\n\nScharkow, Michael. 2017. “Content Analysis,\nAutomatic.” In The International Encyclopedia of\nCommunication Research Methods, edited by Jörg Matthes, Christine\nS. Davis, and Robert F. Potter, 1–14. Hoboken, NJ: Wiley. https://doi.org/10.1002/9781118901731.iecrm0043.\n\n\nTrilling, Damian. 2013. “Following the news:\nPatterns of online and offline news consumption.” {PhD}\nTheses, University of Amsterdam. https://hdl.handle.net/11245/1.394551.\n\n\n———. 2017. “Big Data, Analysis\nof.” In The International Encyclopedia of\nCommunication Research Methods, 1–20. Hoboken, NJ, USA: John Wiley\n& Sons, Inc. https://doi.org/10.1002/9781118901731.iecrm0014.\n\n\nTufte, Edward R. 2006. Beautiful Evidence. Vol. 1. Graphics\nPress Cheshire, CT.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. Vol. 2.\nReading, Mass.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential\nTools for Working with Data. O’Reilly.\n\n\nVermeer, Susan A. M. 2018. “A supervised\nmachine learning method to classify Dutch-language news\nitems.” https://doi.org/10.6084/m9.figshare.7314896.v1.\n\n\nVermeer, Susan A. M., Theo Araujo, Stefan F. Bernritter, and Guda van\nNoort. 2019. “Seeing the wood for the trees:\nHow machine learning can help firms in identifying relevant electronic\nword-of-mouth in social media.” International Journal\nof Research in Marketing 36 (3): 492–508. https://doi.org/10.1016/j.ijresmar.2019.01.010.\n\n\nWaldherr, Annie. 2014. “Emergence of News\nWaves: A Social Simulation Approach.” Journal of\nCommunication 64 (5): 852–73. https://doi.org/10.1111/jcom.12117.\n\n\nWettstein, Martin. 2020. “Simulating hidden\ndynamics : Introducing Agent-Based Models as a tool for linkage\nanalysis.” Computational Communication Research 2\n(1): 1–33. https://doi.org/10.5117/CCR2020.1.001.WETT."
  }
]