[
  {
    "objectID": "index.html#acknowledgements-and-contributing",
    "href": "index.html#acknowledgements-and-contributing",
    "title": "Computational Analysis of Communication",
    "section": "Acknowledgements and contributing",
    "text": "Acknowledgements and contributing\nWe would like to thank colleagues, friends, and students who provided feedback and input on earlier versions of parts of the manuscript: Dmitry Bogdanov, Andreu Casas, Modesto Escobar, Anne Kroon, Nicolas Mattis , Cecil Meeusen, Jesús Sánchez-Oro, Nel Ruigrok, Susan Vermeer, Mehdi Zamani, Marthe Möller. Of course, we also want to thank all others that we might have forgotten to mention here (sorry!) – please contact us if you feel that your name should be here.\nOur intention is for the online version to be a ‘living document’ that we will update as tools (or our insights) change, and hopefully serve as the basis for a second edition in the future. For that reason, all feedback is highly appreciated!\nWhat can you do to help:\n\nCreate a github issue if you see any errors, find anything hard to understand, or have any other sort of suggestions or feedback.\nFix typo’s or other issues directly by editing the relevant file on github and creating a pull request\n\nAny contributors will be acknowledged on this page and in a possible second edition of the book. If you are willing to contribute in a more major way (e.g. rewrite or add an entire chapter), please get in touch with us and we can work something out."
  },
  {
    "objectID": "index.html#citing-this-book",
    "href": "index.html#citing-this-book",
    "title": "Computational Analysis of Communication",
    "section": "Citing this book",
    "text": "Citing this book\nTo cite this book, use:\nVan Atteveldt, W., Trilling, D., & Calderón, C. A. (2022). Computational Analysis of Communication. Wiley Blackwell.\nBibtex:\n@book{vanatteveldt2022computational,\n  title={Computational Analysis of Communication},\n  author={{Van Atteveldt}, Wouter and Trilling, Damian and Calder{\\'o}n, Carlos Arc{\\'\\i}la},\n  year={2022},\n  publisher={Wiley Blackwell}\n}"
  },
  {
    "objectID": "content/chapter01.html#sec-ccs",
    "href": "content/chapter01.html#sec-ccs",
    "title": "1  Introduction",
    "section": "1.1 The Role of Computational Analysis in the Social Sciences",
    "text": "1.1 The Role of Computational Analysis in the Social Sciences\nThe use of computers is nothing new in the social sciences. In fact, one could argue that some disciplines within the social sciences have even been early adopters of computational approaches. Take the gathering and analyzing of large-scale survey data, dating back to the use of the Hollerith Machine in the 1890 US census. Long before every scholar had a personal computer on their desk, social scientists were using punch cards and mainframe computers to deal with such data. If we think of the analysis of communication more specifically, we already see attempts to automate content analysis in the 1960’s (see, e.g. Scharkow 2017).\nHowever, something has profoundly changed in recent decades. The amount and type of data we can collect as well as the computational power we have access to have increased dramatically. In particular, digital traces that we leave when communicating online, from access logs to comments we place, have required new approaches (e.g., Trilling 2017). At the same time, better computational facilities now allow us to ask questions we could not answer before.\nGonzález-Bailón (2017), for instance, argued that the computational analysis of communication now allows us to test theories that were formulated a century ago, such as Tarde’s theory of social imitation. Salganik (2019) tells an impressive methodological story of continuity in showing how new digital research methods build on and relate to established methods such as surveys and experiments, while offering new possibilities by observing behavior in new ways.\nA frequent misunderstanding, then, about computational approaches is that they would somehow be a-theoretical. This is probably fueled by clichés coined during the “Big Data”-hype in the 2010’s, such as the infamous saying that in the age of Big Data, correlation is enough (Mayer-Schönberger and Cukier 2013); but one could not be more wrong: as the work of Kitchin shows (Kitchin 2014a, 2014b), computational approaches can be well situated within existing epistemologies. For the field to advance, computational and theoretical work should be symbiotic, with each informing the other and with neither claiming superiority (Margolin 2019). Thus, the computational scientists’ toolbox includes both more data-driven and more theory-driven techniques; some are more bottom-up and inductive, others are more top-down and deductive. What matters here, and what is often overlooked, is in which stage of the research process they are employed. In other words, both inductive and deductive approaches as they are distinguished in more traditional social-science textbooks (e.g., Bryman 2012) have their equivalent in the computational social sciences.\nTherefore, we suggest that the data collection and data analysis process is thought of as a pipeline. To test, for instance, a theoretically grounded hypothesis about personalization in the news, we could imagine a pipeline that starts with scraping online news, proceeds with some natural-language processing techniques such as Named Entity Recognition, and finally tests whether the mentioning of persons has an influence on the placement of the stories. We can distinguish here between parts of the pipeline that are just necessary but not inherently interesting to us, and parts of the pipeline that answer a genuinely interesting question. In this example, the inner workings of the Named Entity Recognition step are not genuinely interesting for us – we just need to do it to answer our question. We do care about how well it works and especially which biases it may have that could affect our substantive outcomes, but we are not really evaluating any theory on Named Entity Recognition here. We are, however, answering a theoretically interesting question when we look at the pipeline as a whole, that is, when we apply the tools in order to tackle a social scientific problem. Of course, what is genuinely interesting depends on one’s discipline: For a computational linguist, the inner workings of the Named Entity Recognition may actually be the interesting part, and our research question just one possible “downstream task”.\nThis distinction is also sometimes referred to as “building a better mousetrap” versus “understanding”. For instance, Breiman (2001) remarked: “My attitude toward new and/or complicated methods is pragmatic. Prove that you’ve got a better mousetrap and I’ll buy it. But the proof had better be concrete and convincing.” (p. 230). In contrast, many social scientists are using statistical models to test theories and to understand social processes: they want to specifically understand how \\(x\\) relates to \\(y\\), even if \\(y\\) may be better predicted by another (theoretically uninteresting) variable.\nThis book is to some extent about both building mousetraps and understanding. When you are building a supervised machine learning classifier to determine the topic of each text in a large collection of news articles or parliamentary speeches, you are building a (better) mousetrap. But as a social scientist, your work does not stop there. You need to use the mousetrap to answer some theoretically interesting question.\nActually, we expect that the contents of this book will provide a background that helps you to face the current research challenges in both academia and industry. On the one hand, the emerging field of Computational Social Science has become one of the most promising areas of knowledge and many universities and research institutes are looking for scholars with this profile. On the other hand, it is widely known that nowadays the computational skills will increase your job opportunities in private companies, public organizations, or NGOs, given the growing interest in data-driven solutions.\nWhen planning this book, we needed to make a couple of tough choices. We aimed to at least give an introduction to all techniques that students and scholars who want to computationally analyze communication will probably be confronted with. Of course, specific – technical – literature on techniques such as, for instance, machine learning can cover the subject in more depth, and the interested student may indeed want to dive into one or several of the techniques we cover more deeply. Our goal here is to offer enough working knowledge to apply these techniques and to know what to look for. While trying to cover the breadth of the field without sacrificing too much depth when covering each technique, we still needed to draw some boundaries. One technique that some readers may miss is agent-based modeling (ABM). Arguably, such simulation techniques are an important technique in the computational social sciences more broadly (Cioffi-Revilla 2014), and they have recently been applied to the analysis of communication as well (Waldherr 2014; Wettstein 2020). Nevertheless, when reviewing the curricula of current courses teaching the computational analysis of communication, we found that simulation approaches do not seem to be at the core of such analyses (yet). Instead, when looking at the use of computational techniques in fields such as journalism studies (e.g., Boumans and Trilling 2016), media studies (e.g., Rieder 2017), or the text-as-data movement (Grimmer and Stewart 2013), we see a core of techniques that are used over and over again, and that we have therefore included in our book. In particular, besides general data analysis and visualization techniques, these are techniques for gathering data such as web scraping or the use of API’s; techniques for dealing with text such as natural language processing and different ways to turn text into numbers; supervised and unsupervised machine learning techniques; and network analysis."
  },
  {
    "objectID": "content/chapter01.html#sec-whypythonr",
    "href": "content/chapter01.html#sec-whypythonr",
    "title": "1  Introduction",
    "section": "1.2 Why Python and/or R?",
    "text": "1.2 Why Python and/or R?\nBy far most work in the computational social sciences is done using Python and/or R. Sure, for some specific tasks there are standalone programs that are occasionally used; and there are some useful applications written in other languages such as C or Java. But we believe it is fair to say that it is very hard to delve into the computational analysis of communication without learning at least either Python or R, and preferably both of them. There are very few tasks that you cannot do with at least one of them.\nSome people have strong beliefs as to which language is “better” – we do not subscribe to that view. Most techniques that are relevant to us can be done in either language, and personal preference is a big factor. R started out as a statistical programming environment, and that heritage is still visible, for instance in the strong emphasis on vectors, factors, et cetera, or the possibility to estimate complex statistical models in just one line of code. Python started out as a general-purpose programming language, which means that some of the things we do feel a bit more `low-level’ – Python abstracts away less of the underlying programming concepts than R does. This sometimes gives us more flexibility – at the cost of being more wordy. In recent years, however, Python and R have been growing closer to each other: with modules like pandas and statsmodels, Python now has R-like functionality handling data frames and estimating common statistical models on them; and with packages such as quanteda, handling of text – traditionally a strong domain of Python – has become more accessible in R.\nThis is the main reason why we decided to write this “bi-lingual” book. We wanted to teach techniques for the computational analysis of communication, without enforcing a specific implementation. We hope that the reader will learn from our book, say, how to transform a text into features and how to choose an appropriate machine learning model, but will find it of less importance in which language this happens.\nHowever, sometimes, there are good reasons to choose one language above the other. For instance, many machine learning models in the popular caret package in R under the hood create a dense matrix, which severely limits the number of documents and features one can use; also, some complex web scraping tasks are maybe easier to realize in Python. On the other hand, R’s data wrangling and visualization techniques in the tidyverse environment are known for their user-friendliness and quality. In the rare cases where we believe that R or Python is clearly superior for a given task, we indicate this; for the rest, we believe that it is up to the reader to choose."
  },
  {
    "objectID": "content/chapter01.html#sec-howtouse",
    "href": "content/chapter01.html#sec-howtouse",
    "title": "1  Introduction",
    "section": "1.3 How to use this book",
    "text": "1.3 How to use this book\nThis book differs from more technically oriented books on the one hand and more conceptual books on the other hand. We do cover the technical background that is necessary to understand what is going on, but we keep both computer science concepts and mathematical concepts to a minimum. For instance, if we had written a more technical book about programming in Python, we would have introduced rather early and in detail concepts such as classes, inheritance, and instances of classes. Instead, we decided to provide such information only as additional background where necessary, and to focus, rather pragmatically, on the application of techniques for the computational analysis of communication. Vice versa, if we had written a more conceptual book on new methods in our field, we would have given more emphasis to epistemological aspects, and had skipped the programming examples, which are now at the core of this book.\nWe do not expect much prior knowledge from the readers of this book. Sure, some affinity with computers helps, but there is no strict requirement on what you need to know. Also in terms of statistics, it helps if you have heard of concepts such as correlation or regression analysis, but even if your knowledge here is rather limited, you should be able to follow along.\nThis also means that you may be able to skip chapters. For instance, if you already work with R and/or Python, you may not need our detailed instructions at the beginning. Still, the book follows a logical order in which chapters build on previous ones. For instance, when explaining supervised machine learning on textual data, we expect you to be familiar with previous chapters that deal with machine learning in general, or with the handling of textual data.\nThis book is designed in such a way that it can be used as a text book for introductory courses on the computational analysis of communications. Often, such courses will be on the graduate level, but it is equally possible to use this book in an undergraduate course; maybe skipping some parts that may go too deep. All code examples are not only printed in this book, but also available online. Students as well as social-scientists who want to brush up their skillset should therefore also be able to use this book for self-study, without a formal course around it. Lastly, this book can also be a reference for readers asking themselves: “How do I do this again?”. In particular, if the main language you work in is R, you can look up how to do similar things in Python and vice versa.\n\n\n\n\n\n\nCode examples\n\n\n\n\n\nRegardless of the context in which you use this book, one thing is for sure: The only way to learn computational analysis methods is by practicing and playing around. For this reason, the code examples are probably the most important part of the book. Where possible, the examples use real world data that is freely available on the Internet. To make sure that the examples still work in five years’ time, we generally provide a copy of this data on the book website, but we also provide a link to the original source.\nOne thing to note is that to avoid unnecessary repetition the examples are sometimes designed to continue on earlier snippets from that chapter. So, if you seem to be missing a data set, or if some package is not imported yet, make sure you run all the code examples from that chapter.\nNote that although it is possible to copy-paste the code from the website accompanying this book1, we would actually recommend typing the examples yourself. That way, you are more conscious about the commands you are using and you are adding them to your `muscle memory’.\nFinally, realize that the code examples in this book are just examples. There’s often more ways to do something, and our way is not necessarily the only good (let alone the best) way. So, after you get an example to work, spend some time to play around with it: try different options, maybe try it on your own data, or try to achieve the same result in a different way. The most important thing to remember is: you can’t break anything! So just go ahead, have fun, and if nothing works anymore you can always start over from the code example from the book."
  },
  {
    "objectID": "content/chapter01.html#sec-installing",
    "href": "content/chapter01.html#sec-installing",
    "title": "1  Introduction",
    "section": "1.4 Installing R and Python",
    "text": "1.4 Installing R and Python\nR and Python are the most popular programming languages that data scientists and computational scholars have adopted to conduct their work. While many develop a preference for one or the other language, the chances are good that you will ultimately switch back and forth between them, depending on the specific task at hand and the project you are involved in.\nBefore you can start with analyzing data and communication in Python or R, you need to install interpreters for these languages (i.e., programs that can read code in these languages and execute it) on your computer. Interpreters for both Python and R are open source and completely free to download and use. Although there are various web-based services on which you can run code for both languages (such as Google Colab or RStudio Cloud), it is generally better to install an interpreter on your own computer.\nAfter installing Python or R, you can execute code in these languages, but you also want a nice Integrated Development Environment (IDE) to develop your data analysis scripts. For R we recommend RStudio, which is free to install and is currently the most popular environment for working with R. For Python we recommend starting with JupyterLab or JupyterNotebook, which is a browser-based environment for writing and running Python code. All of these tools are available and well documented for Windows, MacOS, and Linux. After explaining how to install R and Python, there is a very important section on installing packages. If you plan to only use either R or Python (for now), feel free to skip the part about the other language.\nIf you are writing longer Python programs (as opposed to, for instance, short data analysis scripts) you probably want to install a full-blown IDE as well. We recommend PyCharm2 for this, which has a free version that has everything you need, and the premium version is also free for students and academic or open source developers. See their website for download and installation instructions.\n\n\n\n\n\n\nAnaconda\n\n\n\n\n\n. An alternative to installing R, Python, and optional libraries separately and as you need them (which we will explain later in this chapter) is to install the so-called Anaconda distribution, one of the most used and extensive platforms to perform data science. Anaconda is free and open-source, and is conceived to run Python and R code for data analysis and machine learning. Installing the complete Anaconda Distribution on your computer3 provides you with everything that you need to follow the examples in this book and includes development environments such as Spyder, Jupyter, and RStudio. It also includes a large set of pre-installed packages often used in data science and its own package manager, conda, which will help you to install and update other libraries or dependencies. In short, Anaconda bundles almost all the important software to perform computational analysis of communication.\nSo, should you install Anaconda, or should you install all software separately as outlined in this chapter? It depends. On the pro side, by downloading Anaconda you have everything installed at once and do not have to worry about dependencies (e.g., Windows users usually do not have a C compiler installed, but some packages may need it). On the con side, it is huge and also installs many things you do not need, so you essentially get a non-standard installation, in which programs and packages are stored in different locations than those you (or your computer) may expect. Nowadays, as almost all computers actually already have some version of Python installed (even though you may not know it), you also end up in a possibly confusing situation where it may be unclear which version you are actually running, or for which version you installed a package. For this reason, our recommendation is to not use Anaconda unless it is already installed or you have a specific reason to do so (for example, if your professor requires you to use it).\n\n\n\n\n1.4.1 Installing R and RStudio\nFirstly, we will install R and its most popular IDE RStudio, and we will learn how to install additional packages and how to run a script. R is an object-based programming language orientated to statistical computing that can be used for most of the stages of computational analysis of communication. If you are completely new to R, but familiar with other popular statistical packages in social sciences (such as SPSS or STATA), you will find that you can perform in R many already-known statistical operations. If you are not familiar with other statistical packages, do not panic, we will guide you from the very beginning. Unlike much traditional software that requires just one complete and initial installation, when working with R, we will first install the raw programming language and then we will continue to install additional components during our journey. It might sound cumbersome, but in fact it will make your work more powerful and flexible, since you will be able to choose the best way to interact with R and especially you will select the packages that are suitable for your project.\nNow, let us install R. The easiest way is to go to the RStudio CRAN page at cran.rstudio.com/. 4 Click on the link for installing R for your operating system, and install the latest version. If you use Linux, you may want to install R via your package manager. For Ubuntu linux, it is best to follow the instructions on cran.r-project.org/bin/linux/ubuntu/.\nAfter installing R, let us immediately install RStudio Desktop (the free version). Go to rstudio.com/products/rstudio/download/#download and download and run the installer for your computer. If you open RStudio you should get a screen similar to Figure 1.1. If this is the first time you open RStudio you probably won’t see the top left pane (the scripts), you can create that pane by creating a new R Script via the file menu or with the green plus icon in the top left corner.\n\n\n\nFigure 1.1: RStudio Desktop.\n\n\nOf the four panes in RStudio, you will probably spend most time in the top left pane, where you can view and edit your analysis scripts. A script is simply a list of commands that the computer should execute one after the other, for example: open your data, do some computations, and make a nice graph.\nTo run a line of code, you can place your cursor anywhere on that line and click the Run icon or press control+Enter. To try that, type the following into your newly opened script:\nprint(\"Hello world\")\nNow, place your cursor on that line and press Run (or control+Enter). What happens is that the line is copied to the Console in the bottom left corner and executed. So, the results of your commands (and any error messages) will be shown in this console view.\nIn contrast to most traditional programming languages, the easiest way to run R code is line by line. You can simply place your cursor on the first line, and repeatedly press control+Enter, which executes a line and then places the cursor on the next line. You can also select multiple lines (or part of a line) to execute those commands together, but in general it is easier to check that everything is going as planned if you run the code line by line.\nYou can also write commands directly in the console and execute them (by pressing Enter). This can be useful for trying things out or to run things that only need to be run once, but in general we would strongly recommend typing all your commands in a script and then executing them. That way, the script serves as a log of the commands you used to analyze your data, so you (or a colleague) can read and understand how you did the analyses.\n\n\n\n\n\n\nRStudio Projects\n\n\n\n\n\nA very good idea to organize your data and code is to work with RStudio Projects. In fact, we would recommend you to now create a new empty project for the examples in this book. To do this, click on the Project button in the top right and select “New Project”. Then, select New Directory and New Project and enter a name for this project and a parent folder for the project if you don’t want it in your Documents. Using a project means that the scripts and data files for your project are all in the same location and you don’t need to mess around with specifying the locations of files (which will probably be different for someone else or on a different computer). Moreover, RStudio remembers which files you were editing for each project, so if you are working on multiple projects, it’s very easy to switch between them. We recommend creating a project now for the book (and/or for any projects you are working on), and always switching to a project when you open RStudio\n\n\n\nOn the right side of the RStudio workspace you will find two additional windows. In the top right pane there are two or more tabs: environment and history, and depending on additional packages you may have installed there may be some more. In environment you can manage your workspace (the set of elements you need to deploy for data analysis) and have a list of the objects you have uploaded to it. You may also import datasets with this tool. In the history tab you have an inventory of code executions, which you can save to a file, or move directly to console or to an R document.\nNote that in the environment you can save and load your “workspace” (all data in the computer memory). However, relying on this functionality is often not a good idea: it will only save the state of your current session, whereas you most likely will want to save your R syntax file and/or your data instead. If you have your raw input data (e.g., as a csv file, see Chapter 5) and your analysis script, you can always reproduce what you have been doing. If you only have a snapshot of your workspace, you know the state in which you arrived, but cannot necessarily reproduce (or change) how you got there.\nIn the bottom right pane there are five additional useful tabs. In files you can explore your computer and manage all the files you may use for the project, including importing datasets. In plots, help and viewer, you can visualize the outputs, figures, documentation and general outcomes, respectively, that you have executed in your script. Finally, the tab for packages will be of great utility since it will let you install or update packages from CRAN or even from a file saved on your computer with a friendly interface.\n\n\n1.4.2 Installing Python and Jupyter Notebook\nPython is an object-orientated programming language and it is probably the favorite language of computational and data scientists in all disciplines around the world. There are different releases of Python, but the biggest difference used to be between Python 2 and Python 3. Fortunately, you will probably never need to install or use Python 2, and in fact, since January 2020 it is no longer supported. Thus, you can just use any recent Python 3 version for this book. When browsing through questions on online fora such as Stackoverflow or reading other people’s code on Github (we will talk about that in Chapter 4), you still may come across legacy code in Python 2. Such code usually does not run directly in a Python 3 interpreter, but in most cases, only minor adaptions are necessary to make it work.\nWe will install and run Python and Jupyter Notebook using a terminal or command line interface. This is a tool that is installed on all computers that allows you to enter commands to the computer directly. First, create a project folder for this book using the File Explorer (Windows) or Finder (MacOS). Then, on Windows you can shift + Right click that folder and select “Open command Window here”. On MacOS, after navigating to the folder you just created, you click on “Finder” in the menu at the top of the screen, then on “Services”, then on “New Terminal at Folder.” In both cases, this should open a new window (usually black or gray) that allows you to type commands.\nNote that on most computers, Python is already installed by default. You can check this by typing the following command in your terminal:\npython3 --version\nOn some versions of Windows, you may need to use py instead of python3:\npy --version\nIn either case, the output of this command should be something like Python 3.8.5. If python --version also returns this version, you are free to use either command (but on older systems python can still refer to Python 2, so make sure that you are using Python 3 for this book!).\nIf Python is not installed on your system, go to www.python.org/downloads/windows/ or www.python.org/downloads/mac-osx/ and download and install the latest stable release (which at the time of writing is 3.9.0). 5 After installing it, open a terminal again and run the command above to verify that it is installed correctly.\nIncluded in any recent Python install is pip, the program that you will use for installing Python packages. You can check that pip is installed correctly by typing the following command on your terminal:\npip3 --version\nWhich should report something like pip 20.0.2 from ... (python 3.8). Again, if pip reports the same version you can also use it instead of pip3. On some systems pip3 will not work, so use pip in that case (but make sure to check that it points to Python 3).\nInstalling Jupyter Notebook. Next, we will install Jupyter Notebook, which you can use to run all the examples in this book and is a great environment for developing Python data analysis scripts. Jupyter Notebooks (in IDE JupyterLab if you installed that), are run as a web application that allows you to create documents that contain code and inline text fragments. One of the nicest things about the Jupyter Notebook is that the code is inserted in fields (so-called “cells”) that you can run one by one, getting its respective output, which when added to the narrative text, will make your script more clean and reproducible. You can also add formatted text blocks (using a simple formatting language called Markdown) to explain to the reader what you are doing. In Section 4.3, we will address notebooks again as a good practice for a computational scientist.\nYou can install Jupyter notebook directly using pip using the following command (executed in a terminal):\npip3 install jupyter-notebook\nNow, you can run Jupyter by executing the following command on the terminal:\njupyter notebook\nThis will print some useful information, including the URL at which you can access the notebook. However, it should also directly open this in a browser (e.g. Chrome) so you can directly start working. In your browser you should see the Jupyter main screen similar to the middle window in Figure 1.2. Create a new notebook by clicking on the New button in the top right and selecting Python 3. This should open a window similar to the bottom window in Figure 1.2.\n\n\n\nFigure 1.2: Jupyter Notebook.\n\n\nIn Jupyter, code is entered into cells. First, type print(\"Hello World\") into the empty cell next to the In [ ]: prompt. Then, click the Run button or press control+Enter. This should execute your command and display the text \"Hello World\" in the output area right below the input cell. Note that you can create more cells using the plus icon or with the insert menu. You can also set the cell type via the Cell menu: select code for analysis scripts (which is the default), or Markdown for text fragments, which can be used to explain the code and/or interpret the results."
  },
  {
    "objectID": "content/chapter01.html#sec-installingpackages",
    "href": "content/chapter01.html#sec-installingpackages",
    "title": "1  Introduction",
    "section": "1.5 Installing Third-Party Packages",
    "text": "1.5 Installing Third-Party Packages\nThe print function used above is automatically included when you start R or Python. Many functions, however, are included in separate packages (also known as libraries or modules), which are generally collections of commands for a certain task or activity.\nAlthough both R and Python come pre-installed with many useful packages, one of the great things of both languages is that they have a very active community that continuously develops, improves, and publishes new packages. Throughout this book, we will be using such third-party packages for a variety of tasks, from data wrangling and visualization to text analysis. For example, we will use the R package tidyverse and the Python packages pandas for data wrangling.\nTo install these packages on your computer, run the following commands: (Note: if you are using Anaconda, replace pip3 install with conda install)\n\nInstalling a package from JupyterInstalling a package in R\n\n\n\n!pip3 install pandas\n# (On some systems, !pip install pandas)\n\n\n\n\ninstall.packages(\"tidyverse\")\n\n\n\n\nThese commands will automatically fetch the package from the right repository6 and install them on your computer. This can take a while, especially for large packages such as tidyverse. Fortunately, this only needs to be done once. Every time you use a package, you also need to activate it using the import (Python) or library (R) command.\nIn general, whenever you get an error No module named 'pandas' (Python) or there is no package called ‘tidyverse’, you can just install the package with that name using the code listed above. If you get an error such as name 'pandas' is not defined (Python) or object 'ggplot' not found (R), it is quite possible you forgot to activate the package that includes that function.\n\n\n\n\n\n\nPackages used in each chapter\n\n\n\n\n\nSome packages, like the tidyverse (R) and pandas (Python) packages for data handling are used in almost every chapter. Many chapters also introduce specific packages such as igraph/networkx for network analysis in Chapter 13. To make it easy to keep track of the packages needed for each chapter, every chapter that includes code in this book starts with a note like this that gives an overview of the main packages introduced in that chapter. It also includes the code needed to install these packages, which of course is only needed if you didn’t install these packages before. Note again that if you are using Anaconda for Python, you should replace !pip3 install with !conda install in that code. On some systems, you may need to use !pip install instead of !pip3 install.\nThese notes also includes a code block to import all the packages used for that chapter, which you need to run every time you use examples from that chapter.\n\n\n\n\n\n\n\nBoumans, Jelle W., and Damian Trilling. 2016. “Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars.” Digital Journalism 4 (1): 8–23. https://doi.org/10.1080/21670811.2015.1096598.\n\n\nBreiman, Leo. 2001. “Statistical modeling: The two cultures.” Statistical Science 16 (3): 199–215. https://doi.org/10.1214/ss/1009213726.\n\n\nBryman, Alan. 2012. Social Research Methods. 4th edition. New York, NY: Oxford University Press.\n\n\nCioffi-Revilla, Claudio. 2014. Introduction to Computational Social Science: Principles and Applications. London, UK: Springer.\n\n\nGonzález-Bailón, Sandra. 2017. Decoding the social world: Data science and the unintended consequences of communication. Cambridge, MA: MIT.\n\n\nGrimmer, J., and B. M. Stewart. 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.” Political Analysis 21 (3): 267–97. https://doi.org/10.1093/pan/mps028.\n\n\nKitchin, Rob. 2014a. “Big Data, new epistemologies and paradigm shifts.” Big Data & Society 1 (1): 1–12. https://doi.org/10.1177/2053951714528481.\n\n\n———. 2014b. The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences. Sage.\n\n\nMargolin, Drew B. 2019. “Computational Contributions: A Symbiotic Approach to Integrating Big, Observational Data Studies into the Communication Field.” Communication Methods and Measures 13 (4): 229–47.\n\n\nMayer-Schönberger, Viktor, and Kenneth Cukier. 2013. Big Data: A Revolution That Will Transform How We Live, Work, and Think. New York, NY: Houghton Mifflin Harcourt.\n\n\nRieder, Bernhard. 2017. “Scrutinizing an Algorithmic Technique: The Bayes Classifier as Interested Reading of Reality.” Information Communication and Society 20 (1): 100–117. https://doi.org/10.1080/1369118X.2016.1181195.\n\n\nSalganik, Matthew. 2019. Bit by Bit: Social Research in the Digital Age. Princeton University Press.\n\n\nScharkow, Michael. 2017. “Content Analysis, Automatic.” In The International Encyclopedia of Communication Research Methods, edited by Jörg Matthes, Christine S. Davis, and Robert F. Potter, 1–14. Hoboken, NJ: Wiley. https://doi.org/10.1002/9781118901731.iecrm0043.\n\n\nTrilling, Damian. 2017. “Big Data, Analysis of.” In The International Encyclopedia of Communication Research Methods, 1–20. Hoboken, NJ, USA: John Wiley & Sons, Inc. https://doi.org/10.1002/9781118901731.iecrm0014.\n\n\nWaldherr, Annie. 2014. “Emergence of News Waves: A Social Simulation Approach.” Journal of Communication 64 (5): 852–73. https://doi.org/10.1111/jcom.12117.\n\n\nWettstein, Martin. 2020. “Simulating hidden dynamics : Introducing Agent-Based Models as a tool for linkage analysis.” Computational Communication Research 2 (1): 1–33. https://doi.org/10.5117/CCR2020.1.001.WETT."
  },
  {
    "objectID": "content/chapter02.html#sec-funtweets",
    "href": "content/chapter02.html#sec-funtweets",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.1 Fun With Tweets",
    "text": "2.1 Fun With Tweets\nThe goal of this chapter is to showcase how you can use R or Python to quickly and easily run some impressive analyses of real world data. For this purpose, we will be using a dataset of tweets about the COVID pandemic that is engulfing much of the world at the time this book is written. Of course, tweets are probably only representative for what is said on Twitter, but the data are (semi-)public and rich, containing text, location, and network characteristics. This makes them ideal for exploring the many ways in which we can analyze and visualize information with Python and R.\nExample 2.1 shows how you can read this dataset into memory using a single command. Note that this does not retrieve the tweets from Twitter itself, but rather downloads our cached version of the tweets. In Chapter 12 we will show how you can download tweets and location data yourself, but to make sure we can get down to business immediately we will start from this cached version.\n\n\n\n\n\n\n\nExample 2.1 Retrieving cached tweets about COVID\n\nPython codeR code\n\n\n\ntw = pd.read_csv(\"https://cssbook.net/d/covid.csv\")\ntw.head()\n\n             status_id  ... reply_to_screen_name\n0  1309535775109926912  ...                  NaN\n1  1309626010221129729  ...                  NaN\n2  1309578234007257088  ...                  NaN\n3  1309557875296083969  ...                  NaN\n4  1309643186827132929  ...                  NaN\n\n[5 rows x 8 columns]\n\n\n\n\n\ntw = read_csv(\"https://cssbook.net/d/covid.csv\")\nhead(tw)\n\n# A tibble: 6 × 8\n  status_id created_at          screen_name  lang  locat…¹ text  retwe…² reply…³\n      <dbl> <dttm>              <chr>        <chr> <chr>   <chr>   <dbl> <chr>  \n1   1.31e18 2020-09-25 16:50:33 ghulamabbas… en    Lahore… \"Sec…    1203 <NA>   \n2   1.31e18 2020-09-25 22:49:07 GeoRebekah   en    Florid… \"On …    1146 <NA>   \n3   1.31e18 2020-09-25 19:39:16 AlexBerenson en    New Yo… \"Upd…     988 <NA>   \n4   1.31e18 2020-09-25 18:18:22 AlexBerenson en    New Yo… \"No …     953 <NA>   \n5   1.31e18 2020-09-25 23:57:22 B52Malmet    en    New Yo… \"Dr.…     946 <NA>   \n6   1.31e18 2020-09-26 08:28:51 iingwen      en    Taipei… \"It’…     436 <NA>   \n# … with abbreviated variable names ¹​location, ²​retweet_count,\n#   ³​reply_to_screen_name\n\n\n\n\n\n\n\n\n\nAs you can see, the dataset contains almost 10000 tweets, listing their sender, their location and language, the text, the number of retweets, and whether it was a reply (retweet). You can read the start of the three most retweeted messages, which contain one (political) tweet from India and two seemingly political and factual tweets from the United States.\nMy first bar plot. Before diving into the textual, network, and geographic data in the dataset, let’s first make a simple visualization of the date on which the tweets were posted. Example 2.2 does this in two steps: first, the number of tweets per hour is counted with an aggregation command. Next, a bar plot is made of this calculated value with some options to make it look relatively clean and professional. If you want to play around with this, you can for example try to plot the number of tweets per language, or create a line plot instead of a bar plot. For more information on visualization, please see Chapter 7. See Chapter 6 for an in-depth explanation of the aggregation command.\n\n\n\n\n\n\n\nExample 2.2 Barplot of tweets over time\n\nPython codeR code\n\n\n\ntw.index = pd.DatetimeIndex(tw[\"created_at\"])\ntw[\"status_id\"].groupby(pd.Grouper(freq=\"H\")).count().plot(kind=\"bar\")\n# (note the use of \\ to split a long line)\n\n\n\n\n\n\n\ntweets_per_hour = tw %>% \n  mutate(hour=round_date(created_at, \"hour\")) %>% \n  group_by(hour) %>% summarize(n=n()) \nggplot(tweets_per_hour, aes(x=hour, y=n)) + \n  geom_col() + theme_classic() + \n  xlab(\"Time\") + ylab(\"# of tweets\") +\n  ggtitle(\"Number of COVID tweets over time\")"
  },
  {
    "objectID": "content/chapter02.html#sec-funtext",
    "href": "content/chapter02.html#sec-funtext",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.2 Fun With Textual Data",
    "text": "2.2 Fun With Textual Data\nCorpus Analysis. Next, we can analyze which hashtags are most frequently used in this dataset. Example 2.3 does this by creating a document-term matrix using the package quanteda (in R) or by manually counting the words using a defaultdict (in Python). The code shows a number of steps that are made to create the final results, each of which represent researcher choices about which data to keep and which to discard as noise. In this case, we select English tweets, convert text to lower case, remove stop words, and keep only words that start with #, while dropping words starting with #corona and #covid. To play around with this example, see if you can adjust the code to e.g. include all words or only at-mentions instead of the hashtags and make a different selection of tweets, for example Spanish language tweets or only popular (retweeted) tweets. Please see Chapter 10 if you want to learn more about corpus analysis, and see Chapter 6 for more information on how to select subsets of your data.\n\n\n\n\n\n\n\nExample 2.3 My First Tag Cloud\n\nPython codeR code\n\n\n\nfreq = defaultdict(int)\nfor tweet in tw[\"text\"]:\n    for tag in re.findall(\"#\\w+\", tweet.lower()):\n        if not re.search(\"#covid|#corona\", tag):\n            freq[tag] += 1\nwc = WordCloud().generate_from_frequencies(freq)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\n\n\n\ndtm_tags = filter(tw, lang==\"en\") %>% \n  corpus() %>% tokens() %>% \n  dfm(tolower = T) %>% \n  dfm_select(pattern = \"#*\") %>% \n  dfm_remove(c(\"#corona*\", \"#covid*\")) \ntextplot_wordcloud(dtm_tags, max_words=100)\n\n\n\n\n\n\n\n\n\n\n\nTopic Model. Where a word cloud (or tag cloud) shows which words occur most frequently, a topic model analysis shows which words co-occur in the same documents. Using the most common topic modeling algorithm, Latent Dirichlet Allocation or LDA, Example 2.4 explores the tweets by automatically clustering the tags selected earlier into 10 topics. Topic modeling is non-deterministic – if you run it again you can get slightly different topics, and topics are swapped around randomly as the topic numbers have no special meaning. By setting the computer’s random seed you can ensure that if you run it again you get the same results. As you can see, some topics seem easily interpretable (such as topic 2 about social distancing, and topic 8 on health care), it is always recommended that you inspect the clustered documents and edge cases in addition to the top words (or tags) as shown here. You can play around with this example by using a different selection of words (modifying the code in Example 2.3) or changing the number of topics. You can also change (or remove) the random seed and see how running the same model multiple times will give different results. See Section 11.5 for more information about fitting, interpreting, and validating topic models.\n\n\n\n\n\n\n\nExample 2.4 Topic Model of the COVID tags\n\nPython codeR code\n\n\n\ntags = [\n    [tag.lower() for tag in re.findall(\"#\\w+\", tweet)] for tweet in tw[\"text\"]\n]\nvoca = corpora.Dictionary(tags)\ncorpus = [voca.doc2bow(doc) for doc in tags]\nm = models.LdaModel(\n    corpus, num_topics=10, id2word=voca, distributed=False, random_state=123\n)\nfor topic, words in m.print_topics(num_words=3):\n    print(f\"{topic}: {words}\")\n\n0: 0.030*\"#coviduk\" + 0.019*\"#covid\" + 0.010*\"#lockdown2\"\n1: 0.132*\"#covid\" + 0.016*\"#salud\" + 0.012*\"#blm\"\n2: 0.105*\"#covid\" + 0.030*\"#vaccine\" + 0.029*\"#lockdown\"\n3: 0.151*\"#covid\" + 0.112*\"#covid19\" + 0.067*\"#coronavirus\"\n4: 0.664*\"#covid\" + 0.010*\"#covidー19\" + 0.005*\"#covidiots\"\n5: 0.023*\"#covid\" + 0.022*\"#mentalhealth\" + 0.017*\"#love\"\n6: 0.197*\"#covid\" + 0.032*\"#coronavirus\" + 0.030*\"#pandemic\"\n7: 0.157*\"#covid\" + 0.032*\"#trump\" + 0.031*\"#florida\"\n8: 0.051*\"#covid\" + 0.039*\"#healthcare\" + 0.033*\"#rss\"\n9: 0.205*\"#covid\" + 0.079*\"#coronavirus\" + 0.033*\"#covid19\"\n\n\n\n\n\nset.seed(1)\nm = convert(dtm_tags, to=\"topicmodel\") %>% \n  LDA(10, method=\"Gibbs\")\nterms(m, 5)\n\n     Topic 1       Topic 2             Topic 3            Topic 4         \n[1,] \"#vintage\"    \"#socialdistancing\" \"#florida\"         \"#maga\"         \n[2,] \"#vote\"       \"#staysafe\"         \"#economy\"         \"#business\"     \n[3,] \"#wfh\"        \"#lockdown\"         \"#sandiego\"        \"#climatechange\"\n[4,] \"#news\"       \"#love\"             \"#fridaythoughts\"  \"#\"             \n[5,] \"#remotework\" \"#stayhome\"         \"#rip_indianmedia\" \"#masks\"        \n     Topic 5      Topic 6    Topic 7         Topic 8       Topic 9        \n[1,] \"#sarscov2\"  \"#vaccine\" \"#pandemic\"     \"#health\"     \"#india\"       \n[2,] \"#china\"     \"#cdc\"     \"#virus\"        \"#healthcare\" \"#rss\"         \n[3,] \"#canada\"    \"#who\"     \"#masks\"        \"#medicine\"   \"#islamophobia\"\n[4,] \"#education\" \"#nhs\"     \"#pence\"        \"#doctor\"     \"#gandhi\"      \n[5,] \"#delhi\"     \"#podcast\" \"#publichealth\" \"#wellness\"   \"#nehru\"       \n     Topic 10\n[1,] \"#trump\"\n[2,] \"#usa\"  \n[3,] \"#biden\"\n[4,] \"#uk\"   \n[5,] \"#ue\""
  },
  {
    "objectID": "content/chapter02.html#sec-fungeo",
    "href": "content/chapter02.html#sec-fungeo",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.3 Fun With Visualizing Geographic Information",
    "text": "2.3 Fun With Visualizing Geographic Information\nFor the final set of examples, we will use the location information contained in the Twitter data. This information is based on what Twitter users enter into their profile, and as such it is incomplete and noisy with many users giving a nonsensical location such as `Ethereally here’ or not filling in any location at all. However, if we assume that most users that do enter a proper location (such as Lahore or Florida in the top tweets displayed above), we can use it to map where most tweets are coming from.\nThe first step in this analysis is to resolve a name such as `Lahore, Pakistan’ to its geographical coordinates (in this case, about 31 degrees north and 74 degrees east). This is called geocoding, and both Google maps and Open Street Maps can be used to perform this automatically. As with the tweets themselves, we will use a cached version of the geocoding results here so we can proceed directly. Please see https://cssbook.net/datasets for the code that was used to create this file so you can play around with it as well.\nExample 2.5 shows how this data can be used to create a map of Twitter activity. First, the cached user data is retrieved, showing the correct location for Lahore but also illustrating the noisiness of the data with the location “Un peu partout”. Next, this data is joined to the Twitter data, so the coordinates are filled in where known. Finally, we plot this information on a map, showing tweets with more retweets as larger dots. See Chapter 7 for more information on visualization.\n\n\n\n\n\n\n\nExample 2.5 Location of COVID tweets\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/covid_users.csv\"\nusers = pd.read_csv(url)\ntw2 = tw.merge(users, on=\"screen_name\", how=\"left\")\nworld = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\ngdf = gpd.GeoDataFrame(tw2, geometry=gpd.points_from_xy(tw2.long, tw2.lat))\nax = world.plot(color=\"white\", edgecolor=\"black\", figsize=(10, 10))\ngdf.plot(ax=ax, color=\"red\", alpha=0.2, markersize=tw[\"retweet_count\"])\nplt.show()\n\n\n\n\n\n\n\nurl = \"https://cssbook.net/d/covid_users.csv\"\nusers = read_csv(url)\ntw2 = left_join(tw, users)\nggplot(mapping=aes(x=long, y=lat)) +\n  geom_polygon(aes(group=group), \n    data=map_data(\"world\"), \n    fill=\"lightgray\", colour = \"white\") +\n  geom_point(aes(size=retweet_count, \n                 alpha=retweet_count), \n             data=tw2, color=\"red\") + \n  theme_void() + theme(aspect.ratio=1) + \n  guides(alpha=FALSE, size=FALSE) + \n  ggtitle(\"Location of COVID tweets\", \n          \"Size indicates number of retweets\")\n\n\n\n\n\n\n\n\n\n\n\nCombining textual and structured information. Since we know the location of a subset of our tweet’s users, we can differentiate between e.g. American, European, and Asian tweets. Example 2.6 creates a very rough identification of North American tweets, and uses that to compute the relative frequency of words in those tweets compared to the rest. Not surprisingly, those tweets are much more about American politics, locations, and institutions. The other tweets talk about UK politics but also use a variety of names to refer to the pandemic. To play around with this, see if you can isolate e.g. Asian or South American tweets, or compare Spanish tweets from different locations.\n\n\n\n\n\n\n\nExample 2.6 Corpus comparison: North American tweets vs. the rest\n\nPython codeR code\n\n\n\nnltk.download(\"stopwords\")\ncn = gdf.query(\"lang=='en'&(long<-60 & lat>25)\")\ncn = Counter(cn[\"text\"].str.cat().lower().split())\ncr = gdf.query(\"lang=='en' & (long>-60 | lat<25)\")\ncr = Counter(cr[\"text\"].str.cat().lower().split())\nfor k in stopwords.words(\"english\"):\n    del cn[k]\n    del cr[k]\nkey = sh.ProportionShift(type2freq_1=cn, type2freq_2=cr)\nkey.get_shift_graph().plot()\n\n\n\n\n\n\n\ndfm = tw2 %>% mutate(northamerica=ifelse(\n    long < -60 & lat > 25,\"N. America\",\"Rest\"))%>%\n  filter(lang==\"en\") %>% \n  corpus(docid_field=\"status_id\") %>% \n  tokens(remove_punct=T) %>%\n  tokens_group(northamerica) %>%\n  dfm(tolower=T) %>% \n  dfm_remove(stopwords(\"en\")) %>%\n  dfm_select(min_nchar=4)\nkey = textstat_keyness(dfm, target=\"N. America\")\ntextplot_keyness(key, margin=0.2) +\n  ggtitle(\"Words preferred by North Americans\", \n          \"(Only English-language tweets)\") + \n  theme_void()"
  },
  {
    "objectID": "content/chapter02.html#sec-funnet",
    "href": "content/chapter02.html#sec-funnet",
    "title": "2  Getting started: Fun with data and visualizations",
    "section": "2.4 Fun With Networks",
    "text": "2.4 Fun With Networks\nTwitter, of course, is a social network as well as a microblogging service: users are connected to other users because they follow each other and retweet and like each others’ tweets. Using the reply_to_screen_name column, we can inspect the retweet network contained in the COVID tweet dataset. Example 2.7 first uses the data summarization commands from tidyverse(R) and pandas(Python) to create a data frame of connections or edges listing how often each user retweets each other user. The second code block shows how the igraph (R) and networkx (Python) packages are used to convert this edge list into a graph. From this graph, we select only the largest connected component and use a clustering algorithm to analyze which nodes (users) form cohesive subnetworks. Finally, a number of options are used to set the color and size of the edges, nodes, and labels, and the resulting network is plotted. As you can see, the central node is Donald Trump, who is retweeted by a large number of users, some of which are then retweeted by other users. You can play around with different settings for the plot options, or try to filter e.g. only tweets from a certain language. You could also easily compute social network metrics such as centrality on this network, and/or export the network for further analysis in specialized social network analysis software. See Chapter 13 for more information on network analysis, and Chapter 6 for the summarization commands used to create the edge list.\n\n\n\n\n\n\n\nExample 2.7 Retweet network in the COVID tweets.\n\nPython codeR code\n\n\n\nedges = tw2[[\"screen_name\", \"reply_to_screen_name\"]]\nedges = edges.dropna().rename(\n    {\"screen_name\": \"from\", \"reply_to_screen_name\": \"to\"}, axis=\"columns\"\n)\nedges.groupby([\"from\", \"to\"]).size().head()\n\nfrom            to            \n007Vincentxxx   ilfattovideo      1\n06CotedUsure    ArianeWalter      1\n1Million4Covid  1Million4Covid    3\n                JustinTrudeau     1\n1ctboy1         LegionPost13      1\ndtype: int64\n\n\n\n\n\nedges = tw2 %>% \n  select(from=screen_name, \n         to=reply_to_screen_name) %>% \n  filter(to != \"\") %>%\n  group_by(to, from) %>% \n  summarize(n=n())\nhead(edges)\n\n# A tibble: 6 × 3\n# Groups:   to [6]\n  to             from                n\n  <chr>          <chr>           <int>\n1 _FutureIsUs    _FutureIsUs         1\n2 _JaylaS_       AfronerdRadio       1\n3 _LoveMTB_      ExpatriateNl        1\n4 _nogueiraneto  ideobisium          1\n5 _NotFakeNews_  panich52            1\n6 _vikasupadhyay SHADABMOHAMMAD7     4\n\n\n\n\n\n\nPython codeR code\n\n\n\ng1 = nx.Graph()\ng1.add_edges_from(edges.to_numpy())\nlargest = max(nx.connected_components(g1), key=len)\ng2 = g1.subgraph(largest)\n\npos = nx.spring_layout(g2)\nplt.figure(figsize=(20, 20))\naxes_info = plt.axis(\"off\")\nsizes = [s * 1e4 for s in nx.centrality.degree_centrality(g2).values()]\nnx.draw_networkx_nodes(g2, pos, node_size=sizes)\nedge_info = nx.draw_networkx_labels(g2, pos)\nnx.draw_networkx_edges(g2, pos)\nplt.show()\n\n\n\n\n\n\n\n# create igraph and select largest component\ng = graph_from_data_frame(edges)\ncomponents <- decompose.graph(g)\nlargest = which.max(sapply(components, gsize))\ng2 = components[[largest]]\n# Color nodes by cluster\nclusters = cluster_spinglass(g2)\nV(g2)$color = clusters$membership\nV(g2)$frame.color = V(g2)$color\n# Set node (user) and edge (arrow) size\nV(g2)$size = degree(g2)^.5\nV(g2)$label.cex = V(g2)$size/3\nV(g2)$label = ifelse(degree(g2)<=1,\"\",V(g2)$name) \nE(g2)$width = E(g2)$n\nE(g2)$arrow.size= E(g2)$width/10\nplot(g2)\n\n\n\n\n\n\n\n\n\n\n\nGeographic networks. In the final example of this chapter, we will combine the geographic and network information to show which regions of the world interact with each other. For this, in Example 2.8 we join the user information to the edges data frame created above twice: once for the sender, once for the replied-to user. Then, we adapt the earlier code for plotting the map by adding a line for each node in the network. As you can see, users in the main regions (US, EU, India) mostly interact with each other, with almost all regions also interacting with the US.\n\n\n\n\n\n\n\nExample 2.8 Reply Network of Tweets\n\nPython codeR code\n\n\n\nu = users.drop([\"location\"], axis=1)\nuf = u.rename(\n    {\"screen_name\": \"from\", \"lat\": \"lat_from\", \"long\": \"long_from\"}, axis=1\n)\nut = u.rename({\"screen_name\": \"to\", \"lat\": \"lat_to\", \"long\": \"long_to\"}, axis=1)\nedges = edges.merge(uf).merge(ut).query(\"long_to!=long_from & lat_to!=lat_from\")\n\nworld = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\ng_to = gpd.GeoDataFrame(\n    edges.copy(), geometry=gpd.points_from_xy(edges.long_to, edges.lat_to)\n)\ng_from = gpd.GeoDataFrame(\n    edges.copy(), geometry=gpd.points_from_xy(edges.long_from, edges.lat_from)\n)\n\nax = world.plot(color=\"white\", edgecolor=\"black\", figsize=(10, 10))\ng_from.plot(ax=ax, color=\"red\", alpha=0.2)\ng_to.plot(ax=ax, color=\"blue\", alpha=0.2)\n\ne = g_from.join(g_to, lsuffix=\"_from\", rsuffix=\"_to\")\ne = e[[\"geometry_from\", \"geometry_to\"]]\npx = lambda point: point.x\npy = lambda point: point.y\n\n# WVA: This code no longer works but gives\n#      UnsupportedOperationException: getX called on empty Point\n# x_values = list(zip(e[\"geometry_from\"].map(px),\n#                    e[\"geometry_to\"].map(px)))\n# y_values = list(zip(e[\"geometry_from\"].map(py),\n#                    e[\"geometry_to\"].map(py)))\n# plt.plot(x_values, y_values, linewidth = 1,\n#    linestyle = \"-\", color = \"green\", alpha=.3)\n# plt.show()\n\n\n\n\nedges2 = edges %>% \n  inner_join(users, by=c(\"from\"=\"screen_name\"))%>%\n  inner_join(users, by=c(\"to\"=\"screen_name\"), \n             suffix=c(\"\", \".to\")) %>% \n  filter(lat != lat.to | long != long.to )\nggplot(mapping=aes(x = long, y = lat)) +\n  geom_polygon(aes(group=group),map_data(\"world\"),\n    fill=\"lightgray\", colour = \"white\") +\n  geom_point(aes(size=retweet_count, \n  alpha=retweet_count), data=tw2, color=\"red\")+\n  geom_curve(aes(xend=long.to,yend=lat.to,size=n),\n             edges2, curvature=.1, alpha=.5) +\n  theme_void() + guides(alpha=FALSE, size=FALSE) +\n  ggtitle(\"Retweet network of COVID tweets\", \n  \"Bubble size indicates total no. of retweets\")"
  },
  {
    "objectID": "content/chapter03.html#sec-datatypes",
    "href": "content/chapter03.html#sec-datatypes",
    "title": "3  Programming concepts for data analysis",
    "section": "3.1 About Objects and Data Types",
    "text": "3.1 About Objects and Data Types\nNow that you have seen what R and Python can do in Chapter 2, it is time to take a small step back and learn more about how it all actually works under the hood.\nIn both languages, you write a script or program containing the commands for the computer. But before we get to some real programming and exciting data analyses, we need to understand how data can be represented and stored.\nNo matter whether you use R or Python, both store your data in memory as objects. Each of these objects has a name, and you create them by assigning a value to a name. For example, the command x=10 creates a new object[^1], named x, and stores the value 10 in it. This object is now stored in memory and can be used in later commands. Objects can be simple values such as the number 10, but they can also be pieces of text, whole data frames (tables), or analysis results. We call this distinction the type or class of an object.\n\n\n\n\n\n\nObjects, pointers, and variables.\n\n\n\n\n\nIn programming, a distinction is often made between an object (such as the number 10) and the variable in which it is stored (such as x). The latter is also called a “pointer”. However, this distinction is not very relevant for most of our purposes. Moreover, in statistics, the word variable often refers to a column of data, rather than to the name of, for instance, the object containing the whole data frame (or table). For that reason, we will use the word object to refer to both the actual object or value and its name. (If you want some extra food for thought and want to challenge your brain a bit, try to see the relationship between the idea of a pointer and the discussion about mutable and immutable objects below.)\n\n\n\nLet us create an object that we call a (an arbitrary name, you can use whatever you want), assign the value 100 to it, and use the class function (R) or type function (Python) to check what kind of object we created (Example 3.1). As you can see, R reports the type of the number as “numeric”, while Python reports it as “int”, short for integer or whole number. Although they use different names, both languages offer very similar data types. Table 3.1 provides an overview of some common basic data types.\n\n\n\n\n\n\n\nExample 3.1 Determining the type of an object\n\nPython codeR code\n\n\n\na = 100\nprint(type(a))\n\n<class 'int'>\n\n\n\n\n\na = 100\nprint(class(a))\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\n\n\n\nTable 3.1: Most used basic data types in Python and R\n\n\nPython\n\nR\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nName\nExample\nName\nExample\n\n\n\nint\n1\ninteger\n1L\nwhole numbers\n\n\nfloat\n1.3\nnumeric\n1.3\nnumbers with decimals\n\n\nstr\n\"Spam\", 'ham'\ncharacter\n\"Spam\", 'ham'\ntextual data\n\n\nbool\nTrue, False\nlogical\nTRUE, FALSE\nthe truth values\n\n\n\n\nLet us have a closer look at the code in Example 3.1 above. The first line is a command to create the object a and store its value 100; and the second is illustrative and will give you the class of the created object, in this case “numeric”. Notice that we are using two native functions of R, print and class, and including a as an argument of class, and the very same class(a) as an argument of print. The only difference between R and Python, here, is that the relevant Python function is called type instead of class.\nOnce created, you can now perform multiple operations with a and other values or new variables as shown in Example 3.2. For example, you could transform a by multiplying a by 2, create a new variable b of value 50 and then create another new object c with the result of a + b.\n\n\n\n\n\n\n\nExample 3.2 Some simple operations\n\nPython codeR code\n\n\n\na = 100\na = a * 2  # equivalent to (shorter) a*=2\nb = 50\nc = a + b\nprint(a, b, c)\n\n200 50 250\n\n\n\n\n\na = 100\na = a*2\nb = 50\nc = a + b\nprint(a)\n\n[1] 200\n\nprint(b)\n\n[1] 50\n\nprint(c)\n\n[1] 250\n\n\n\n\n\n\n\n\n\n\n3.1.1 Storing Single Values: Integers, Floating-Point Numbers, Booleans\nWhen working with numbers, we distinguish between integers (whole numbers) and floating point numbers (numbers with a decimal point, called “numeric” in R). Both Python and R automatically determine the data type when creating an object, but differ in their default behavior when storing a number that can be represented as an int: R will store it as a float anyway and you need to force it to do otherwise, for Python it is the other way round (Example 3.3). We can also convert between types later on, even though converting a float to an int might not be too good an idea, as you truncate your data.\nSo why not just always use a float? First, floating point operations usually take more time than integer operations. Second, because floating point numbers are stored as a combination of a coefficient and an exponent (to the base of 2), many decimal fractions can only approximately be stored as a floating point number. Except for specific domains (such as finance), these inaccuracies are often not of much practical importance. But it explains why calculating 6*6/10 in Python returns 3.6, while 6*0.6 or 6*(6/10) returns 3.5999999999999996. Therefore, if a value can logically only be a whole number (anything that is countable, in fact), it makes sense to restrict it to an integer.\nWe also have a data type that is even more restricted and can take only two values: true or false. It is called “logical” (R) or “bool” (Python). Just notice that boolean values are case sensitive: while in R you must capitalize the whole value (TRUE, FALSE), in Python we only capitalize the first letter: True, False. As you can see in Example 3.3, such an object behaves exactly as an integer that is only allowed to be 0 or 1, and it can easily be converted to an integer.\n\n\n\n\n\n\n\nExample 3.3 Floating point numbers, integers, and boolean values.\n\nPython codeR code\n\n\n\nd = 20\nprint(type(d))\n# forcing python to treat 20 as a float\n\n<class 'int'>\n\nd2 = 20.0\nprint(type(d2))\n\n<class 'float'>\n\ne = int(20.7)\nprint(type(e))\n\n<class 'int'>\n\nprint(e)\n\n20\n\nf = True\nprint(type(f))\n\n<class 'bool'>\n\nprint(int(f))\n\n1\n\nprint(int(False))\n\n0\n\n\n\n\n\nd = 20\nprint(class(d))\n\n[1] \"numeric\"\n\n# forcing R to treat 20 as an int\nd2 = 20L\nprint(class(d2))\n\n[1] \"integer\"\n\ne = as.integer(20.7)\nprint(class(e))\n\n[1] \"integer\"\n\nprint(e)\n\n[1] 20\n\nf = TRUE\nprint(class(f))\n\n[1] \"logical\"\n\nprint(as.integer(f))\n\n[1] 1\n\nprint(as.integer(FALSE))\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 Storing Text\nAs a computational analyst of communication you will usually work with text objects or strings of characters. Commonly simply known as “strings”, such text objects are also referred to as “character vector objects” in R. Every time you want to analyze a social-media message, or any other text, you will be dealing with such strings.\n\n\n\n\n\n\n\nExample 3.4 Strings and bytes.\n\nPython codeR code\n\n\n\ntext1 = \"This is a text\"\nprint(f\"Type of text1: {type(text1)}\")\n\nType of text1: <class 'str'>\n\ntext2 = \"Using 'single' and \\\"double\\\" quotes\"\ntext3 = 'Using \"single\" and \"double\" quotes'\nprint(f\"Are text2 and text3 equal?{text2==text3}\")\n\nAre text2 and text3 equal?False\n\n\n\n\n\ntext1 = \"This is a text\"\nglue(\"Class of text1: {class(text1)}\")\n\nClass of text1: character\n\ntext2 = \"Using 'single' and \\\"double\\\" quotes\"\ntext3 = 'Using \\'single\\' and \"double\" quotes'\nglue(\"Are text2 and text3 equal? {text2==text3}\")\n\nAre text2 and text3 equal? TRUE\n\n\n\n\n\n\nPython codeR code\n\n\n\nsomebytes = text1.encode(\"utf-8\")\nprint(type(somebytes))\n\n<class 'bytes'>\n\nprint(somebytes)\n\nb'This is a text'\n\n\n\n\n\nsomebytes= charToRaw(text1)\nprint(class(somebytes))\n\n[1] \"raw\"\n\nprint(somebytes)\n\n [1] 54 68 69 73 20 69 73 20 61 20 74 65 78 74\n\n\n\n\n\n\n\n\n\nAs you see in Example 3.4, you can create a string by enclosing text in quotation marks. You can use either double or single quotation marks, but you need to use the same mark to begin and end the string. This can be useful if you want to use quotation marks within a string, then you can use the other type to denote the beginning and end of the string. If you need to use a single quotation mark within a single-quoted string, you can escape the quotation mark by prepending it with a backslash (\\'), and similarly for double-quoted strings. To include an actual backslash in a text, you also escape it with a backslash, so you end up with a double backslash (\\\\).\nThe Python example also shows a concept introduced in Python 3.6: the f-string. These are strings that are prefixed with the letter f and are formatted strings. This means that these strings will automatically insert a value where curly brackets indicate that you wish to do so. This means that you can write: print(f\"The value of i is {i}\") in order to print “The value of i is 5” (given that i equals 5). In R, the glue package allows you to use an f-string-like syntax as well: glue(\"The value of i is \\{i\\}\").\nAlthough this will be explained in more detail in Section 5.2.2 9.1, it is good to introduce how computers store text in memory or files. It is not too difficult to imagine how a computer internally handles integers: after all, even though the number may be displayed as a decimal number to us, it can be trivially converted and stored as a binary number (effectively, a series of zeros and ones) — we do not have to care about that. But when we think about text, it is not immediately obvious how a string should be stored as a sequence of zeros and ones, especially given the huge variety of writing systems used for different languages.\nIndeed, there are several ways of how textual characters can be stored as bytes, which are called encodings. The process of moving from bytes (numbers) to characters is called decoding, and the reverse process is called encoding. Ideally, this is not something you should need to think of, and indeed strings (or character vectors) already represent decoded text. This means that often when you read from or write data to a file, you need to specify the encoding (usually UTF-8). However, both Python and R also allow you to work with the raw data (e.g. before decoding) in the form of bytes (Python) or raw (R) data, which is sometimes necessary if there are encoding problems. This is shown briefly in the bottom part of var4. Note that while R shows the underlying hexadecimal byte values of the raw data (so 54 is T, 68 is h and so on) and Python displays the bytes as text characters, in both cases the underlying data type is the same: raw (non-decoded) bytes.\n\n\n3.1.3 Combining Multiple Values: Lists, Vectors, And Friends\nUntil now, we have focused on the basic, initial data types or “vector objects”, as they are called in R. Often, however, we want to group a number of these objects. For example, we do not want to manually create thousands of objects called tweet0001, tweet0002, …, tweet9999 – we’d rather have one list called tweets that contains all of them. You will encounter several names for such combined data structures: lists, vectors, arrays, series, and more. The core idea is always the same: we take multiple objects (be it numbers, strings, or anything else) and then create one object that combines all of them (Example 3.5).\n\n\n\n\n\n\n\nExample 3.5 Collections arrays (such as vectors in R or lists in Python) can contain multiple values\n\nPython codeR code\n\n\n\nscores = [8, 8, 7, 6, 9, 4, 9, 2, 8, 5]\nprint(type(scores))\n\n<class 'list'>\n\ncountries = [\"Netherlands\", \"Germany\", \"Spain\"]\nprint(type(countries))\n\n<class 'list'>\n\n\n\n\n\nscores = c(8, 8, 7, 6, 9, 4, 9, 2, 8, 5)\nprint(class(scores))\n\n[1] \"numeric\"\n\ncountries = c(\"Netherlands\", \"Germany\", \"Spain\")\nprint(class(countries))\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\nAs you see, we now have one name (such as scores) to refer to all of the scores. The Python object in Example 3.5 is called a list, the R object a vector. There are more such combined data types, which have slightly different properties that can be important to know about: first, whether you can mix different types (say, integers and strings); second, what happens if you change the array. We will discuss both points below and show how this relates to different specific types of arrays in Python and R which you can choose from. But first, we will show how to work with them.\nOperations on vectors and lists. One of the most basic operations you can perform on all types of one-dimensional arrays is indexing. It lets you locate any given element or group of elements within a vector using its or their positions. The first item of a vector in R is called 1, the second 2, and so on; in Python, we begin counting with 0. You can retrieve a specific element from a vector or list by simply putting the index between square brackets [] (Example 3.6).\n\n\n\n\n\n\n\nExample 3.6 Slicing vectors and converting data types\n\nPython codeR code\n\n\n\nscores = [\"8\", \"8\", \"7\", \"6\", \"9\", \"4\", \"9\", \"2\", \"8\", \"5\"]\n\nprint(scores[4])\n\n9\n\nprint([scores[0], scores[9]])\n\n['8', '5']\n\nprint(scores[0:4])\n\n# Convert the first 4 scores into numbers\n# Note the use of a list comprehension [.. for ..]\n# This will be explained in the section on loops\n\n['8', '8', '7', '6']\n\nscores_new = [int(e) for e in scores[1:4]]\nprint(type(scores_new))\n\n<class 'list'>\n\nprint(scores_new)\n\n[8, 7, 6]\n\n\n\n\n\nscores=c(\"8\",\"8\",\"7\",\"6\",\"9\",\"4\",\"9\",\"2\",\"8\",\"5\")\n\nscores[5]\n\n[1] \"9\"\n\nscores[c(1, 10)]\n\n[1] \"8\" \"5\"\n\nscores[1:4]\n\n[1] \"8\" \"8\" \"7\" \"6\"\n\n# Convert the first 4 scores into numbers\nscores_new = as.numeric(scores[1:4])\nclass(scores_new)\n\n[1] \"numeric\"\n\nscores_new\n\n[1] 8 8 7 6\n\n\n\n\n\n\n\n\n\nIn the first case, we asked for the score of the 5th student (“9”); in the second we asked for the 1st and 10th position (“8” “5”); and finally for all the elements between the 1st and 4th position (“8” “8” “7” “6”). We can directly indicate a range by using a :. After the colon, we provide the index of the last element (in R), while Python stops just before the index.1 If we want to pass multiple single index values instead of a range in R, we need to create a vector of these indices by using c() (Example 3.6). Take a moment to compare the different ways of indexing between Python and R in Example 3.6!\nIndexing is very useful to access elements and also to create new objects from a part of another one. The last line of our example shows how to create a new array with just the first four entries of scores and store them all as numbers. To do so, we use slicing to get the first four scores and then either change its class using the function as.numeric (in R) or convert the elements to integers one-by-one (Python) (Example 3.6).\n\n\n\n\n\n\n\nExample 3.7 Some more operations on one-dimensional arrays\n\nPython codeR code\n\n\n\n# Appending a new value to a list:\nscores.append(7)\n\n# Create a new list instead of overwriting:\nscores4 = scores + [7]\n\n# Removing an entry:\ndel scores[-10]\n\n# Creating a list containing various ranges\nlist(range(1, 21))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n\nlist(range(-5, 6))\n\n# A range of fractions: 0, 0.2, 0.4, ... 1.0\n# Because range only handles integers, we first\n#   make a range of 0, 2, etc, and divide by 10\n\n[-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n\nmy_sequence = [e / 10 for e in range(0, 11, 2)]\n\n\n\n\n# appending a new value to a vector\nscores = c(scores, 7)\n\n# Create a new list instead of overwriting:\nscores4 = c(scores, 7)\n\n# removing an entry from a vector\nscores = scores[-10]\n\n# Creating a vector containing various ranges\nrange1 = 1:20\nrange2 = -5:5\n\n# A range of fractions: 0, 0.2, 0.4, ... 1.0 \nmy_sequence = seq(0,1, by=0.2)\n\n\n\n\n\n\n\n\nWe can do many other things like adding or removing values, or creating a vector from scratch by using a function (Example 3.7). For instance, rather than just typing a large number of values by hand, we often might wish to create a vector from an operator or a function, without typing each value. Using the operator : (R) or the functions seq (R) or range (Python), we can create numeric vectors with a range of numbers.\nCan we mix different types?. There is a reason that the basic data types (numeric, character, etc.) we described above are called “vector objects” in R: The vector is a very important structure in R and consists of these objects. A vector can be easily created with the c function and can only combine elements of the same type (numeric, integer, complex, character, logical, raw). Because the data types within a vector correspond to only one class, when we create a vector with for example numeric data, the class function will display “numeric” and not “vector”.\nIf we try to create a vector with two different data types, R will force some elements to be transformed, so that all elements belong to the same class. For example, if you re-build the vector of scores with a new student who has been graded with the letter b instead of a number (Example 3.8), your vector will become a character vector. If you print it, you will see that the values are now displayed surrounded by \".\n\n\n\n\n\n\n\nExample 3.8 R enforces that all elements of a vector have the same data type ## R code\n\nscores2 = c(8, 8, 7, 6, 9, 4, 9, 2, 8, 5, \"b\")\nprint(class(scores2))\n\n[1] \"character\"\n\nprint(scores2)\n\n [1] \"8\" \"8\" \"7\" \"6\" \"9\" \"4\" \"9\" \"2\" \"8\" \"5\" \"b\"\n\n\n\n\n\n\nIn contrast to a vector, a list is much less restricted: a list does not care whether you mix numbers and text. In Python, such lists are the most common type for creating a one-dimensional array. Because they can contain very different objects, running the type function on them does not return anything about the objects inside the list, but simply states that we are dealing with a list (Example 3.5). In fact, lists can even contain other lists, or any other object for that matter.\nIn R you can also use lists, even though they are much less popular in R than they are in Python, because vectors are better if all objects are of the same type. R lists are created in a similar way as vectors, except that we have to add the word list before declaring the values. Let us build a list with four different kinds of elements, a numeric object, a character object, a square root function (sqrt), and a numeric vector (Example 3.9). In fact, you can use any of the elements in the list through indexing – even the function sqrt that you stored in there to get the square root of 16!\n\n\n\n\n\n\n\nExample 3.9 Lists can store very different objects of multiple data types and even functions\n\nPython codeR code\n\n\n\nmy_list = [33, \"Twitter\", np.sqrt, [1, 2, 3, 4]]\nprint(type(my_list))\n\n# this resolves to sqrt(16):\n\n<class 'list'>\n\nprint(my_list[2](16))\n\n4.0\n\n\n\n\n\nmy_list = list(33, \"Twitter\", sqrt, c(1,2,3,4))\nclass(my_list)\n\n[1] \"list\"\n\n# this resolves to sqrt(16):\nmy_list[[3]](16)\n\n[1] 4\n\n\n\n\n\n\n\n\n\nPython users often like the fact that lists give a lot of flexibility, as they happily accept entries of very different types. But also Python users sometimes may want a stricter structure like R’s vector. This may be especially interesting for high-performance calculations, and therefore, such a structure is available from the numpy (which stands for Numbers in Python) package: the numpy array. This will be discussed in more detail when we deal with data frames in Chapter 5.\n\n\n\n\n\n\nObject references and mutable objects.\n\n\n\n\n\nA subtle difference between Python and R is how they deal with copying objects. Suppose we define \\(x\\) containing the numbers \\(1,2,3\\) (x=[1,2,3] in Python or x=c(1,2,3) in R) and then define an object \\(y\\) to equal \\(x\\) (y=x). In R, both objects are kept separate, so changing \\(x\\) does not affect \\(y\\), which is probably what you expect. In Python, however, we now have two variables (names) that both point to or reference the same object, and if we change \\(x\\) we also change \\(y\\) and vice versa, which can be quite unexpected. Note that if you really want to copy an object in Python, you can run x.copy(). See Example 3.10 for an example.\nNote that this is only important for mutable objects, that is, objects that can be changed. For example, lists in Python and R and vectors in R are mutable because you can replace or append members. Strings and numbers, on the other hand, are immutable: you cannot change a number or string, a statement such as x=x*2 creates a new object containing the value of x*2 and stores it under the name x.\n\n\n\n\n\n\n\n\n\n\nExample 3.10 The (unexpected) behavior of mutable objects\n\nPython codeR code\n\n\n\nx = [1, 2, 3]\ny = x\ny[0] = 99\nprint(x)\n\n[99, 2, 3]\n\n\n\n\n\nx = c(1,2,3)\ny = x\ny[1] = 99\nprint(x)\n\n[1] 1 2 3\n\n\n\n\n\n\n\n\n\nSets and Tuples. The vector (R) and list (Python) are the most frequently used collections for storing multiple objects. In Python there are two more collection types you are likely to encounter. First, tuples are very similar to lists, but they cannot be changed after creating them (they are immutable). You can create a tuple by replacing the square brackets by regular parentheses: x=(1,2,3).\nSecond, in Python there is an object type called a set. A set is a mutable collection of unique elements (you cannot repeat a value) with no order. As it is not properly ordered, you cannot run any indexing or slicing operation on it. Although R does not have an explicit set type, it does have functions for the various set operations, the most useful of which is probably the function unique which removes all duplicate values in a vector. Example 3.11 shows a number of set operations in Python and R, which can be very useful, e.g. finding all elements that occur in two lists.\n\n\n\n\n\n\n\nExample 3.11 Sets\n\nPython codeR code\n\n\n\na = {3, 4, 5}\nmy_list = [3, 2, 3, 2, 1]\nb = set(my_list)\nprint(f\"Set a: {a}; b: {b}\")\n\nSet a: {3, 4, 5}; b: {1, 2, 3}\n\nprint(f\"intersect:  a & b = {a & b}\")\n\nintersect:  a & b = {3}\n\nprint(f\"union:      a | b = {a | b}\")\n\nunion:      a | b = {1, 2, 3, 4, 5}\n\nprint(f\"difference: a - b = {a - b}\")\n\ndifference: a - b = {4, 5}\n\n\n\n\n\na = c(3, 4, 5)\nmy_vector = c(3, 2, 3, 2, 1)\nb = unique(my_vector)\nprint(b)\n\n[1] 3 2 1\n\nprint(intersect(a,b))\n\n[1] 3\n\nprint(union(a,b))\n\n[1] 3 4 5 2 1\n\nprint(setdiff(a,b))\n\n[1] 4 5\n\n\n\n\n\n\n\n\n\n\n\n3.1.4 Dictionaries\nPython dictionaries are a very powerful and versatile data type. Dictionaries contain unordered2 and mutable collections of objects that contain certain information in another object. Python generates this data type in the form of {key : value} pairs in order to map any object by its key and not by its relative position in the collection. Unlike in a list, in which you index with an integer denoting the position in a list, you can index a dictionary using the key. This is the case shown in Example 3.12, in which we want to get the values of the object “positive” in the dictionary sentiments and of the object “A” in the dictionary grades. You will find dictionaries very useful in your journey as a computational scientist or practitioner, since they are flexible ways to store and retrieve structured information. We can create them using the curly brackets {} and including each key-value pair as an element of the collection (Example 3.12).\nIn R, the closest you can get to a Python dictionary is to use lists with named elements. This allows you to assign and retrieve values by key, however the key is restricted to names, while in Python most objects can be used as keys. You create a named list with d = list(name=value) and access individual elements with either d$name or d[[\"name\"]].\n\n\n\n\n\n\n\nExample 3.12 Key-value pairs in Python dictionaries and R named lists\n\nPython codeR code\n\n\n\nsentiments = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\nprint(type(sentiments))\n\n<class 'dict'>\n\nprint(\"Sentiment for positive:\", sentiments[\"positive\"])\n\nSentiment for positive: 1\n\ngrades = {}\ngrades[\"A\"] = 4\ngrades[\"B\"] = 3\ngrades[\"C\"] = 2\ngrades[\"D\"] = 1\n\nprint(f\"Grade for A: {grades['A']}\")\n\nGrade for A: 4\n\nprint(grades)\n\n{'A': 4, 'B': 3, 'C': 2, 'D': 1}\n\n\n\n\n\nsentiments = list(positive=1, neutral=0, \n                  negative=-1)\nprint(class(sentiments))\n\n[1] \"list\"\n\nprint(glue(\"Sentiment for positive: \",\n           sentiments$positive))\n\nSentiment for positive: 1\n\ngrades =  list()\ngrades$A = 4\ngrades$B = 3\ngrades$C = 2\ngrades$D = 1\n# Note: grades[[\"A\"]] is equivalent to grades$A\nprint(glue(\"Grade for A: {grades[['A']]}\"))\n\nGrade for A: 4\n\nprint(glue(\"Grade for A: {grades$A}\"))\n\nGrade for A: 4\n\nprint(grades)\n\n$A\n[1] 4\n\n$B\n[1] 3\n\n$C\n[1] 2\n\n$D\n[1] 1\n\n\n\n\n\n\n\n\n\nA good analogy for a dictionary is a telephone book (imagine a paper one, but it actually often holds true for digital phone books as well): the names are the keys, and the associated phone numbers the values. If you know someone’s name (the key), it is very easy to look up the corresponding values: even in a phone book of thousands of pages, it takes you maybe 10 or 20 seconds to look up the name (key). But if you know someone’s phone number (the value) instead and want to look up the name, that’s very inefficient: you need to read the whole phone book until you find the number.\nJust as the elements of a list can be of any type, and you can have lists of lists, you can also nest dictionaries to get dicts of dicts. Think of our phone book example: rather than storing just a phone number as value, we could store another dict with the keys “office phone”, “mobile phone”, etc. This is very often done, and you will come across many examples dealing with such data structures. You have one restriction, though: the keys in a dictionary (as opposed to the values) are not allowed to be mutable. After all, imagine that you could use a list as a key in a dictionary, and if at the same time, some other pointer to that very same list could just change it, this would lead to a quite confusing situation.\n\n\n3.1.5 From One to More Dimensions: Matrices and \\(n\\)-Dimensional Arrays\nMatrices are two-dimensional rectangular datasets that include values in rows and columns. This is the kind of data you will have to deal with in many analyses shown in this book, such as those related to machine learning. Often, we can generalize to higher dimensions.\n\n\n\n\n\n\n\nExample 3.13 Working with two- or \\(n\\)-dimensional arrays\n\nPython codeR code\n\n\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(matrix)\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\narray2d = np.array(matrix)\nprint(array2d)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\n\n\n\nmy_matrix = matrix(c(0, 0, 1, 1, 0, 1), \n    nrow = 2, ncol = 3, byrow = TRUE)\nprint(dim(my_matrix))\n\n[1] 2 3\n\nprint(my_matrix)\n\n     [,1] [,2] [,3]\n[1,]    0    0    1\n[2,]    1    0    1\n\nmy_matrix2 = matrix(c(0, 0, 1, 1, 0, 1), \n    nrow = 2, ncol = 3, byrow = FALSE)\nprint(my_matrix2)\n\n     [,1] [,2] [,3]\n[1,]    0    1    0\n[2,]    0    1    1\n\n\n\n\n\n\n\n\n\nIn Python, the easiest representation is to simply construct a list of lists. This is, in fact, often done, but has the disadvantage that there are no easy ways to get, for instance, the dimensions (the shape) of the table, or to print it in a neat(er) format. To get all that, one can transform the list of lists into an array, a datastructure provided by the package numpy (see Chapter 5 for more details).\nTo create a matrix in R, you have to use the function matrix and create a vector of values with the indication of how many rows and columns will be on it. We also have to tell R if the order of the values is determined by the row or not. In Example 3.13, we create two matrices in which we vary the byrow argument to be TRUE and FALSE, respectively, to illustrate how it changes the values of the matrix, even when the shape (\\(2 \\times3\\)) remains identical. As you may imagine, we can operate with matrices, such as adding up two of them.\n\n\n3.1.6 Making Life Easier: Data Frames\nSo far, we have discussed the general built-in collections that you find in most programming languages such as the list and array. However, in data science and statistics you are very likely to encounter a specific collection type that we haven’t discussed yet: the Data frame. Data frames are discussed in detail in Chapter 5, but for completeness we will also introduce them briefly here.\nData frames are user-friendly data structures that look very much like what you find in SPSS, Stata, or Excel. They will help you in a wide range of statistical analysis. A data frame is a tabular data object that includes rows (usually the instances or cases) and columns (the variables). In a three-column data frame, the first variable can be numeric, the second character and the third logical, but the important thing is that each variable is a vector and that all these vectors must be of the same length. We create data frames from scratch using the data.frame() function. Let’s generate a simple data frame of three instances (each case is an author of this book) and three variables of the types numeric (age), character (country where they obtained their master degree) and logic (living abroad, whether they currently live outside the country in which they were born) (Example 3.14). Notice that you have the label of the variables at the top of each column and that it creates an automatic numbering for indexing the rows.\n\n\n\n\n\n\n\nExample 3.14 Creating a simple data frame\n\nPython codeR code\n\n\n\nauthors = pd.DataFrame(\n    {\n        \"age\": [38, 36, 39],\n        \"countries\": [\"Netherlands\", \"Germany\", \"Spain\"],\n        \"living_abroad\": [False, True, True],\n    }\n)\nprint(authors)\n\n   age    countries  living_abroad\n0   38  Netherlands          False\n1   36      Germany           True\n2   39        Spain           True\n\n\n\n\n\nauthors = data.frame(age = c(38, 36, 39), \n  countries = c(\"Netherlands\",\"Germany\",\"Spain\"), \n  living_abroad= c(FALSE, TRUE, TRUE))\nprint(authors)\n\n  age   countries living_abroad\n1  38 Netherlands         FALSE\n2  36     Germany          TRUE\n3  39       Spain          TRUE"
  },
  {
    "objectID": "content/chapter03.html#sec-controlstructures",
    "href": "content/chapter03.html#sec-controlstructures",
    "title": "3  Programming concepts for data analysis",
    "section": "3.2 Simple Control Structures: Loops and Conditions",
    "text": "3.2 Simple Control Structures: Loops and Conditions\n\n\n\n\n\n\nControl structures in Python and R.\n\n\n\n\n\nThis section and the next explain the working of control structures such as loops, conditions, and functions. These exist (and are very useful) in both Python and R. In R, however, you do not need them as much because most functions can work on whole columns in one go, while in Python you often run things on each row of a column and sometimes do not use data frames at all. Thus, if you are primarily interested in using R you could consider skipping the remainder of this chapter for now and returning later when you are ready to learn more. If you are learning Python, we strongly recommend continuing with this chapter, as control structures are used in many of the examples in the book.\n\n\n\nHaving a clear understanding of objects and data types is a first step towards comprehending how object-orientated languages such as R and Python work, but now we need to get some literacy in writing code and interacting with the computer and the objects we created. Learning a programming language is just like learning any new language. Imagine you want to speak Italian or you want to learn how to play the piano. The first thing will be to learn some words or musical notes, and to get familiarized with some examples or basic structures – just as we did in Chapter 2. In the case of Italian or the piano, you would then have to learn some grammar: how to form sentences, how play some chords; or, more generally, how to reproduce patterns. And this is exactly how we now move on to acquiring computational literacy: by learning some rules to make the computer do exactly what you want.\nRemember that you can interact with R and Python directly on their consoles just by typing any given command. However, when you begin to use several of these commands and combine them you will need to put all these instructions into a script that you can then run partially or entirely. Recall Section 1.4, where we showed how IDEs such as RStudio (and Pycharm) offer both a console for directly typing single commands and a larger window for writing longer scripts.\nBoth R and Python are interpreted languages (as opposed to compiled languages), which means that interacting with them is very straightforward: You provide your computer with some statements (directly or from a script), and your computer reacts. We call a sequence of these statements a computer program. When we created objects by writing, for instance, a = 100, we already dealt with a very basic statement, the assignment statement. But of course the statements can be more complex.\nIn particular, we may want to say more about how and when statements need to be executed. Maybe we want to repeat the calculation of a value for each item on a list, or maybe we want to do this only if some condition is fulfilled.\nBoth R and Python have such loops and conditional statements, which will make your coding journey much easier and with more sophisticated results because you can control the way your statements are executed. By controlling the flow of instructions you can deal with a lot of challenges in computer programming such as iterating over unlimited cases or executing part of your code as a function of new inputs.\nIn your script, you usually indicate such loops and conditions visually by using indentation. Logical empty spaces – two in R and four in Python – depict blocks and sub-blocks on your code structure. As you will see in the next section, in R, using indentation is optional, and curly brackets will indicate the beginning ({) and end (}) of a code block; whereas in Python, indentation is mandatory and tells your interpreter where the block starts and ends.\n\n3.2.1 Loops\nLoops can be used to repeat a block of statements. They are executed once, indefinitely, or until a certain condition is reached. This means that you can operate over a set of objects as many times as you want just by giving one instruction. The most common types of loops are for, while, and repeat (do-while), but we will be mostly concerned with so-called for-loops. Imagine you have a list of headlines as an object and you want a simple script to print the length of each message. Of course you can go headline by headline using indexing, but you will get bored or will not have enough time if you have thousands of cases. Thus, the idea is to operate a loop in the list so you can get all the results, from the first until the last element, with just one instruction. The syntax of the for-loop is:\n\nPython codeR code\n\n\n\nfor val in sequence:\n    statement1\n    statement2\n    statement3\n\n\n\n\nfor (val in sequence) {\n    statement1 \n    statement2 \n    statement3\n}\n\n\n\n\nAs Example 3.15 illustrates, every time you find yourself repeating something, for instance printing each element from a list, you can get the same results easier by iterating or looping over the elements of the list, in this case. Notice that you get the same results, but with the loop you can automate your operation writing few lines of code. As we will stress in this book, a good practice in coding is to be efficient and harmonious in the amount of code we write, which is another justification for using loops.\n\n\n\n\n\n\n\nExample 3.15 For-loops let you repeat operations.\n\nPython codeR code\n\n\n\nheadlines = [\n    \"US condemns terrorist attacks\",\n    \"New elections forces UK to go back to the UE\",\n    \"Venezuelan president is dismissed\",\n]\n# Manually counting each element\nprint(\"manual results:\")\n\nmanual results:\n\nprint(len(headlines[0]))\n\n29\n\nprint(len(headlines[1]))\n\n44\n\nprint(len(headlines[2]))\n# and the second is using a for-loop\n\n33\n\nprint(\"for-loop results:\")\n\nfor-loop results:\n\nfor x in headlines:\n    print(len(x))\n\n29\n44\n33\n\n\n\n\n\nheadlines = list(\"US condemns terrorist attacks\", \n  \"New elections forces UK to go back to the UE\",\n  \"Venezuelan president is dismissed\")\n# Manually counting each element\nprint(\"manual results:  \")\n\n[1] \"manual results:  \"\n\nprint(nchar(headlines[1]))\n\n[1] 29\n\nprint(nchar(headlines[2]))\n\n[1] 44\n\nprint(nchar(headlines[3]))\n\n[1] 33\n\n# Using a for-loop\nprint(\"for-loop results:\")\n\n[1] \"for-loop results:\"\n\nfor (x in headlines){\n  print(nchar(x))\n}\n\n[1] 29\n[1] 44\n[1] 33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t repeat yourself!\n\n\n\n\n\nYou may be used to copy-pasting syntax and slightly changing it when working with some statistics program: you run an analysis and then you want to repeat the same analysis with different datasets or different specifications. But this is error-prone and hard to maintain, as it involves a lot of extra work if you want to change something. In many cases where you find yourself pasting multiple versions of your code, you would probably be better using a for-loop instead.\n\n\n\nAnother way to iterate in Python is using list comprehensions (not available natively in R), which are a stylish way to create list of elements automatically even with conditional clauses. This is the syntax:\nnewlist  = [expression for item in list if conditional]\nIn Example 3.16 we provide a simple example (without any conditional clause) that creates a list with the number of characters of each headline. As this example illustrates, list comprehensions allow you to essentially write a whole for-loop in one line. Therefore, list comprehensions are very popular in Python.\n\n\n\n\n\n\n\nExample 3.16 List comprehensions are very popular in Python ## Python code\n\nlen_headlines = [len(x) for x in headlines]\nprint(len_headlines)\n\n# Note: the \"list comprehension\" above is\n#   equivalent to the more verbose code below:\n\n[29, 44, 33]\n\nlen_headlines = []\nfor x in headlines:\n    len_headlines.append(len(x))\nprint(len_headlines)\n\n[29, 44, 33]\n\n\n\n\n\n\n\n\n3.2.2 Conditional Statements\nConditional statements will allow you to control the flow and order of the commands you give the computer. This means you can tell the computer to do this or that, depending on a given circumstance. These statements use logic operators to test if your condition is met (True) or not (False) and execute an instruction accordingly. Both in R and Python, we use the clauses if, else if (elif in Python), and else to write the syntax of the conditional statements. Let’s begin showing you the basic structure of the conditional statement:\n\nPython codeR code\n\n\n\nif condition:\n    statement1\nelif other_condition:\n    statement2\nelse:\n    statement3\n\n\n\n\nif (condition) {\n    statement1\n} else if (other_condition) {\n    statement2\n} else {\n    statement3\n}\n\n\n\n\nSuppose you want to print the headlines of Example 3.15 only if the text is less than 40 characters long. To do this, we can include the conditional statement in the loop, executing the body only if the condition is met (Example 3.17)\n\n\n\n\n\n\n\nExample 3.17 A simple conditional control structure\n\nPython codeR code\n\n\n\nfor x in headlines:\n    if len(x) < 40:\n        print(x)\n\nUS condemns terrorist attacks\nVenezuelan president is dismissed\n\n\n\n\n\nfor (x in headlines){\n  if (nchar(x)<40) {\n    print(x)}\n  }\n\n[1] \"US condemns terrorist attacks\"\n[1] \"Venezuelan president is dismissed\"\n\n\n\n\n\n\n\n\n\nWe could also make it a bit more complicated: first check whether the length is smaller than 40, then check whether it is exactly 44 (elif / else if), and finally specify what to do if none of the conditions was met (else).\nIn Example 3.18, we will print the headline if it is shorter than 40 characters, print the string “What a coincidence!” if it is exactly 44 characters, and print “Too Low” in all other cases. Notice that we have included the clause elif in the structure (in R it is noted else if). elif is a combination of else and if: if the previous condition is not satisfied, this condition is checked and the corresponding code block (or else block) is executed. This avoids having to nest the second if within the else, but otherwise the reasoning behind the control flow statements remains the same.\n\n\n\n\n\n\n\nExample 3.18 A more complex conditional control structure\n\nPython codeR code\n\n\n\nfor x in headlines:\n    if len(x) < 30:\n        print(x)\n    elif len(x) == 44:\n        print(\"What a coincidence!\")\n    else:\n        print(\"Too low\")\n\nUS condemns terrorist attacks\nWhat a coincidence!\nToo low\n\n\n\n\n\nfor (x in headlines) {\n  if (nchar(x)<30) {\n    print(x)\n  } else if (nchar(x)==44) {\n      print(\"What a coincidence!\")\n  } else {\n      print(\"Too low\")\n  }\n}\n\n[1] \"US condemns terrorist attacks\"\n[1] \"What a coincidence!\"\n[1] \"Too low\""
  },
  {
    "objectID": "content/chapter03.html#sec-functions",
    "href": "content/chapter03.html#sec-functions",
    "title": "3  Programming concepts for data analysis",
    "section": "3.3 Functions and Methods",
    "text": "3.3 Functions and Methods\nFunctions and methods are fundamental concepts in writing code in object-orientated programming. Both are objects that we use to store a set of statements and operations that we can use later without having to write the whole syntax again. This makes our code simpler and more powerful.\nWe have already used some built-in functions, such as length and class (R) and len and type (Python) to get the length of an object and the class to which it belongs. But, as you will learn in this chapter, you can also write your own functions. In essence, a function takes some input (the arguments supplied between brackets) and returns some output. Methods and functions are very similar concepts. The difference between them is that the functions are defined independently from the object, while methods are created based on a class, meaning that they are associated with an object. For example, in Python, each string has an associated method lower, so that writing 'HELLO'.lower() will return ‘hello’. In R, in contrast, one uses a function, tolower('HELLO'). For now, it is not really important to know why some things are implemented as a method and some are implemented as a function; it is partly an arbitrary choice that the developers made, and to fully understand it, you need to dive into the concept of classes, which is beyond the scope of this book.\n\n\n\n\n\n\nTab completion.\n\n\n\n\n\nBecause methods are associated with an object, you have a very useful trick at your disposal to find out which methods (and other properties of an object) there are: TAB completion. In Jupyter, just type the name of an object followed by a dot (e.g., a.<TAB> in case you have an object called a) and hit the TAB key. This will open a drop-down menu to choose from.\n\n\n\nWe will illustrate how to create simple functions in R and Python, so you will have a better understanding of how they work. Imagine you want to create two functions: one that computes the 60% of any given number and another that estimates this percentage only if the given argument is above the threshold of 5. The general structure of a function in R and Python is:\n\nPython codeR code\n\n\n\ndef f(par1, par2=0):\n    statements\n    return return_value\n\nresult = f(arg1, arg2)\nresult = f(par1=arg1, par2=arg2)\nresult = f(arg1, par2=arg2)\nresult = f(arg1)\n\n\n\n\nf = function(par1, par2=0) {\n   statements \n   return_value\n}\nresult = f(arg1, arg2)\nresult = f(par1=arg1, par2=arg2)\nresult = f(arg1, par2=arg2)\nresult = f(arg1)\n\n\n\n\nIn both cases, this defines a function called f, with two arguments, arg_1 and arg_2. When you call the function, you specify the values for these parameters (the arguments) between brackets after the function name. You can then store the result of the function as an object as normal.\nAs you can see in the syntax above, you have some choices when specifying the arguments. First, you can specify them by name or by position. If you include the name (f(param1=arg1)) you explicitly bind that argument to that parameter. If you don’t include the name (f(arg1, arg2)) the first argument matches the first parameter and so on. Note that you can mix and match these choices, specifying some parameters by name and others by position.\nSecond, some functions have optional parameters, for which they provide a default value. In this case, par2 is optional, with default value 0. This means that if you don’t specify the parameter it will use the default value instead. Usually, the mandatory parameters are the main objects used by the function to do its work, while the optional parameters are additional options or settings. It is recommended to generally specify these options by name when you call a function, as that increases the readability of the code. Whether to specify the mandatory arguments by name depends on the function: if it’s obvious what the argument does, you can specify it by position, but if in doubt it’s often better to specify them by name.\nFinally, note that in Python you explicitly indicate the result value of the function with return value. In R, the value of the last expression is automatically returned, although you can also explicitly call return(value).\nExample 3.19 shows how to write our function and how to use it.\n\n\n\n\n\n\n\nExample 3.19 Writing functions\n\nPython codeR code\n\n\n\n# The first function just computes 60% of the value\ndef perc_60(x):\n    return x * 0.6\n\nprint(perc_60(10))\n\n6.0\n\nprint(perc_60(4))\n\n# The second function only computes 60% it the\n#  value is bigger than 5\n\n2.4\n\ndef perc_60_cond(x):\n    if x > 5:\n        return x * 0.6\n    else:\n        return x\n\nprint(perc_60_cond(10))\n\n6.0\n\nprint(perc_60_cond(4))\n\n4\n\n\n\n\n\n#The first function just computes 60% of the value\nperc_60 = function(x) x*0.6\n\nprint(perc_60(10))\n\n[1] 6\n\nprint(perc_60(4))\n\n[1] 2.4\n\n# The second function only computes 60% it the\n#  value is bigger than 5\nperc_60_cond = function(x) {\n  if (x>5) {\n    return(x*0.6)\n  } else {\n    return(x)\n  }\n}\nprint(perc_60_cond(10))\n\n[1] 6\n\nprint(perc_60_cond(4))\n\n[1] 4\n\n\n\n\n\n\n\n\n\nThe power of functions, though, lies in scenarios where they are used repeatedly. Imagine that you have a list of 5 (or 5 million!) scores and you wish to apply the function perc_60_cond to all the scores at once using a loop. This costs you only two extra lines of code (Example 3.20).\n\n\n\n\n\n\n\nExample 3.20 Functions are particular useful when used repeatedly\n\nPython codeR code\n\n\n\n# Apply the function in a for-loop\nscores = [3, 4, 5, 7]\nfor x in scores:\n    print(perc_60_cond(x))\n\n3\n4\n5\n4.2\n\n\n\n\n\n# Apply the function in a for-loop\nscores = list(3,4,5,6,7)\nfor (x in scores) {\n  print(perc_60_cond(x))\n}\n\n[1] 3\n[1] 4\n[1] 5\n[1] 3.6\n[1] 4.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nA specific type of Python function that you may come across at some point (for instance, in Section 12.2.2) is the generator. Think of a function that returns a list of multiple values. Often, you do not need all values at once: you may only need the next value at a time. This is especially interesting when calculating the whole list would take a lot of time or a lot of memory. Rather than waiting for all values to be calculated, you can immediately begin processing the first value before the next arrives; or you can work with data so large that it doesn’t all fit into your memory at the same time. You recognize a generator by the yield keyword instead of a return keyword (Example 3.21)\n\n\n\n\n\n\n\n\n\n\nExample 3.21 Generators behave like lists in that you can iterate (loop) over them, but each element is only calculated when it is needed. Hence, they do not have a length. ## Python code\n\nmylist = [35, 2, 464, 4]\n\ndef square1(somelist):\n    listofsquares = []\n    for i in somelist:\n        listofsquares.append(i**2)\n    return listofsquares\n\nmysquares = square1(mylist)\nprint(\n    f\"\"\"As a list:\ntype: {type(mysquares)}\nrepresentation: {mysquares}\nentries: {[x for x in mysquares]}\nsecond pass: {[x for x in mysquares]}\nlength: {len(mysquares)}\"\"\"\n)\n\nAs a list:\ntype: <class 'list'>\nrepresentation: [1225, 4, 215296, 16]\nentries: [1225, 4, 215296, 16]\nsecond pass: [1225, 4, 215296, 16]\nlength: 4\n\ndef square2(somelist):\n    for i in somelist:\n        yield i**2\n\nmysquares = square2(mylist)\nprint(\n    f\"\"\"As a generator:\ntype: {type(mysquares)}\nrepresentation: {mysquares}\nentries: {[x for x in mysquares]}\nsecond pass: {[x for x in mysquares]}\"\"\"\n)\n# This throws an error (generators have no length)\n# print(f\"length: {len(mysquares)}\")\n\nAs a generator:\ntype: <class 'generator'>\nrepresentation: <generator object square2 at 0x7f6020629770>\nentries: [1225, 4, 215296, 16]\nsecond pass: []\n\n\n\n\n\n\nSo far you have taken your first steps as a programmer, but there are many more advanced things to learn that are beyond the scope of this book. You can find a lot of literature, online documentation and even wonderful Youtube tutorials to keep learning. We can recommend the books by Crawley (2012) and VanderPlas (2016) to have more insights into R and Python, respectively. In the next chapter, we will go deeper into the world of code in order to learn how and why you should re-use existing code, what to do if you get stuck during your programming journey and what are the best practices when coding. [^1]: In both R and Python, the equals sign (=) can be used to assign values. In R, however, the traditional way of doing this is using an arrow (<-). In this book we will use the equals sign for assignment in both languages, but remember that for R, x=10 and x<-10 are essentially the same.\n\n\n\n\nCrawley, Michael J. 2012. The r Book. 2nd Edition. Wiley.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly."
  },
  {
    "objectID": "content/chapter04.html#sec-code",
    "href": "content/chapter04.html#sec-code",
    "title": "4  How to write code",
    "section": "4.1 Re-using Code: How Not to Re-Invent the Wheel",
    "text": "4.1 Re-using Code: How Not to Re-Invent the Wheel\nJust as in any human language, programming languages also consist of a vocabulary, syntax rules, and expressions. Using the proper words and grammar, you can build from scratch any idea your imagination allows. That’s a wonderful thing! But, let’s be honest: the language itself, the expressions, ideas, and all the abstract constructs seldom come originally from you. And in fact, that’s great as well: otherwise, you’d have to deeply think of every element before talking and expressing any thought. Instead, you use pre-existing rules, ideas, perceptions, and many different narratives to create your own messages to interact with the world. It’s the same with coding: you never start from scratch.\nOf course you can code anything you want from the very beginning, even just using 0’s and 1’s! When reading through the previous chapters, maybe you even started to think that complex operations will be exhausting and will take a really long time. After all, from the basic operations we did to a useful statistical model seems like a long way to go.\nLuckily, this is not the case. There is almost no project in which computational scientists, data analysts, or developers do not re-use earlier code in order to achieve their goals more quickly and efficiently. The more common a task is, the greater the chance that you do not have to re-invent the wheel. Of course, you have to give credit where credit is due, but it is not uncommon to paste code snippets from others into your own code and adapt them. This is especially true for standard operations, for which there are only so many ways to achieve the desired result.\nThere are different ways to re-use earlier code. One is to copy and adapt raw lines of code written by someone else or by yourself in the past. In fact, there are many online repositories such as GitHub or BitBucket that contain many programs and well-documented code examples (see Section 4.3). When conducting computational analyses, you will spend a significant part of your time in such repositories trying to understand what others have done and figuring out how you can use it in your own work. Of course, make sure that the license of the code allows you to use it in the way you want. Also, give credit where credit is due: at the very least, place a comment with a link in your code to indicate what inspired you.\nAnother way is to build or import functions that summarize many lines of code into a simpler command, as we explained in Section 3.3. Functions are indeed powerful ways of reusing code, since you do not have to write the same code over and over again if you need it in multiple places. Packages are probably the most elegant approach to recycle the work done by other colleagues. In Section 1.4 you already learned how to install a package, and you probably noticed how easy it is to bring many pre-built functionalities onto your workspace. You can also write and publish your own package in the future to help your colleagues to write less code and to be more efficient in their daily job (see also Section 15.3)!\nMany questions can arise here: what to re-use? When to use a function written by someone else instead of writing the code yourself? Which scripts and sources are trustworthy? Which is the best package to choose? How many packages should we use within the same project? Should we care about package versions? And must we know every package that is released in our field? There are of course multiple answers to these questions and it will be probably a matter of practice how to obtain the most appropriate ones. In general, we can say that one premise is to re-use and share code as much as you can. This idea is limited by constraints of quality, availability, parsimony, updates, and expertise. In other words, when recycling code we should think of the reputation of the source, the difficulty of accessing it, the risk of having an excessive and messy number of inputs, the need to share the last developments with your colleagues, and the fact that you will never be able to know everything.\nLet’s take an example. Imagine you want to compute the Levenshtein distance between two strings. That’s a pretty straightforward metric that answers the question: “How many edits (removing/changing/inserting characters) do I need to apply to transform string1 into string2?” It can be used for plagiarism detection, but may be interesting for us to determine, for instance, whether a newspaper copied some content from somewhere else, even if small changes have been applied. You could now try to write some code to calculate that (and we are sure you could do that if you invested some time in it!), but it is such a common problem that it has been solved multiple times before. You could, for instance, look up some functions that are known to solve the problem and copy-paste them into your code. You can find a large number of different implementations for both Python and R here: en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance. You can then choose and copy-paste the one which is most appropriate for you. One that is very fast, because you want to compare a huge set of strings? One that is easy to understand? One that uses only a few lines of code to not distract the reader? Alternatively, if you look for available packages for Python and R, you see that there are multiple packages that you can install with install.packages (R) or pip (Python) and then import. If you go for that route, you don’t need to care about the internal workings and can “abstract away” and outsource the problem – on the other hand, the users of your code now have one more dependency to install before they can use your code.\nIn the case of package selection, we understand it can be quite overwhelming, with so many different packages from different contributors. In fact, sometimes the same task, such as topic modeling, can be done using multiple different packages. So, how to find and choose the best package? Besides resources like this book, the most important guide is probably the community around you: using packages that a lot of other people also use means that the package is probably well maintained and documented, and that there is a community to ask for help if needed. Since all packages on Pypi and CRAN are free to download and install, however, you can also shop around and see what the various packages do. When comparing different packages, it is always good to check their documentation and their GitHub page: packages that are well documented and that are updated frequently are often a good choice.\nFor example, the authors of this book had several intensive discussions of which packages to mention and use in the proposed exercises, an issue that became complex given the variety of topics addressed in this book. In the case of text analysis, a library such as NLTK for Python was incredibly popular among computational analysts until a few years ago becoming a package of reference in the field, but it has – at least for some applications – been overpassed by friendly and sophisticated new packages for natural language processing like SpaCy. So, which should we have included in this book? The one which is well-known (with excellent documentation by the way) and still used by thousands of practitioners and students around the world, or the one which is penetrating the market because of its easiness and advantages? Moreover, when choosing the second option, are we sure a more trendy package is going to be stable in time or is it going to be superseded by a different one in just few months?\nThere isn’t the one golden way of how to re-use code and packages, but this dynamic scenario also depicts an exciting and provocative field that forces us to keep ourselves up to date."
  },
  {
    "objectID": "content/chapter04.html#sec-errors",
    "href": "content/chapter04.html#sec-errors",
    "title": "4  How to write code",
    "section": "4.2 Understanding Errors and Getting Help",
    "text": "4.2 Understanding Errors and Getting Help\nEven though re-using code makes writing programs easier and less error-prone, every programmer makes mistakes. Programming can be a frustrating endeavor, and you will encounter error messages, bugs, and problems that you don’t know how to fix. This section shows how error messages are useful rather than scary and lists the main error messages encountered in the beginning. It explains how to search for help within the R/Python documentation, how to use online resources, and how to formulate questions to your instructor or community so you get a useful answer.\nIf you tried out some of the concepts in Chapter 3, you have probably already come across some typical or basic errors in programming. Maybe you tried to call a function from a library that you forgot to load before, or maybe you tried to multiply a string with a float. There are thousands of errors that you will encounter, and there is no exhaustive list of them, so you won’t find a complete structured catalogue to solve your problems when coding. This might seem a rough road for any scientist but in fact you will get used to finding the answers by different means.\n\n4.2.1 Error Messages\nThere are two common strategies to avoid getting stuck and move on with your task: one is to understand the type of error you are getting, and the other is to know where to go to obtain valuable help. We would add a third one: be patient and do not despair!\nBoth R and Python produce warning or error messages when something is wrong in your code. Beginning computational researchers may sometimes feel afraid, confused, or even frustrated when they get such a painful message (we have all felt this) and some then would become so anxious that they don’t pay enough attention to the text of the error message thinking it will not be helpful to solve the problem and blaming themselves for not being a perfect programmer. But the more you code, the more you realize that getting these error messages is just part of the routine and that it is very useful to carefully read the warning instead of skipping it.\nIn most cases, the error message in your console will tell you exactly where the problem is: a specific line or operation within your code. With this information in many cases you will quickly identify what the problem is about and you will know how to solve it. One of the most common causes for errors is just very silly typos!\nNext to the location (the line number), the error message will also tell you more about the problem. For example, when trying to multiply the float object a by the string object b you will get “Error in a * b : non-numeric argument to binary operator” in R or “TypeError: can’t multiply sequence by non-int of type ‘float’” in Python. As intimidating as this language may sound in the first place, if you re-read it, you will realize that it, in fact, explains exactly what went wrong. This helps you to understand what you did wrong and enable you to fix it.\nIf you get a warning error that you don’t understand or get an incorrect result in your code you have three options to get more information: use the help commands to know more about any object or function (help(object) in both R and Python); read the documentation of base R, base Python or of any individual package (there are plenty of them online!); and look at the wonderful community of worldwide coders, read what they have discussed so far or even pose a question to challenge their minds.\nLet’s consider this third option. Imagine you read the text of an error message and you feel you don’t understand it. It may be because the wording is too complex or because it just gives an “error code” (i.e. “error 401 - Unauthorized” when trying to connect to the Twitter API). If your first thought is to try searching for it in Google, then this is completely correct: it might take you to code documentation, or better to an online discussion in sites such as Stack Overflow, which is a useful question and answer website for coders (see Figure 4.1). It is very likely that some colleagues have already posed a question about the meaning of that error and others have already provided an answer to what it means and especially help with how to fix it.\n\n\n\nFigure 4.1: A online discussion in Stack Overflow about a warning message\n\n\nDepending on the complexity and novelty of your problem you might find a helpful answer in a few minutes or it might take you hours. Never get desperate if you visit many discussions without understanding everything directly: you may have to come back to some of them after reading all. Moreover, some answers will include the exact code you need (ready for copy-and-paste), code to be adapted (i.e. changing the name of your variables) or in pseudocode (informal description of the code). In all of the cases you will be the responsible for making sense of the huge (and sometimes messy) amount of sources you will come across.\nIt is of course possible that you don’t get what you need in previous discussions. In that case you will be able to create your own question and wait for someone to reply. If you decide to do this, take some advice into account. First, be sure that the answer is not elsewhere within the same website (a first answer could be just a link to a previous post!). Second, don’t worry that your question is silly or too basic: you will find in the community all kinds of coders, from those who are taking their first steps to those who are very advanced. Third, be clear, specific, and focus on what you need to solve. This is probably the most important advice since it is necessary that other coders understand what you need in a few words (not philosophical discussions or previous elaborated rationales) so they can decide to spend some minutes of their time and help you. It is a very common practice that you copy in the questions the warning message or the code you are having trouble with because peers can even fix it themselves and give the solution right away. Do not worry if your post receives a lot of replies after getting what you needed. This thread might also help others in the future!\n\n\n4.2.2 Debugging Strategies\nIt’s not always straightforward to understand what is going wrong. Maybe your script does not even produce an error message, but just produces some unexpected result.\nOf course, every program is different and there is not one way to solve every issue, but there are some simple strategies that help you debugging your code. The underlying core principle is to better understand what exactly is happening.\n\nPrint more. For example, if you have a for-loop, just add a print statement to the loop that prints the current value that is processed, or some other information that helps you understanding what data exactly are processed, or what intermediate result is achieved at which step. There are more advanced tools for keeping track of values, such as the so-called debuggers in advanced IDEs or the logging module in Python, but a couple of extra print functions can serve the same purpose.\n\nKeep track of which code blocks have been executed how often. For instance, maybe you have some if statement, but the condition is simply never True, so that the whole code block is never executed. You can create an integer with value 0 at the beginning of the code, and then increment it by one within the code block. If you print it afterwards, you know how often the block has been visited.\nCut it down. Remove (comment out) everything that is not strictly necessary and see whether you can make a simplified version of your code run, before you extend it.\nAdd consistency checks. For instance, if from a theoretical point of view, two lists need to have the same length, check it with the length function; similarly, if you know that an object must have a specific value (e.g., because you know the result), check this assumption.\n\n\nFinally, when you know that some typical errors may arise and you don’t want your script to stop or crash, you can add an exception in your code. Suppose for example that you are building a function to connect to an API (see Section 12.1). There might be many reasons for getting an error, such as an Internet connection problem, a server issue, or a missing document. You might decide to skip the error and continue the next lines or you could even give more detailed instructions of what to do (i.e. wait five minutes and try again). The inclusion of these exceptions are in fact a good practice and will help your code to be more robust and stable.\nLet’s make Example 3.17 from the previous chapter more robust so that it does not fail if an invalid headline is passed. For instance, in Python, the object None has no defined length; and in R, it is illegal to calculate the number of characters in a factor. It is a good idea to think about how you want to deal with this: either you want your script to just fail (and clean up the data), or you may want to deal with the error in some way. Especially if you have little control over the input data and/or if the process you are dealing with takes a long time, you may want to handle these errors rather than having your script fail. In Example 4.1, we show how to use such a try/except construction: you indicate which code block you want to try (e.g., run as normal); and in the next block, you indicate what should happen if that results in an error.\nNote that using try … except statements like this is fairly common in Python code, in R it is not needed as frequently. In many cases where a Python function like int raises an exception if the input cannot be converted to an integer, the R function as.numeric just returns a missing value. Thus, in R you normally only encounter these statements when using external resources, for example when using an API or scraping a web page. See Chapter 12 for more details on these topics.\n\n\n\n\n\n\n\nExample 4.1 Error handling.\n\nPython codeR code\n\n\n\nheadlines = (\n    \"US condemns terrorist  attacks\",\n    None,\n    \"Venezuelan president is dismissed\",\n)\n\nfor x in headlines:\n    try:\n        # Getting len of None will raise an error\n        if len(x) < 40:\n            print(x)\n    except:\n        print(f\"{x} is not a valid headline\")\n\nUS condemns terrorist  attacks\nNone is not a valid headline\nVenezuelan president is dismissed\n\n\n\n\n\nheadlines = list(\"US condemns terrorist attacks\", \n  NA, \"Venezuelan president is dismissed\")\n\nfor (x in headlines){\n  tryCatch(\n      # Getting nchar of NA will raise an error\n      if (nchar(x)<40) print(x),\n      error=function(error_condition) {\n        print(paste(x, \"is not a valid headline\"))\n    }\n  )\n}\n\n[1] \"US condemns terrorist attacks\"\n[1] \"NA is not a valid headline\"\n[1] \"Venezuelan president is dismissed\""
  },
  {
    "objectID": "content/chapter04.html#sec-practices",
    "href": "content/chapter04.html#sec-practices",
    "title": "4  How to write code",
    "section": "4.3 Best Practice: Beautiful Code, GitHub, and Notebooks",
    "text": "4.3 Best Practice: Beautiful Code, GitHub, and Notebooks\nThis section gives a brief explanation of “computational hygiene”: how to structure your code so you can understand it later, the importance of naming and documentation, the use of versioning and online repositories such as GitHub, and the use of literate programming (such as through the use of RMarkdown or Jupyter notebooks) to explain, share, and publish code.\nCoding is more than learning the basic rules and creating a message. If you want to use code to communicate ideas and to work with peers you have to take care of many content and shape details in order to guarantee the comprehension and reproducibility of the scripts. It even applies to the code you write for “private use” because it is highly likely that you will forget your original thoughts from one day to another, or that you later realize that you need to share it with someone else to ask for help. Thus instead of writing personal, hidden and illegible code without adopting any social conventions, you should dedicate some extra effort to make your scripts easy and ready to share.\nThe first step of the computational hygiene is within the code itself. Every time you create an object, a variable, or a function, you have to take many apparently unimportant decisions such as giving a name, separating words, lines, or blocks, and including comments. These decisions are personal, but should mostly depend on social conventions in order to be useful. As you may imagine, there are many of these conventions for general programming and specially for specific languages. To mention just a few, you can find an “official” style guide for Python1 or Google’s R style guide2. Some of these guides are extensive (they cover every detail) and some are more general or abstract. You do not have to see them as a “bible” that needs to be strictly adhered to in each and every situation, but they offer very good guidance for best practice. In fact, even when you find them useful it is true that you will probably learn more of these practices from reading good examples, and especially from interacting with a specific community and its rules.\nWe will mention some general guidelines that apply for both R and Python. If it is the first time you are venturing into the world of code you will find this advice useful, but if you are a more advanced learner you will probably get more specific knowledge in the more detailed sources for each language and community.\nIn the case of naming, we encourage you to use meaningful names or standard abbreviations to objects, using lower-case or mixed-case (remember both Python and R are case-sensitive!), avoiding special characters and operators (such as &, @ or %), and not exceeding 32 characters. You normally begin with a letter3 (an upper-case when defining a class), followed by other letters or numbers, and using underscores to separate the words if necessary (i.e. data_2020 or Filter_Text). Some suggest that variable names should be nouns and function names should be verbs, which seems logical if you think of the nature of these objects.\nWhen writing code, please also take into consideration white space and indentations, because you should use them to give proper structure to the code by creating the block statements. In the case of R, also pay attention to the use of curly braces: the convention is that the opening curly brace begins after some code and is always followed by a new line; and the closing curly brace is in its own line except if there are more instructions in the block. Do not write very long lines (more than 80 characters) to help your code fit the screen and avoid lateral scrolling. Good separation of words, lines and blocks will make your script more readable!\nNow, if you want to make your code highly understandable and shareable, you have to include documentation. This is probably a very basic dimension of coding but unfortunately some authors forget to take the few minutes it takes to describe what their script does (and why), making your journey more difficult. An essential good practice in coding is to include enough information to clarify your code when it is not clear by itself. You can do this in different ways (even by writing a separate codebook), but the most natural and straightforward manner is to include some comments in the code. These comments should be included both at the beginning of the script to give an overview or introduction to the code, and within the script (in independent lines or at the end of a line) to give specific orientations. In many cases, you will need to read your code later (for example when you need to revise an article or analysis), and a short time spent documenting your code will save you a lot of time later.\nR and Python use the hash sign \\# to create these comments. The comment will always begin after the hash. If the first character in your line is a \\# all the text included will be considered as a comment; but if you have already written some code in a line and include a \\# after the code, the initial code will be executed and you will always see the comment by its side. You will normally combine these two ways of documenting your script. As a rule of thumb, insert a comment if the code itself is not obvious, and explain the choices and intentions of the code. So, if a line says df = df - 1, a comment like Decrease df by one is not very useful (as that is obvious from the code), but a comment like Remove one degree of freedom since we estimated the mean does help, as it makes it clear why we are subtracting one from the df object.\nAdditionally, Python and R encourage the use of so-called docstrings: In Python, place a string surrounded by triple quotation marks at the start of a function; in R, place a comment #' right above the function. 4 In this documentation, you can explain what the function does and what parameters it requires. The nice thing is that if properly used, docstrings are automatically displayed in help functions and automatically generated documentation.\nAnother way to make your code more beautiful and, crucially, easier to re-use by others and yourself is to make your code as generic as possible. For instance, imagine you need to calculate the sum of the length of two texts, “Good morning!” and “Goodbye!”. You could just write x = 13 + 8. But what if the strings change in the future? And how to remember what 13 + 8 was supposed to mean? Instead of using such hardcoded values, you can therefore write it better as x = len(\"Good morning!\") + len(\"Goodbye\") (for R, replace len by nchar). But the strings themselves are still hardcoded, so you can create these strings and assign them the names s1 and s2 first, and then just calculate x = len(s1) + len(s2). In practice, these types of generalization often involve the uses of functions (Section 3.3) and loops (Section 3.2.1). So, don’t use hard-coded values or “magic numbers”: circumference=6.28*r is much less clear than PI=3.14; circumference=2*PI*r.\nMoreover, you must be aware that your code is dynamic and it will normally evolve over time. For example, you may have different files (.py or .R) containing different versions of your script, though this is normally inefficient and chaotic. In order to have a more powerful control of versions and to track changes, coders usually use online repositories to host their scripts for private use and especially to share them. And there are many of these sites, but we believe that GitHub5 is the most well-known and is preferred by data scientists (Figure 4.2 shows the repository we used to write this book).\n\n\n\nFigure 4.2: The online repository GitHub.\n\n\nOnce you upload (or commit and push) your code to GitHub, you can access it from anywhere, and will be able to track the historical changes, which in practice will allow you to have multiple versions in the very same place. You will decide if you make the code public or keep it private, and who to invite to edit the repository. When working collaboratively you it will feel like editing a wiki of code, while having a webpage for your project and a network of friends (followers), will be similar to social media. You can work locally or even from a web interface, and then synchronize the changes. When you allow colleagues to download (or clone) your repository you are then making a good contribution to the community of developers and you can also monitor your impact. In addition to code, you can also upload other kinds of files, including notebooks, and organize them in folders, just as you have on your own computer.\nOne extended good practice when sharing code is the use of literate programming, which is an elegant, practical, and pedagogic way to document and execute the base code. We have already mentioned in this section the importance of including documentation within your code (i.e. using the \\# sign and docstrings), but you also have the opportunity to extend this documentation (with formatted texts, images and even equations!) and put everything together to present in a logical structure everything necessary to understand the code and to run the executable lines step by step.\nThere are different approaches to implement this literate programming in web and local environments, but the standard in R and Python is the use of notebooks. In a notebook you can alternate a text processor with an executable cell to place formatted text between blocks of code. By doing this you can include complete documentation of your scripts, and even more important you can execute each cell one step at a time (loading the results in memory while the notebook is open). This last point allows you avoid the risk of executing the whole script at once, and also gives you more control of the intermediate outputs produced in your code. Once you get used to notebooks, you will probably never write code for data analysis in a basic editor again!\nThe usual tool in R is the R Markdown Notebook, and in Python the Jupyter Notebook (see figure 4.3), but in practice you can also deploy Python in Markdown and R in Jupyter. Both tools can help you with similar tasks to organize your script, though their internal technical procedures are quite different. We have chosen Jupyter to develop the examples in this book because it is a web-based interactive tool. Moreover, there are several services such as Google Colab6 (Figure 4.4), that allow you to remotely run these notebooks online without installing anything on your computer, making the code highly reproducible.\n\n\n\nFigure 4.3: Markdown (left) and Jupyter (right) Notebooks\n\n\nSo far you have seen many of the possibilities that the world of code offers you from a technical and collaboration perspective. We will come back to ethical and normative considerations throughout the book, in particular in Section 12.4 and Section 16.3.\n\n\n\nFigure 4.4: Jupyter notebook in Google Colab"
  },
  {
    "objectID": "content/chapter05.html#sec-dataframes",
    "href": "content/chapter05.html#sec-dataframes",
    "title": "5  From file to data frame and back",
    "section": "5.1 Why and When Do We Use Data Frames?",
    "text": "5.1 Why and When Do We Use Data Frames?\nIn Section 3.1, we introduced basic data types: strings (which contain text), integers (which contain whole numbers, or numbers without anything “behind the dot”), floats (floating point numbers; numbers with decimals), and bools (boolean values, True or False). We also learned that a series of multiple values (e.g., multiple integers, multiple strings) can be stored in what we call a vector (R) or a list (Python).\nIn most social-scientific applications, however, we do not deal with isolated series of values. We rather want to link multiple values to each other. One way to achieve this is by the use of dictionaries (see Section 3.1). Such data structures are really useful for nested data: For example, if we do not want to only store people’s ages, but also their addresses, we could store a dict within a dict.\nIn fact, as we will see later in this chapter, much of the data used by computational social scientists comes in such a format. For instance, data about an online product can contain many reviews which in turn have various pieces of information on the review author.\nBut ultimately, for many social-scientific analyses, a tabular data format is preferred. We are used to thinking of observations (cases) as rows with columns containing information or measurements about these observations (e.g., age, gender, days per week of newspaper reading, …). It also simplifies how we can run many statistical analyses later on.\nWe could simply construct a list of lists to achieve such a tabular data format. In fact, this list-of-lists technique is often used to store tabular data or matrices, and you will probably encounter it in some examples in this book or elsewhere. The list-of-lists approach is very low-level, though: if we wanted, for instance, to insert a column or a row at a specific place, writing the code to do so could be cumbersome. There are also no things like column headers, and no consistency checks: nothing would warn us if one row actually contained more “columns” than another, which should not be the case in a rectangular table.\nTo make our lives easier, we can therefore use a data structure called a data frame. Data frames can be generated from list-of-list structures, from dictionaries, and many others. One way of doing this is shown in Example 5.1, but very often, you’d rather read data from a file or an online resource directly into a data frame (see Section 5.2).\n\n\n\n\n\n\n\nExample 5.1 Creating a data frame from other data structures\n\nPython codeR code\n\n\n\n# Create two lists that will be columns\nlist1 = [\"Anna\", \"Peter\", \"Sarah\", \"Kees\"]\nlist2 = [40, 33, 40, 77]\n\n# or we could have a list of lists instead\nmytable = [[\"Anna\", 40], [\"Peter\", 33], [\"Sarah\", 40], [\"Kees\", 77]]\n\n# Convert an array to a dataframe\ndf = pd.DataFrame(mytable)\n\n# Or create the data frame directly from vectors\ndf2 = pd.DataFrame.from_records(zip(list1, list2))\n\n# No. of rows, no. of columns, and shape\nprint(f\"{len(df)} rows x {len(df.columns)} cols\")\n\n4 rows x 2 cols\n\nprint(f\"Its shape is {df.shape}\")\n\nIts shape is (4, 2)\n\nprint(\"Element-wise equality of df and df2:\")\n\nElement-wise equality of df and df2:\n\nprint(df == df2)\n\n      0     1\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n\n\n\n\n\n# Create two vectors that will be columns\nvector1 <- c(\"Anna\",\"Peter\",\"Sarah\",\"Kees\")\nvector2 <- c(40,33,40,77)\n\n# Create an array of four rows and two columns\nmyarray <- array(c(vector1,vector2), dim=c(4,2))\n\n# Convert an array to a dataframe\ndf1=data.frame(myarray)\n\n# Or create the data frame directly from vectors\ndf2=data.frame(vector1, vector2)\n\n# No. of rows, no. of columns, and dimension\nprint(glue(\"{ncol(df1)} rows x {nrow(df1)} cols\"))\n\n2 rows x 4 cols\n\nprint(dim(df1))\n\n[1] 4 2\n\nprint(\"Element-wise equality of df1 and df2:\")\n\n[1] \"Element-wise equality of df1 and df2:\"\n\nprint(df1 == df2)\n\n       X1   X2\n[1,] TRUE TRUE\n[2,] TRUE TRUE\n[3,] TRUE TRUE\n[4,] TRUE TRUE\n\n\n\n\n\n\n\n\n\nIn this book, we use data frames a lot, because they are very convenient for handling tabular data, and because they provide a lot of useful functionalities, instead of requiring us to re-invent the wheel all the time. In the next section, we will discuss some of them.\nOf course, there are some situations when data frames are not a good choice to organize your data: - Your data is one-dimensional. Think, for example, of resources like a list of stopwords, or a list of texts without any meta-information. - Your data do not have a tabular structure. Think, for example, of deeply nested data, network data or of very messy data. - Your data are so large that you cannot (or do not want to) load it into memory. For instance, if you want to process the text of all articles on Wikipedia, you probably want to process them one-by-one instead of loading all articles at the same time.\nTherefore, you will come across (and we will introduce you to) examples in which we do not use data frames to organize our data. But in most cases we will, because they make our life easier: once we have constructed our data frame, we have a range of handy functions at our disposal that allow us to select rows or columns, add new rows or columns, apply functions to them, and so on. We will discuss these in Chapter 6.\nBut how do we – toy examples like those in Example 5.1 aside – get data into and out of data frames?"
  },
  {
    "objectID": "content/chapter05.html#sec-reading",
    "href": "content/chapter05.html#sec-reading",
    "title": "5  From file to data frame and back",
    "section": "5.2 Reading and Saving Data",
    "text": "5.2 Reading and Saving Data\n\n5.2.1 The Role of Files\nIn statistical software like SPSS or Stata, or in all typical office applications for that matter, you open a file, do some work on it, and then save the changes to the same file once you are done. You basically “work on that file”.\nThat’s not how your typical workflow in R or Python looks. Here, you work on one or multiple data frames (or some other data structures). That means that you might start by reading the contents of some file into a data frame, but once that is done, there is no link between the data frame and that file any more. Once your work is done, you can save your data frame to a file, of course, but it is a good practice not to overwrite your input file, so that you can always go back to where you started. A typical workflow would look like this:\n\nRead raw data from file myrawdata.csv into data frame df - Do some operations and analyses on df - Save df to file myfinaldata.csv Note that the last step is not even necessary, but may be handy if running the script takes very long, or if you want to re-distribute the resulting file.\n\nThe format in which we read files into a data frame and the format to which we save our final data frame by no means needs to be identical. We can, for example, read data created by someone else in Stata’s proprietary .dta format into a data frame and later save it to a .csv table.\nWhile we sometimes do not have the choice in which format we get our input data, we have a range of options regarding our output data. We usually prefer formats that are open and interoperable for this, which ensures that they can be used by as many people as possible, and that they are not tied to any specific (proprietary) software tool which might not be available to everyone and can be discontinued in the future.\nThe most common file formats that are relevant to us are listed in Table 5.1. txt files are particularly useful for long texts (think of one file containing one newspaper article or even a whole book), but they are bad for storing associated meta data. csv files are the default choice for tabular data, and json files allow us to store nested data in a dictionary-like format.\nFor the sake of completeness, we also listed the native Python and R formats pickle, RDS, and RDA. Because of their lack of interoperability, they are not very suitable for long-term storage or for sharing data, but they can have a place in a workflow as an intermediate step to solve the issue that none of the other formats are able to store all properties of a data frame (e.g., the csv file cannot store whether a given column in an R data frame is to be understood as containing strings such as “man”, “woman”, “non-binary” or a factor with the three levels man, woman, non-binary). If it is important to store an object (such as a data frame) exactly as-it-is, we can use these formats. One of the rare instances where we use these formats is in Example 11.8, where we store machine learning models for later reuse.\n\n\nTable 5.1: Basics of data frame handling\n\n\n\nUsed for?\nopen\ninteroperable?\n\n\n\n\ntxt\nplain text\nyes\nyes\n\n\ncsv\ntabular data\nyes\nyes\n\n\njson\nnested data, key-value pairs\nyes\nyes\n\n\npickle\nPython objects\nyes\nno\n\n\nRDS/RDA\nR objects\nyes\nno\n\n\n\n\n\n\n5.2.2 Encodings and Dialects\nPlain txt files, csv files, and json files are all files that are based on text. Unlike binary file formats, you can read them in any text editor. Try it yourself to understand what is going on under the hood.\nDownload a csv file (such as cssbook.net/d/gun-polls.csv) and open it in a text editor of your choice. Some people swear that their preferred editor is the best (google to learn about the vi versus emacs war for some entertainment), but if you have no strong feeling, then Notepad++, Atom, or Sublime may be good choices that you may want to look into.\nAs you will see (Figure 5.1), a csv file internally just looks like a bunch of text in which each line represents a row and in which the columns are separated by a comma (hence the name comma separated values (csv)). Looking at the data in a text editor is a very good way to find out what happens if reading your files into a data frame does not work as expected – which can happen more frequently than you would expect.\nMostly due to historical reasons, not every text based file (which, as we have seen, includes csv files) is internally stored in the same way. For a long time, it was common to encode in such a way that one character mapped to one byte. That was easy from a programming perspective (after all, the \\(n\\)th character of a text can be directly read from and written to the \\(n\\)th byte of a file) and was also storage-efficient. But given that a byte consists of 8 bits, that means that there are only 256 possible characters. All letters in the alphabet in uppercase, again in lowercase, numbers, punctuation, some control characters – and you are out of characters. Due to this limitation, there were different encodings or codepages for different languages that told a program which value should be interpreted as which character.\nWe all know the phenomenon of garbled special characters, like German umlauts or Scandinavian characters like ø, å, or œ being displayed as something completely different. This happens when files are read with a different encoding than the encoding that was used for creating them.\nIn principle, this issue has been solved due to the advent of Unicode. Unicode allows all characters from all scripts to be handled, including emoticons, Korean and Chinese characters, and so on. The most popular encoding for Unicode characters is called UTF-8, and it has been around for decades.\nTo avoid any data loss, it is advisable to make sure that your whole workflow uses UTF-8 files. Most modern applications support UTF-8, even though some still by default use a different encoding (e.g., “Windows-1252”) to store data. As Figure 5.1 illustrates, you can use a text editor to find out what encoding your data has, and many editors also offer an option to change the encoding. However, you cannot recover what has been lost (e.g., if at one point you saved your data with an encoding that only allows 256 different characters, it follows logically that you cannot recover that information).\n\n\n\nFigure 5.1: A csv file opened in a text editor, illustrating that the columns are separated by commas, and showing the encoding and the line endings.\n\n\nAs we will show in the practical code examples below, you can also force Python and R to use a specific encoding, which can come in handy if your data arrives in a legacy encoding.\nRelated to the different encodings a file can have, but less problematic, are different conventions of how a line ending is denoted. Windows-based programs have been using a Carriage Return followed by a Line Feed (denoted as \\r\\n), very old versions of MacOS used a Carriage Return only (\\r), and newer versions of MacOS as well as Linux use a Line Feed only (n). In our field, the Linux (or Unix) style line endings have become most dominant, and Python 3 even automatically converts Windows style line endings to Unix style line endings when reading a file – even on Windows itself.\nA third difference is the use of so-called byte-order markers (BOM). In essence, a BOM is an additional byte added to the beginning of a text file to indicate that it is a UTF-encoded file and to indicate in which order the bytes are to be read (the so-called endianness). While informative, this can cause trouble if your program does not expect that byte to be there. In that case, you might either want to remove it or explicitly specify the encoding as such. For instance, you can add an argument such as encoding=\"UTF-8\" or encoding=\"UTF-8bom\" to the open (Python) or scan (R) command.\nIn short, the most standard form in which you probably want to encode your data is in UTF-8 with Linux-style line endings without the use of a byte-order marker.\nIn the case of reading and writing csv files, we thus need to know the encoding, and potentially also the line ending conventions and the presence of a byte-order marker. However, there are also some additional variations that we need to consider. There is no single definition of what a csv file needs to look like, and there are multiple dialects that are widely used. They mainly differ in two aspects: the delimiter that is chosen, and the quoting and/or escaping of values.\nFirst, even though csv stands for comma separated values, one could use other characters instead of a comma to separate the columns. In fact, because many countries use a comma instead of a dot as a decimal separator ($10.30 versus 10,30€), in these countries a semicolon (;) is used instead of a comma as the column delimiter. To avoid any possible confusion, others use a tab character (t) to separate columns. Sometimes, these files are then called a tab-separated file, and instead of .csv, they may have a file extension such as .tsv, .tab, or even .txt. However, this does not change the way how you can read them – but what you need to know is whether your columns are separated by ,, ;, or t.\nSecond, there may be different ways to deal with strings as values in a csv file. For instance, it may be that a specific value contains the same character that is also used as a delimiter. These cases are usually resolved by either putting all strings into quotes, putting only strings that contain such ambiguities in quotes, or by prepending the ambiguous character with a specific escape character. Most likely, all of this is just handled automatically under the hood, but in case of problems, you might want to look into this and check out the documentation of the packages you are using on how to specify which strategy is to be used.\nLet’s get practical and try out reading and writing files into a data frame (Example 5.2).\n\n\n\n\n\n\n\nExample 5.2 Reading files into a data frame\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/media.csv\"\n# Directly read a csv file from internet\ndf = pd.read_csv(url)\n\n# We can also explicitly specify delimiter etc.\ndf = pd.read_csv(url, delimiter=\",\")\n# Note: use help(pd.read_csv) to see all options\n\n# Save dataframe to a csv:\ndf.to_csv(\"mynewcsvfile.csv\")\n\n\n\n\nurl = \"https://cssbook.net/d/media.csv\"\n# Directly read a csv file from internet\ndf = read_csv(url)\n\n# We can also explicitly specify delimiter etc.\ndf = read_delim(url, delim = \",\")\n# Note: use ?read_csv to see all options\n\n# Save dataframe to a csv:\nwrite_csv(df,\"mynewcsvfile.csv\")\n\n\n\n\n\n\n\n\nOf course, we can read more than just csv files. In the Python example, you can use tabcompletion to get an overview of all file formats Python supports: type pd.read and then press the TAB key to get a list of all supported files. For instance, you could pd.read_excel('test.xlsx'), df3 = pd.read_stata('test.dta'), or df4 = pd.read_json('test.json') Similarly, for R, you can hit TAB after typing haven:: to get an overview over functions such as read_spss.\n\n\n5.2.3 File handling beyond data frames\nData frames are a very useful data structure for organizing and analyzing data, and will occur in many examples in this book. However, not all things that we might want to read from a file needs to go into a data frame. Imagine if we have a list of words that we later want to remove from some texts (so-called stopwords, see Chapter 9). We could make a list (or vector) of such words directly in our code. But if we have more than a couple of such words, it is easier and more readable to keep them in an external file. We could create a file stopwords.txt in a text editor with one of such words per line:\nand\nor\na\nan\nIf you do not wish to create this list yourself, you could also download one from cssbook.net/d/stopwords.txt and save it in the same directory as your Python or R script.\nThen, you can read this file into a vector or list (see Example 5.3).\n\n\n\n\n\n\n\nExample 5.3 Reading files without data frames\n\nPython codeR code\n\n\n\n# Define stopword list in the code itself\nstopwords = [\"and\", \"or\", \"a\", \"an\", \"the\"]\n\n# Better idea: Download stopwords file and read it\nurl = \"https://cssbook.net/d/stopwords.txt\"\nurllib.request.urlretrieve(url, \"stopwords.txt\")\n\n('stopwords.txt', <http.client.HTTPMessage object at 0x7f8bb6c7e650>)\n\nwith open(\"stopwords.txt\") as f:\n    stopwords = [w.strip() for w in f]\nstopwords\n\n['and', 'or', 'a', 'an', 'the']\n\n\n\n\n\n# Define stopword list in the code itself \nstopwords = c(\"and\", \"or\", \"a\", \"an\", \"the\")\n\n# Better idea: Download stopwords file and read it\nurl = \"https://cssbook.net/d/stopwords.txt\"\ndownload.file(url, \"stopwords.txt\")\nstopwords =  scan(\"stopwords.txt\", what=\"string\")\nstopwords\n\n[1] \"and\" \"or\"  \"a\"   \"an\"  \"the\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.4 More examples for reading from and writing to files.\n\nPython codeR code\n\n\n\n# Modify the stopword list and save it:\nstopwords += [\"somenewstopword\", \"andanotherone\"]\nwith open(\"newstopwords.txt\", mode=\"w\") as f:\n    f.writelines(stopwords)\n\n# Use json to read/write dictionaries\nsomedict = {\"label\": \"Report\", \"entries\": [1, 2, 3, 4]}\n\nwith open(\"test.json\", mode=\"w\") as f:\n    json.dump(somedict, f)\n\nwith open(\"test.json\", mode=\"r\") as f:\n    d = json.load(f)\nprint(d)\n\n{'label': 'Report', 'entries': [1, 2, 3, 4]}\n\n\n\n\n\n# Modify the stopword list and save it:\nstopwords = c(stopwords, \n              \"somenewstopword\", \"andanotherone\")\nfileConn<-file(\"newstopwords.txt\")\nwriteLines(stopwords, fileConn)\nclose(fileConn)\n\n# Use json to read/write named lists\nsomedict = list(label=\"Report\",\n               entries=c(1,2,3,4))\n\nwrite_json(somedict, \"/tmp/x.json\", auto_unbox=T)\n\nd=read_json(\"/tmp/x.json\", simplifyVector = T)\nprint(d)\n\n$label\n[1] \"Report\"\n\n$entries\n[1] 1 2 3 4\n\n\n\n\n\n\n\n\n\nExample 5.4 provides you with some more elaborate code examples that allows us to dig a bit deeper into the general way of handling files.\nIn the Python example, we can open a file and assign a handle to it that allows us to refer to it (the name of the handle is arbitrary, let’s just call it f here). Then, we can use a for loop to iterate over all lines in the file and add it to a list.\nThe mode = 'r' specifies that we want to read from the file. mode = 'w' would open the file for writing, create it if necessary, and immediately deletes all content that may have been in there if the file already existed (!). Note that the .strip() is necessary to remove the line ending itself, and also any possible whitespace at the beginning or end of a line. If we want to save our stopwords, we can do this in a similar way: we first open the file (this time, for writing), and then use the file handle’s methods to write to it. We are not limited to plain text files, here. For instance, we can use the same approach to read json files into a Python dict or to store a Python dict into a json file.\nWe could also combine this with a for loop that goes over all files in a dictionary. Imagine we have a folder full of positive movie reviews, and another one full of negative movie reviews that we want to use to train a machine learning classifier (see Section 11.4). Let’s further assume that all these reviews are saved as .txt files. We can iterate over all of them, as shown in Example 11.1. If you want to read text files into a data frame in R, the readtext package may be interesting for you."
  },
  {
    "objectID": "content/chapter05.html#sec-gathering",
    "href": "content/chapter05.html#sec-gathering",
    "title": "5  From file to data frame and back",
    "section": "5.3 Data from online sources",
    "text": "5.3 Data from online sources\nMany data that are interesting to those analyzing communication are nowadays gathered online. In Chapter 12, you will learn how to use APIs to retrieve data from web services, and how to write your own web scraper to automatically download large numbers of web pages and extract relevant information. For instance, you might want to retrieve customer reviews from a website or articles from news sites.\nIn this section, however, we will focus on how to re-use existing datasets that others have made available online. For instance, the open science movement has led to more and more datasets being shared openly using repositories such as Dataverse, Figshare, or others. Re-using existing data can be very good for several reasons: first, to confirm (or not) the conclusions drawn by others; second, to avoid wasting resources by re-collecting very similar or even identical data all over again; and third, because gathering a large, high-quality dataset might just not be feasible with your means. This is especially true when you need annotated (i.e., hand-coded) data for supervised machine learning purposes (Chapter 8).\nWe can distinguish between two types of existing online datasets: datasets that are inherently interesting, and so-called toy datasets.\nToy datasets may include made-up data, but often, they contain real data. However, they are not analyzed to gain scientific insights (any more), as they may be too small, outdated, or already analyzed all-over again. These provide a great way, though, to learn and explore new techniques: after all, the results and the characteristics of the data are already known. Hence, such toy datasets are often even included in R and Python packages. Some of them are really well-known in teaching (e.g., the iris dataset containing measurements of some flowers; or the titanic dataset containing statistics on survival rates of passengers on the Titanic; MINIST for image classification; or the MPG dataset on car fuel consumption). Many of these are included in packages like scikit-learn, seaborn, or ggplot2– and you can have a look at their documentation.\nFor instance, the 20 Newsgroups dataset contains \\(18846\\) posts from newsgroups plus the groups where they were posted (Example 5.5). This can be an interesting resource for practicing with natural language processing, unsupervised, and supervised machine learning. Other interesting resource are collections of political speeches, such as the state-of-the-union speeches from the US, which are available in multiple packages (Example 5.6). Other interesting datasets with large collections of textual data may be the Financial News dataset compiled by Chen (2017) or the political news dataset compiled by Horne et al. (2018).\n\n\n\n\n\n\n\nExample 5.5 In Python, scikit-learn has a convenience function to automatically download the 20 newsgroup dataset and automatically clean it up. In R, you can download the raw version (there are multiple copies floating around on the internet) and perform the cleaning yourself.\n\nPython codeR code\n\n\n\n# Note: use fetch_20newsgroups? for more options\nd = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"))\ndf = pd.DataFrame(zip(d[\"data\"], d[\"target_names\"]))\ndf.head()\n\n                                                   0                         1\n0  I was wondering if anyone out there could enli...               alt.atheism\n1  A fair number of brave souls who upgraded thei...             comp.graphics\n2  well folks, my mac plus finally gave up the gh...   comp.os.ms-windows.misc\n3  \\nDo you have Weitek's address/phone number?  ...  comp.sys.ibm.pc.hardware\n4  From article <C5owCB.n3p@world.std.com>, by to...     comp.sys.mac.hardware\n\n\n\n\n\nurl = \"https://cssbook.net/d/20_newsgroups.csv\"\nd = read_csv(url)\nhead(d)\n\n# A tibble: 6 × 3\n   ...1 target text                                                             \n  <dbl>  <dbl> <chr>                                                            \n1     0      9 \"From: cubbie@garnet.berkeley.edu (                             …\n2     1      4 \"From: gnelson@pion.rutgers.edu (Gregory Nelson) Subject: Thanks…\n3     2     11 \"From: crypt-comments@math.ncsu.edu Subject: Cryptography FAQ 10…\n4     3      4 \"From:  () Subject: Re: Quadra SCSI Problems??? Organization: Ap…\n5     4      0 \"From: keith@cco.caltech.edu (Keith Allan Schneider) Subject: Re…\n6     5      4 \"From: taihou@chromium.iss.nus.sg (Tng Tai Hou) Subject: ADB and…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.6 A collection of US state-of-the-union speeches is available in multiple packages in various forms.\n\nPython codeR code\n\n\n\n# Note: download is only needed once...\nnltk.download(\"state_union\")\n\nTrue\n\n[nltk_data] Downloading package state_union to\n[nltk_data]     /home/runner/nltk_data...\n[nltk_data]   Unzipping corpora/state_union.zip.\n\nsentences = state_union.sents()\nprint(f\"There are {len(sentences)} sentences.\")\n\nThere are 17930 sentences.\n\n\n\n\n\nspeeches = sotu_meta\n# show only first 50 characters\nspeeches %>% \n    mutate(text = substr(sotu_text,0,50)) %>%\n    head()\n\n  X         president year years_active       party sotu_type\n1 1 George Washington 1790    1789-1793 Nonpartisan    speech\n2 2 George Washington 1790    1789-1793 Nonpartisan    speech\n3 3 George Washington 1791    1789-1793 Nonpartisan    speech\n4 4 George Washington 1792    1789-1793 Nonpartisan    speech\n5 5 George Washington 1793    1793-1797 Nonpartisan    speech\n6 6 George Washington 1794    1793-1797 Nonpartisan    speech\n                                                  text\n1   Fellow-Citizens of the Senate and House of Represe\n2 \\n\\n Fellow-Citizens of the Senate and House of Repr\n3 \\n\\n Fellow-Citizens of the Senate and House of Repr\n4   Fellow-Citizens of the Senate and House of Represe\n5 \\n\\n Fellow-Citizens of the Senate and House of Repr\n6 \\n\\n Fellow-Citizens of the Senate and House of Repr\n\n\n\n\n\n\n\n\n\nThere are also some more generic resources that you may want to consider for finding more datasets to play around with. On datasetsearch.research.google.com, you can search for datasets of all kinds, both really interesting ones and toy datasets. Another great research is kaggle.com, a site that hosts data science competitions.\n\n\n\n\n\n\n\nChen, Lizi. 2017. “News-Processed-Dataset.” https://doi.org/10.6084/m9.figshare.5296357.v1.\n\n\nHorne, Benjamin D., William Dron, Sara Khedr, and Sibel Adali. 2018. “Sampling the News Producers: A Large News and Feature Data Set for the Study of the Complex Media Landscape.” In 12th International AAAI Conference on Web and Social Media (ICWSM), 518–27. Icwsm. http://arxiv.org/abs/1803.10124."
  },
  {
    "objectID": "content/chapter06.html#filtering-selecting-and-renaming",
    "href": "content/chapter06.html#filtering-selecting-and-renaming",
    "title": "6  Data Wrangling",
    "section": "6.1 Filtering, Selecting, and Renaming",
    "text": "6.1 Filtering, Selecting, and Renaming\nSelecting and renaming columns. A first clean up step we often want to do is removing unnecessary columns and renaming columns with unclear or overly long names. In particular, it is often convenient to rename columns that contain spaces or non-standard characters, so it is easier to refer to them later.\nSelecting rows. As a next step, we can decide to filter certain rows. For example, we might want to use only a subset of the data, or we might want to remove certain rows because they are incomplete or incorrect.\nAs an example, FiveThirtyEight published a quiz about American public opinion about guns, and were nice enough to also publish the underlying data1. Example 6.1 gives an example of loading and cleaning this dataset, starting with the function read_csv (included in both tidyverse and pandas) to load the data directly from the Internet. This dataset contains one poll result per row, with a Question column indicating which question was asked, and the columns listing how many Americans (adults or registered voters) were in favor of that measure, in total and for Republicans and Democrats. Next, the columns Republican and Democratic Support are renamed to shorten the names and remove the space. Then, the URL column is dropped using the tidyverse function select in R or the pandas function drop in Python. Notice that the result of these operations is assigned to the same object d. This means that the original d is overwritten.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn R, the tidyverse function select is quite versatile. You can specify multiple columns using select(d, column1, column2) or by specifying a range of columns: select(d, column1:column3). Both commands keep only the specified columns. As in the example, you can also specify a negative selection with the minus sign: select(d, -column1) drops column1, keeping all other columns. Finally, you can rename columns in the select command as well: select(d, column1=col1, column2) renames col to column1, keeps that column and column2, and drops all other columns.\n\n\n\nWe then filter the dataset to list only the polls on whether teachers should be armed (you can understand this is close to our heart). This is done by comparing the value of the Question column to the value 'arm-teachers'. This comparison is done with a double equal sign (==). In both Python and R, a single equals sign is used for assignment, and a double equal sign is used for comparison. A final thing to notice is that while in R we used the dplyr function (filter) to filter out rows, in Python we index the data frame using square brackets on the pandas DataFrame attribute loc(ation): d.loc[].\nNote that we chose to assign the result of this filtering to d2, so after this operation we have the original full dataset d as well as the subset d2 at our disposal. In general, it is your choice whether you overwrite the data by assigning to the same object, or create a copy by assigning to a new name2. If you will later need to work with a different subset, it is smart to keep the original so you can subset it again later. On the other hand, if all your analyses will be on the subset, you might as well overwrite the original. We can always re-download it from the internet (or reload it from our harddisk) if it turns out we needed the original anyway.\n\n\n\n\n\n\n\nExample 6.1 Filtering\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/guns-polls.csv\"\nd = pd.read_csv(url)\nd = d.rename(columns={\"Republican Support\": \"rep\", \"Democratic Support\": \"dem\"})\nd = d.drop(columns=\"URL\")\n# alternatively, we can write:\n# d.drop(columns=\"URL\", inplace=True)\nd2 = d.loc[d.Question == \"arm-teachers\"]\nd2\n\n        Question    Start      End  ... Support rep  dem\n7   arm-teachers  2/23/18  2/25/18  ...      41  69   20\n8   arm-teachers  2/20/18  2/23/18  ...      44  68   20\n9   arm-teachers  2/27/18  2/28/18  ...      43  71   24\n10  arm-teachers  2/27/18  2/28/18  ...      41  68   18\n11  arm-teachers   3/3/18   3/5/18  ...      40  77   10\n12  arm-teachers  2/26/18  2/28/18  ...      43  80   11\n\n[6 rows x 8 columns]\n\n\n\n\n\nurl=\"https://cssbook.net/d/guns-polls.csv\"\nd = read_csv(url)\nd = rename(d, rep=`Republican Support`, \n           dem=`Democratic Support`)\nd = select(d, -URL)\n\nd2 = filter(d, Question == \"arm-teachers\")\nd2\n\n# A tibble: 6 × 8\n  Question     Start   End     Pollster        Population    Support   rep   dem\n  <chr>        <chr>   <chr>   <chr>           <chr>           <dbl> <dbl> <dbl>\n1 arm-teachers 2/23/18 2/25/18 YouGov/Huffpost Registered V…      41    69    20\n2 arm-teachers 2/20/18 2/23/18 CBS News        Adults             44    68    20\n3 arm-teachers 2/27/18 2/28/18 Rasmussen       Adults             43    71    24\n4 arm-teachers 2/27/18 2/28/18 NPR/Ipsos       Adults             41    68    18\n5 arm-teachers 3/3/18  3/5/18  Quinnipiac      Registered V…      40    77    10\n6 arm-teachers 2/26/18 2/28/18 SurveyMonkey    Registered V…      43    80    11"
  },
  {
    "objectID": "content/chapter06.html#sec-calculate",
    "href": "content/chapter06.html#sec-calculate",
    "title": "6  Data Wrangling",
    "section": "6.2 Calculating Values",
    "text": "6.2 Calculating Values\nVery often, we need to calculate values for new columns or change the content of existing columns. For example, we might wish to calculate the difference between two columns, or we may need to clean a column by correcting clerical errors or converting between data types.\nIn these steps, the general pattern is that a column is assigned a new value based on a calculation that generally involves other columns. In both R and Python, there are two general ways to accomplish this. First, you can simply assign to an existing or new column, using the column selection notation discussed in Section 3.1: df[\"column\"] = ... in Python, or df$column = ... in R.\nBoth Python and R also offer a function that allows multiple columns to be changed, returning a new copy of the data frame rather than changing the original data frame. In R, this is done using the tidyverse function mutate, which is the recommended way to compute values. The Python equivalent, pandas function assign, is used more rarely as it does not offer many advantages over direct assignment.\nIn either case, you can use arithmetic: e.g. rep - dem to compute the difference between these columns. This works directly in R mutate, but in Python or in R direct assignment you also need to specify the name of the data frame. In Python, this would be d[\"rep\"] - d[\"dem\"] 3, while in R this is d$rep - d$dem.\nIn many cases, however, you want to use various functions to perform tasks like cleaning and data conversion (see Section 3.3 for a detailed explanation of built-in and custom functions). For example, to convert a column to numeric you would use the base R function as.numeric in R or the pandas function to_numeric in Python. Both functions take a column as argument and convert it to a numeric column.\nAlmost all R functions work on whole columns like that. In Python, however, many functions work on individual values rather than columns. To apply a function on each element of a column col, you can use df.col.apply(my_function) (where df and col are the names of your data frame and column). In contast, Pandas columns have multiple useful methods that – because they are methods of that column – apply to the whole column4. For example, the method df.col.fillna replaces missing values in the column col, and df.col.str.replace conducts a find and replace. Unlike functions that expect individual values rather than columns as an input, there is no need to explicitly apply such a method. As always, you can use tab completion (pressing the TAB key after writing df.col.) to get a menu that includes all available methods.\n\n\n\n\n\n\n\nExample 6.2 Mutate\n\nPython codeR code\n\n\n\n# version of the guns polls with some errors\nurl = \"https://cssbook.net/d/guns-polls-dirty.csv\"\nd2 = pd.read_csv(url)\n\n# Option 1: clean with direct assignment\n# Note that when creating a new column,\n# you have to use df[\"col\"] rather than df.col\nd2[\"rep2\"] = d2.rep.str.replace(\"[^0-9\\\\.]\", \"\")\nd2[\"rep2\"] = pd.to_numeric(d2.rep2)\nd2[\"Support2\"] = d2.Support.fillna(d.Support.mean())\n\n# Alternatively, clean with .assign\n# Note the need to use an anonymous function\n# (lambda) to chain calculations\ncleaned = d2.assign(\n    rep2=d2.rep.str.replace(\"[^0-9\\\\.]\", \"\"),\n    rep3=lambda d2: pd.to_numeric(d2.rep2),\n    Support2=d2.Support.fillna(d2.Support.mean()),\n)\n\n# Finally, you can create your own function\ndef clean_num(x):\n    x = re.sub(\"[^0-9\\\\.]\", \"\", x)\n    return int(x)\n\ncleaned[\"rep3\"] = cleaned.rep.apply(clean_num)\ncleaned.head()\n\n       Question    Start      End         Pollster  ... dem  rep2 Support2  rep3\n0  arm-teachers  2/23/18  2/25/18  YouGov/Huffpost  ...  20    69     41.0    69\n1  arm-teachers  2/20/18  2/23/18         CBS News  ...  20    68     41.6    68\n2  arm-teachers  2/27/18  2/28/18        Rasmussen  ...  24    71     43.0    71\n3  arm-teachers  2/27/18  2/28/18        NPR/Ipsos  ...  18    68     41.0    68\n4  arm-teachers   3/3/18   3/5/18       Quinnipiac  ...  10    77     40.0    77\n\n[5 rows x 11 columns]\n\n\n\n\n\n# version of the guns polls with some errors\nurl=\"https://cssbook.net/d/guns-polls-dirty.csv\"\nd2 = read_csv(url)\n\n# Option 1: clean with direct assignment. \n# Note the need to specify d2$ everywhere\nd2$rep2=str_replace_all(d2$rep, \"[^0-9\\\\.]\", \"\")\nd2$rep2 = as.numeric(d2$rep2)\nd2$Support2 = replace_na(d2$Support, \n                        mean(d2$Support, na.rm=T))\n\n# Alternative, clean with mutate\n# No need to specify d2$, \n# and we can assign to a new or existing object\ncleaned = mutate(d2, \n    rep2 = str_replace_all(rep, \"[^0-9\\\\.]\", \"\"),\n    rep2 = as.numeric(rep2),\n    Support2 = replace_na(Support, \n        mean(Support, na.rm=TRUE)))\n\n# Finally, you can create your own function\nclean_num = function(x) {\n    x = str_replace_all(x, \"[^0-9\\\\.]\", \"\")\n    as.numeric(x)\n}\ncleaned = mutate(cleaned, rep3 = clean_num(rep))\nhead(cleaned)\n\n# A tibble: 6 × 11\n  Question   Start End   Polls…¹ Popul…² Support   rep   dem  rep2 Suppo…³  rep3\n  <chr>      <chr> <chr> <chr>   <chr>     <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl>\n1 arm-teach… 2/23… 2/25… YouGov… Regist…      41    69    20    69    41      69\n2 arm-teach… 2/20… 2/23… CBS Ne… Adults       NA    68    20    68    41.6    68\n3 arm-teach… 2/27… 2/28… Rasmus… Adults       43    71    24    71    43      71\n4 arm-teach… 2/27… 2/28… NPR/Ip… Adults       41    68    18    68    41      68\n5 arm-teach… 3/3/… 3/5/… Quinni… Regist…      40    77    10    77    40      77\n6 arm-teach… 2/26… 2/28… Survey… Regist…      43    80    11    80    43      80\n# … with abbreviated variable names ¹​Pollster, ²​Population, ³​Support2\n\n\n\n\n\n\n\n\n\nTo illustrate some of the many possibilities, Example 6.2 has code for cleaning a version of the gun polls in which we intentionally introduced two problems: we added some typos to the rep column and introduced a missing value in the Support column. To clean this, we perform three steps: First, we remove all non-numeric characters using a regular expression (see Section 9.2 for more information on text handling and regular expressions). Next, we need to explicitly convert the resulting column into a numeric column so we can later use it in calculations. Finally, we replace the missing value by the column mean (of course, it is doubtful that that is the best strategy for imputing missing values here, we do it mainly to show how one can deal with missing values technically. You will find some more discussion about missing values in Section 7.1).\nThe cleaning process is actually performed twice: lines 5-10 use direct assignment, while lines 12-19 use the mutate/assign function. Finally, lines 21-27 show how you can define and apply a custom function to combine the first two cleaning steps. This can be quite useful if you use the same cleaning steps in multiple places, since it reduces the repetition of code and hence the possibility of introducing bugs or inconsistencies.\nNote that all these versions work fine and produce the same result. In the end, it is up to the researcher to determine which feels most natural given the circumstances. As noted above, in R we would generally prefer mutate over direct assignment, mostly because it fits nicely into the tidyverse workflow and you do not need to repeat the data frame name. In Python, we would generally prefer the direct assignment, unless a copy of the data with the changes made is convenient, in which case assign can be more useful."
  },
  {
    "objectID": "content/chapter06.html#sec-grouping",
    "href": "content/chapter06.html#sec-grouping",
    "title": "6  Data Wrangling",
    "section": "6.3 Grouping and Aggregating",
    "text": "6.3 Grouping and Aggregating\nThe functions we used to change the data above operated on individual rows. Sometimes, however, we wish to compute summary statistics of groups of rows. This essentially shifts the unit of analysis to a higher level of abstraction. For example, we could compute per-school statistics from a data file containing information per student; or we could compute the average number of mentions of a politician per day from data file containing information per articles (each date might have multiple articles and each article multiple mentions to politicians!).\nIn data analysis, this is called aggregation. In both Python and R, it consists of two steps: First, you define which rows are grouped together to form a new unit by specifying which column identifies these groups. In the previous examples, this would be the school name or the date of each article. It is also possible to group by multiple columns, for example to compute the average per day per news source.\nThe next step is to specify one or more summary (or aggregation) functions to be computed over the desired value columns. These functions compute a summary value, like the mean, sum, or standard deviation, over all the values belonging to each group. In the example, to compute average test scores per school we would apply the average (or mean) function to the test score value column. In general, you can use multiple functions (e.g. mean and variance) and multiple columns (e.g. mean test score and mean parental income).\nThe resulting dataset is reduced both in rows and in columns. Each row now represents a group of previuos cases (e.g. school or date), and the columns are now only the grouping columns and the computed summary scores.\nExample 6.3 shows the code in R and Python to define groups and compute summary values. First, we group by poll question; and for each question, we compute the average and standard deviation. The syntax is a little different for R and Python, but the idea is the same: first we create a new variable groups that stores the grouping information, and then we create the aggregate statistics. In this example, we do not store the result of the computation, but print it on the screen. To store the results, simply assign it to a new object as normal.\n\n\n\n\n\n\n\nExample 6.3 Aggregation. Note that in the Python example, we can specify often-used functions such as “mean” simply as a string, but instead, we could also pass functions directly, such as numpy’s np.mean\n\nPython codeR code\n\n\n\ngroups = d.groupby(\"Question\")\ngroups.agg({\"Support\": [\"mean\", \"std\"]})\n\n                               Support          \n                                  mean       std\nQuestion                                        \nage-21                       75.857143  6.011893\narm-teachers                 42.000000  1.549193\nbackground-checks            87.428571  7.322503\nban-assault-weapons          61.750000  6.440285\nban-high-capacity-magazines  67.285714  3.860669\nmental-health-own-gun        85.833333  5.455884\nrepeal-2nd-amendment         10.000000       NaN\nstricter-gun-laws            66.454545  5.145165\n\n\n\n\n\ngroups = group_by(d, Question)\nsummarize(groups, m=mean(Support), sd=sd(Support))\n\n# A tibble: 8 × 3\n  Question                        m    sd\n  <chr>                       <dbl> <dbl>\n1 age-21                       75.9  6.01\n2 arm-teachers                 42    1.55\n3 background-checks            87.4  7.32\n4 ban-assault-weapons          61.8  6.44\n5 ban-high-capacity-magazines  67.3  3.86\n6 mental-health-own-gun        85.8  5.46\n7 repeal-2nd-amendment         10   NA   \n8 stricter-gun-laws            66.5  5.15\n\n\n\n\n\n\n\n\n\nIn R, you use the dplyr function group_by to define the groups, and then call the function summarize to compute summary values by specifying name=function(value).\nIn Python, the grouping step is quite similar. In the summarization step, however, you specify which summaries to compute in a dictionary5. The keys of the dictionary list the value columns to compute summaries of, and the values contain the summary functions to apply, so 'value': function or 'value': [list of functions].\n\n6.3.1 Combining Multiple Operations\nIn the examples above, each line of code (often called a statement) contained a single operation, generally a call to a function or method (see Section 3.3). The general shape of each line in R was data = function(data, arguments), that is, the data is provided as the first argument to the function. In Python, we often used methods that “belong to” objects such as data frames or columns. Here, we therefore specify the object itself followed by a period and its method that is to be called, i.e. object = object.method(arguments).\nAlthough there is nothing wrong with limiting each line to a single operation, both languages allow multiple operations to be chained together. Especially for grouping and summarizing, it can make sense to link these operations together as they can be thought of as a single “data wrangling” step.\nIn Python, this can be achieved by adding the second .method() directly to the end of the first statement. Essentially, this calls the second method on the result of the first method: data = data.method1(arguments).method2(arguments). In R, the data needs, of course, to be included in the function arguments. But we can also chain these function calls. This is done using the pipe operator (%>%) from the (cutely named) magrittr package. The pipe operator inserts the result of the first function as the first argument of the second function. More technically, f1(d) %>% f2() is equivalent to f2(f1(d)). This can be used to chain multiple commands together, e.g. data = data %>% function1(arguments) %>% function2(arguments).\n\n\n\n\n\n\n\nExample 6.4 Combining multiple functions or methods. The result is identical to\n\nPython codeR code\n\n\n\nd.groupby(\"Question\").agg({\"Support\": [\"mean\", \"std\"]})\n\n                               Support          \n                                  mean       std\nQuestion                                        \nage-21                       75.857143  6.011893\narm-teachers                 42.000000  1.549193\nbackground-checks            87.428571  7.322503\nban-assault-weapons          61.750000  6.440285\nban-high-capacity-magazines  67.285714  3.860669\nmental-health-own-gun        85.833333  5.455884\nrepeal-2nd-amendment         10.000000       NaN\nstricter-gun-laws            66.454545  5.145165\n\n\n\n\n\nd %>% group_by(Question) %>% \n  summarize(m=mean(Support), sd=sd(Support))\n\n# A tibble: 8 × 3\n  Question                        m    sd\n  <chr>                       <dbl> <dbl>\n1 age-21                       75.9  6.01\n2 arm-teachers                 42    1.55\n3 background-checks            87.4  7.32\n4 ban-assault-weapons          61.8  6.44\n5 ban-high-capacity-magazines  67.3  3.86\n6 mental-health-own-gun        85.8  5.46\n7 repeal-2nd-amendment         10   NA   \n8 stricter-gun-laws            66.5  5.15\n\n\n\n\n\n\n\n\n\nExample 6.4 shows the same operation as in Example 6.3, but chained into a single statement.\n\n\n6.3.2 Adding Summary Values\nRather than reducing a data frame to contain only the group-level information, it is sometimes desirable to add the summary values to the original data. For example, if we add the average score per school to the student-level data, we can then determine whether individual students outperform the school average.\nOf course, the summary scores are the same for all rows in the same group: all students in the same school have the same school average. So, these values will be repeated for these rows, essentially mixing individual and group level variables in the same data frame.\n\n\n\n\n\n\n\nExample 6.5 Adding summary values to individual cases\n\nPython codeR code\n\n\n\n# Note the use of ( ) to split a long line\nd[\"mean\"] = d.groupby(\"Question\")[\"Support\"].transform(\"mean\")\nd[\"deviation\"] = d[\"Support\"] - d[\"mean\"]\nd.head()\n\n  Question    Start      End  ... dem       mean  deviation\n0   age-21  2/20/18  2/23/18  ...  86  75.857143  -3.857143\n1   age-21  2/27/18  2/28/18  ...  92  75.857143   6.142857\n2   age-21   3/1/18   3/4/18  ...  76  75.857143  -8.857143\n3   age-21  2/22/18  2/26/18  ...  92  75.857143   8.142857\n4   age-21   3/3/18   3/5/18  ...  93  75.857143   2.142857\n\n[5 rows x 10 columns]\n\n\n\n\n\nd = d %>% group_by(Question) %>% \n  mutate(mean = mean(Support), \n         deviation=Support - mean)\nhead(d)\n\n# A tibble: 6 × 10\n# Groups:   Question [1]\n  Question Start   End     Pollster    Popul…¹ Support   rep   dem  mean devia…²\n  <chr>    <chr>   <chr>   <chr>       <chr>     <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 age-21   2/20/18 2/23/18 CNN/SSRS    Regist…      72    61    86  75.9   -3.86\n2 age-21   2/27/18 2/28/18 NPR/Ipsos   Adults       82    72    92  75.9    6.14\n3 age-21   3/1/18  3/4/18  Rasmussen   Adults       67    59    76  75.9   -8.86\n4 age-21   2/22/18 2/26/18 Harris Int… Regist…      84    77    92  75.9    8.14\n5 age-21   3/3/18  3/5/18  Quinnipiac  Regist…      78    63    93  75.9    2.14\n6 age-21   3/4/18  3/6/18  YouGov      Regist…      72    65    80  75.9   -3.86\n# … with abbreviated variable names ¹​Population, ²​deviation\n\n\n\n\n\n\n\n\n\nExample 6.5 shows how this can be achieved in Python and R, computing the mean support per question and then calculating how each poll deviates from this mean.\nIn R, the code is very similar to Example 6.4 above, simply replacing the dplyr function summarize by the function mutate discussed above. In this function you can mix summary functions and regular functions, as shown in the example: first the mean per group is calculated, followed by the deviation of this mean.\nThe Python code also uses the same syntax used for computing new columns. The first line selects the Support column on the grouped dataset, and then calls the pandas method transform on that column to compute the mean per group, adding it as a new column by assigning it to the column name. The second line uses the regular assignment syntax to create the deviation based on the support and calculated mean."
  },
  {
    "objectID": "content/chapter06.html#sec-join",
    "href": "content/chapter06.html#sec-join",
    "title": "6  Data Wrangling",
    "section": "6.4 Merging Data",
    "text": "6.4 Merging Data\nIn many cases, we need to combine data from different sources or data files. For example, we might have election poll results in one file and socio-economic data per area in another. To test whether we can explain variance in poll results from factors such as education level, we would need to combine the poll results with the economic data. This process is often called merging or joining data.\n\n6.4.1 Equal Units of Analysis\n\n\n\n\n\n\n\nExample 6.6 Private and Public Capital data (source: Piketty 2014).\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/private_capital.csv\"\nprivate = pd.read_csv(url)\nprivate.tail()\n\n    Year  U.S.  Japan  Germany  France  U.K.  Italy  Canada  Australia  Spain\n36  2006  4.88   5.83     3.78    5.34  5.19   6.37    3.88       5.32   7.69\n37  2007  4.94   5.79     3.79    5.53  5.23   6.42    4.02       5.55   7.92\n38  2008  4.36   5.87     3.90    5.53  4.91   6.61    3.83       5.44   7.86\n39  2009  4.06   6.19     4.15    5.63  5.04   6.91    4.13       5.04   7.89\n40  2010  4.10   6.01     4.12    5.75  5.22   6.76    4.16       5.18   7.55\n\n\n\n\n\nurl=\"https://cssbook.net/d/private_capital.csv\"\nprivate = read_csv(url)\ntail(private)\n\n# A tibble: 6 × 10\n   Year  U.S. Japan Germany France  U.K. Italy Canada Australia Spain\n  <dbl> <dbl> <dbl>   <dbl>  <dbl> <dbl> <dbl>  <dbl>     <dbl> <dbl>\n1  2005  4.7   5.74    3.84   5     4.99  6.24   3.73      5.22  7.24\n2  2006  4.88  5.83    3.78   5.34  5.19  6.37   3.88      5.32  7.69\n3  2007  4.94  5.79    3.79   5.53  5.23  6.42   4.02      5.55  7.92\n4  2008  4.36  5.87    3.9    5.53  4.91  6.61   3.83      5.44  7.86\n5  2009  4.06  6.19    4.15   5.63  5.04  6.91   4.13      5.04  7.89\n6  2010  4.1   6.01    4.12   5.75  5.22  6.76   4.16      5.18  7.55\n\n\n\n\n\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = pd.read_csv(url)\npublic.tail()\n\n    Year  U.S.  Japan  Germany  France  U.K.  Italy  Canada  Australia  Spain\n36  2006  0.51   0.36     0.02    0.37  0.32  -0.54   -0.10       0.69   0.20\n37  2007  0.54   0.38     0.06    0.46  0.32  -0.52   -0.03       0.69   0.26\n38  2008  0.49   0.34     0.08    0.43  0.28  -0.52    0.00       0.71   0.25\n39  2009  0.36   0.24     0.07    0.35  0.19  -0.65   -0.02       0.71   0.14\n40  2010  0.21   0.14     0.04    0.31  0.06  -0.68   -0.04       0.67   0.05\n\n\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = read_csv(url)\ntail(public)\n\n# A tibble: 6 × 10\n   Year  U.S. Japan Germany France  U.K. Italy Canada Australia Spain\n  <dbl> <dbl> <dbl>   <dbl>  <dbl> <dbl> <dbl>  <dbl>     <dbl> <dbl>\n1  2005  0.48  0.34    0.04   0.28  0.32 -0.56  -0.16      0.67  0.13\n2  2006  0.51  0.36    0.02   0.37  0.32 -0.54  -0.1       0.69  0.2 \n3  2007  0.54  0.38    0.06   0.46  0.32 -0.52  -0.03      0.69  0.26\n4  2008  0.49  0.34    0.08   0.43  0.28 -0.52   0         0.71  0.25\n5  2009  0.36  0.24    0.07   0.35  0.19 -0.65  -0.02      0.71  0.14\n6  2010  0.21  0.14    0.04   0.31  0.06 -0.68  -0.04      0.67  0.05\n\n\n\n\n\n\n\n\n\nThe easiest joins are when both datasets have the same unit of analysis, i.e. the rows represent the same units. For example, consider the data on public and private capital ownership published by Piketty (2017) alongside his landmark book Capital in the 21st Century. As shown in Example 6.6, he released separate files for public and private capital ownership. If we wished to analyze the relationship between these (for example to recreate Figure 3.6 on page 128 of that book), we first need to combine them into a single data frame.\nTo combine these data frames, we use the pandas data frame method merge in Python or the dplyr method full_join in R. Both methods join the data frames on one or more key columns. The key column(s) identify the units in both data frames, so in this case the Year column. Often, the key column is some sort of identifier, like a respondent or location ID. The resulting data frame will contain the shared key column(s), and all other columns from both joined data frames.\nIn both Python and R, all columns that occur in both data frames are by default assumed to be the key columns. In many cases, this is the desired behavior as both data frames may contain e.g. a Year or RepondentID column. Sometimes, however, this is not the case. Possibly, the key column is called differently in both data frames, e.g. respID in one and Respondent in the other. It is also possible that the two frames contain columns with the same name, but which contain actual data that should not be used as a key. For example, in the Piketty data shown above the key column is called Year in both frames, but they also share the columns for the countries which are data columns.\nIn these cases, it is possible to explicitly specify which columns to join on (using the on= (Python) / by= (R) argument). However, we would generally recommend preprocessing the data first and select and/or rename columns such that the only shared columns are the key columns. The reason for that is that if columns in different data frames mean the same thing (i.e. respID and Respondent), they should generally have the same name to avoid confusion. In the case of “accidentally” shared column names, such as the country names in the current example, it is also better to rename them so it is obvious which is which in the resulting dataset: if shared columns are not used in the join, by default they get “.x” and “.y” (R) or “_x” and “_y” (Python) appended to their name, which is not very meaningful. Even if the key column is the only shared column, however, it can still be good to explicitly select that column to make it clear to the reader (or for yourself in the future) what is happening.\n\n\n\n\n\n\n\nExample 6.7 Merging private and public data for France.\n\nPython codeR code\n\n\n\nprivate_fr = private[[\"Year\", \"France\"]].rename(\n    columns={\"France\": \"fr_private\"}\n)\npublic_fr = public[[\"Year\", \"France\"]].rename(columns={\"France\": \"fr_public\"})\ncapital_fr = pd.merge(private_fr, public_fr)\n# Data for Figure 3.6 (Piketty, 2014, p 128)\ncapital_fr.head()\n\n   Year  fr_private  fr_public\n0  1970        3.10       0.41\n1  1971        3.04       0.43\n2  1972        3.07       0.45\n3  1973        3.05       0.46\n4  1974        3.03       0.48\n\n\n\n\n\nprivate_fr = private %>% \n    select(Year, fr_private=France)\npublic_fr = public %>% \n    select(Year, fr_public=France)\ncapital_fr = full_join(private_fr, public_fr)\n# Data for Figure 3.6 (Piketty, 2014, p 128)\nhead(capital_fr)\n\n# A tibble: 6 × 3\n   Year fr_private fr_public\n  <dbl>      <dbl>     <dbl>\n1  1970       3.1       0.41\n2  1971       3.04      0.43\n3  1972       3.07      0.45\n4  1973       3.05      0.46\n5  1974       3.03      0.48\n6  1975       3.17      0.53\n\n\n\n\n\n\nPython codeR code\n\n\n\n# Are private and public capital correlated?\nr, p = scipy.stats.pearsonr(capital_fr.fr_private, capital_fr.fr_public)\nprint(f\"Pearson correlation: rho={r:.2},p={p:.3}\")\n\nPearson correlation: rho=-0.32,p=0.0404\n\n\n\n\n\n# Are private and public capital correlated?\ncor.test(capital_fr$fr_private, \n         capital_fr$fr_public)\n\n\n    Pearson's product-moment correlation\n\ndata:  capital_fr$fr_private and capital_fr$fr_public\nt = -2.1204, df = 39, p-value = 0.04039\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.57252488 -0.01537337\nsample estimates:\n       cor \n-0.3215032 \n\n\n\n\n\n\n\n\n\nThis is shown in Example 6.7. The first two lines select only the Year and France columns, and rename the France column to indicate whether it is the private or public data. Line 3 does the actual join, with and without the explicit selection of key column, respectively. This is then used to compute the correlation between private and public capital, which shows that there is a weak but (just) significant negative correlation (\\(\\rho=-.32, p=.04\\)) 6.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNext to merge, /textitPandas data frames also have a method called join. It is a simplified version for joining on indices (i.e., the row labels). If you have two data frames in which corresponding rows have the same row number, you can simply write df1.join(df2). In short: both methods do the same, but merge provides more options, and join is easier if you want to join on the indices.\n\n\n\n\n\n6.4.2 Inner and Outer Joins\nIn the example above, both datasets had exactly one entry for each unit (year), making it the most straightforward case. If either (or both) of the datasets have missing units, however, you need to specify how to deal with this.\nTable 6.1 list the four possible ways of joining, keeping all rows (outer join), only rows present in both (inner join), or all rows from one of the sets and matching rows from the other (left or right join). Left and right here literally refer to the order in which you type the data frame names. Figure 6.1 and Table 6.1 give an overview. In all cases except inner joins, this can create units where information from one of the datasets is missing. This will be lead to missing values (NA/NaN) being inserted in the columns of the datasets with missing units.\n\n\n\nFigure 6.1: The solid area indicates whether the cases in the resulting datasets need to appear in one, both, or any of the datasets.\n\n\n\n\nTable 6.1: Different types of joins between datasets d1 and d2\n\n\n\n\n\n\n\n\nType\nDescription\nR\nPython\n\n\n\n\nOuter\nAll units from both sets\nfull_join(d1,d2)\nd1.merge(d2, how='outer')\n\n\nInner\nOnly units that are in both sets\ninner_join(d1,d2)\nd1.merge(d2, how='inner')\n\n\nLeft\nAll units from left-hand set\nleft_join(d1,d2)\nd1.merge(d2, how='left')\n\n\nRight\nAll units from right-hand set\nright_join(d1,d2)\nd1.merge(d2, how='right')\n\n\n\n\nIn most cases, you will either use inner join or left join. Inner join is useful when information should be complete, or where you are only interested in units with information in both datasets. In general, when joining sets with the same units, it is smart to check the number of rows before and after the operation. If it decreases, this shows that there are units where information is missing in either set. If it increases, it shows that apparently the sets are not at the same level of analysis, or there are duplicate units in the data. In either case, an unexpected change in the number of rows is a good indicator that something is wrong.\nLeft joins are useful when you are adding extra information to a “primary” dataset. For example, you might have your main survey results in a dataset, to which you want to add metadata or extra information about your respondents. If this data is not available for all respondents, you can use a left join to add the information where it is available, and simply leave the other respondents with missing values.\nA similar use case is when you have a list of news items, and a separate list of items that were coded or found with some search term. Using a left join will let you keep all news items, and add the coding where it is available. Especially if items that had zero hits of a search term are excluded from the search results, you might use a left join followed by a calculation to replace missing values by zeros to indicate that the counts for items aren’t actually missing, but were zero.\nOf course, you could also use a right join to achieve the same effect. It is more natural, however, to work from your primary dataset and add the secondary data, so you will generally use left joins rather than right joins.\nOuter (or full) joins can be useful when you are adding information from e.g. multiple survey waves, and you want to include any respondent that answered any of the waves. Of course, you will have to carefully think about how to deal with the resulting missing values in the substantive analysis.\n\n\n6.4.3 Nested Data\nThe sections above discuss merging two datasets at the same level of analysis, i.e. with rows representing the same units (respondents, items, years) in both sets. It is also possible, however, to join a more aggregate (high level) set with a more detailed dataset. For example, you might have respondents that are part of a school or organizational unit. It can be desirable to join the respondent level information with the school level information, for example to then explore differences between schools or do multilevel modeling.\nFor this use the same commands as for equal joins. In the resulting merged dataset, information from the group level will be duplicated for all individuals in that group.\nFor example, take the two datasets shown in Example 6.8. The results dataset shows how many votes each US 2016 presidential primary candidate received in each county: Bernie Sanders got 544 votes in Autauga County in the US state of Alabama, which was 18.2% of all votes cast in the Democratic primary. Conversely, the counties dataset shows a large number of facts about these counties, such as population, change in population, gender and education distribution, etc.\n\n\n\n\n\n\n\nExample 6.8 2016 Primary results and county-level metadata. Note that to avoid duplicate output, we display the counties data in the Python example and the results data in the R example\n\nPython codeR code\n\n\n\nr = \"https://cssbook.net/d/2016_primary_results.csv\"\nresults = pd.read_csv(r)\nresults.head()\n\n     state state_abbrev   county  ...        candidate votes fraction_votes\n0  Alabama           AL  Autauga  ...   Bernie Sanders   544          0.182\n1  Alabama           AL  Autauga  ...  Hillary Clinton  2387          0.800\n2  Alabama           AL  Baldwin  ...   Bernie Sanders  2694          0.329\n3  Alabama           AL  Baldwin  ...  Hillary Clinton  5290          0.647\n4  Alabama           AL  Barbour  ...   Bernie Sanders   222          0.078\n\n[5 rows x 8 columns]\n\n\n\n\n\nr=\"https://cssbook.net/d/2016_primary_results.csv\"\nresults = read_csv(r) \nhead(results)\n\n# A tibble: 6 × 8\n  state   state_abbrev county   fips party    candidate       votes fraction_v…¹\n  <chr>   <chr>        <chr>   <dbl> <chr>    <chr>           <dbl>        <dbl>\n1 Alabama AL           Autauga  1001 Democrat Bernie Sanders    544        0.182\n2 Alabama AL           Autauga  1001 Democrat Hillary Clinton  2387        0.8  \n3 Alabama AL           Baldwin  1003 Democrat Bernie Sanders   2694        0.329\n4 Alabama AL           Baldwin  1003 Democrat Hillary Clinton  5290        0.647\n5 Alabama AL           Barbour  1005 Democrat Bernie Sanders    222        0.078\n6 Alabama AL           Barbour  1005 Democrat Hillary Clinton  2567        0.906\n# … with abbreviated variable name ¹​fraction_votes\n\n\n\n\n\n\nPython codeR code\n\n\n\nc = \"https://cssbook.net/d/2016_primary_county.csv\"\ncounties = pd.read_csv(c)\ncounties.head()\n\n   fips       area_name state_abbrev  ...  Building_permits   Land_area  Pop_density\n0     0   United States          NaN  ...           1046363  3531905.43         87.4\n1  1000         Alabama          NaN  ...             13369    50645.33         94.4\n2  1001  Autauga County           AL  ...               131      594.44         91.8\n3  1003  Baldwin County           AL  ...              1384     1589.78        114.6\n4  1005  Barbour County           AL  ...                 8      884.88         31.0\n\n[5 rows x 54 columns]\n\n\n\n\n\nc=\"https://cssbook.net/d/2016_primary_county.csv\"\ncounties = read_csv(c)\nhead(counties)\n\n# A tibble: 6 × 54\n   fips area_n…¹ state…² Pop_2…³ Pop_2…⁴ Pop_c…⁵ Pop_2…⁶ Age_u…⁷ Age_u…⁸ Age_o…⁹\n  <dbl> <chr>    <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1     0 United … <NA>     3.19e8  3.09e8     3.3  3.09e8     6.2    23.1    14.5\n2  1000 Alabama  <NA>     4.85e6  4.78e6     1.4  4.78e6     6.1    22.8    15.3\n3  1001 Autauga… AL       5.54e4  5.46e4     1.5  5.46e4     6      25.2    13.8\n4  1003 Baldwin… AL       2.00e5  1.82e5     9.8  1.82e5     5.6    22.2    18.7\n5  1005 Barbour… AL       2.69e4  2.75e4    -2.1  2.75e4     5.7    21.2    16.5\n6  1007 Bibb Co… AL       2.25e4  2.29e4    -1.8  2.29e4     5.3    21      14.8\n# … with 44 more variables: Sex_female_pct <dbl>, Race_white_pct <dbl>,\n#   Race_black_pct <dbl>, Race_native_pct <dbl>, Race_asian_pct <dbl>,\n#   Race_island_pct <dbl>, Race_mixed_pct <dbl>, Race_hispanic_pct <dbl>,\n#   Race_white_not_hispanic_pct <dbl>, Pop_same_house_pct <dbl>,\n#   Pop_foreign_born_pct <dbl>, Pop_nonenglish_home_pct <dbl>,\n#   Pop_hs_grad_pct <dbl>, Pop_college_grad_pct <dbl>,\n#   Pop_veterans_count <dbl>, Pop_avg_commute_mins <dbl>, …\n\n\n\n\n\n\n\n\n\nSuppose we hypothesize that Hillary Clinton would do relatively well in areas with more black voters. We would then need to combine the county level data about ethnic composition with the county \\(\\times\\) candidate level data on vote outcomes.\nThis is achieved in Example 6.9 in two steps. First, both datasets are cleaned to only contain the relevant data: for the results dataset only the Democrat rows are kept, and only the fips (county code), candidate, votes, and fraction columns. For the counties dataset, all rows are kept but only the county code, name, and Race_white_pct columns are kept.\n\n\n\n\n\n\n\nExample 6.9 Joining data at the result and the county level\n\nPython codeR code\n\n\n\nc = counties[[\"fips\", \"area_name\", \"Race_black_pct\"]]\n\nr = results.loc[results.candidate == \"Hillary Clinton\"]\nr = r[[\"fips\", \"votes\", \"fraction_votes\"]]\nr = r.merge(c)\nr.head()\n\n     fips  votes  fraction_votes       area_name  Race_black_pct\n0  1001.0   2387           0.800  Autauga County            18.7\n1  1003.0   5290           0.647  Baldwin County             9.6\n2  1005.0   2567           0.906  Barbour County            47.6\n3  1007.0    942           0.755     Bibb County            22.1\n4  1009.0    564           0.551   Blount County             1.8\n\n\n\n\n\nc = counties %>% \n  select(\"fips\", \"area_name\", \"Race_black_pct\")\nr = results %>% \n  filter(candidate == \"Hillary Clinton\") %>% \n  select(fips, votes, fraction_votes)\nr = inner_join(r, c)\ncor.test(r$Race_black_pct, r$fraction_votes)\n\n\n    Pearson's product-moment correlation\n\ndata:  r$Race_black_pct and r$fraction_votes\nt = 50.944, df = 2806, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6734586 0.7119165\nsample estimates:\n      cor \n0.6931806 \n\n\n\n\n\n\n\n\n\nIn the next step, both sets are joined using an inner join from the results dataset. Note that we could also have used a left join here, but with an inner join it will be immediately obvious if county level data is missing, as the number of rows will then decrease. In fact, in this case the number of rows does decrease, because some results do not have corresponding county data. As a puzzle, can you use the dataset filtering commands discussed above to find out which results these are?\nNote also that the county level data contains units that are not used, particularly the national and state level statistics. These, and the results that do not correspond to counties, are automatically filtered out by using an inner join.\nFinally, we can create a scatter plot or correlation analysis of the relation between ethnic composition and electoral success (see how to create the scatter plot in Section 7.2). In this case, it turns out that Hillary Clinton did indeed do much better in counties with a high percentage of black residents. Note that we cannot take this to mean there is a direct causal relation, there could be any number of underlying factors, including the date of the election which is very important in primary races. Statistically, since observations within a state are not independent, we should really control for the state-level vote here. For example, we could use a partial correlation, but we would still be violating the independence assumption of the errors, so it would be better to take a more sophisticated (e.g. multilevel) modeling approach. This, however, is well beyond the scope of this chapter."
  },
  {
    "objectID": "content/chapter06.html#sec-pivot",
    "href": "content/chapter06.html#sec-pivot",
    "title": "6  Data Wrangling",
    "section": "6.5 Reshaping Data: Wide To Long And Long To Wide",
    "text": "6.5 Reshaping Data: Wide To Long And Long To Wide\nData that you find or create does not always have the shape that you need it to be for your analysis. In many cases, for further data wrangling or for analyses you want each observation to be in its own row. However, many data sources list multiple observations in columns. For example, data from panel surveys asking the same question every week will often have one row per respondent, and one column for each weekly measurement. For a time-series analysis, however, each row should be a single measurement, i.e. the unit of analysis is a respondent per week.\nGenerally, data with multiple observations of the same unit is called wide data (as there are many columns), while a dataset with one row for each observation is called long data (as there are many rows). In most cases, long data is easiest to work with, and in fact in tidyverse jargon such data is called tidy data.\nAs a first relatively simple example, consider the datasets containing public and private capital. This data is “wide” in the sense that the measurements for the different countries are contained in the columns. To make this data “long” we would have to create rows for each country–year combination. This will make it much easier to do further data wrangling or analysis, as you can now e.g. directly merge the datasets and compute the pooled correlation between these variables. In fact, when we merged these datasets earlier in Example 6.10, we selected only the measurements for France, essentially turning it into long data.\n\n\n\n\n\n\n\nExample 6.10 Converting wide to long data to facilitate merging and visualizing.\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/private_capital.csv\"\nprivate = pd.read_csv(url)\nprivate = private.melt(id_vars=\"Year\", var_name=\"country\", value_name=\"capital\")\nprivate.head()\n\n   Year country  capital\n0  1970    U.S.     3.42\n1  1971    U.S.     3.41\n2  1972    U.S.     3.49\n3  1973    U.S.     3.39\n4  1974    U.S.     3.21\n\n\n\n\n\nurl = \"https://cssbook.net/d/private_capital.csv\"\nprivate = read_csv(url)\nprivate = private %>% pivot_longer(cols = -Year,\n    names_to=\"country\", values_to=\"capital\")\nhead(private)\n\n# A tibble: 6 × 3\n   Year country capital\n  <dbl> <chr>     <dbl>\n1  1970 U.S.       3.42\n2  1970 Japan      2.99\n3  1970 Germany    2.25\n4  1970 France     3.1 \n5  1970 U.K.       3.06\n6  1970 Italy      2.39\n\n\n\n\n\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = pd.read_csv(url)\npublic = public.melt(id_vars=\"Year\", var_name=\"country\", value_name=\"capital\")\nd = pd.concat([private.assign(type=\"private\"), public.assign(type=\"public\")])\n\ncountries = {\"France\", \"U.K.\", \"Germany\"}\nd = d.loc[d.country.isin(countries)]\nd.reset_index(inplace=True)\nplt = sns.lineplot(data=d, x=\"Year\", y=\"capital\", hue=\"country\", style=\"type\")\nplt.set(ylabel=\"Capital (% of national income\")\nplt.set_title(\n    \"Capital in Europe, 1970 - 2010\" \"\\nPartial reproduction of Piketty fig 4.4\"\n)\n\n\n\n\n\n\n\nurl = \"https://cssbook.net/d/public_capital.csv\"\npublic = read_csv(url) %>% pivot_longer(-Year,\n    names_to=\"country\", values_to=\"capital\")\n\nd = bind_rows(\n    private %>% add_column(type=\"private\"),\n    public %>% add_column(type=\"public\"))\ncountries = c(\"Germany\", \"France\", \"U.K.\")\nd %>% filter(country %in% countries) %>% \n  ggplot(aes(x=Year, y=capital, \n             color=country, lty=type)) + \n  geom_line()+ \n  ylab(\"Capital (% of national income)\") +\n  guides(colour=guide_legend(\"Country\"), \n         linetype=guide_legend(\"Capital\")) + \n  theme_classic() + \n  ggtitle(\"Capital in Europe, 1970 - 2010\", \n    \"Partial reproduction of Piketty fig 4.4\")\n\n\n\n\n\n\n\n\n\n\n\nExample 6.10 shows how you can “pivot” the capital data to long format using pivot_longer (R) and melt (Pandas). The second part of this example then goes on to do this for both datasets, merge them, and partially reproduce Figure 4.4 from Piketty (2017)."
  },
  {
    "objectID": "content/chapter06.html#restructuring-messy-data",
    "href": "content/chapter06.html#restructuring-messy-data",
    "title": "6  Data Wrangling",
    "section": "6.6 Restructuring Messy Data",
    "text": "6.6 Restructuring Messy Data\nAs a final example, we will look at the data on income and wage shares from Piketty (supplemental tables S8.1 and S8.2). We want to visualize the income and wage share going to the top 1% earners in France and the US. Figure 6.2 shows a screen shot of this data in Libre Office, with the US data having a similar shape. For the previous examples, we used a clean csv version of this data, but now we will tackle the additional challenge of dealing with the Excel file including extra header rows and column names aimed at human consumption rather than easy computing.\n\n\n\nFigure 6.2: Data on top incomes as provided in Piketty (2014; digital appendix).\n\n\nIn order to perform our visualization, we want a dataset containing a single measurement column (percentage share), and a row for each year–country–type combination, i.e. one row for wage inequality in 1910 in the US. One of the most important skills in computational social science (and data-driven analysis in general) is understanding which series of generally small steps are needed to go from one data format to the other. Although there is not a fixed set of steps that are always needed, the steps to get from the raw data visualized in Figure 6.2 to a “tidy” dataset are fairly typical:\n\nInput: read the data into data frames. In this case, reading from an Excel sheet and skipping the extra header rows\n\nReshape: pivoting the data into long format\nNormalize: normalize names, value types, etc. In this case, also separate a header like “Top 1% income share” into income type (income, wage) and percentile (10%, 1%, etc)\nFilter: filter for the desired data\nAnalyze: create the visualization\n\n\nFortunately, these steps have been discussed before: reading csv data in Section 5.2; pivot to long data in Section 6.5; add a column in Section 6.2; joining data in Section 6.4; and visualizing in Section 7.2.\nExample 6.11 shows how to perform these steps for the US case. First, we use the readxl (R) and xlrd (Python) to read a sheet from an Excel file into a data frame, manually specifying the number of header and footer rows to skip. Then, we pivot the columns into a long format. In step 3, we split the header into two columns using separate (R) and split (Python). Finally, steps 4 and 5 take the desired subset and create a line plot.\nThe missing step, splitting a header into two columns, is done using separate (R) and split (Python).\n\n\n\n\n\n\n\nExample 6.11 Dealing with ``messy’’ data.\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/Chapitre8.xls\"\n# 1 Input: Read the data into a data frame\nd = pd.read_excel(url, sheet_name=\"TS8.2\", skiprows=4, skipfooter=3)\n\nd = d.rename(columns={\"Unnamed: 0\": \"year\"})\n\n# 2 Reshape: Pivoting to long, dropping missing\nd = d.melt(value_name=\"share\", id_vars=\"year\")\n\n# 3 Normalize\ncols = [\"_top\", \"percentile\", \"type\", \"_share\", \"capital_gains\"]\nd[cols] = d.variable.str.split(n=4, expand=True)\nd = d.drop(columns=[\"variable\", \"_top\", \"_share\"])\nd[\"capital_gains\"] = d[\"capital_gains\"].notna()\nd.head()\n\n   year  share percentile    type  capital_gains\n0  1900  0.405        10%  income          False\n1  1901    NaN        10%  income          False\n2  1902    NaN        10%  income          False\n3  1903    NaN        10%  income          False\n4  1904    NaN        10%  income          False\n\n\n\n\n\n#1 Input: Read the data into a data frame\nurl=\"https://cssbook.net/d/Chapitre8.xls\"\ndest = tempfile(fileext=\".xls\")\ndownload.file(url, dest)\nd = read_excel(dest,sheet=\"TS8.2\",skip=4)\nd = d%>% rename(\"year\"=1)\n\n#2 Reshape: Pivoting to long, dropping missing\nd = d%>%pivot_longer(-year, values_to=\"share\")%>% \n        na.omit()\n\n#3 Normalize\ncols = c(NA,\"percent\",\"type\",NA,\"capital_gains\")\nd = d %>% separate(name, into=cols,\n   sep=\" \", extra=\"merge\", fill=\"right\") %>% \n  mutate(year=as.numeric(year), \n         capital_gains=!is.na(capital_gains))\nhead(d)\n\n# A tibble: 6 × 5\n   year percent type   capital_gains  share\n  <dbl> <chr>   <chr>  <lgl>          <dbl>\n1  1900 10%     income FALSE         0.405 \n2  1900 10%     income TRUE          0.403 \n3  1910 10%     income FALSE         0.406 \n4  1910 10%-5%  income FALSE         0.0989\n5  1910 5%-1%   income FALSE         0.129 \n6  1910 1%      income FALSE         0.178 \n\n\n\n\n\n\nPython codeR code\n\n\n\n# 4 Filter for the desired data\nsubset = d[\n    (d.year >= 1910) & (d.percentile == \"1%\") & (d.capital_gains == False)\n]\n\n# 5 Analyze and/or visualize\nplt = sns.lineplot(data=subset, hue=\"type\", x=\"year\", y=\"share\")\nplt.set(xlabel=\"Year\", ylabel=\"Share of income going to top-1%\")\n\n\n\n\n\n\n\n#4 Filter for the desired data\nsubset = d %>% filter(year >=1910, \n                      percent==\"1%\", \n                      capital_gains==F)\n\n#5 Analyze and/or visualization\nggplot(subset, aes(x=year, y=share, color=type)) +\n  geom_line() + xlab(\"Year\") + \nylab(\"Share of income going to top-1%\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPiketty, Thomas. 2017. Capital in the Twenty-First Century. Cambridge, MA: Harvard University Press."
  },
  {
    "objectID": "content/chapter07.html#sec-simpleeda",
    "href": "content/chapter07.html#sec-simpleeda",
    "title": "7  Exploratory data analysis",
    "section": "7.1 Simple Exploratory Data Analysis",
    "text": "7.1 Simple Exploratory Data Analysis\nNow that you are familiar with data structures (Chapter 5) and data wrangling (Chapter 6) you are probably eager to get some real insights into your data beyond the basic techniques we briefly introduced in Chapter 2.\nAs we outlined in Chapter 1, the computational analysis of communication can be bottom-up or top-down, inductive or deductive. Just as in traditional research methods Bryman (2012), sometimes, an inductive bottom-up approach is a goal in itself: after all, explorative analyses are invaluable for generating hypotheses that can be tested in follow-up research. But even when you are conducting a deductive, hypothesis-testing study, it is a good idea to start by describing your dataset using the tools of exploratory data analysis to get a better picture of your data. In fact, we could even go as far as saying that obtaining details like frequency tables, cross-tabulations, and summary statistics (mean, median, mode, etc.) is always necessary, even if your research questions or hypotheses require further complex analysis. For the computational analysis of communication, a significant amount of time may actually be invested at this stage.\nExploratory data analysis (EDA), as originally conceived by Tukey (1977), can be a very powerful framework to prepare and evaluate data, as well as to understand its properties and generate insights at any stage of your research. It is mandatory to do some EDA before any sophisticated analysis to know if the data is clean enough, if there are missing values and outliers, and how the distributions are shaped. Furthermore, before making any multivariate or inferential analysis we might want to know the specific frequencies for each variable, their measures of central tendency, their dispersion, and so on. We might also want to integrate frequencies of different variables into a single table to have an initial picture of their interrelations.\nTo illustrate how to do this in R and Python, we will use existing representative survey data to analyze how support for migrants or refugees in Europe changes over time and differs per country. The Eurobarometer (freely available at the Leibniz Institute for the Social Sciences – GESIS) has contained these specific questions since 2015. We might pose questions about the variation of a single variable or also describe the covariation of different variables to find patterns in our data. In this section, we will compute basic statistics to answer these questions and in the next section we will visualize them by plotting within and between variable behaviors of a selected group of features of the Eurobarometer conducted in November 2017 to 33193 Europeans.\nFor most of the EDA we will use tidyverse in R and pandas as well as numpy and scipy in Python (Example 7.1). After loading a clean version of the survey data1 stored in a csv file (using the tidyverse function read_csv in R and the pandas function read_csv in R), checking the dimensions of our data frame (33193 x 17), we probably want to get a global picture of each of our variables by getting a frequency table. This table shows the frequency of different outcomes for every case in a distribution. This means that we can know how many cases we have for each number or category in the distribution of every variable, which is useful in order to have an initial understanding of our data.\n\n\n\n\n\n\npandas versus pure numpy/scipy\n\n\n\n\n\nIn this book, we use pandas data frames a lot: they make our lives easier compared to native data types (Section 3.1), and they already integrate a lot of functionality of underlying math and statistics packages such as numpy and scipy. However, you do not have to force your data into a data frame if a different structure makes more sense in your script. numpy and scipy will happily calculate mean, media, skewness, and kurtosis of the values in a list, or the correlation between two lists. It’s up to you.\n\n\n\n\n\n\n\n\n\n\nExample 7.1 Load data from Eurobarometer survey and select some variables\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/eurobarom_nov_2017.csv\"\nd2 = pd.read_csv(url)\nprint(\"Shape of my filtered data =\", d2.shape)\n\nShape of my filtered data = (33193, 17)\n\nprint(d2.columns)\n\nIndex(['survey', 'uniqid', 'date', 'country', 'marital_status', 'educational',\n       'gender', 'age', 'occupation', 'type_community',\n       'household_composition', 'support_refugees', 'support_migrants',\n       'date_n', 'support_refugees_n', 'support_migrants_n', 'educational_n'],\n      dtype='object')\n\n\n\n\n\nurl=\"https://cssbook.net/d/eurobarom_nov_2017.csv\"\nd2= read_csv(url, col_names = TRUE)\nglue(\"{nrow(d2)} row x {ncol(d2)} columns\")\n\n33193 row x 17 columns\n\ncolnames(d2)\n\n [1] \"survey\"                \"uniqid\"                \"date\"                 \n [4] \"country\"               \"marital_status\"        \"educational\"          \n [7] \"gender\"                \"age\"                   \"occupation\"           \n[10] \"type_community\"        \"household_composition\" \"support_refugees\"     \n[13] \"support_migrants\"      \"date_n\"                \"support_refugees_n\"   \n[16] \"support_migrants_n\"    \"educational_n\"        \n\n\n\n\n\n\n\n\n\nLet us first get the distribution of the categorical variable gender by creating tables that include absolute and relative frequencies. The frequency tables (using the dplyr functions group_by and summarize in R, and pandas function value_counts in Python) reveals that 17716 (53.38%) women and 15477 (46.63%) men answered this survey (Example 7.2). We can do the same with the level of support of refugees [support_refugees] (To what extent do you agree or disagree with the following statement: our country should help refugees) and obtain that 4957 (14.93%) persons totally agreed with this statement, 12695 (38.25%) tended to agree, 5931 (16.24%) tended to disagree and 3574 (10.77%) totally disagreed.\n\n\n\n\n\n\n\nExample 7.2 Absolute and relative frequencies of support of refugees and gender.\n\nPython codeR code\n\n\n\nprint(d2[\"gender\"].value_counts())\n\nWoman    17716\nMan      15477\nName: gender, dtype: int64\n\nprint(d2[\"gender\"].value_counts(normalize=True))\n\nWoman    0.533727\nMan      0.466273\nName: gender, dtype: float64\n\n\n\n\n\nd2 %>%\n  group_by(gender) %>%\n  summarise(frequency = n()) %>%\n  mutate(rel_freq = frequency / sum(frequency))   \n\n# A tibble: 2 × 3\n  gender frequency rel_freq\n  <chr>      <int>    <dbl>\n1 Man        15477    0.466\n2 Woman      17716    0.534\n\n\n\n\n\n\nPython codeR code\n\n\n\nprint(d2[\"support_refugees\"].value_counts())\n\nTend to agree       12695\nTend to disagree     5391\nTotally agree        4957\nTotally disagree     3574\nName: support_refugees, dtype: int64\n\nprint(d2[\"support_refugees\"].value_counts(normalize=True, dropna=False))\n\nTend to agree       0.382460\nNaN                 0.198114\nTend to disagree    0.162414\nTotally agree       0.149339\nTotally disagree    0.107673\nName: support_refugees, dtype: float64\n\n\n\n\n\nd2 %>%\n  group_by(support_refugees) %>%\n  summarise(frequency = n()) %>%\n  mutate(rel_freq = frequency / sum(frequency)) \n\n# A tibble: 5 × 3\n  support_refugees frequency rel_freq\n  <chr>                <int>    <dbl>\n1 Tend to agree        12695    0.382\n2 Tend to disagree      5391    0.162\n3 Totally agree         4957    0.149\n4 Totally disagree      3574    0.108\n5 <NA>                  6576    0.198\n\n\n\n\n\n\n\n\n\nBefore diving any further into any between variables analysis, you might have noticed that there might be some missing values in the data. These values represent an important amount of data in many real social and communication analysis (just remember that you cannot be forced to answer every question in a telephone or face-to-face survey!). From a statistical point of view, we can have many approaches to address missing values: For example, we can drop either the rows or columns that contain any of them, or we can impute the missing values by predicting them based on their relation with other variables – as we did in Section 6.2 by replacing the missing values with the column mean. It goes beyond the scope of this chapter to explain all the imputation methods (and, in fact, mean imputation has some serious drawbacks when used in subsequent analysis), but at least we need to know how to identify the missing values in our data and how to drop the cases that contain them from our dataset.\nIn the case of the variable support_refugees we can count its missing data (6576 cases) with base R function is.na and the pandas method isna2. Then we may decide to drop all the records that contain these values in our dataset using the tidyr function drop_na in R and the Pandas function dropna in Python3 (Example 7.3). By doing this we get a cleaner dataset and can continue with a more sophisticated EDA with cross-tabulation and summary statistics for the group of cases.\n\n\n\n\n\n\n\nExample 7.3 Drop missing values\n\nPython codeR code\n\n\n\nn_miss = d2[\"support_refugees\"].isna().sum()\nprint(f\"# of missing values: {n_miss}\")\n\n# of missing values: 6576\n\nd2 = d2.dropna()\nprint(f\"Shape after dropping NAs: {d2.shape}\")\n\nShape after dropping NAs: (23448, 17)\n\n\n\n\n\nn_miss = sum(is.na(d2$support_refugees))\nprint(glue(\"# of missing values: {n_miss}\"))\n\n# of missing values: 6576\n\nd2 = d2 %>% drop_na()\nprint(glue(\"Rows after dropping NAs: {nrow(d2)}\"))\n\nRows after dropping NAs: 23448\n\n\n\n\n\n\n\n\n\nNow let us cross tabulate the gender and support_refugees to have an initial idea of what the relationship between these two variables might be. With this purpose we create a contingency table or cross-tabulation to get the frequencies in each combination of categories (using dplyr functions group_by, summarize and spread in R, and the pandas function crosstab in Python; example 7.4). From this table you can easily see that 2178 women totally supported helping refugees and 1524 men totally did not. Furthermore, other interesting questions about our data might now arise if we compute summary statistics for a group of cases (using again dplyr functions group_by, summarize and spread, and base mean in R; and pandas function groupby and base mean in Python). For example, you might wonder what the average ages of the women were that totally supported (52.42) or not (53.2) to help refugees. This approach will open a huge amount of possible analysis by grouping variables and estimating different statistics beyond the mean, such as count, sum, median, mode, minimum or maximum, among others.\n\n\n\n\n\n\n\nExample 7.4 Cross tabulation of support of refugees and gender, and summary statistics\n\nPython codeR code\n\n\n\nprint(\"Crosstab gender and support_refugees:\")\n\nCrosstab gender and support_refugees:\n\nprint(pd.crosstab(d2[\"support_refugees\"], d2[\"gender\"]))\n\ngender             Man  Woman\nsupport_refugees             \nTend to agree     5067   5931\nTend to disagree  2176   2692\nTotally agree     2118   2178\nTotally disagree  1524   1762\n\nprint(\"Summary statistics for group of cases:\")\n\nSummary statistics for group of cases:\n\nprint(d2.groupby([\"support_refugees\", \"gender\"])[\"age\"].mean())\n\nsupport_refugees  gender\nTend to agree     Man       54.073022\n                  Woman     53.373799\nTend to disagree  Man       52.819853\n                  Woman     52.656761\nTotally agree     Man       53.738905\n                  Woman     52.421947\nTotally disagree  Man       52.368110\n                  Woman     53.203746\nName: age, dtype: float64\n\n\n\n\n\nprint(\"Crosstab gender and support_refugees:\")\n\n[1] \"Crosstab gender and support_refugees:\"\n\nd2 %>%\n  group_by(gender, support_refugees)%>%\n  summarise(n=n())%>%\n  pivot_wider(values_from=\"n\",names_from=\"gender\")\n\n# A tibble: 4 × 3\n  support_refugees   Man Woman\n  <chr>            <int> <int>\n1 Tend to agree     5067  5931\n2 Tend to disagree  2176  2692\n3 Totally agree     2118  2178\n4 Totally disagree  1524  1762\n\nprint(\"Summary statistics for group of cases:\")\n\n[1] \"Summary statistics for group of cases:\"\n\nd2 %>%\n  group_by(support_refugees, gender)%>%\n  summarise(mean_age=mean(age, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   support_refugees [4]\n  support_refugees gender mean_age\n  <chr>            <chr>     <dbl>\n1 Tend to agree    Man        54.1\n2 Tend to agree    Woman      53.4\n3 Tend to disagree Man        52.8\n4 Tend to disagree Woman      52.7\n5 Totally agree    Man        53.7\n6 Totally agree    Woman      52.4\n7 Totally disagree Man        52.4\n8 Totally disagree Woman      53.2"
  },
  {
    "objectID": "content/chapter07.html#sec-visualization",
    "href": "content/chapter07.html#sec-visualization",
    "title": "7  Exploratory data analysis",
    "section": "7.2 Visualizing Data",
    "text": "7.2 Visualizing Data\nData visualization is a powerful technique for both understanding data yourself and communicating the story of your data to others. Based on ggplot2 in R and matplotlib and seaborn in Python, this section covers histograms, line and bar graphs, scatterplots and heatmaps. It touches on combining multiple graphs, communicating uncertainty with boxplots and ribbons, and plotting geospatial data. In fact, visualizing data is an important stage in both EDA and advanced analytics, and we can use graphs to obtain important insights into our data. For example, if we want to visualize the age and the support for refugees of European citizens, we can plot a histogram and a bar graph, respectively.\n\n\n\n\n\n\nR: GGPlot syntax\n\n\n\n\n\nOne of the nicest features of using R for data exploration is the ggplot2 package for data visualization. This is a package that brings a unified method for visualizing with generally good defaults but that can be customized in every way if desired. The syntax, however, can look a little strange at first. Let’s consider the command from Example 7.5:\nggplot (data=d2) + geom_bar(mapping=aes(x= support_refugees), fill=\"blue\")\nWhat you can see here is that every ggplot is composed of multiple sub-commands that are added together with the plus sign. At a minimum, every ggplot needs two sub-commands: ggplot, which initiates the plot and can be seen as an empty canvas, and one or more geom commands which add geometries to the plot, such as bars, lines, or points. Moreover, each geometry needs a data source, and an aesthetic mapping which tells ggplot how to map columns in the data (in this case the support_refugees column) to graphical (aesthetic) elements of the plot, in this case the \\(x\\) position of each bar. Graphical elements can also be set to a constant value rather than mapped to a column, in which case the argument is placed outside the aes function, as in the fill=\"blue\" above.\nEach aesthetic mapping is assigned a scale. This scale is initialized with a sensible default which depends on the data type. For example, the color of the lines in Example 7.9 are mapped to the group column. Since that is a nominal value (character column), ggplot automatically assigns colors to each group, in this case blue and red. In Example 7.15, on the other hand, the fill color is mapped to the score column, which is numerical (interval) data, to which ggplot by default assigns a color range of white to blue.\nAlmost every aspect of ggplot can be customized by adding more subcommands. For example, you can specify the title and axis labels by adding + labs(title=\"Title\", x=\"Axis Label\") to the plot, and you can completely alter the look of the graph by applying a theme. For example, the ggthemes package defines an Economist theme, so by simply adding + theme_economist() to your plot you get the characteristic layout of plots from that magazine. You can also customize the way scales are mapped using the various scale_variable_mapping functions. For example, Example 7.19 uses scale_fill_viridis_c(option = \"B\") to use the viridis scale for the fill aesthetic, specifying that scale B should be used. Similar commands can be used to e.g. change the colors of color ranges, the size of points, etc.\nBecause all geometries start with geom_, all scales start with scale_, all themes start with theme_, etc., you can use the RStudio autocompletion to browse through the complete list of options: simply type geom_, press tab or control+space, and you get a list of the options with a short description, and you can press F1 to get help on each option. The help for every geometry also lists all aesthetic elements that can or must be supplied.\nBesides the built-in help, there are a number of great (online) resources to learn more. Specifically, we recommend the book Data Visualization: A practical introduction by Kieran Healy4. Another great resource is the R Graph Gallery5, which has an enormous list of possible visualizations, all with R code included and most of them based on ggplot. Finally, we recommend the Data-to-Viz6 website, which allows you to explore a number of graph types depending on your data, lists the do’s and don’ts for each graph, and links to the Graph Gallery for concrete examples.\n\n\n\n\n7.2.1 Plotting Frequencies and Distributions\nIn the case of nominal data, the most straightforward way to visualize them is to simply count the frequency of value and then plot them as a bar chart. For instance, when we depict the support to help refugees (Example 7.5) you can quickly get that the option “tend to agree” is the most frequently voiced answer.\n\n\n\n\n\n\n\nExample 7.5 Barplot of support for refugees\n\nPython codeR code\n\n\n\nd2[\"support_refugees\"].value_counts().plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\n\n\nggplot(data=d2) +\n  geom_bar(mapping = aes(x= support_refugees))\n\n\n\n\n\n\n\n\n\n\n\nIf we have continuous variables, however, having such a bar chart would lead to too many bars: we may lose oversight (and creating the graph may be resource-intensive). Instead, we want to group the data into bins, such as age groups. Hence, a histogram is used to examine the distribution of a continuous variable (ggplot2 function geom_histogram in R and pandas function hist in Python) and a bar graph to inspect the distribution of a categorical one (ggplot2 function geom_bar() in R and matplotlib function plot in Python). In Example 7.6 you can easily see the shape of the distribution of the variable age, with many values close to the average and a slightly bigger tail to the right (not that far from the normal distribution!).\n\n\n\n\n\n\n\nExample 7.6 Histogram of Age\n\nPython codeR code\n\n\n\nd2.hist(column=\"age\", bins=15)\nplt.show()\n\n\n\n\n\n\n\nggplot(data=d2) +\n  geom_histogram(mapping = aes(x= age), bins = 15)\n\n\n\n\n\n\n\n\n\n\n\nAnother way to show distributions is using bloxplots, which are powerful representations of the distribution of our variables through the use of quartiles that are marked with the 25th, 50th (median) and 75th percentiles of any given variable. By examining the lower and upper levels of two or more distributions you can compare their variability and even detect possible outliers. You can generate multiple boxplots to compare the ages of the surveyed citizens by country and quickly see that in terms of age the distributions of Spain and Greece are quite similar, but we can identify some differences between Croatia and the Netherlands. In R we use the base function geom_boxplot, while in Python we use the seaborn function boxplot.\n\n\n\n\n\n\n\nExample 7.7 Bloxplots of age by country\n\nPython codeR code\n\n\n\nd2 = d2.sort_values(by=\"country\")\nplt.figure(figsize=(8, 8))\nsns.boxplot(x=\"age\", y=\"country\", data=d2)\nplt.show()\n\n\n\n\n\n\n\nggplot(d2, aes(y=fct_rev(country), x=age))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Plotting Relationships\nAfter having inspected distributions of single variables, you may want to check how two variables are related. We are going to discuss two ways of doing so: plotting data over time, and scatterplots to illustrate the relationship between two continuous variables.\nThe Eurobarometer collects data for 15 days (in the example from November 5 to 19, 2017) and you may wonder if the level of support to refugees or even to general migrants changes over the time. This is actually a simple time series and you can use a line graph to represent it. Firstly you must use a numerical variable for the level of support (support_refugees_n, which ranges from 1 to 4, 4 being the maximum support) and group it by day in order to get the average for each day. In the case of R, you can plot the two series using the base function plot, or you can use the ggplot2 function geom_line. In the case of Python you can use the matplotlib function plot or the seaborn function lineplot. To start, Example 7.8 shows how to create a graph for the average support for refugees by day.\n\n\n\n\n\n\n\nExample 7.8 Line graph of average support for refugees by day\n\nPython codeR code\n\n\n\nsupport_refugees = d2.groupby([\"date_n\"])[\"support_refugees_n\"].mean()\nsupport_refugees = support_refugees.to_frame()\n\nplt.plot(support_refugees.index, support_refugees[\"support_refugees_n\"])\nplt.xlabel(\"Day\")\nplt.ylabel(\"Support for refugees\")\nplt.show()\n\n\n\n\n\n\n\nsupport_refugees = d2 %>%\n  group_by(date_n) %>%\n  summarise(support=mean(support_refugees_n, \n                         na.rm = TRUE))\nggplot(support_refugees,aes(x=date_n, y=support))+\n  geom_line() + \n  xlab(\"Day\") + \n  ylab(\"Support for refugees\")\n\n\n\n\n\n\n\n\n\n\n\nTo also plot the support for migrants, you can combine multiple subgraphs in a single plot, giving the reader a broader and more comparative perspective (Example 7.9). In R, the geom_line also takes a color aesthetic, but this requires the data to be in long format. So, we first reshape the data and also change the factor labels to get a better legend (see Section 6.5). In Python, you can plot the two lines as separate figures and add the pyplot function show to display an integrated figure.\n\n\n\n\n\n\n\nExample 7.9 Plotting multiple lines in one graph\n\nPython codeR code\n\n\n\n# Combine data\nsupport_combined = d2.groupby([\"date_n\"]).agg(\n    refugees=(\"support_refugees_n\", \"mean\"),\n    migrants=(\"support_migrants_n\", \"mean\"),\n)\n\n# plot\nsns.lineplot(x=\"date_n\", y=\"refugees\", data=support_combined, color=\"blue\")\nsns.lineplot(x=\"date_n\", y=\"migrants\", data=support_combined, color=\"red\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Level of support\")\nplt.title(\"Support of refugees and migrants\")\nplt.show()\n\n\n\n\n\n\n\n# Combine data\nsupport_combined = d2 %>% group_by(date_n) %>%\n summarise(\n  refugees=mean(support_refugees_n, na.rm = TRUE),\n  migrants=mean(support_migrants_n, na.rm = TRUE))\n\n# Pivot to long format and plot \nsupport_long = support_combined %>% \n  pivot_longer(-date_n, names_to=\"group\", \n               values_to=\"support\")\nggplot(support_long, \n       aes(x=date_n, y=support, colour=group)) +\n  geom_line(size = 1.5) +\n  labs(title=\"Support for refugees and migrants\", \n       x=\"Day\", y=\"Level of Support\") \n\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can create multiple subplots, one for each group that you want to show (Example 7.10). In ggplot (R), you can use the facet_grid function to automatically create subplots that each show one of the groups. In the case of Python you can use the matplotlib function subplots that allows you to configure multiple plots in a single one.\n\n\n\n\n\n\n\nExample 7.10 Creating subfigures)\n\nPython codeR code\n\n\n\nf, axes = plt.subplots(2, 1)\nsns.lineplot(x=\"date_n\", y=\"refugees\", data=support_combined, ax=axes[0])\nsns.lineplot(x=\"date_n\", y=\"migrants\", data=support_combined, ax=axes[1])\n\nsns.lineplot(x=\"date_n\", y=\"support_refugees_n\", data=d2, ci=0, ax=axes[0])\nsns.lineplot(x=\"date_n\", y=\"support_migrants_n\", data=d2, ci=0, ax=axes[1])\nplt.show()\n\n\n\n\n\n\n\nggplot(support_long, aes(x=date_n, y=support)) +  \n  geom_line() + facet_grid(rows=vars(group)) +\n  xlab(\"Day\") + ylab(\"Support\")\n\n\n\n\n\n\n\n\n\n\n\nNow if you want to explore the possible correlation between the average support for refugees (mean_support_refugees_by_day) and the average support to migrants by year (mean_support_migrants_by_day), you might need a scatterplot, which is a better way to visualize the type and strength of this relationship scatter.\n\n\n\n\n\n\n\nExample 7.11 Scatterplot of average support for refugees and migrants by year\n\nPython codeR code\n\n\n\nsns.scatterplot(data=support_combined, x=\"refugees\", y=\"migrants\")\n\n\n\n\n\n\n\nggplot(support_combined, \n       aes(x=refugees, y=migrants))+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nA scatterplot uses dots to depict the values of two variables in a Cartesian plane (with coordinates for the axes \\(x\\) and \\(y\\)). You can easily plot this figure in R using the ggplot2 function geom_point (and geom_smooth to display a regression line!), or in Python using seaborn function scatterplot (lmplot to include the regression line as shown in Example 7.12).\n\n\n\n\n\n\n\nExample 7.12 Scatterplot with regression line\n\nPython codeR code\n\n\n\nsns.lmplot(data=support_combined, x=\"refugees\", y=\"migrants\")\n\n\n\nplt.show()\n\n\n\n\n\n\n\nggplot(support_combined,\n       aes(x=refugees, y= migrants))+\n  geom_point()+\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\n\n\n\nLooking at the dispersion of points in the provided example you can infer that there might be a positive correlation between the two variables, or in other words, the more the average support to refugees the more the average support to migrants over time.\nWe can check and measure the existence of this correlation by computing the Pearson correlation coefficient or Pearson’s r, which is the most well-known correlation function. As you probably remember from your statistics class, a correlation refers to a relationship between two continuous variables and is usually applied to measure linear relationships (although there also exist nonlinear correlation coefficients, such as Spearman’s \\(\\rho\\)). Specifically, Pearson’s \\(r\\) measures the linear correlation between two variables (X and Y) producing a value between \\(-1\\) and \\(+1\\), where 0 depicts the absence of correlation and values near to 1 a strong correlation. The signs (\\(+\\) or \\(-\\)) represent the direction of the relationship (being positive if two variables variate in the same direction, and negative if they vary in the opposite direction). The correlation coefficient is usually represented with r or the Greek letter \\(\\rho\\) and mathematically expressed as:\nYou can estimate this correlation coefficient with the pandas function corr in Python and the base R function cor in R. As shown in Example 7.13 the two variables plotted above are highly correlated with a coefficient of 0.95.\n\n\n\n\n\n\n\nExample 7.13 Pearson correlation coefficient\n\nPython codeR code\n\n\n\nprint(\n    support_combined[\"refugees\"].corr(\n        support_combined[\"migrants\"], method=\"pearson\"\n    )\n)\n\n0.9541243084907629\n\n\n\n\n\ncor.test(support_combined$refugees, \n         support_combined$migrants, \n         method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  support_combined$refugees and support_combined$migrants\nt = 9.0133, df = 8, p-value = 1.833e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8127522 0.9893855\nsample estimates:\n      cor \n0.9541243 \n\n\n\n\n\n\n\n\n\nAnother useful representation is the heatmap. This figure can help you plot a continuous variable using a color scale and shows its relation with another two variables. This means that you represent your data as colors, which might be useful for understanding patterns. For example, we may wonder what the level of support for refugees is given the nationality and the gender of the individuals. For this visualization, it is necessary to create a proper data frame (Example 7.14) to plot the heatmap, in which each number of your continuous variable *_refugees_n* is included in a table where each axis (x= gender, y=country) represents the categorical variables. This pivoted table stored in an object called pivot_data can be generated using some of the already explained commands.\n\n\n\n\n\n\n\nExample 7.14 Create a data frame to plot the heatmap\n\nPython codeR code\n\n\n\npivot_data = pd.pivot_table(\n    d2, values=\"support_refugees_n\", index=[\"country\"], columns=\"gender\"\n)\n\n\n\n\npivot_data= d2 %>% \n  select(gender, country, support_refugees_n) %>%\n  group_by(country, gender) %>%\n  summarise(score = mean(support_refugees_n))\n\n\n\n\n\n\n\n\nIn the first resulting figure proposed in Example 7.15, the lighter the blue the greater the support in each combination of country \\(\\times\\) gender. You can see that level of support is similar in countries such as Slovenia or Spain, and is different in the Czech Republic or Austria. It also seems that women have a higher level of support. For this default heatmap we can use the ggplot2 function geom_tile in R and seaborn function heatmap in Python. To personalize the scale colors (e.g. if we want a scale of blues) we can use the ggplot2 function scale_fill_gradient in R or the parameter cmap of the seaborn function heatmap in Python.\n\n\n\n\n\n\n\nExample 7.15 Heatmap of country gender and support for refugees\n\nPython codeR code\n\n\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_data, cmap=\"Blues\", cbar_kws={\"label\": \"support_refugees_n\"})\nplt.show()\n\n\n\n\n\n\n\nggplot(pivot_data, aes(x = gender, \n    y = fct_rev(country), fill = score)) + \n  geom_tile()+\n  scale_fill_gradient2(low=\"white\", high=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\nAs you will notice, one of the goals of EDA is exploring the variance of our variables, which includes some uncertainty about their behavior. We will introduce you to two basic plots to visually communicate this uncertainty. Firstly, ribbons and area plots can help us to clearly identify a predefined interval of a variable in order to interpret its variance over some cases. Let us mark this interval in 0.15 points in the above-mentioned plots of the average support to refugees or migrants by day, and we can see that the lines tend to converge more on the very last day and are more separated by day four. This simple representation can be conducted in R using the ggplot2 function geom_ribbon and in Python using the parameter ci of the seaborn function lineplot.\n\n\n\n\n\n\n\nExample 7.16 Add ribbons to the graph lines of support to refugees and migrants\n\nPython codeR code\n\n\n\nsns.lineplot(\n    x=\"date_n\",\n    y=\"support_refugees_n\",\n    data=d2,\n    color=\"blue\",\n    ci=100,\n    label=\"Refugees\",\n)\nsns.lineplot(\n    x=\"date_n\",\n    y=\"support_migrants_n\",\n    data=d2,\n    color=\"red\",\n    ci=100,\n    label=\"Migrants\",\n)\nplt.xlabel(\"Day\")\nplt.ylabel(\"Level of support\")\nplt.title(\"Support for refugees and migrants\")\nplt.show()\n\n\n\n\n\n\n\nggplot(support_long, \n       aes(x=date_n, y=support, color=group)) + \n  geom_line(size=1.5) + \n  geom_ribbon(aes(fill=group, ymin=support-0.15,\n                  ymax=support+0.15),\n              alpha=.1, lty=0) +\n  ggtitle(\"Support for refugees and migrants\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Plotting Geospatial Data\nPlotting geospatial data is a more powerful tool to compare countries or other regions. Maps are very easy to understand and can have greater impact to all kinds of readers, which make them a useful representation for a wide range of studies that any computational analyst has to deal with. Geospatial data is based on the specific location of any country, region, city or geographical area, marked by its coordinates, latitude and longitude, that can later build points and polygon areas. The coordinates are normally mandatory to plot any data on a map, but are not always provided in our raw data. In those cases, we must join the geographical information we have (i.e. the name of a country) with its coordinates in order to have an accurate data frame for plotting geospatial data. Some libraries in R and Python might directly read and interpret different kinds of geospatial information by recognizing strings such as “France” or “Paris”, but in the end they will be converted into coordinates.\nUsing the very same data as our example, we might want to plot in a map the level of support to European refugees by country. Firstly, we should create a data frame with the average level of support to refugees by country (supports_country). Secondly, we must install an existing library that provides you with accurate geospatial information. In the case of R, we recommend the package maps which contains the function map_data that helps you generate an object with geospatial information of specific areas, countries or regions, that can be easily read and plotted by ggplot2. Even if not explained in this book, we also recommend ggmap in R (Kahle and Wickham, 2013). When working with Python we recommend geopandas that works very well with pandas and matplotlib (it will also need some additional packages such as descartes).\nIn Example 7.17 we illustrate how to plot a world map (from existing geographical information). We then save a partial map into the object some_eu_maps containing the European countries that participated in the survey. After we merge supports_country and some_eu_maps (by region) and get a complete data frame called support_map with coordinates for each country (Example 7.18). Finally, we plot it using the ggplot2 function geom_polygon in R and the geopandas method plot in Python (Example 7.19). Voilà: a nice and comprehensible representation of our data with a scale of colors!\n\n\n\n\n\n\n\nExample 7.17 Simple world map\n\nPython codeR code\n\n\n\nsupports_country = (\n    d2.groupby([\"country\"])[\"support_refugees_n\"]\n    .mean()\n    .to_frame()\n    .reset_index()\n)\n\n# Load a world map and plot it\nwmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\nwmap = wmap.rename(columns={\"name\": \"country\"})\nwmap.plot()\n\n\n\n\n\n\n\nsupports_country = d2 %>%\n  group_by(country) %>%\n  summarise(m=mean(support_refugees_n,na.rm=TRUE))\n\n#Load a world map and plot it\nwmap = map_data(\"world\")\nggplot(wmap, aes(x=long,y=lat,group=group)) +\n  geom_polygon(fill=\"lightgray\", colour = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.18 Select EU countries and joint the map with Eurobarometer data\n\nPython codeR code\n\n\n\ncountries = [\n    \"Portugal\",\n    \"Spain\",\n    \"France\",\n    \"Germany\",\n    \"Austria\",\n    \"Belgium\",\n    \"Netherlands\",\n    \"Ireland\",\n    \"Denmark\",\n    \"Poland\",\n    \"UK\",\n    \"Latvia\",\n    \"Cyprus\",\n    \"Croatia\",\n    \"Slovenia\",\n    \"Hungary\",\n    \"Slovakia\",\n    \"Czech republic\",\n    \"Greece\",\n    \"Finland\",\n    \"Italy\",\n    \"Luxemburg\",\n    \"Sweden\",\n    \"Sweden\",\n    \"Bulgaria\",\n    \"Estonia\",\n    \"Lithuania\",\n    \"Malta\",\n    \"Romania\",\n]\nm = wmap.loc[wmap[\"country\"].isin(countries)]\nm = pd.merge(supports_country, m, on=\"country\")\n\n\n\n\ncountries = c(\n  \"Portugal\", \"Spain\", \"France\", \"Germany\",\n  \"Austria\", \"Belgium\", \"Netherlands\", \"Ireland\",\n  \"Denmark\", \"Poland\", \"UK\", \"Latvia\", \"Cyprus\",\n  \"Croatia\", \"Slovenia\", \"Hungary\", \"Slovakia\",\n  \"Czech republic\", \"Greece\", \"Finland\", \"Italy\",\n  \"Luxemburg\", \"Sweden\", \"Sweden\", \"Bulgaria\", \n  \"Estonia\", \"Lithuania\", \"Malta\", \"Romania\")\nm = wmap %>% rename(country=region) %>% \n  filter(country %in% countries) %>%\n  left_join(supports_country, by=\"country\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.19 Map of Europe with the average level of support for refugees by country\n\nPython codeR code\n\n\n\nm = gpd.GeoDataFrame(m, geometry=m[\"geometry\"])\nm.plot(\n    column=\"support_refugees_n\",\n    legend=True,\n    cmap=\"OrRd\",\n    legend_kwds={\"label\": \"Level of suppport\"},\n).set_title(\"Support of refugees by country\")\n\n\n\n\n\n\n\nggplot(m, aes(long, lat, group=group))+\n  geom_polygon(aes(fill = m), color=\"white\")+\n  scale_fill_viridis_c(option=\"B\")+\n  labs(title=\"Support for refugees by country\", \n       fill=\"Level of support\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.4 Other Possibilities\nThere are many other ways of visualizing data. For EDA we have covered in this chapter only some of the most used techniques but they might be still limited for your future work. There are many books that cover data visualization in detail, such as Tufte (2006), Cairo (2019), and Kirk (2016). There are also many online resources, such as the Python Graph Gallery 7 and the R Graph Gallery 8, which introduce you to other useful plot types. These sites include code examples, many using the ggplot, matplotlib and seaborn packages introduced here, but also using other packages such as bokeh or plotly for interactive plots."
  },
  {
    "objectID": "content/chapter07.html#sec-clustering",
    "href": "content/chapter07.html#sec-clustering",
    "title": "7  Exploratory data analysis",
    "section": "7.3 Clustering and Dimensionality Reduction",
    "text": "7.3 Clustering and Dimensionality Reduction\nSo far, we have reviewed traditional statistical exploratory and visualization techniques that any social scientist should be able to apply. A more computational next step in your EDA workflow is using machine learning (ML) to let your computer “learn” about our data and in turn give more initial insights. ML is a branch of artificial intelligence that uses algorithms to interact with data and obtain some patterns or rules that characterize that data. We normally distinguish between supervised machine learning (SML) and unsupervised machine learning (UML). In Chapter 8, we will come back to this distinction. For now, it may suffice to say that the main characteristic of unsupervised methods is that we do not have any measurement available for a dependent variable, label, or categorization, which we want to predict. Instead, we want to identify patterns in the data without knowing in advance what these may look like. In this, unsupervised machine learning is very much of a inductive, bottom-up technique (see Chapter 1 and Boumans and Trilling (2016)).\nIn this chapter, we will focus on UML as a means of finding groups and latent dimensions in our data, which can also help to reduce our number of variables. Specifically, we will use base R and Python’s scikit-learn to conduct \\(k\\)-means clustering, hierarchical clustering, and principal component analysis (PCA) as well as the closely related singular value decomposition (SVD).\nIn data mining, we use clustering as a UML technique that aims to find the relationship between a set of descriptive variables. By doing cluster analysis we can identify underlying groups in our data that we will call clusters. Imagine we want to explore how European countries can be grouped based on their average support to refugees/migrants, age and educational level. We might create some a priori groups (such as southern versus northern countries), but cluster analysis would be a great method to let the data “talk” and then create the most appropriate groups for this specific case. As in all UML, the groups will come unlabeled and the computational analyst will be in charge of finding an appropriate and meaningful label for each cluster to better communicate the results.\n\n7.3.1 \\(k\\)-means Clustering\n\\(k\\)-means is a very frequently used algorithm to perform cluster analysis. Its main advantage is that, compared to the hierarchical clustering methods we will discuss later, it is very fast and does not consume many resources. This makes it especially useful for larger datasets.\n\\(k\\)-means cluster analysis is a method that takes any number of observations (cases) and groups them into a given number of clusters based on the proximity of each observation to the mean of the formed cluster (centroid). Mathematically, we measure this proximity as the distance of any given point to its cluster center, and can be expressed as\nwhere \\(\\|x_n - \\mu_k\\|\\) is the distance between the data point \\(x_n\\) and the center of the cluster \\(\\mu_k\\).\nInstead of taking the mean, some variations of this algorithm take the median (\\(k\\)-medians) or a representative observation, also called medoid (\\(k\\)-medoids or partitioning around medoids, PAM) as a way to optimize the initial method.\nBecause \\(k\\)-means clustering calculates distances between cases, these distances need to be meaningful – which is only the cases if the scales on which the variables are measured are comparable. If all your variables are measured on the same (continuous) scale with the same endpoints, you may be fine. In most cases, you need to normalize your data by transforming them into, for instance, \\(z\\)-scores9, or a scale from 0 to 1.\nHence, the first thing we do in our example, is to prepare a proper dataset with only continuous variables, scaling the data (for comparability) and avoiding missing values (drop or impute). In Example 7.20, we will use the variables support to refugees (support_refugees_n), support to migrants (support_migrants_n), age (age) and educational level (number of years of education) (educational_n) and will create a data frame d3 with the mean of all these variables for each country (each observation will be a country). \\(k\\)-means requires us to specify the number of clusters, \\(k\\), in advance. This is a tricky question, and (besides arbitrarily deciding \\(k\\)!), you essentially need to re-estimate your model multiple times with different \\(k\\)s.\nThe simplest method to obtain the optimal number of clusters is to estimate the variability within the groups for different runs. This means that we must run \\(k\\)-means for different number of clusters (e.g. 1 to 15 clusters) and then choose the number of clusters that decreases the variability maintaining the highest number of clusters. When you generate and plot a vector with the variability, or more technically, the within-cluster sum of squares (WSS) obtained after each execution, it is easy to identify the optimal number: just look at the bend (knee or elbow) and you will find the point where it decreases the most and then get the optimal number of clusters (three clusters in our example).\n\n\n\n\n\n\n\nExample 7.20 Getting the optimal number of clusters\n\nPython codeR code\n\n\n\n# Average variables by country and scale\nd3 = d2.groupby([\"country\"])[\n    [\"support_refugees_n\", \"support_migrants_n\", \"age\", \"educational_n\"]\n].mean()\n\nscaler = StandardScaler()\nd3_s = scaler.fit_transform(d3)\n\n# Store sum of squares for 1..15 clusters\nwss = []\nfor i in range(1, 15):\n    km_out = KMeans(n_clusters=i, n_init=20)\n    km_out.fit(d3_s)\n    wss.append(km_out.inertia_)\nplt.plot(range(1, 15), wss, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Within groups sum of squares\")\nplt.show()\n\n\n\n\n\n\n\n# Average variables by country and scale\nd3_s = d2%>%\n  group_by(country)%>%\n  summarise(\n    m_refugees=mean(support_refugees_n, na.rm=T), \n    m_migrants=mean(support_migrants_n, na.rm=T),\n    m_age=mean(age, na.rm=T),\n    m_edu=mean(educational_n, na.rm=T)) %>%\n  column_to_rownames(var=\"country\") %>%\n  scale()\n# Store sum of squares for 1..15 clusters\nwss = list()\nfor (i in 1:15) {\n  km.out = kmeans(d3_s, centers=i, nstart=25)\n  wss[[i]] = tibble(k=i, ss=km.out$tot.withinss)\n}\nwss = bind_rows(wss)\nggplot(wss, aes(x=k, y=ss)) + \n  geom_line() + geom_point() + \n  xlab(\"Number of Clusters\") + \n  ylab(\"Within groups sum of squares\")\n\n\n\n\n\n\n\n\n\n\n\nNow we can estimate our final model (Example 7.21). We generate 25 initial random centroids (the algorithm will choose the one that optimizes the cost). The default of this parameter is 1, but it is recommended to set it with a higher number (i.e. 20 to 50) to guarantee the maximum benefit of the method. The base R function kmeans and scikit-learn function KMeans in Python will produce the clustering. You can observe the mean (scaled) for each variable in each cluster, as well as the corresponding cluster for each observation.\n\n\n\n\n\n\n\nExample 7.21 Using Kmeans to group countries based on the average support of refugees and migrants, age, and educational level\n\nPython codeR code\n\n\n\n# Compute k-means with k = 3\nkm_res = KMeans(n_clusters=3, n_init=25).fit(d3_s)\nprint(km_res)\n\nKMeans(n_clusters=3, n_init=25)\n\nprint(\"K-means cluste sizes:\", np.bincount(km_res.labels_[km_res.labels_ >= 0]))\n\nK-means cluste sizes: [13 12  3]\n\nprint(f\"Cluster means: {km_res.cluster_centers_}\")\n\nCluster means: [[-0.89000978 -0.82574663 -0.3892184  -0.21560025]\n [ 0.66164163  0.64025687 -0.02468681 -0.39044418]\n [ 1.2101425   1.01720791  1.78536032  2.49604445]]\n\nprint(\"Clustering vector:\")\n\nClustering vector:\n\nprint(np.column_stack((d3.index, km_res.labels_)))\n\n[['Austria' 1]\n ['Belgium' 1]\n ['Bulgaria' 0]\n ['Croatia' 0]\n ['Cyprus' 1]\n ['Czech republic' 0]\n ['Denmark' 2]\n ['Estonia' 0]\n ['Finland' 2]\n ['France' 1]\n ['Germany' 1]\n ['Greece' 0]\n ['Hungary' 0]\n ['Ireland' 1]\n ['Italy' 0]\n ['Latvia' 0]\n ['Lithuania' 0]\n ['Luxemburg' 1]\n ['Malta' 1]\n ['Netherlands' 1]\n ['Poland' 0]\n ['Portugal' 1]\n ['Romania' 0]\n ['Slovakia' 0]\n ['Slovenia' 0]\n ['Spain' 1]\n ['Sweden' 2]\n ['UK' 1]]\n\nprint(\"Within cluster sum of squares:\")\n\nWithin cluster sum of squares:\n\nprint(km_res.inertia_)\n\n42.50488485460034\n\n\n\n\n\nset.seed(123)\nkm.res = kmeans(d3_s, 3, nstart=25)\nprint(km.res)\n\nK-means clustering with 3 clusters of sizes 13, 3, 12\n\nCluster means:\n  m_refugees m_migrants       m_age      m_edu\n1 -0.8739722 -0.8108671 -0.38220489 -0.2117152\n2  1.1883363  0.9988783  1.75318903  2.4510670\n3  0.6497192  0.6287198 -0.02424197 -0.3834086\n\nClustering vector:\n       Austria        Belgium       Bulgaria        Croatia         Cyprus \n             3              3              1              1              3 \nCzech republic        Denmark        Estonia        Finland         France \n             1              2              1              2              3 \n       Germany         Greece        Hungary        Ireland          Italy \n             3              1              1              3              1 \n        Latvia      Lithuania      Luxemburg          Malta    Netherlands \n             1              1              3              3              3 \n        Poland       Portugal        Romania       Slovakia       Slovenia \n             1              3              1              1              1 \n         Spain         Sweden             UK \n             3              2              3 \n\nWithin cluster sum of squares by cluster:\n[1] 20.485188  2.984011 17.517654\n (between_SS / total_SS =  62.0 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\n\n\n\nUsing the function fviz_cluster of the library factoextra in R, or the pyplot function scatter in Python, you can get a visualization of the clusters. In Example 7.22 you can clearly identify that the clusters correspond to Nordic countries (more support to foreigners, more education and age), Central and Southern European countries (middle support, lower education and age), and Eastern European countries (less support, lower education and age)10 .\n\n\n\n\n\n\n\nExample 7.22 Visualization of clusters\n\nPython codeR code\n\n\n\nfor cluster in range(km_res.n_clusters):\n    plt.scatter(\n        d3_s[km_res.labels_ == cluster, 0], d3_s[km_res.labels_ == cluster, 1]\n    )\nplt.scatter(\n    km_res.cluster_centers_[:, 0],\n    km_res.cluster_centers_[:, 1],\n    s=250,\n    marker=\"*\",\n)\nplt.legend(scatterpoints=1)\nplt.show()\n\n\n\n\n\n\n\nfviz_cluster(km.res, d3_s, ellipse.type=\"norm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.2 Hierarchical Clustering\nAnother method to conduct a cluster analysis is hierarchical clustering, which builds a hierarchy of clusters that we can visualize in a dendogram. This algorithm has two versions: a bottom-up approach (observations begin in their own clusters), also called agglomerative, and a top-down approach (all observations begin in one cluster), also called divisive. We will follow the bottom-up approach in this chapter and when you look at the dendogram you will realize how this strategy repeatedly combines the two nearest clusters at the bottom into a larger one in the top. The distance between clusters is initially estimated for every pair of observation points and then put every point in its own cluster in order to get the closest pair of points and iteratively compute the distance between each new cluster and the previous ones. This is the internal rule of the algorithm and we must choose a specific linkage method (complete, single, average or centroid, or Ward’s linkage). Ward’s linkage is a good default choice: it minimizes the variance of the clusters being merged. In doing so, it tends to produce roughly evenly sized clusters and is less sensitive to noise and outliers than some of the other methods. In Example 7.23 we will use the function hcut of the package factoextra in R and scikit-learn function AgglomerativeClustering in Python, to compute the hierarchical clustering.\nA big advantage of hierarchical clustering is that, once estimated, you can freely choose the number of clusters in which to group your cases without re-estimating the model. If you decide, for instance, to use four instead of three clusters, then the cases in one of your three clusters are divided into two subgroups. With \\(k\\)-means, in contrast, a three-cluster solution can be completely different from a four-cluster solution. However, this comes at a big cost: hierarchical clustering requires a lot more computing resources and may therefore not be feasible for large datasets.\n\n\n\n\n\n\n\nExample 7.23 Using hierarchical clustering to group countries based on the average support of refugees and migrants, age and educational level\n\nPython codeR code\n\n\n\nhc_res = AgglomerativeClustering(affinity=\"euclidean\", linkage=\"complete\")\nhc_res.fit_predict(d3_s)\n\narray([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 0])\n\nprint(hc_res)\n\nAgglomerativeClustering(linkage='complete')\n\n\n\n\n\nhc.res <- hcut(d3_s, hc_method=\"complete\")\nsummary(hc.res)\n\n            Length Class  Mode     \nmerge        54    -none- numeric  \nheight       27    -none- numeric  \norder        28    -none- numeric  \nlabels       28    -none- character\nmethod        1    -none- character\ncall          3    -none- call     \ndist.method   1    -none- character\ncluster      28    -none- numeric  \nnbclust       1    -none- numeric  \nsilinfo       3    -none- list     \nsize          2    -none- numeric  \ndata        112    -none- numeric  \n\n\n\n\n\n\n\n\n\nWe can then plot the dendogram with base R function plot and scipy (module cluster.hierarchy) function dendogram in Python. The summary of the initial model suggest two clusters (size=2) but by looking at the dendogram you can choose the number of clusters you want to work with by choosing a height (for example four to get three clusters).\n\n\n\n\n\n\n\nExample 7.24 Dendogram to visualize the hierarchical clustering\n\nPython codeR code\n\n\n\ndendrogram = sch.dendrogram(\n    sch.linkage(d3_s, method=\"complete\"),\n    labels=list(d3.index),\n    leaf_rotation=90,\n)\n\n\n\n\nplot(hc.res, cex=0.5)\n\n\n\n\n\n\n\n\n\n\n\nIf you re-run the hierarchical clustering for three clusters (Example 7.25) and visualize it (Example 7.26) you will get a graph similar to the one produced by \\(k\\)-means.\n\n\n\n\n\n\n\nExample 7.25 Re-run hierarchical clustering with three clusters\n\nPython codeR code\n\n\n\nhc_res = AgglomerativeClustering(\n    n_clusters=3, affinity=\"euclidean\", linkage=\"ward\"\n)\nhc_res.fit_predict(d3_s)\n\narray([0, 0, 2, 0, 0, 2, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0,\n       0, 2, 0, 0, 1, 0])\n\nprint(hc_res)\n\nAgglomerativeClustering(n_clusters=3)\n\n\n\n\n\nhc.res = hcut(d3_s, k=3, hc_method=\"complete\") \nsummary(hc.res)\n\n            Length Class  Mode     \nmerge        54    -none- numeric  \nheight       27    -none- numeric  \norder        28    -none- numeric  \nlabels       28    -none- character\nmethod        1    -none- character\ncall          3    -none- call     \ndist.method   1    -none- character\ncluster      28    -none- numeric  \nnbclust       1    -none- numeric  \nsilinfo       3    -none- list     \nsize          3    -none- numeric  \ndata        112    -none- numeric  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.26 Re-run hierarchical clustering with three clusters\n\nPython codeR code\n\n\n\nfor cluster in range(hc_res.n_clusters):\n    plt.scatter(\n        d3_s[hc_res.labels_ == cluster, 0], d3_s[hc_res.labels_ == cluster, 1]\n    )\nplt.legend(scatterpoints=1)\nplt.show()\n\n\n\n\n\n\n\nfviz_cluster(hc.res, d3_s, ellipse.type=\"convex\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.3 Principal Component Analysis and Singular Value Decomposition\nCluster analyses are in principle used to group similar cases. Sometimes, we want to group similar variables instead. A well-known method for this is principal component analysis (PCA)11. This unsupervised method is useful to reduce the dimensionality of your data by creating new uncorrelated variables or components that describe the original dataset. PCA uses linear transformations to create principal components that are ordered by the level of explained variance (the first component will catch the largest variance). We will get as many principal components as number of variables we have in the dataset, but when we look at the cumulative variance we can easily select only few of these components to explain most of the variance and thus work with a smaller and summarized data frame that might be more convenient for many tasks (i.e. those that require avoiding multicollinearity or just need to be more computationally efficient). By simplifying the complexity of our data we can have a first understanding of how our variables are related and also of how our observations might be grouped. All components have specific loadings for each original variable, which can tell you how the old variables are represented in the new components. This statistical technique is especially useful in EDA when working with high dimensional datasets but it can be used in many other situations.\nThe mathematics behind PCA can be relatively easy to understand. However, for the sake of simplicity, we will just say that in order to obtain the principal components the algorithm firstly has to compute the mean of each variable and then compute the covariance matrix of the data. This matrix contains the covariance between the elements of a vector and the output will be a square matrix with an identical number of rows and columns, corresponding to the total number of dimensions of the original dataset. Specifically, we can calculate the covariance matrix of the variables X and y with the formula:\nSecondly, using the covariance matrix the algorithm computes the eigenvectors and their corresponding eigenvalues, and then drop the eigenvectors with the lowest eigenvalues. With this reduced matrix it transforms the original values to the new subspace in order to obtain the principal components that will synthesize the original dataset.\nLet us now conduct a PCA over the Eurobarometer data. In Example 7.27 we will re-use the sub-data frame d3 containing the means of 4 variables (support to refugees, support to migrants, age and educational level) for each of the 30 European countries. The question is can we have a new data frame containing less than 4 variables but that explains most of the variance, or in other words, that represents our original dataset well enough, but with fewer dimensions? As long as our features are measured on different scales, it is normally suggested to center (to mean 0) and scale (to standard deviation 1) the data. You may also know this transformation as “calculating \\(z\\)-scores”. We can perform the PCA in R using the base function prcomp and in Python using the function PCA of the module decomposition of scikit-learn.\n\n\n\n\n\n\n\nExample 7.27 Principal component analysis (PCA) of a data frame with 30 records and 4 variables\n\nPython codeR code\n\n\n\npca_m = PCA()\npca = pca_m.fit(d3_s)\npca_n = PCA()\npca = pca_n.fit_transform(d3_s)\npca_df = pd.DataFrame(data=pca, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"])\npca_df.index = d3.index\nprint(pca_df.head())\n\n               PC1       PC2       PC3       PC4\ncountry                                         \nAustria  -0.103285 -1.220018 -0.535673  0.066888\nBelgium  -0.029355 -0.084707  0.051515  0.227609\nBulgaria -1.660518  0.949533 -0.480337 -0.151837\nCroatia  -1.267502 -0.819093 -0.920657  0.843682\nCyprus    0.060590 -0.195928  0.573670  0.812519\n\npca_df_2 = pd.DataFrame(\n    data=pca_n.components_.T, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"]\n)\npca_df_2.index = d3.columns\nprint(pca_df_2)\n\n                         PC1       PC2       PC3       PC4\nsupport_refugees_n  0.573292 -0.369010  0.139859  0.718058\nsupport_migrants_n  0.513586 -0.533140 -0.094283 -0.665659\nage                 0.445117  0.558601  0.670994 -0.199005\neducational_n       0.457642  0.517261 -0.722023  0.041073\n\n\n\n\n\npca = prcomp(d3_s, scale=TRUE)\nhead(pca$x)\n\n                       PC1         PC2         PC3         PC4\nAustria         0.10142430  1.19803416 -0.52602091  0.06568251\nBelgium         0.02882642  0.08318037  0.05058673  0.22350741\nBulgaria        1.63059623 -0.93242257 -0.47168110 -0.14910143\nCroatia         1.24466229  0.80433304 -0.90406722  0.82847927\nCyprus         -0.05949790  0.19239739  0.56333279  0.79787757\nCzech republic  2.17979559 -0.66348027 -0.74991773 -0.08459459\n\npca$rotation\n\n                  PC1        PC2         PC3         PC4\nm_refugees -0.5732924  0.3690096  0.13985877  0.71805799\nm_migrants -0.5135857  0.5331396 -0.09428317 -0.66565948\nm_age      -0.4451170 -0.5586008  0.67099390 -0.19900549\nm_edu      -0.4576422 -0.5172613 -0.72202312  0.04107309\n\n\n\n\n\n\n\n\n\nThe generated object with the PCA contains different elements (in R sdev, rotation, center, scale and x) or attributes in Python (components_, explained_variance_, explained_variance_ratio, singular_values_, mean_, n_components_, n_features_, n_samples_, and noise_variance_). In the resulting object we can see the values of four principal components of each country, and the values of the loadings, technically called eigenvalues, for the variables in each principal component. In our example we can see that support for refugees and migrants are more represented on PC1, while age and educational level are more represented on PC2. If we plot the first two principal components using base function biplot in R and the library bioinfokit in Python (Example 7.28), we can clearly see how the variables are associated with either PC1 or with PC2 (we might also want to plot any pair of the four components!). But we can also get a picture of how countries are grouped based only in these two new variables.\n\n\n\n\n\n\n\nExample 7.28 Plot PC1 and PC2\n\nPython codeR code\n\n\n\nvar1 = round(pca_n.explained_variance_ratio_[0], 2)\nvar2 = round(pca_n.explained_variance_ratio_[1], 2)\nbioinfokit.visuz.cluster.biplot(\n    cscore=pca,\n    loadings=pca_n.components_,\n    labels=pca_df_2.index.values,\n    var1=var1,\n    var2=var2,\n    show=True,\n)\n\n\n\n\n\n\n\nbiplot(x = pca, scale = 0, cex = 0.6, \n       col = c(\"blue4\", \"brown3\"))\n\n\n\n\n\n\n\n\n\n\n\nSo far we are not sure how many components are enough to accurately represent our data, so we need to know how much variance (which is the square of the standard deviation) is explained by each component. We can get the values (Example 7.29) and plot the proportion of explained variance (Example 7.30). We get that the first component explains 57.85% of the variance, the second 27.97%, the third 10.34% and the fourth just 3.83%.\n\n\n\n\n\n\n\nExample 7.29 Proportion of variance explained\n\nPython codeR code\n\n\n\nprint(\"Proportion of variance explained:\")\n\nProportion of variance explained:\n\nprint(pca_n.explained_variance_ratio_)\n\n[0.57848569 0.27974794 0.10344996 0.03831642]\n\n\n\n\n\nprint(\"Proportion of variance explained:\")\n\n[1] \"Proportion of variance explained:\"\n\nprop_var = tibble(pc=1:4,\n    var=pca$sdev^2 / sum(pca$sdev^2))\nprop_var\n\n# A tibble: 4 × 2\n     pc    var\n  <int>  <dbl>\n1     1 0.578 \n2     2 0.280 \n3     3 0.103 \n4     4 0.0383\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.30 Plot of the proportion of variance explained\n\nPython codeR code\n\n\n\nplt.bar([1, 2, 3, 4], pca_n.explained_variance_ratio_)\nplt.ylabel(\"Proportion of variance explained\")\nplt.xlabel(\"Principal component\")\nplt.xticks([1, 2, 3, 4])\nplt.show()\n\n\n\n\n\n\n\nggplot(prop_var, aes(x=pc, y=var)) +\n  geom_col() +\n  scale_y_continuous(limits = c(0,1)) +\n  xlab(\"Principal component\") + \n  ylab(\"Proportion of variance explained\")\n\n\n\n\n\n\n\n\n\n\n\nWhen we estimate (Example 7.31) and plot (Example 7.32) the cumulative explained variance it is easy to identify that with just the two first components we explain 88.82% of the variance. It might now seem a good deal to reduce our dataset from four to two variables, or let’s say half of the data, but retaining most of the original information.\n\n\n\n\n\n\n\nExample 7.31 Cumulative explained variance\n\nPython codeR code\n\n\n\ncvar = np.cumsum(pca_n.explained_variance_ratio_)\ncvar\n\narray([0.57848569, 0.85823362, 0.96168358, 1.        ])\n\n\n\n\n\ncvar = cumsum(prop_var)\ncvar\n\n# A tibble: 4 × 2\n     pc   var\n  <int> <dbl>\n1     1 0.578\n2     3 0.858\n3     6 0.962\n4    10 1    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.32 Plot of the cumulative explained variance\n\nPython codeR code\n\n\n\nplt.plot(cvar)\nplt.xlabel(\"number of components\")\nplt.xticks(np.arange(len(cvar)), np.arange(1, len(cvar) + 1))\nplt.ylabel(\"cumulative explained variance\")\nplt.show()\n\n\n\n\n\n\n\nggplot(cvar, aes(x=pc, y=var)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  xlab(\"Principal component\") +\n  ylab(\"Cumulative explained variance\")\n\n\n\n\n\n\n\n\n\n\n\nAnd what if we want to use this PCA and deploy a clustering (as explained above) with just these two new variables instead of the four original ones? Just repeat the \\(k\\)-means procedure but now using a new smaller data frame selecting PC1 and PC2 from the PCA. After estimating the optimal number of clusters (three again!) we can compute and visualize the clusters, and get a very similar picture to the one obtained in the previous examples, with little differences such as the change of cluster of the Netherlands (more similar now to the Nordic countries!). This last exercise is a good example of how to combine different techniques in EDA.\n\n\n\n\n\n\n\nExample 7.33 Combining PCA to reduce dimensionality and \\(k\\)-means to group countries\n\nPython codeR code\n\n\n\n# Generate a new dataset with first components\nd5 = pca[:, 0:2]\nd5[0:5]\n\n# Get optimal number of clusters\n\narray([[-0.10328545, -1.22001827],\n       [-0.02935539, -0.08470675],\n       [-1.66051792,  0.94953266],\n       [-1.26750204, -0.81909268],\n       [ 0.06058969, -0.19592792]])\n\nwss = []\nfor i in range(1, 15):\n    km_out = KMeans(n_clusters=i, n_init=20)\n    km_out.fit(d5)\n    wss.append(km_out.inertia_)\n\n# Plot sum of squares vs. number of clusters\n\nKMeans(n_clusters=1, n_init=20)\nKMeans(n_clusters=2, n_init=20)\nKMeans(n_clusters=3, n_init=20)\nKMeans(n_clusters=4, n_init=20)\nKMeans(n_clusters=5, n_init=20)\nKMeans(n_clusters=6, n_init=20)\nKMeans(n_clusters=7, n_init=20)\nKMeans(n_init=20)\nKMeans(n_clusters=9, n_init=20)\nKMeans(n_clusters=10, n_init=20)\nKMeans(n_clusters=11, n_init=20)\nKMeans(n_clusters=12, n_init=20)\nKMeans(n_clusters=13, n_init=20)\nKMeans(n_clusters=14, n_init=20)\n\nplt.plot(range(1, 15), wss, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Within groups sum of squares\")\nplt.show()\n\n# Compute again with k = 3 and visualize\n\n\n\nkm_res_5 = KMeans(n_clusters=3, n_init=25).fit(d5)\nfor cluster in range(km_res_5.n_clusters):\n    plt.scatter(\n        d3_s[km_res_5.labels_ == cluster, 0],\n        d3_s[km_res_5.labels_ == cluster, 1],\n    )\nplt.scatter(\n    km_res_5.cluster_centers_[:, 0],\n    km_res_5.cluster_centers_[:, 1],\n    s=250,\n    marker=\"*\",\n)\nplt.legend(scatterpoints=1)\nplt.show()\n\n\n\n\n\n\n\n#Generate a new dataset with first components\nd5 = pca$x[, c(\"PC1\", \"PC2\")]\nhead(d5)\n\n                       PC1         PC2\nAustria         0.10142430  1.19803416\nBelgium         0.02882642  0.08318037\nBulgaria        1.63059623 -0.93242257\nCroatia         1.24466229  0.80433304\nCyprus         -0.05949790  0.19239739\nCzech republic  2.17979559 -0.66348027\n\n#Get optimal number of clusters\nwss = list()\nfor (i in 1:15) {\n  km.out = kmeans(d5, centers = i, nstart = 20)\n  wss[[i]] = tibble(k=i, ss=km.out$tot.withinss)\n}\nwss = bind_rows(wss)\n\n# Plot sum of squares vs. number of clusters\nggplot(wss, aes(x=k, y=ss)) + geom_line() + \n     xlab(\"Number of Clusters\") + \n     ylab(\"Within groups sum of squares\")\n\n\n\n# Compute again with k = 3 and visualize\nset.seed(123)\nkm.res_5 <- kmeans(d5, 3, nstart = 25)\nfviz_cluster(km.res_5, d5, ellipse.type = \"norm\")\n\n\n\n\n\n\n\n\n\n\n\nWhen your dataset gets bigger, though, you may actually not use PCA but the very much related singular value decomposition, SVD. They are closely interrelated, and in fact SVD can be used “under the hood” to estimate a PCA. While PCA is taught in a lot of classical textbooks for statistics in the social sciences, SVD is usually not. Yet, it has a great advantage: in the way that it is implemented in scikit-learn, it does not require to store the (dense) covariance matrix in memory (see the feature box in Section 11.4.1 for more information on sparse versus dense matrices). This means that once your dataset grows bigger than typical survey datasets, a PCA maybe quickly become impossible to estimate, whereas the SVD can still be estimated without much resource required. Therefore, especially when you are working with textual data, you will see that SVD is used instead of PCA. For all practical purposes, the way that you can use and interpret the results stays the same.\n\n\n\n\nBoumans, Jelle W., and Damian Trilling. 2016. “Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars.” Digital Journalism 4 (1): 8–23. https://doi.org/10.1080/21670811.2015.1096598.\n\n\nBryman, Alan. 2012. Social Research Methods. 4th edition. New York, NY: Oxford University Press.\n\n\nCairo, Alberto. 2019. How Charts Lie. WW Norton & Company.\n\n\nKirk, Andy. 2016. Data Visualisation: A Handbook for Data Driven Design. London, UK: SAGE.\n\n\nTufte, Edward R. 2006. Beautiful Evidence. Vol. 1. Graphics Press Cheshire, CT.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. Vol. 2. Reading, Mass."
  },
  {
    "objectID": "content/chapter08.html#sec-prediction",
    "href": "content/chapter08.html#sec-prediction",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.1 Statistical Modeling and Prediction",
    "text": "8.1 Statistical Modeling and Prediction\nMachine learning, many people joke, is nothing other than a fancy name for statistics. And, in fact, there is some truth to this: if you say “logistic regression”, this will sound familiar to both statisticians and machine learning practitioners. Hence, it does not make much sense to distinguish between statistics on the one hand and machine learning on the other hand. Still, there are some differences between traditional statistical approaches that you may have learned about in your statistics classes and the machine learning approach, even if some of the same mathematical tools are used. One may say that the focus is a different one, and the objective we want to achieve may differ.\nLet us illustrate this with an example. media.csv1 contains a few columns from survey data on how many days per week respondents turn to different media types (radio, newspaper, tv and Internet) in order to follow the news2. It also contains their age (in years), their gender (coded as female = 0, male = 1), and their education (on a 5-point scale).\nA straightforward question to ask is how far the sociodemographic characteristics of the respondents explain their media use. Social scientists would typically approach this question by running a regression analysis. Such an analysis tells us how some independent variables \\(x_1, x_2, \\ldots, x_n\\) can explain \\(y\\). In an ordinary least square regression (OLS), we would estimate \\(y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\\).\nIn a typical social-science paper, we would then interpret the coefficients that we estimated, and say something like: when \\(x_1\\) increases by one unit, \\(y\\) increases by \\(\\beta_1\\). We sometimes call this “the effect of \\(x_1\\) on \\(y\\)” (even though, of course, it depends on the study design whether the relationship can really be interpreted as a causal effect). Additionally, we might look at the explained variance \\(R^2\\), to assess how well the model fits our data. In Example 8.1 we use this regression approach to model the relationship of age and gender over the number of days per week a person reads a newspaper. We fit the linear model using the stats function lm in R and the statsmodels function ols (imported from the module statsmodels.formula.api) in Python.\n\n\n\n\n\n\n\nExample 8.1 Obtaining a model through estimating an OLS regression\n\nPython codeR code\n\n\n\ndf = pd.read_csv(\"https://cssbook.net/d/media.csv\")\nmod = smf.ols(formula=\"newspaper ~ age + gender\", data=df).fit()\n# mod.summary() would give a lot more info,\n# but we only care about the coefficients:\nmod.params\n\nIntercept   -0.089560\nage          0.067620\ngender       0.176665\ndtype: float64\n\n\n\n\n\ndf = read.csv(\"https://cssbook.net/d/media.csv\")\nmod = lm(formula = \"newspaper ~ age + gender\",\n         data = df)\n# summary(mod) would give a lot more info, \n# but we only care about the coefficients:\nmod\n\n\nCall:\nlm(formula = \"newspaper ~ age + gender\", data = df)\n\nCoefficients:\n(Intercept)          age       gender  \n   -0.08956      0.06762      0.17666  \n\n\n\n\n\n\n\n\n\nMost traditional social-scientific analyses stop after reporting and interpreting the coefficients of age (\\(\\beta = 0.0676\\)) and gender (\\(\\beta = -0.0896\\)), as well as their standard errors, confidence intervals, p-values, and the total explained variance (19%). But we can go a step further. Given that we have already estimated our regression equation, why not use it to do some prediction?\nWe have just estimated that\nBy just filling in the values for a 20 year old man, or a 40 year old woman, we can easily calculate the expected number of days such a person reads the newspaper per week, even if no such person exists in the original dataset.\nWe learn that\nThis was easy to do by hand, but of course, we could do this automatically for a large and essentially unlimited number of cases. This could be as simple as shown in Example 8.2.\n\n\n\n\n\n\n\nExample 8.2 Using the OLS model we estimated before to predict the dependent variable for new data where the dependent variable is unknown.\n\nPython codeR code\n\n\n\nnewdata = pd.DataFrame([{\"gender\": 1, \"age\": 20}, {\"gender\": 0, \"age\": 40}])\nmod.predict(newdata)\n\n0    1.439508\n1    2.615248\ndtype: float64\n\n\n\n\n\ngender = c(1,0)\nage = c(20,40)\nnewdata = data.frame(age, gender)\npredict(mod, newdata)\n\n       1        2 \n1.439508 2.615248 \n\n\n\n\n\n\n\n\n\nIn doing so, we shift our attention from the interpretation of coefficients to the prediction of the dependent variable for new, unknown cases. We do not care about the actual values of the coefficients, we just need them for our prediction. In fact, in many machine learning models, we will have so many of them that we do not even bother to report them.\nAs you see, this implies that we proceed in two steps: first, we use some data to estimate our model. Second, we use that model to make predictions.\nWe used an OLS regression for our first example, because it is very straightforward to interpret and most of our readers will be familiar with it. However, a model can take the form of any function, as long as it takes some characteristics (or “features”) of the cases (in this case, people) as input and returns a prediction.\nUsing such a simple OLS regression approach for prediction, as we did in our example, can come with a couple of problems, though. One problem is that in some cases, such predictions do not make much sense. For instance, even though we know that the output should be something between 0 and 7 (as that is the number of days in a week), our model will happily predict that once a man reaches the age of 105 (rare, but not impossible), he will read a newspaper on 7.185 out of 7 days. Similarly, a one year old girl will even have a negative amount of newspaper reading. A second problem relates to the models’ inherent assumptions. For instance, in our example it is quite an assumption to make that the relationships between these variables are linear –- we will therefore discuss multiple models that do not make such assumptions later in this chapter. And, finally, in many cases, we are actually not interested in getting an accurate prediction of a continuous number (a regression task), but rather in predicting a category. We may want to predict whether a tweet goes viral or not, whether a user comment is likely to contain offensive language or not, whether an article is more likely to be about politics, sports, economy, or lifestyle. In machine learning terms, these tasks are known as classification.\nIn the next section, we will outline key terms and concepts in machine learning. After that, we will discuss specific models that you can use for different use applications."
  },
  {
    "objectID": "content/chapter08.html#sec-principles",
    "href": "content/chapter08.html#sec-principles",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.2 Concepts and Principles",
    "text": "8.2 Concepts and Principles\nThe goal of Supervised Machine Learning can be summarized in one sentence: estimate a model based on some data, and then use the model to predict the expected outcome for some new cases, for which we do not know the outcome yet. This is exactly what we have done in the introductory example in Section 8.1.\nBut when do we need it?\nIn short, in any scenario where the following two preconditions are fulfilled. First, we have a large dataset (say, \\(100000\\) headlines) for which we want to predict to which class they belong to (say, whether they are clickbait or not). Second, for a random subset of the data (say, \\(2000\\) of the headlines), we already know the class. For example because we have manually coded (“annotated”) them.\nBefore we start using SML, though, we first need to have a common terminology. At the risk of oversimplifying matters, Table 8.1 provides a rough guideline of how some typical machine learning terms translate to statistical terms that you may be familiar with.\n\n\nTable 8.1: Some common machine learning terms explained\n\n\n\n\n\n\nmachine learning lingo\nstatistics lingo\n\n\n\n\nfeature\nindependent variable\n\n\nlabel\ndependent variable\n\n\nlabeled dataset\ndataset with both independent and dependent variables\n\n\nto train a model\nto estimate\n\n\nclassifier (classification)\nmodel to predict nominal outcomes\n\n\nto annotate\nto (manually) code (content analysis)\n\n\n\n\nLet us explain them more in detail by walking through a typical SML workflow.\nBefore we start, we need to get a labeled dataset. It may be given to us, or we may need to create it ourselves. For instance, often we can draw a random sample of our data and use techniques of manual content analysis (e.g., Riffe et al. 2019) to annotate (i.e., to manually code) the data. You can download an example for this process (annotating the topic of news articles) from dx.doi.org/10.6084/m9.figshare.7314896.v1 (Vermeer 2018).\nIt is hard to give a rule of thumb for how much labeled data you need. It depends heavily on the type of data you have (for instance, if it is a binary as opposed to a multi-class classification problem), and on how evenly distributed (class balance) they are (after all, having \\(10000\\) annotated headlines doesn’t help you if \\(9990\\) are not clickbait and only \\(10\\) are). These reservations notwithstanding, it is fair to say that typical sizes in our field are (very roughly) speaking often in the order of \\(1000\\) to \\(10000\\) when classifying longer texts (see Burscher et al. 2014), even though researchers studying less rich data sometimes annotate larger datasets (e.g., \\(60000\\) social media messages in Vermeer et al. 2019).\nOnce we have established that this labeled dataset is available and have ensured that it is of good quality, we randomly split it into two datasets: a training dataset and a test dataset.3 We will use the first one to train our model, and the second to test how well our model performs. Common ratios range from 50:50 to 80:20; and especially if the size of your labeled dataset is rather limited, you may want to have a slightly larger training dataset at the expense of a slightly smaller test dataset.\nIn Example 8.3, we prepare the dataset we already used in Section 8.1 for classification by creating a dichotomous variable (the label) and splitting it into a training and a test dataset. We use y_train to denote the training labels and X_train to denote the feature matrix of the training dataset; y_test and X_test is the corresponding test dataset. We set a so-called random-state seed to make sure that the random splitting will be the same when re-running the code. We can easily split these datasets using the rsample function initial_split in R and the sklearn function train_test_split in Python.\n\n\n\n\n\n\n\nExample 8.3 Preparing a dataset for supervised machine learning\n\nPython codeR code\n\n\n\ndf = pd.read_csv(\"https://cssbook.net/d/media.csv\")\n\ndf[\"uses-internet\"] = (df[\"internet\"] > 0).replace(\n    {True: \"user\", False: \"non-user\"}\n)\ndf.dropna(inplace=True)\nprint(\"How many people used online news at all?\")\n\nHow many people used online news at all?\n\nprint(df[\"uses-internet\"].value_counts())\n\nuser        1262\nnon-user     803\nName: uses-internet, dtype: int64\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df[[\"age\", \"education\", \"gender\"]],\n    df[\"uses-internet\"],\n    test_size=0.2,\n    random_state=42,\n)\nprint(f\"We have {len(X_train)} training and \" f\"{len(X_test)} test cases.\")\n\nWe have 1652 training and 413 test cases.\n\n\n\n\n\ndf = read.csv(\"https://cssbook.net/d/media.csv\")\ndf = na.omit(df %>% mutate(\n    usesinternet=recode(internet, \n            .default=\"user\", `0`=\"non-user\")))\n\nset.seed(42)\ndf$usesinternet = as.factor(df$usesinternet)\nprint(\"How many people used online news at all?\")\n\n[1] \"How many people used online news at all?\"\n\nprint(table(df$usesinternet))\n\n\nnon-user     user \n     803     1262 \n\nsplit = initial_split(df, prop = .8)\ntraindata = training(split)\ntestdata  = testing(split)\n\nX_train = select(traindata, \n                 c(\"age\", \"gender\", \"education\"))\ny_train = traindata$usesinternet\nX_test = select(testdata, \n                c(\"age\", \"gender\", \"education\"))\ny_test = testdata$usesinternet\n\nglue(\"We have {nrow(X_train)} training and {nrow(X_test)} test cases.\")\n\nWe have 1652 training and 413 test cases.\n\n\n\n\n\n\n\n\n\nWe now can train our classifier (i.e., estimate our model using the training dataset contained in the objects X_train and y_train). This can be as straightforward as estimating a logistic regression equation (we will discuss different classifiers in Section 8.3). It may be that we first need to create new independent variables, so-called features, a step known as feature engineering, for example by transforming existing variables, combining them, or by converting text to numerical word frequencies. Example 8.4 shows how easy it is to train a classifier using the Naïve Bayes algorithm with packages caret/naivebayes in R and sklearn in Python (this approach will be better explained in Section 8.3.1).\n\n\n\n\n\n\n\nExample 8.4 A simple Naïve Bayes classifier\n\nPython codeR code\n\n\n\nmyclassifier = GaussianNB()\nmyclassifier.fit(X_train, y_train)\n\nGaussianNB()\n\ny_pred = myclassifier.predict(X_test)\n\n\n\n\nmyclassifier = train(x = X_train, y = y_train, \n                     method = \"naive_bayes\")\ny_pred = predict(myclassifier, newdata = X_test)\n\n\n\n\n\n\n\n\nBut before we can actually use this classifier to do some useful work, we need to test how capable it is to predict the correct labels, given a set of features. One might think that we could just feed it the same input data (i.e., the same features) again and see whether the predicted labels match the actual labels of the test dataset. In fact, we could do that. But this test would not be strict enough: after all, the classifier has been trained on exactly these data, and therefore one would expect it to perform pretty well. In particular, it may be that the classifier is very good in predicting its own training data, but fails at predicting other data, because it overgeneralizes some idiosyncrasy in the data, a phenomenon known as overfitting (see Figure 8.1).\n\n\n\nFigure 8.1: Underfitting and overfitting. Example adapted from https://scikit-learn.org/stable/auto _ examples/model _ selection/plot _ underfitting _ overfitting.html\n\n\nInstead, we use the features of the test dataset (stored in the objects X_test and y_test) as input for our classifier, and evaluate how far the predicted labels match the actual labels. Remember: the classifier has at no point in time seen the actual labels. Therefore, we can in fact calculate how often the prediction is right.4\n\n\n\n\n\n\n\nExample 8.5 Calculating precision and recall\n\nPython codeR code\n\n\n\nprint(\"Confusion matrix:\")\n\nConfusion matrix:\n\nprint(confusion_matrix(y_test, y_pred))\n\n[[ 55 106]\n [ 40 212]]\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n    non-user       0.58      0.34      0.43       161\n        user       0.67      0.84      0.74       252\n\n    accuracy                           0.65       413\n   macro avg       0.62      0.59      0.59       413\nweighted avg       0.63      0.65      0.62       413\n\n\n\n\n\nprint(confusionMatrix(y_pred, y_test))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction non-user user\n  non-user       62   53\n  user           99  199\n                                          \n               Accuracy : 0.632           \n                 95% CI : (0.5834, 0.6786)\n    No Information Rate : 0.6102          \n    P-Value [Acc > NIR] : 0.1958408       \n                                          \n                  Kappa : 0.1843          \n                                          \n Mcnemar's Test P-Value : 0.0002623       \n                                          \n            Sensitivity : 0.3851          \n            Specificity : 0.7897          \n         Pos Pred Value : 0.5391          \n         Neg Pred Value : 0.6678          \n             Prevalence : 0.3898          \n         Detection Rate : 0.1501          \n   Detection Prevalence : 0.2785          \n      Balanced Accuracy : 0.5874          \n                                          \n       'Positive' Class : non-user        \n                                          \n\nprint(\"Confusion matrix:\")\n\n[1] \"Confusion matrix:\"\n\nconfmat = table(testdata$usesinternet, y_pred)\nprint(confmat)\n\n          y_pred\n           non-user user\n  non-user       62   99\n  user           53  199\n\nprint(\"Precision for predicting True internet\")\n\n[1] \"Precision for predicting True internet\"\n\nprint(\"users and non-internet-users:\")\n\n[1] \"users and non-internet-users:\"\n\nprecision = diag(confmat) / colSums(confmat)\nprint(precision)\n\n non-user      user \n0.5391304 0.6677852 \n\nprint(\"Recall for predicting True internet\")\n\n[1] \"Recall for predicting True internet\"\n\nprint(\"users and non-internet-users:\")\n\n[1] \"users and non-internet-users:\"\n\nrecall = (diag(confmat) / rowSums(confmat))\nprint(recall)\n\n non-user      user \n0.3850932 0.7896825 \n\n\n\n\n\n\n\n\n\nAs shown in Example 8.5, we can create a confusion matrix (generated with caret function confusionMatrix in R and sklearn function confusion_matrix in Python), and then estimate two measures: precision and recall (using base R calculations in R and sklearn function classification_report in Python). In a binary classification, the confusion matrix is a useful table in which each column usually represents the number of cases in a predicted class, and each row the number of cases in the real or actual class. With this matrix (see Figure 8.2) we can then estimate the number of true positives (TP) (correct prediction), false positives (FP) (incorrect prediction), true negatives (TN) (correct prediction) and false negatives (FN) (incorrect prediction).\n\n\n\nFigure 8.2: Visual representation of a confusion matrix.\n\n\nFor a better understanding of these concepts, imagine that we build a sentiment classifier, that predicts – based on the text of a movie review – whether it is a positive review or a negative review. Let us assume that the goal of training this classifier is to build an app that recommends only good movies to the user. There are two things that we want to achieve: we want to find as many positive films as possible (recall), but we also want that the selection we found only contains positive films (precision).\nPrecision is calculated as \\(\\frac{\\rm{TP}}{\\rm{TP}+\\rm{FP}}\\), where TP are true positives and FP are false positives. For example, if our classifier retrieves 200 articles that it classifies as positive films, but only 150 of them indeed are positive films, then the precision is \\(\\frac{150}{150+50} = \\frac{150}{200} = 0.75\\).\nRecall is calculated as \\(\\frac{\\rm{TP}}{\\rm{TP}+\\rm{FN}}\\), where TP are true positives and FN are false negatives. If we know that the classifier from the previous paragraph missed 20 positive films, then the recall is \\(\\frac{150}{150+20} = \\frac{150}{170}= 0.88\\).\nIn other words: recall measures how many of the cases we wanted to find we actually found. Precision measures how much of what we have found is actually correct.\nOften, we have to make a trade-off between precision and recall. For example, just retrieving every film would give us a recall of 1.0 (after all, we didn’t miss a single positive film). But on the other hand, we retrieved all the negative films as well, so precision will be extremely low. It can depend on the task at hand whether precision or recall is more important. In ?sec-validation, we discuss this trade-off in detail, as well as other metrics such as accuracy, \\(F_1\\)-score or the area under the curve (AUC)."
  },
  {
    "objectID": "content/chapter08.html#sec-nb2dnn",
    "href": "content/chapter08.html#sec-nb2dnn",
    "title": "8  Statistical Modeling and Supervised Machine Learning",
    "section": "8.3 Classical Machine Learning: From Naïve Bayes to Neural Networks",
    "text": "8.3 Classical Machine Learning: From Naïve Bayes to Neural Networks\nTo do supervised machine learning, we can use several models, all of which have different advantages and disadvantages, and are more useful for some use cases than for others. We limit ourselves to the most common ones in this chapter. The website of scikit-learn (www.scikit-learn.org) gives a good overview of more alternatives.\n\n8.3.1 Naïve Bayes\nThe Naïve Bayes classifier is a very simple classifier that is often used as a “baseline”. Before estimating more complicated and resource-intensive models, it is a good idea to estimate a simpler model first, to assess how much better the other model actually is. Sometimes, the simple model might even be just fine.\nThe Naïve Bayes classifier allows you to predict a binary outcome, such as: “Is this message spam or not?”, “Is this article about politics or not?”, “Will this go viral or not?”. It, in fact, also allows you to do the same with more than one category, and both the Python and the R implementation will happily let you train a Naïve Bayes classifier on nominal data, such as whether an article is about politics, sports, the economy, or something different.\nFor the sake of simplicity, we will discuss a binary example, though.\nAs its name suggests, a Naïve Bayes classifier is based on Bayes’ theorem, and it is “naïve”. It may sound a bit weird to call a model “naïve”, but what it actually means is not so much that it is stupid, but that it makes very far-reaching assumptions about the data (hence, it is naïve). Specifically, it assumes that all features are independent from each other. Of course, that is hardly ever the case – for instance, in a survey data set, while age and gender indeed are generally independent from each other, this is not the case for education, political interest, media use, and so on. And in textual data, whether a word \\(W_1\\) is used is not independent from the use of word \\(W_2\\) – after all, both are not randomly drawn from a dictionary, but depend on the topic of the text (and other things). Astonishingly, even though these assumptions are regularly violated, the Naïve Bayes classifier works reasonably well in practice.\nThe Bayes part of the Naïve Bayes classifier comes from the fact that it uses Bayes’ formula, $ P(\n\n\n\n:::\n::: ::: :::\n\n\n8.3.2 Train, Validate, Test\nBy now, we have established which measures we can use to decide which model to use. For all of them, we have assumed that we split our labeled dataset into two: a training dataset and a test dataset. The logic behind it was simple: if we calculate precision and recall on the training data itself, our assessment would be too optimistic – after all, our models have been trained on exactly these data, so predicting the label isn’t too hard. Assessing the models on a different dataset, the test dataset, instead, gives us an assessment of what precision and recall look like if the labels haven’t been seen earlier – which is exactly what we want to know.\nUnfortunately, if we calculate precision and recall (or any other metric) for multiple models on the same test dataset, and use these results to determine which metric to use, we can run into a problem: we may avoid overfitting of our model on the training data, but we now risk overfitting it on the test data! After all, we could tweak our models until they fit our test data perfectly, even if this makes the predictions for other cases worse.\nOne way to avoid this is to split the original data into three datasets instead of two: a training dataset, a validation dataset, and a test dataset. We train multiple model configurations on the training dataset and calculate the metrics of interest for all of them on the validation dataset. Once we have decided on a final model, we calculate its performance (once) on the test dataset, to get an unbiased estimate of its performance.\n\n\n8.3.3 Cross-validation and Grid Search\nIn an ideal world, we would have a huge labeled dataset and would not need to worry about the decreasing size of our training dataset as we set aside our validation and test datasets.\nUnfortunately, our labeled datasets in the real world have a limited size, and setting aside too many cases can be problematic. Especially if you are already on a tight budget, setting aside not only a test dataset, but also a validation dataset of meaningful size may lead to critically small training datasets. While we have addressed the problem of overfitting, this could lead to underfitting: we may have removed the only examples of some specific feature combination, for instance.\nA common approach to address this issue is \\(k\\)-fold cross-validation. To do this, we split our training data into \\(k\\) partitions, known as folds. We then estimate our model \\(k\\) times, and each time leave one of the folds aside for validation. Hence, every fold is exactly one time the validation dataset, and exactly \\(k-1\\) times part of the training data. We then simply average the results of our \\(k\\) values for the evaluation metric we are interested in.\nIf our classifier generalizes well, we would expect that our metric of interest (e.g., the accuracy, or the \\(F_1\\)-score, …) is very similar in all folds. Example 8.6 performs a cross-validation based on the logistic regression classifier we built above. We see that the standard deviation is really low, indicating that there are almost no changes between the runs, which is great.\nRunning the same cross-validation on our random forest, instead, would produce not only worse (lower) means, but also worse (higher) standard deviations, even though also here, there are no dramatic changes between the runs.\n\n\n\n\n\n\n\nExample 8.6 Crossvalidation\n\nPython codeR code\n\n\n\nmyclassifier = LogisticRegression(solver=\"lbfgs\")\nacc = cross_val_score(\n    estimator=myclassifier, X=X_train, y=y_train, scoring=\"accuracy\", cv=5\n)\nprint(acc)\n\n[0.64652568 0.64048338 0.62727273 0.64242424 0.63636364]\n\nprint(f\"M={acc.mean():.2f}, SD={acc.std():.3f}\")\n\nM=0.64, SD=0.007\n\n\n\n\n\nmyclassifier = train(x = X_train, y = y_train,\n    method = \"glm\", family=\"binomial\",\n    metric=\"Accuracy\", trControl = trainControl(\n     method = \"cv\", number = 5, \n     returnResamp =\"all\", savePredictions=TRUE),)\nprint(myclassifier$resample)\n\n   Accuracy     Kappa parameter Resample\n1 0.6646526 0.2564808      none    Fold1\n2 0.6616314 0.2441998      none    Fold2\n3 0.6606061 0.2057079      none    Fold3\n4 0.6575758 0.2099241      none    Fold4\n5 0.6333333 0.1670491      none    Fold5\n\nprint(myclassifier$results)\n\n  parameter  Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.6555598 0.2166724 0.01267959 0.03525159\n\n\n\n\n\n\n\n\n\nVery often, cross-validation is used when we want to compare many different model specifications, for example to find optimal hyperparameters. Hyperparameters are parameters of the model that are not estimated from the data. These depend on the model, but could for example be the estimation method to use, the number of times a bootstrap should be repeated, etc. Very good examples are the hyperparameters of support vector machines (see above): it is hard to know how soft our margins should be (the \\(C\\)), and we may also be unsure about the right kernel (Example 8.8), or in the case of a polynomial kernel, how many degrees we want to consider.\nUsing the help function (e.g., RandomForestClassifier? in Python), you can look up which hyperparameters you can specify. For a random forest classifier, for instance, this includes the number of estimators in the model, the criterion, and whether or not to use bootstrapping. Example 8.7, Example 8.8, and Example 8.9 illustrate how you can automatically assess which values you should choose.\nNote that in R, not all parameters are “tunable” using standard caret. Therefore, an exact replication of the grid searches in Example 8.7 and Example 8.8 would requires either manual comparisons or writing a so-called caret extension.\n\n\n\n\n\n\n\nExample 8.7 A simple gridsearch in Python ## Python code\n\nf1scorer = make_scorer(f1_score, pos_label=\"user\")\n\n\nmyclassifier = RandomForestClassifier()\n\ngrid = {\n    \"n_estimators\": [10, 50, 100, 200],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"bootstrap\": [True, False],\n}\nsearch = GridSearchCV(\n    estimator=myclassifier, param_grid=grid, scoring=f1scorer, cv=5\n)\nsearch.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=RandomForestClassifier(),\n             param_grid={'bootstrap': [True, False],\n                         'criterion': ['gini', 'entropy'],\n                         'n_estimators': [10, 50, 100, 200]},\n             scoring=make_scorer(f1_score, pos_label=user))\n\nprint(search.best_params_)\n\n{'bootstrap': True, 'criterion': 'gini', 'n_estimators': 100}\n\nprint(classification_report(y_test, search.predict(X_test)))\n\n              precision    recall  f1-score   support\n\n    non-user       0.43      0.40      0.41       161\n        user       0.63      0.66      0.65       252\n\n    accuracy                           0.56       413\n   macro avg       0.53      0.53      0.53       413\nweighted avg       0.55      0.56      0.56       413\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 8.8 A gridsearch in Python using multiple CPUs ## Python code\n\nmyclassifier = SVC(gamma=\"scale\")\n\ngrid = {\"C\": [100, 1e4], \"kernel\": [\"linear\", \"rbf\", \"poly\"], \"degree\": [3, 4]}\n\nsearch = GridSearchCV(\n    estimator=myclassifier,\n    param_grid=grid,\n    scoring=f1scorer,\n    cv=5,\n    n_jobs=-1,  # use all cpus\n    verbose=10,\n)\nsearch.fit(X_train_scaled, y_train)\n\nFitting 5 folds for each of 12 candidates, totalling 60 fits\nGridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n             param_grid={'C': [100, 10000.0], 'degree': [3, 4],\n                         'kernel': ['linear', 'rbf', 'poly']},\n             scoring=make_scorer(f1_score, pos_label=user), verbose=10)\n\nprint(f\"Hyperparameters {search.best_params_} \" \"give the best performance:\")\n\nHyperparameters {'C': 100, 'degree': 3, 'kernel': 'poly'} give the best performance:\n\nprint(classification_report(y_test, search.predict(X_test_scaled)))\n\n              precision    recall  f1-score   support\n\n    non-user       0.58      0.04      0.08       161\n        user       0.62      0.98      0.76       252\n\n    accuracy                           0.62       413\n   macro avg       0.60      0.51      0.42       413\nweighted avg       0.60      0.62      0.49       413\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 8.9 A gridsearch in R. ## R code\n\n# Create the grid of parameters\ngrid = expand.grid(Loss=c(\"L1\",\"L2\"),\n                   cost=c(100,1000))\n\n# Train the model using our previously defined \n# parameters\ngridsearch = train(x = X_train, y = y_train,\n    preProcess = c(\"center\", \"scale\"), \n    method = \"svmLinear3\", \n    trControl = trainControl(method = \"cv\", \n            number = 5),\n    tuneGrid = grid)\ngridsearch\n\nL2 Regularized Support Vector Machine (dual) with Linear Kernel \n\n1652 samples\n   3 predictor\n   2 classes: 'non-user', 'user' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1322, 1322, 1321, 1321, 1322 \nResampling results across tuning parameters:\n\n  Loss  cost  Accuracy   Kappa    \n  L1     100  0.6458555  0.1994112\n  L1    1000  0.5587091  0.1483755\n  L2     100  0.6525185  0.2102270\n  L2    1000  0.6525185  0.2102270\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were cost = 100 and Loss = L2.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nSupervised machine learning is one of the areas where you really see differences between Python and R. While in Python, virtually all you need is available via scikit-learn, in R, we often need to combine caret with various libraries providing the actual models. In contrast, all components we need for machine learning in Python are developed within one package, which leads to less friction. This is what you see in the gridsearch examples in this section. In scikit-learn, any hyperparameter can be part of the grid, but no hyperparameter has to be. Note that in R, in contrast, you cannot (at least, not easily) put any parameter of the model in the grid. Instead, you can look up the “tunable parameters” which must be present as part of the grid in the caret documentation. This means that an exact replication of the grid searches in Example 8.7 and Example 8.8 is not natively supported using caret and requires either manual testing or writing a so-called caret extension.\nWhile in the end, you can find a supervised machine learning solution for all your use cases in R as well, if supervised machine learning is at the core of your project, it may save you a lot of cursing to do this in Python. Hopefully, the package will provide a better solution for machine learning in R in the near future.\n\n\n\n\n\n\n\nBurscher, Björn, Daan Odijk, Rens Vliegenthart, Maarten de Rijke, and Claes H. de Vreese. 2014. “Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis.” Communication Methods and Measures 8 (3): 190–206. https://doi.org/10.1080/19312458.2014.937527.\n\n\nRiffe, Daniel, Stephen Lacy, Frederick Fico, and Brendan Watson. 2019. Analyzing Media Messages. Using Quantitative Content Analysis in Research. 4th edition. New York, NY: Routledge.\n\n\nTrilling, Damian. 2013. “Following the news: Patterns of online and offline news consumption.” {PhD} Theses, University of Amsterdam. https://hdl.handle.net/11245/1.394551.\n\n\nVermeer, Susan A. M. 2018. “A supervised machine learning method to classify Dutch-language news items.” https://doi.org/10.6084/m9.figshare.7314896.v1.\n\n\nVermeer, Susan A. M., Theo Araujo, Stefan F. Bernritter, and Guda van Noort. 2019. “Seeing the wood for the trees: How machine learning can help firms in identifying relevant electronic word-of-mouth in social media.” International Journal of Research in Marketing 36 (3): 492–508. https://doi.org/10.1016/j.ijresmar.2019.01.010."
  },
  {
    "objectID": "content/chapter09.html#sec-unicode",
    "href": "content/chapter09.html#sec-unicode",
    "title": "9  Processing text",
    "section": "9.1 Text as a String of Characters",
    "text": "9.1 Text as a String of Characters\n\n\n\n\n\n\nImportant: Unicode and Encodings\n\n\n\n\n\nTechnically speaking, text is represented as bytes (numbers) rather than characters. The Unicode standard determines how these bytes should be interpreted or “decoded”. This chapter assumes that the bytes in a file are already “decoded” into characters (or Unicode code points), and we can just work with the characters. Especially if you are not working with English text, it is very important to make sure you understand Unicode and encodings and check that the texts you work with are decoded properly. Please see Section 5.2.2 for more information on how this works.\n\n\n\nWhen we think about text, we might think of sentences or words, but the computer only “thinks” about letters: text is represented internally as a string of characters. This is reflected of course in the type name, with R calling it a character vector and Python a string.\n\n\n\n\n\n\n\nExample 9.1 Internal representation and of single and multiple texts.\n\nPython codeR code\n\n\n\ntext = \"This is text.\"\nprint(f\"type(text): {type(text)}\")\n\ntype(text): <class 'str'>\n\nprint(f\"len(text): {len(text)}\")\n\nlen(text): 13\n\nprint(f\"text[0]: '{text[0]}'\")\n\ntext[0]: 'T'\n\nprint(f\"text[5:7]: '{text[5:7]}'\")\n\ntext[5:7]: 'is'\n\nprint(f\"text[-1]: '{text[-1]}'\")\n\ntext[-1]: '.'\n\nprint(f\"text[-4:]: '{text[-5:]}'\")\n\ntext[-4:]: 'text.'\n\n\n\n\n\ntext = \"This is text.\"\nglue(\"class(text): {class(text)}\")\n\nclass(text): character\n\nglue(\"length(text): {length(text)}\")\n\nlength(text): 1\n\nglue(\"text[1]: {text[1]}\")\n\ntext[1]: This is text.\n\nglue(\"str_length(text): {str_length(text)}\")\n\nstr_length(text): 13\n\nglue(\"str_sub(text, 6,7): {str_sub(text, 6,7)}\")\n\nstr_sub(text, 6,7): is\n\n\n\n\n\n\nPython codeR code\n\n\n\nwords = [\"These\", \"are\", \"words\"]\nprint(f\"type(words): {type(words)}\")\n\ntype(words): <class 'list'>\n\nprint(f\"len(words): {len(words)}\")\n\nlen(words): 3\n\nprint(f\"words[0]: '{words[0]}'\")\n\nwords[0]: 'These'\n\nprint(f\"words[1:3]: '{words[1:3]}'\")\n\nwords[1:3]: '['are', 'words']'\n\n\n\n\n\nwords = c(\"These\", \"are\", \"words\")\nglue(\"class(words): {class(words)}\")\n\nclass(words): character\n\nprint(\"length(words): {length(words)}\")\n\n[1] \"length(words): {length(words)}\"\n\nglue(\"words[1]: {words[1]}\")\n\nwords[1]: These\n\n# Note: use collapse to convert to single value\nwords_2_3 = str_c(words[2:3], collapse=\", \")\nglue(\"words[2:3]: {words_2_3}\")\n\nwords[2:3]: are, words\n\n\n\n\n\n\n\n\n\nAs a simple example, the figure at the top of Example 9.1 shows how the text “This is text.” is represented. This text is split into separate characters, with each character representing a letter (or space, punctuation, emoji, or Chinese character). These characters are indexed starting from the first one, with (as always) R counting from one, but Python counting from zero.\nIn Python, texts are represented as str (string) objects, in which we can directly address the individual characters by their position: text[0] is the first character of text, and so on. In R, however, texts (like all objects) represent columns (or vectors) rather than individual values. Thus, text[1] in R is the first text in a series of text. To access individual characters in a text, you have to use a function such as str_length and str_sub that will be discussed in more detail below. This also means that in Python, if you have a column (or list) of strings that you need to apply an operation to, you either need to use one of /textitPandas’ methods shown below or use a for loop or list comprehension to iterate over all the strings (see also section 3.2).\n\n9.1.1 Methods for Dealing With Text\n\n\n\n\n\n\nStringi, stringr, and base string operations in R\n\n\n\n\n\nAs is so often the case, R has multiple packages that partially replicate functionality for basic text handling. In this book we will mainly use the stringr package, which is part of tidyverse. This is not because that package is necessarily better or easier than the alternative stringi package or the built-in (base) methods. However, the methods are well-documented, clearly named, and consistent with other tidyverse functions, so for now it is easiest to stick to stringr. In particular, stringr is very similar to stringi (and in fact is partially based on it). So, to give one example, the function str_detect is more or less the same as stringi::str_detect and base::grepl.\n\n\n\nThe first thing to keep in mind is that once you load any text in R or Python, you usually store this content as a character or string object (you may also often use lists or dictionaries, but they will have strings inside them), which means that basic operations and conditions of this data type apply, such as indexing or slicing to access individual characters or substrings (see Section 3.1). In fact, base strings operations are very powerful to clean your text and eliminate a large amount of noise. Table 9.1 summarizes some useful operations on strings in R and Python that will help you in this stage.\n\n\n\n\n\nTable 9.1: Useful strings operations in R and Python to clean noise.\n\n\n\n\n\n\n\n\n\nString operation\nR (stringr)\nPython\nPandas\n\n\n\n\n\n\n(whole column)\n(single string)\n(whole column)\n\n\n\nCount characters in s\nstr_length(s)\nlen(s)\ns.str.len()\n\n\n\nExtract a substring\nstr_sub(s, n1, n2)\ns[n1:n2]\ns.str.slice(n1, n2)\n\n\n\nTest if s contains s2\nstr_detect(s, s2)*\ns2 in s\ns.str.match(s2)*\n\n\n\nStrip spaces\ntrimws(s)\ns.strip()\ns.str.strip()\n\n\n\nConvert to lowercase\ntolower(s)\ns.lower()\ns.str.lower()\n\n\n\nConvert to uppercase\ntoupper(s)\ns.upper()\ns.str.upper()\n\n\n\nFind s1 and replace by s2\nstr_replace(s, s1, s2)*\ns.replace(s1, s2)\ns.str.replace(s1, s2)*\n\n\n\n\n\n\nTable notes\n*) The R functions str_detect and str_replace and the Pandas function s.str.match and s.str.replace use regular expressions to define what to find and replace. See Section 9.2 below for more information.\n\nLet us apply some of these functions/methods to a simple Wikipedia text that contains HTML tags, or boilerplate, and upper/lower case letters. Using the stringr function str_replace_all in R and replace in Python we can do a find-and-replace and replace substrings by others (in our case, replace <b> with a space, for instance). To remove unnecessary double spaces we apply the str_squish function provided by stringr and in Python, we first chunk our string into a list of words by using the split string method, before we use the join method to join them again with now a single space. In the case of converting letters from upper to lower case, we use the base R function tolower and the string method lower in Python. Finally, the base R function trimws and the Python string method strip remove the white space from the beginning and end of the string. Example 9.2 shows how to conduct this cleaning process.\nWhile you can get quite far with these techniques, there are more advanced and flexible approaches possible. For instance, you probably do not want to list all possible HTML tags in separate replace methods or str_replace_all functions. In the next section, we therefore show how to use so-called regular expressions to formulate such generalizable patterns.\n\n\n\n\n\n\n\nExample 9.2 Some basic text cleaning approaches\n\nPython codeR code\n\n\n\ntext = \"\"\"   <b>Communication</b>    \n    (from Latin communicare, meaning to share) \"\"\"\n# remove tags:\ncleaned = text.replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n# normalize white space\ncleaned = \" \".join(cleaned.split())\n# lower case\ncleaned = cleaned.lower()\n# trim spaces from start and end\ncleaned = cleaned.strip()\n\nprint(cleaned)\n\ncommunication (from latin communicare, meaning to share)\n\n\n\n\n\ntext = \"    <b>Communication</b>    \n     (from Latin communicare, meaning to share)  \"\ncleaned = text %>% \n  # remove HTML tags:\n  str_replace_all(\"<b>\", \" \")  %>% \n  str_replace_all(\"</b>\", \" \")  %>% \n  # normalize white space \n  str_squish() %>%\n  # lower case\n  tolower()  %>% \n  # trim spaces at start and end\n  trimws()\n\nglue(cleaned)\n\ncommunication (from latin communicare, meaning to share)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo regex or not to regex in Python\n\n\n\n\n\nYou may wonder why we introduce basic string methods like replace or the split-then-join trick, if everything can be done with regular expressions anyway. There are a couple of reasons for still using these methods: first, they are easy and don’t have any dependencies. If you just want to replace a single thing, then you don’t need to import any additional module. Second, regular expressions are considerably slower than string methods – in most cases, you won’t notice, but if you do a lot of replacements (think in thousands per news article, for a million articles), then this may matter. Third, you can use the join trick also for other things like punctuation removal – in this case, by generating a list of all characters in a string called text provided they are no punctuation characters, and then joining them directly to each other: from string import punctuation; \"\".join([c for c in text if c not in punctuation])"
  },
  {
    "objectID": "content/chapter09.html#sec-regular",
    "href": "content/chapter09.html#sec-regular",
    "title": "9  Processing text",
    "section": "9.2 Regular Expressions",
    "text": "9.2 Regular Expressions\nA regular expression or regex is a powerful language to locate strings that conform to a given pattern. For instance, we can extract usernames or email-addresses from text, or normalize spelling variations and improve the cleaning methods covered in the previous section. Specifically, regular expressions are a sequence of characters that we can use to design a pattern and then use this pattern to find strings (identify or extract) and also replace those strings by new ones.\nRegular expressions look complicated, and in fact they take time to get used to initially. For example, a relatively simple (and not totally correct) expression to match an email address is [\\w\\.-]+@[\\w\\.-]+\\.\\w\\w+, which doesn’t look like anything at all unless you know what you are looking for. The good news is that regular expression syntax is the same in R and Python (and many other languages), so once you learn regular expressions you will have acquired a powerful and versatile tool for text processing.\nIn the next section, we will first review general expression syntax without reference to running them in Python or R. Subsequently, you will see how you can apply these expressions to inspect and clean texts in both languages.\n\n9.2.1 Regular Expression Syntax\nAt its core, regular expressions are patters for matching sequences of characters. In the simplest case, a regular letter just matches that letter, so the pattern “cat” matches the text “cat”. Next, there are various wildcards, or ways to match different letters. For example, the period (.) matches any character, so c.t matches both “cat” and “cot”. You can place multiple letters between square brackets to create a character class that matches all the specified letters, so c[au]t matches “cat” and “cut”, but not “cot”. There are also a number of pre-defined classes, such as \\w which matches “word characters” (letters, digits, and (curiously) underscores).\nFinally, for each character or group of characters you can specify how often it should occur. For example, a+ means one or more a’s while a? means zero or one a, so lo+l matches lol',lool’, etc., and lo?l matches lol' orll’. This raises the question, of course, of how to look for actual occurrences of a plus, question mark, or period. The solution is to escape these special symbols by placing a backslash (\\) before them: a\\+ matches the literal text “a+”, and \\\\w (with a double backslash) matches the literal text “”.\nNow, we can have another look at the example email address pattern given above. The first part, [\\w\\.-] creates a character class containing word characters, (literal) periods, and dashes. Thus, [\\w\\.-]+@[\\w\\.-]+ means one or more letters, digits, underscores, periods, or dashes, followed by an at sign, followed by one or more letters, digits, etc. Finally, the last part \\.\\w\\w+ means a literal period, a word character, and one or more word characters. In other words, we are looking for a name (possibly containing dashes or periods) before the at sign, followed by a domain, followed by a top level domain (like .com) of at least two characters.\nIn essence, thinking in terms of what you want to match and how often you want to match it is all there is to regular expressions. However, it will take some practice to get comfortable with turning something sensible (such as an email address) into a correct regular expression pattern. The next subsection will explain regular expression syntax in more detail, followed by an explanation of grouping, and in the final subsection we will see how to use these regular expressions in R and Python to do text cleaning.\n\n\n\n\n\nTable 9.2: Regular expression syntax\n\n\n\n\n\n\n\n\n\nFunction\nSyntax\nExample\nMatches\n\n\n\n\n\nSpecifier: What to match\n\n\n\n\n\n\nAll characters except for new lines\n.\nd.g\ndig,d!g\n\n\n\nWord characters*(letters, digits,_)\n\\w\nd\\wg\ndig,dog\n\n\n\nDigits*(0 to 9)\n\\d\n202\\d\n2020,2021\n\n\n\nWhitespace*(space, tab, newline)\n\\s\n\n\n\n\n\nNewline\n\\n\n\n\n\n\n\nBeginning of the string\n\\^\n\\^go\ngogo go\n\n\n\nEnding of the string\n\\$\ngo\\$\ngo gogo\n\n\n\nBeginning or end of word\n\\b\n\\bword\\b\na word!\n\n\n\nEither first or second option\n…|…\ncat|dog\ncat,dog\n\n\n\nQuantifier: How many to match\n\n\n\n\n\n\nZero or more\n*\nd.*g\ndg,drag,d = g\n\n\n\nZero or more (non-greedy)\n*?\nd.*?g\ndogg\n\n\n\nOne or more\n+\n\\d+%\n1%,200%\n\n\n\nOne or more (non-greedy)\n+?\n\\d+%\n200%\n\n\n\nZero or one\n?\ncolou?r\ncolor,colour\n\n\n\nExactly n times\n{n}\n\\d{4}\n1940,2020\n\n\n\nAt least n times\n{n,}\n\n\n\n\n\nBetween n and m times\n{n,m}\n\n\n\n\n\nOther constructs\n\n\n\n\n\n\nGroups\n(…)\n'(bla )+'\n'bla bla bla'\n\n\n\nSelection of characters\n[…]\nd[iuo]g\ndig,dug,dog\n\n\n\nRange of characters in selection\n[a-z]\n\n\n\n\n\nEverything except selection\n[\\^...]\n\n\n\n\n\nEscape special character\n\\\n3\\.14\n3.14\n\n\n\nUnicode character properties†\n\n\n\n\n\n\nLetters*\n\\p{LETTER}\n\nwords,単語\n\n\n\nPunctuation*\n\\p{PUNCTUATION}\n\n. , :\n\n\n\nQuotation marks*\n\\p{QUOTATION MARK}\n\n' ` \" «\n\n\n\nEmoji*\n\\p{EMOJI}\n\n😊\n\n\n\nSpecific scripts, e.g. Hangul*\n\\p{HANG}\n\n한글\n\n\n\n\n\n\nTable notes\n*) These selectors can be inverted by changing them into capital letters. Thus, \\W matches everything except word characters, and \\P\\{PUNCTUATION\\} matches everything except punctuation.\n†) See www.unicode.org/reports/tr44/#Property_Index for a full list of Unicode properties. Note that when using Python, these are only available if you use regex, which is a drop-in replacement for the more common re.\n\nIn Table 9.2 you will find an overview of the most important parts of regular expression syntax.1 The first part shows a number of common specifiers for determining what to match, e.g. letters, digits, etc., followed by the quantifiers available to determine how often something should be matched. These quantifiers always follow a specifier, i.e. you first say what you’re looking for, and then how many of those you need. Note that by default quantifiers are greedy, meaning they match as many characters as possible. For example, <.*> will match everything between angle brackets, but if you have something like <p>a paragraph</p> it will happily match everything from the first opening bracket to the last closing bracket. By appending a question mark (?) to the quantifier, it becomes non-greedy. so, <.*?> will match the individual <p> and </p> substrings.\nThe third section discusses other constructs. Groups are formed using parentheses () and are useful in at least three ways. First, by default a quantifier applies to the letter directly before it, so no+ matches “no”, “nooo”, etc. If you group a number of characters you can apply a quantifier to the group. So, that's( not)? good matches either “that’s not good” or “that’s good”. Second, when using a vertical bar (|) to have multiple options, you very often want to put them into a group so you can use it as part of a larger pattern. For example, a( great| fantastic)? victory matches either “a victory”, “a great victory”, or “a fantastic victory”. Third, as will be discussed below in Section 9.3, you can use groups to capture (extract) a specific part of a string, e.g. to get only the domain part of a web address.\nThe other important construct are character classes, formed using square brackets []. Within a character class, you can specify a number of different characters that you want to match, using a dash (-) to indicate a range. You can add as many characters as you want: [A-F0-9] matches digits and capital letters A through F. You can also invert this selection using an initial caret: [^a-z] matches everything except for lowercase Latin letters. Finally, you sometimes need to match a control character (e.g. +, ?, \\). Since those characters have a special meaning within a regular expressing, they cannot be used directly. The solution is to add a backslash (\\) behind them to escape them: . matches any character, but \\. matches an actual period. \\\\ matches an actual backslash.\n\n\n\n9.2.2 Example Patterns\nUsing the syntax explained in the previous section, we can now make patterns for common tasks in cleaning and analyzing text. Table 9.3 lists a number of regular expressions for common tasks such as finding dates or stripping HTML artifacts.\n\n\nTable 9.3: Regular expression syntax in Python and R\n\n\nGoal\nPattern\nExample\n\n\n\n\nUS Zip Code\n\\d{5}\n90210\n\n\nUS Phone number\n(\\d{3}-)?\\d{3}-\\d{4}\n202-456-1111,456-1111\n\n\nDutch Postcode\n\\d{4} ?[A-Za-z]{2}\n1015 GK\n\n\nISO Date\n\\d{4}-\\d{2}-\\d{2}\n2020-07-20\n\n\nGerman Date\n\\d{1,2}\\.\\d{1,2}\\.\\d{4}\n25.6.1988\n\n\nInternational phone number\n\\+(\\d[-]?){7,}\\d\n+1 555-1234567\n\n\nURL\nhttps?://\\S+\nhttps://example.com?a=b\n\n\nE-mail address\n[\\w\\.-]+@[\\w\\.-]+\\.\\w+\nme@example.com\n\n\nHTML tags\n</?\\w[^>]*>\n</html>\n\n\nHTML Character escapes\n&[^;]+;\n&nbsp;\n\n\n\n\nPlease note that most of these patterns do not correctly distinguish all edge cases (and hence may lead to false negatives and/or false positives) and are provided for educational purposes only.\nWe start with a number of relatively simple patterns for Zip codes and phone numbers. Starting with the simplest example, US Zip codes are simply five consecutive numbers. Next, a US phone number can be written down as three groups of numbers separated by parentheses, where the first group is made optional for local phone numbers using parentheses to group these numbers so the question mark applies to the whole group. Next, Dutch postal codes are simply four numbers followed by two letters, and we allow an optional space in between. Similarly simple, dates in ISO format are three groups of numbers separated by dashes. German dates follow a different order, use periods as separator, and allow for single-digit day and month numbers. Note that these patterns do not check for the validity of dates. A simple addition would be to restrict months to 01-12, e.g. using (0[1-9]|1[0-2]). However, in general validation is better left to specialized libraries, as properly validating the day number would require taking the month (and leap years) into account.\nA slightly more complicated pattern is the one given for international phone numbers. They always start with a plus sign and contain at least eight numbers, but can contain dashes and spaces depending on the country. So, after the literal + (which we need to escape since + is a control character), we look for seven or more numbers, optionally followed by a single dash or space, and end with a single number. This allows dashes and spaces at any position except the start and end, but does not allow for e.g. double dashes. It also makes sure that there are at least eight numbers regardless of how many dashes or spaces there are.\nThe final four examples are patterns for common notations found online. For URLs, we look for http:// or https:// and take everything until the next space or end of the string. For email addresses, we define a character class for letters, periods, or dashes and look for it before and after the at sign. Then, there needs to be at least one period and a top level domain containing only letters. Note that the dash within the character class does not need to be escaped because it is the final character in the class, so it cannot form a range. For HTML tags and character escapes, we anchor the start (< and &) and end (> and ;) and allow any characters except for the ending character in between using an inverted character class.\nNote that these example patterns would also match if the text is enclosed in a larger text. For example, the zip code pattern would happily match the first five numbers of a 10-digit number. If you want to check that an input value is a valid zip code (or email address, etc.), you probably want to check that it only contains that code by surrounding it with start-of-text and end-of-text markers: ^\\d{5}$. If you want to extract e.g. zip codes from a longer document, it is often useful to surround them with word boundary markers: \\b\\d{5}\\b.\nPlease note that many of those patterns are not necessarily fully complete and correct, especially the final patterns for online notations. For example, email addresses can contain plus signs in the first part, but not in the domain name, while domain names are not allowed to start with a dash – a completely correct regular expression to match email addresses is over 400 characters long! Even worse, complete HTML tags are probably not even possible to describe using regular expressions, because HTML tags frequently contain comments and nested escapes within attributes. For a better way to deal with analyzing HTML, please see Chapter 12. In the end, patterns like these are fine for a (somewhat) noisy analysis of (often also somewhat noisy) source texts as long as you understand the limitations."
  },
  {
    "objectID": "content/chapter09.html#sec-regextract",
    "href": "content/chapter09.html#sec-regextract",
    "title": "9  Processing text",
    "section": "9.3 Using Regular Expressions in Python and R",
    "text": "9.3 Using Regular Expressions in Python and R\nNow that you hopefully have a firm grasp of the syntax of regular expressions, it is relatively easy to use these patterns in Python or R (or most other languages). Table 9.4 lists the commands for four of the most common use cases: identifying matching texts, removing and replacing all matching text, extracting matched groups, and splitting texts.\n\n\nTable 9.4: Regular expression syntax\n\n\n\n\n\n\n\n\nOperation\nR (stringr)\nPython\nPandas\n\n\n\n\n\n(whole column)\n(single string)\n(whole column)\n\n\nDoes pattern p occur in text t?\nstr_detect(t, p)\nre.search(p, t)\nt.str.contains(p)\n\n\nDoes text t start with pattern p?\nstr_detect(t, \"\\^p\")\nre.match(p, t)\nt.str.match(p)\n\n\nCount occurrences of p in t\nstr_count(t, \"\\^p\")\nre.match(p, t)\nt.str.count(p)\n\n\nRemove all occurences of p in t\nstr_remove_all(t, p)\nre.sub(p, \"\", t)\nt.str.replace(p, \"\")\n\n\nReplace p by r in text t\nstr_replace_all(t, p, r)\nre.sub(p, r, t)\nt.str.replace(p, r)\n\n\nExtract the first match of p in t\nstr_extract(t, p)\nre.search(p, t).group(1)\nt.str.extract(p)\n\n\nExtract all matches of p in t\nstr_extract_all(t, p)\nre.findall(p, t)\nt.str.extractall(p)\n\n\nSplit t on matches of p\nstr_split(t, p)\nre.split(p, t)\nt.str.split(p)\n\n\n\n\nNote: if using Unicode character properties (\\p), use the same functions in package regex instead of re\nFor R, we again use the functions from the stringr package. For Python, you can use either the re or regex package, which both support the same functions and syntax so you can just import one or the other. The re package is more common and significantly faster, but does not support Unicode character properties (\\p). We also list the corresponding commands for pandas, which are run on a whole column instead of a single text (but note that pandas does not support Unicode character properties.)\nFinally, a small but important note about escaping special characters by placing a backslash (\\) before them. The regular expression patterns are used within another language (in this case, Python or R), but these languages have their own special characters which are also escaped. In Python, you can create a raw string by putting a single r before the opening quotation mark: r\"\\d+\" creates the regular expression pattern \\d. From version 4.0 (released in spring 2020), R has a similar construct: r\"(\\d+)\". In R, the parentheses are part of the string delimiters, but you can use more parentheses within the string without a problem. The only thing you cannot include in a string is the closing sequence )\", but as you are also allowed to use square or curly brackets instead of parentheses and single instead of double quotes to delimit the raw string you can generally avoid this problem: to create the pattern \"(cat|dog)\" (i.e. cat or dog enclosed in quotation marks), you can use r\"{\"(cat|dog)\"}\" or r'(\"(cat|dog)\")' (or even more legible: r'{\"(cat|dog)\"}').\nUnfortunately, in earlier versions of R (and in any case if you don’t use raw strings), you need to escape special characters twice: first for the regular expression, and then for R. So, the pattern \\d becomes \"\\\\d\". To match a literal backslash you would use the pattern \\\\, which would then be represented in R as \"\\\\\\\\\"!\nExample 9.3 cleans the same text as Example 9.2 above, this time using regular expressions. First, it uses <[^>+]> to match all HTML tags: an angular opening bracket, followed by anything except for a closing angular bracket ([^>]), repeated one or more times (+), finally followed by a closing bracket. Next, it replaces one or more whitespace characters (\\s+) by a single space. Finally, it uses a vertical bar to select either space at the start of the string (^\\s+), or at the end (\\s+$), and removes it. As you can see, you can express a lot of patterns using regular expressions in this way, making for more generic (but sometimes less readable) clean-up code.\n\n\n\n\n\n\n\nExample 9.3 Using regular expressions to clean a text\n\nPython codeR code\n\n\n\ntext = \"\"\"   <b>Communication</b>    \n    (from Latin communicare, meaning to share) \"\"\"\n# remove tags:\ncleaned = re.sub(\"<[^>]+>\", \"\", text)\n# normalize white space\ncleaned = re.sub(\"\\s+\", \" \", cleaned)\n# trim spaces from start and end\ncleaned = re.sub(\"^\\s+|\\s+$\", \"\", cleaned)\ncleaned = cleaned.strip()\n\nprint(cleaned)\n\nCommunication (from Latin communicare, meaning to share)\n\n\n\n\n\ntext = \"    <b>Communication</b>    \n     (from Latin communicare, meaning to share)  \"\ncleaned = text %>% \n  # remove HTML tags:\n  str_replace_all(\"<[^>]+>\", \" \")  %>% \n  # normalize white space \n  str_replace_all(\"\\\\p{space}+\", \" \")  %>% \n  # trim spaces at start and end\n  str_remove_all(\"^\\\\s+|\\\\s+$\")\n\ncleaned\n\n[1] \"Communication (from Latin communicare, meaning to share)\"\n\n\n\n\n\n\n\n\n\nFinally, Example 9.4 shows how you can run the various commands on a whole column of text rather than on individual strings, using a small set of made-up tweets to showcase various operations. First, we determine whether a pattern occurs, in this case for detecting hashtags. This is very useful for e.g. subsetting a data frame to only rows that contain this pattern. Next, we count how many at-mentions are contained in the text, where we require that the character before the mention needs to be either whitespace or the start of the string (^), to exclude email addresses and other non-mentions that do contain at signs. Then, we extract the (first) url found in the text, if any, using the pattern discussed above. Finally, we extract the plain text of the tweet in two chained operations: first, we remove every word starting with an at-sign, hash, or http, removing everything up to the next whitespace character. Then, we replace everything that is not a letter by a single space.\n\n\n\n\n\n\n\nExample 9.4 Using regular expressions on a data frame\n\nPython codeR code\n\n\n\n\n\n\nurl = \"https://cssbook.net/d/example_tweets.csv\"\ntweets = pd.read_csv(url, index_col=\"id\")\n# identify tweets with hashtags\ntweets[\"tag\"] = tweets.text.str.contains(r\"#\\w+\")\n# How many at-mentions are there?\ntweets[\"at\"] = tweets.text.str.count(r\"(^|\\s)@\\w+\")\n# Extract first url\ntweets[\"url\"] = tweets.text.str.extract(r\"(https?://\\S+)\")\n# Remove urls, tags, and @-mentions\nexpr = r\"(^|\\s)(@|#|https?://)\\S+\"\ntweets[\"plain2\"] = tweets.text.str.replace(expr, \" \", regex=True).replace(\n    r\"\\W+\", \" \"\n)\ntweets\n\n                                       text  ...                   plain2\nid                                           ...                         \n1   RT: @john_doe https://example.com/ne...  ...  RT:   very interesting!\n2                      tweet with just text  ...     tweet with just text\n3   http://example.com/pandas #breaking ...  ...                         \n4               @me and @myself #selfietime  ...                    and  \n\n[4 rows x 5 columns]\n\n\n\n\n\nlibrary(tidyverse)\nurl=\"https://cssbook.net/d/example_tweets.csv\"\ntweets = read_csv(url)\ntweets = tweets %>% mutate(\n    # identify tweets with hashtags\n    has_tag=str_detect(text, \"#\\\\w+\"),\n    # How many at-mentions are there?\n    n_at = str_count(text, \"(^|\\\\s)@\\\\w+\"),\n    # Extract first url\n    url = str_extract(text, \"(https?://\\\\S+)\"),\n    # Remove at-mentions, tags, and urls\n    plain2 = str_replace_all(text, \n       \"(^|\\\\s)(@|#|https?://)\\\\S+\", \" \") %>% \n             str_replace_all(\"\\\\W+\", \" \")\n    )\ntweets\n\n# A tibble: 4 × 6\n     id text                                          has_tag  n_at url   plain2\n  <dbl> <chr>                                         <lgl>   <int> <chr> <chr> \n1     1 RT: @john_doe https://example.com/news very … FALSE       1 http… \"RT v…\n2     2 tweet with just text                          FALSE       0 <NA>  \"twee…\n3     3 http://example.com/pandas #breaking #mustread TRUE        0 http… \" \"   \n4     4 @me and @myself #selfietime                   TRUE        2 <NA>  \" and…\n\n\n\n\n\n\n\n\n\n\n9.3.1 Splitting and Joining Strings, and Extracting Multiple Matches\nSo far, the operations we used all took a single string object and returned a single value, either a cleaned version of the string or e.g. a boolean indicating whether there is a match. This is convenient when using data frames, as you can transform a single column into another column. There are three common operations, however, that complicate matters: you can split a string into multiple substrings, or extract multiple matches from a string, and you can join multiple matches together.\n\n\n\n\n\n\n\nExample 9.5 Splitting extracting and joining a single text\n\nPython codeR code\n\n\n\ntext = \"apples, pears, oranges\"\n# Three ways to achieve the same thing:\nitems = text.split(\", \")\nitems = regex.split(r\"\\p{PUNCTUATION}\\s*\", text)\nitems = regex.findall(r\"\\p{LETTER}+\", text)\nprint(f\"Split text into items: {items}\")\n\nSplit text into items: ['apples', 'pears', 'oranges']\n\njoined = \" & \".join(items)\nprint(joined)\n\napples & pears & oranges\n\n\n\n\n\ntext = \"apples, pears, oranges\"\nitems=strsplit(text,\", \", fixed=T)[[1]]\nitems=str_split(text,\"\\\\p{PUNCTUATION}\\\\s*\")[[1]]\nitems=str_extract_all(text,\"\\\\p{LETTER}+\")[[1]]\nprint(items)\n\n[1] \"apples\"  \"pears\"   \"oranges\"\n\njoined = str_c(items, collapse=\" & \")\nprint(joined)\n\n[1] \"apples & pears & oranges\"\n\n\n\n\n\n\n\n\n\nExample 9.5 shows the “easier” case of splitting up a single text and joining the result back together. We show three different ways to split: using a fixed pattern to split on (in this case, a comma plus space); using a regular expression (in this case, any punctuation followed by any space); and by matching the items we are interested in (letters) rather than the separator. Finally, we join these items together again using join (Python) and str_c (R).\nOne thing to note in the previous example is the use of the index [[1]] in R to select the first element in a list. This is needed because in R, splitting a text actually splits all the given texts, returning a list containing all the matches for each input text. If there is only a single input text, it still returns a list, so we select the first element of the list.\nIn many cases, however, you are not working on a single text but rather on a series of texts loaded into a data frame, from tweets to news articles and open survey questions. In the example above, we extracted only the first url from each tweet. If we want to extract e.g. all hash tags from each tweet, we cannot simply add a “tags” column, as there can be multiple tags in each tweet. Essentially, the problem is that the URLs per tweet are now nested in each row, creating a non-rectangular data structure.\nAlthough there are multiple ways of dealing with this, if you are working with data frames our advice is to normalize the data structure to a long format. In the example, that would mean that each tweet is now represented by multiple rows, namely one for each hash tag. Example 9.6 shows how this can be achieved in both R and Pandas. One thing to note is that in pandas, t.str.extractall automatically returns the desired long format, but it is essential that the index of the data frame actually contains the identifier (in this case, the tweet (status) id). t.str.split, however, returns a data frame with a column containing lists, similar to how both R functions return a list containing character vectors. We can normalize this to a long data frame using t.explode (pandas) and pivot_longer (R). After this, we can use all regular data frame operations, for example to join and summarize the data.\nA final thing to note is that while you normally use a function like mean to summarize the values in a group, you can also join strings together as a summarization. The only requirement for a summarization function is that it returns a single value for a group of values, which of course is exactly what joining a multiple string together does. This is shown in the final line of the example, where we split a tweet into words and then reconstruct the tweet from the individual words.\n\n\n\n\n\n\n\nExample 9.6 Applying split and extract _ all on text columns\n\nPython codeR code\n\n\n\ntags = tweets.text.str.extractall(\"(#\\\\w+)\")\ntags.merge(tweets, left_on=\"id\", right_on=\"id\")\n\n              0  ...   plain2\nid               ...         \n3     #breaking  ...         \n3     #mustread  ...         \n4   #selfietime  ...    and  \n\n[3 rows x 6 columns]\n\n\n\n\n\ntags = tweets %>% mutate(\n    tag=str_extract_all(tweets$text,\"(#\\\\w+)\"))%>%\n  select(id, tag)\ntags_long = tags  %>% unnest(tag)\nleft_join(tags_long, tweets)\n\n# A tibble: 3 × 7\n     id tag         text                              has_tag  n_at url   plain2\n  <dbl> <chr>       <chr>                             <lgl>   <int> <chr> <chr> \n1     3 #breaking   http://example.com/pandas #break… TRUE        0 http… \" \"   \n2     3 #mustread   http://example.com/pandas #break… TRUE        0 http… \" \"   \n3     4 #selfietime @me and @myself #selfietime       TRUE        2 <NA>  \" and…\n\n\n\n\n\n\nPython codeR code\n\n\n\nwords = tweets.text.str.split(\"\\\\W+\")\nwords_long = words.explode()\n\n\n\n\nwords = tweets %>% mutate(\n    word=str_split(tweets$text, \"\\\\W+\")) %>% \n  select(id, word)\nwords_long = words %>% unnest(word)\nhead(words_long)\n\n# A tibble: 6 × 2\n     id word    \n  <dbl> <chr>   \n1     1 RT      \n2     1 john_doe\n3     1 https   \n4     1 example \n5     1 com     \n6     1 news    \n\n\n\n\n\n\nPython codeR code\n\n\n\nwords_long.groupby(\"id\").agg(\"_\".join)\n\nid\n1    RT_john_doe_https_example_com_news_v...\n2                       tweet_with_just_text\n3    http_example_com_pandas_breaking_mus...\n4                  _me_and_myself_selfietime\nName: text, dtype: object\n\n\n\n\n\nwords_long %>% \n  group_by(id) %>% \n  summarize(joined=str_c(word, collapse=\"_\"))\n\n# A tibble: 4 × 2\n     id joined                                              \n  <dbl> <chr>                                               \n1     1 RT_john_doe_https_example_com_news_very_interesting_\n2     2 tweet_with_just_text                                \n3     3 http_example_com_pandas_breaking_mustread           \n4     4 _me_and_myself_selfietime"
  },
  {
    "objectID": "content/chapter10.html#sec-dtm",
    "href": "content/chapter10.html#sec-dtm",
    "title": "10  Text as data",
    "section": "10.1 The Bag of Words and the Term-Document Matrix",
    "text": "10.1 The Bag of Words and the Term-Document Matrix\nBefore you can conduct any computational analysis of text, you need to solve a problem: computations are usually done on numerical data – but you have text. Hence, you must find a way to represent the text by numbers. The document-term matrix (DTM, also called the term-document matrix or TDM) is one common numerical representation of text. It represents a corpus (or set of documents) as a matrix or table, where each row represents a document, each column represents a term (word), and the numbers in each cell show how often that word occurs in that document.\n\n\n\n\n\n\n\nExample 10.1 Example document-term matrix\n\nPython codeR code\n\n\n\ntexts = [\n    \"The caged bird sings with a fearful trill\",\n    \"for the caged bird sings of freedom\",\n]\ncv = CountVectorizer()\nd = cv.fit_transform(texts)\n# Create a dataframe of the word counts to inspect\n# - todense transforms the dtm into a dense matrix\n# - get_feature_names() gives a list words\npd.DataFrame(d.todense(), columns=cv.get_feature_names())\n\n   bird  caged  fearful  for  freedom  of  sings  the  trill  with\n0     1      1        1    0        0   0      1    1      1     1\n1     1      1        0    1        1   1      1    1      0     0\n\n\n\n\n\ntexts = c(\n    \"The caged bird sings with a fearful trill\", \n    \"for the caged bird sings of freedom\")\nd = tokens(texts) %>% dfm()\n# Inspect by converting to a (dense) matrix\nconvert(d, \"matrix\") \n\n       features\ndocs    the caged bird sings with a fearful trill for of freedom\n  text1   1     1    1     1    1 1       1     1   0  0       0\n  text2   1     1    1     1    0 0       0     0   1  1       1\n\n\n\n\n\n\n\n\n\nAs an example, Example 10.1 shows a DTM made from two lines from the famous poem by Mary Angelou. The resulting matrix has two rows, one for each line; and 11 columns, one for each unique term (word). In the columns you see the document frequencies of each term: the word “bird” occurs once in each line, but the word “with” occurs only in the first line (text1) and not in the second (text2).\nIn R, you can use the dfm function from the quanteda package (Benoit et al. 2018). This function can take a vector or column of texts and transforms it directly into a DTM (which quanteda actually calls a document-feature matrix, hence the function name dfm). In Python, you achieve the same by creating an object of the CountVectorizer class, which has a fit_transform function.\n\n10.1.1 Tokenization\nIn order to turn a corpus into a matrix, each text needs to be tokenized, meaning that it must be split into a list (vector) of words. This seems trivial, as English (and most western) text generally uses spaces to demarcate words. However, even for English there are a number of edge cases. For example, should “haven’t” be seen as a single word, or two?\n\n\n\n\n\n\n\nExample 10.2 Differences between tokenizers\n\nPython codeR code\n\n\n\ntext = \"I haven't seen John's derring-do\"\ntokenizer = CountVectorizer().build_tokenizer()\nprint(tokenizer(text))\n\n['haven', 'seen', 'John', 'derring', 'do']\n\n\n\n\n\ntext = \"I haven't seen John's derring-do\"\ntokens(text)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"I\"          \"haven't\"    \"seen\"       \"John's\"     \"derring-do\"\n\n\n\n\n\n\n\n\n\nExample 10.2 shows how Python and R deal with the sentence “I haven’t seen John’s derring-do”. For Python, we first use CountVectorizer.build_tokenizer to access the built-in tokenizer. As you can see in the first line of input, this tokenizes “haven’t” to haven, which of course has a radically different meaning. Moreover, it silently drops all single-letter words, including the 't, 's, and I.\nIn the box “Tokenizing in Python” below, we therefore discuss some alternatives. For instance, the TreebankWordTokenizer included in the nltk package is a more reasonable tokenizer and splits “haven’t” into have and n't, which is a reasonable outcome. Unfortunately, this tokenizer assumes that text has already been split into sentences, and it also includes punctuation as tokens by default. To circumvent this, we can introduce a custom tokenizer based on the Treebank tokenizer, which splits text into sentences (using nltk.sent_tokenize) – see the box for more details.\nFor R, we simply call the tokens function from the quanteda package. This keeps haven't and John's as a single word, which is probably less desirable than splitting the words but at least better than outputting the word haven.\nAs this simple example shows, even a relatively simple sentence is tokenized differently by the tokenizers considered here (and see the box on tokenization in Python). Depending on the research question, these differences might or might not be important. However, it is always a good idea to check the output of this (and other) preprocessing steps so you understand what information is kept or discarded.\n\n\n\n\n\n\nTokenization in Python\n\n\n\n\n\nAs you can see in the example, the built-in tokenizer in scikit-learnis not actually very good. For example, haven’t is tokenized to haven, which is an entirely different word. Fortunately, there are other tokenizers in the nltk.tokenize package that do better.\nFor example, the TreebankTokenizer uses the tokenization rules for the Penn Treebank to tokenize, which produces better results:\n\ntext = \"\"\"I haven't seen John's derring-do. \n          Second sentence!\"\"\"\nprint(TreebankWordTokenizer().tokenize(text))\n\n['I', 'have', \"n't\", 'seen', 'John', \"'s\", 'derring-do.', 'Second', 'sentence', '!']\n\n\nAnother example is the WhitespaceTokenizer, which simply uses whitespace to tokenize, which can be useful if your input has already been tokenized, and is used in Example 10.11 below for tweets to conserve hash tags.\n\nprint(WhitespaceTokenizer().tokenize(text))\n\n['I', \"haven't\", 'seen', \"John's\", 'derring-do.', 'Second', 'sentence!']\n\n\nYou can also write your own tokenizer if needed. For example, the TreebankTokenizer assumes that text has already been split into sentences (which is why the period is attached to the word derring-do.). The code below shows how we can make our own tokenizer class, which uses nltk.sent_tokenize to first split the text into sentences, and then uses the TreebankTokenizer to tokenize each sentence, keeping only tokens that include at least one letter character. Although a bit more complicated, this approach can give you maximum flexibility.\n\nnltk.download(\"punkt\")\n\n\nclass MyTokenizer:\n    def tokenize(self, text):\n        tokenizer = TreebankWordTokenizer()\n        result = []\n        word = r\"\\p{letter}\"\n        for sent in nltk.sent_tokenize(text):\n            tokens = tokenizer.tokenize(sent)\n            tokens = [t for t in tokens if regex.search(word, t)]\n            result += tokens\n        return result\n\nmytokenizer = MyTokenizer()\nprint(mytokenizer.tokenize(text))\n\n['I', 'have', \"n't\", 'seen', 'John', \"'s\", 'derring-do', 'Second', 'sentence']\n\n\n\n\n\n\n\n\n\n\n\n\nExample 10.3 Tokenization of Japanese verse.\n\nPython codeR code\n\n\n\n# this snippet uses the tokenizer created above\n# (example \"Tokenization with Python\")\nhaiku = (\"\\u53e4\\u6c60\\u86d9\"\n         \"\\u98db\\u3073\\u8fbc\\u3080\"\n         \"\\u6c34\\u306e\\u97f3\")\nprint(f\"Default: {mytokenizer.tokenize(haiku)}\")\n\nDefault: ['古池蛙飛び込む水の音']\n\nprint(f\"Nagisa: {nagisa.tagging(haiku).words}\")\n\nNagisa: ['古', '池蛙', '飛び込む', '水', 'の', '音']\n\n\n\n\n\nhaiku = \"\\u53e4\\u6c60\\u86d9\n         \\u98db\\u3073\\u8fbc\\u3080\n         \\u6c34\\u306e\\u97f3\"\ntokens(haiku)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"古池\"     \"蛙\"       \"飛び込む\" \"水\"       \"の\"       \"音\"      \n\n\n\n\n\n\n\n\n\n\n\n\nNote that for languages such as Chinese, Japanese, and Korean, which do not use spaces to delimit words, the story is more difficult. Although a full treatment is beyond the scope of this book, Example 10.3 shows a small example of tokenizing Japanese text, in this case the famous haiku “the sound of water” by Bashō. The default tokenizer in quanteda actually does a good job, in contrast to the default Python tokenizer that simply keeps the whole string as one word (which makes sense since this tokenizer only looks for whitespace or punctuation). For Python the best bet is to use a custom package for tokenizing Japanese, such as the nagisa package. This package contains a tokenizer which is able to tokenize the Japanese text, and we could use this in the CountVectorizer much like we used the TreebankWordTokenizer for English earlier. Similarly, with heavily inflected languages such as Hungarian or Arabic, it might be better to use preprocessing tools developed specifically for these languages, but treating those is beyond the scope of this book.\n\n\n10.1.2 The DTM as a Sparse Matrix\n\n\n\n\n\n\n\nExample 10.4 Example document-term matrix\n\nPython codeR code\n\n\n\n# this snippet uses the tokenizer created above\n# (example \"Tokenization with Python\")\nsotu = pd.read_csv(\"https://cssbook.net/d/sotu.csv\")\ncv = CountVectorizer(tokenizer=mytokenizer.tokenize)\nd = cv.fit_transform(sotu[\"text\"])\nd\n\n<85x17185 sparse matrix of type '<class 'numpy.int64'>'\n    with 132900 stored elements in Compressed Sparse Row format>\n\n\n\n\n\nurl = \"https://cssbook.net/d/sotu.csv\"\nsotu = read_csv(url) %>% \n       mutate(doc_id=paste(lubridate::year(Date), \n                           President, delivery))\nd = corpus(sotu) %>% tokens() %>% dfm()\nd\n\nDocument-feature matrix of: 85 documents, 18,165 features (91.07% sparse) and 6 docvars.\n                        features\ndocs                      to  the congress  :  in considering state   of union\n  1945 Roosevelt written 247  642       14  6 236           1     5  376     2\n  1945 Roosevelt spoken  110  238        8  3  90           0     1  137     0\n  1946 Truman written    738 2141       74 17 669           4    24 1264     8\n  1947 Truman spoken     227  473       27  7 132           1     5  292     7\n  1948 Truman spoken     175  325       15  2  98           0     7  252     5\n  1949 Truman spoken     139  239       17  2  69           1     1  150     1\n                        features\ndocs                        ,\n  1945 Roosevelt written  351\n  1945 Roosevelt spoken   139\n  1946 Truman written    1042\n  1947 Truman spoken      236\n  1948 Truman spoken      155\n  1949 Truman spoken      148\n[ reached max_ndoc ... 79 more documents, reached max_nfeat ... 18,155 more features ]\n\n\n\n\n\n\n\n\n\nExample 10.4 shows a more realistic example. It downloads all US “State of the Union” speeches and creates a document-term matrix from them. Since the matrix is now easily too large to print, both Python and R simply list the size of the matrix. R lists \\(85\\) documents (rows) and \\(17999\\) features (columns), and Python reports that its size is \\(85\\times17185\\). Note the difference in the number of columns (unique terms) due to the differences in tokenization as discussed above.\n\n\n\n\n\n\n\nExample 10.5 A look inside the DTM.\n\nPython codeR code\n\n\n\ndef termstats(dfm, vectorizer):\n    \"\"\"Helper function to calculate term and\n    document frequency per term\"\"\"\n    # Frequencies are the column sums of the DFM\n    frequencies = dfm.sum(axis=0).tolist()[0]\n    # Document frequencies are the binned count\n    # of the column indices of DFM entries\n    docfreqs = np.bincount(dfm.indices)\n    freq_df = pd.DataFrame(\n        dict(frequency=frequencies, docfreq=docfreqs),\n        index=vectorizer.get_feature_names(),\n    )\n    return freq_df.sort_values(\"frequency\", ascending=False)\n\n\ntermstats(d, cv).iloc[[0, 10, 100, 1000, 10000]]\n\n            frequency  docfreq\nthe             34996       85\nis               5472       85\nenergy            707       68\nscientific         73       28\nescalate            2        2\n\nwords = [\"the\", \"is\", \"energy\", \"scientific\", \"escalate\"]\nindices = [cv.vocabulary_[x] for x in words]\n\nd[[[0], [25], [50], [75]], indices].todense()\n\nmatrix([[642,  78,   0,   0,   0],\n        [355,  66,   1,   0,   0],\n        [182,  45,   2,   0,   0],\n        [326,  59,  15,   1,   0]])\n\n\n\n\n\ntextstat_frequency(d)[c(1, 10, 100, 1000, 10000), ]\n\n                feature frequency rank docfreq group\n1                   the     34999    1      85   all\n10                  our      9334   10      85   all\n100               first       750  100      83   all\n1000        investments        76  988      34   all\n10000 service-connected         2 8696       2   all\n\nas.matrix(d[\n  c(3, 25, 50, 75),\n  c(\"the\",\"first\",\"investment\",\"defrauded\")])\n\n                     features\ndocs                   the first investment defrauded\n  1946 Truman written 2141    21          9         0\n  1965 Johnson spoken  283    14          0         0\n  1984 Reagan spoken   209     8          1         0\n  2009 Obama spoken    269     8          4         0\n\n\n\n\n\n\n\n\n\nIn Example 10.5 we show how you can look at the content of the DTM. First, we show the overall term and document frequencies of each word, where we showcase words at different frequencies. Unsurprisingly, the word the tops both charts, but further down there are minor differences. In all cases, the highly frequent words are mostly functional words like them or first. More informative words such as investments are by their nature used much less often. Such term statistics are very useful to check for noise in the data and get a feeling of the kind of language that is used. Second, we take a look at the frequency of these same words in four speeches from Truman to Obama. All use words like the and first, but none of them talk about defrauded – which is not surprising, since it was only used once in all the speeches in the corpus.\nHowever, the words that ranked around 1000 in the top frequency are still used in less than half of the documents. Since there are about 17000 even less frequent words in the corpus, you can imagine that most of the document-term matrix consists of zeros. The output also noted this sparsity in the first output above. In fact, R reports that the dtm is \\(91\\%\\) sparse, meaning 91% percent of all entries are zero. Python reports a similar figure, namely that there are only just under 150000 non-zero entries out of a possible \\(8\\times22219\\), which boils down to a 92% sparse matrix.\nNote that to display the matrix we turned it from a sparse matrix representation into a dense matrix. Briefly put, in a dense matrix, all entries are stored as a long list of numbers, including all the zeros. In a sparse matrix, only the non-zero entries and their location are stored. This conversion (using the function as.matrix and the method todense respectively), however, was only performed after selecting a small subset of the data. In general, it is very inefficient to store and work with the matrix in a dense format. For a reasonably large corpus with tens of thousands of documents and different words, this can quickly run to billions of numbers, which can cause problems even on modern computers and is, moreover, very inefficient. Because sparsity values are often higher than 99%, using a sparse matrix representation can easily reduce storage requirements by a hundred times, and in the process speed up calculations by reducing the number of entries that need to be inspected. Both quanteda and scikit-learnstore DTMs as sparse matrices by default, and most analysis tools are able to deal with sparse matrices very efficiently (see, however, Section 11.4.1 for problems with machine learning on sparse matrices in R).\nA final note on the difference between Python and R in this example. The code in R is much simpler and produces nicer results since it also shows the words and the speech names. In Python, we wrote our own helper function to create the frequency statistics which is built into the R quanteda package. These differences between Python and R reflect a pattern that is true in many (but not all) cases: in Python libraries such as numpyand scikit-learnare setup to maximize performance, while in R a library such as quanteda or tidyverse is more geared towards ease of use. For that reason, the DTM in Python does not “remember” the actual words, it uses the index of each word, so it consumes less memory if you don’t need to use the actual words in e.g. a machine learning setup. R, on the other hand, stores the words and also the document IDs and metadata in the DFM object. This is easier to use if you need to look up a word or document, but it consumes (slightly) more memory.\n\n\n\n\n\n\nNote\n\n\n\n\n\nPython: Why fit_transform? In Python, you don’t have a function that directly transforms text into a DTM. Instead, you create an transformer called a CountVectorizer, which can then be used to “vectorize” texts (turn it into a row of numbers) by counting how often each word occurs. This uses the fit_transform function which is offered by all scikit-learntransformers. It “fits” the model on the training data, which in this case means learning the vocabulary. It can then be used to transform other data into a DTM with the exact same columns, which is often required for algorithms. Because the feature names (the words themselves) are stored in the CountVectorizer rather than the document-term matrix, you generally need to keep both objects.\n\n\n\n\n\n10.1.3 The DTM as a “Bag of Words”\nAs you can see already in these simple examples, the document-term matrix discards quite a lot of information from text. Specifically, it disregards the order or words in a text: “John fired Mary” and “Mary fired John” both result in the same DTM, even though the meaning of the sentences is quite different. For this reason, a DTM is often called a bag of words, in the sense that all words in the document are simply put in a big bag without looking at the sentences or context of these words.\nThus, the DTM can be said to be a specific and “lossy” representation of the text, that turns out to be quite useful for certain tasks: the frequent occurrence of words like “employment”, “great”, or “I” might well be good indicators that a text is about the economy, is positive, or contains personal expressions respectively. As we will see in the next chapter, the DTM representation can be used for many different text analyses, from dictionaries to supervised and unsupervised machine learning.\nSometimes, however, you need information that is encoded in the order of words. For example, in analyzing conflict coverage it might be quite important to know who attacks whom, not just that an attack took place. In the Section 10.3 we will look at some ways to create a richer matrix-representation by using word pairs. Although it is beyond the scope of this book, you can also use automatic syntactic analysis to take grammatical relations into account as well. As is always the case with automatic analyses, it is important to understand what information the computer is looking at, as the computer cannot find patterns in information that it doesn’t have.\n\n\n10.1.4 The (Unavoidable) Word Cloud\nOne of the most famous text visualizations is without doubt the word cloud. Essentially, a word cloud is an image where each word is displayed in a size that is representative of its frequency. Depending on preference, word position and color can be random, depending on word frequency, or in a decorative shape.\nWord clouds are often criticized since they are (sometimes) pretty but mostly not very informative. The core reason for that is that only a single aspect of the words is visualized (frequency), and simple word frequency is often not that informative: the most frequent words are generally uninformative “stop words” like “the” and “I”.\nFor example, Example 10.6 shows the word cloud for the state of the union speeches downloaded above. In R, this is done using the quanteda function textplot_wordcloud. In Python we need to work a little harder, since it only has the counts, not the actual words. So, we sum the DTM columns to get the frequency of each word, and combine that with the feature names (words) from the CountVectorized object cv. Then we can create the word cloud and give it the frequencies to use. Finally, we plot the cloud and remove the axes.\n\n\n\n\n\n\n\nExample 10.6 Word cloud of the US State of the Union corpus\n\nPython codeR code\n\n\n\ndef wordcloud(dfm, vectorizer, **options):\n    freq_dict = dict(\n        zip(vectorizer.get_feature_names(), dfm.sum(axis=0).tolist()[0])\n    )\n    wc = WordCloud(**options)\n    return wc.generate_from_frequencies(freq_dict)\n\nwc = wordcloud(d, cv, background_color=\"white\")\nplt.imshow(wc)\nplt.axis(\"off\")\n\n\n\n\ntextplot_wordcloud(d, max_words=200)\n\n\n\n\n\n\n\n\n\n\n\nThe results from Python and R look different at first – for one thing, R is nice and round but Python has more colors! However, if you look at the cloud you can see both are not very meaningful: the largest words are all punctuation or words like “a”, “and”, or “the”. You have to look closely to find words like “federal” or “security” that give a hint on what the texts were actually about."
  },
  {
    "objectID": "content/chapter10.html#sec-dtmselect",
    "href": "content/chapter10.html#sec-dtmselect",
    "title": "10  Text as data",
    "section": "10.2 Weighting and Selecting Documents and Terms",
    "text": "10.2 Weighting and Selecting Documents and Terms\nSo far, the DTMs you made in this chapter simply show the count of each word in each document. Many words, however, are not informative for many questions. This is especially apparent if you look at a word cloud, essentially a plot of the most frequent words in a corpus (set of documents).\n\n\n\n\n\n\nVectors and a geometric interpretation of document-term matrices\n\n\n\n\n\nWe said that a document is represented by a “vector” of numbers, where each number (for a document-term matrix) is the frequency of a specific word in that document. This term is also seen in the name for the tokenizer scikit-learn: a vectorizer or function to turn texts into vectors.\nThe term vector here can be read as just a fancy word for a group of numbers. In this meaning, the term is also often used in R, where a column of a data frame is called a vector, and where functions that can be called on a whole vector at once are called vectorized.\nMore generally, however, a vector in geometry is a point (or line from the origin) in an \\(n\\)-dimensional space, where \\(n\\) is the length or dimensionality of the vector. This is also a very useful interpretation for vectors in text analysis: the dimensionality of the space is the number of unique words (columns) in the document-term matrix, and each document is a point in that \\(n\\)-dimensional space.\nIn that interpretation, various geometric distances between documents can be calculated as an indicator for how similar two documents are. Techniques that reduce the number of columns in the matrix (such as clustering or topic modeling) can then be seen as dimensionality reduction techniques since they turn the DTM into a matrix with lower dimensionality (while hopefully retaining as much of the relevant information as possible).\n\n\n\nMore formally, a document-term matrix can be seen as a representation of data points about documents: each document (row) is represented as a vector containing the count per word (column). Although it is a simplification compared to the original text, an unfiltered document-term matrix contains a lot of relevant information. For example, if a president uses the word “terrorism” more often than the word “economy”, that could be an indication of their policy priorities.\nHowever, there is also a lot of noise crowding out this signal: as seen in the word cloud in the previous section the most frequent words are generally quite uninformative. The same holds for words that hardly occur in any document (but still require a column to be represented) and noisy “words” such as punctuation or technical artifacts like HTML code.\nThis section will discuss a number of techniques for cleaning a corpus or document-term matrix in order to minimize the amount of noise: removing stop words, cleaning punctuation and other artifacts, and trimming and weighting. As a running example in this section, we will use a collection of tweets from US president Donald Trump. Example 10.7 shows how to load these tweets into a data frame containing the ID and text of the tweets. As you can see, this dataset contains a lot of non-textual features such as hyperlinks and hash tags as well as regular punctuation and stop words. Before we can start analyzing this data, we need to decide on and perform multiple cleaning steps such as detailed below.\n\n\n\n\n\n\n\nExample 10.7 Top words used in Trump Tweets\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/trumptweets.csv\"\ntweets = pd.read_csv(url, usecols=[\"status_id\", \"text\"], index_col=\"status_id\")\ntweets.head()\n\n                                                                text\nstatus_id                                                           \nx1864367186        Read a great interview with Donald Trump that ...\nx9273573134835712  Congratulations to Evan Lysacek for being nomi...\nx29014512646       I was on The View this morning. We talked abou...\nx7483813542232064  Tomorrow night's episode of The Apprentice del...\nx5775731054        Donald Trump Partners with TV1 on New Reality ...\n\n\n\n\n\nurl = \"https://cssbook.net/d/trumptweets.csv\"\ntweets = read_csv(url, \n    col_types=cols_only(text=\"c\", status_id=\"c\")) \nhead(tweets)\n\n# A tibble: 6 × 2\n  status_id          text                                                       \n  <chr>              <chr>                                                      \n1 x1864367186        Read a great interview with Donald Trump that appeared in …\n2 x9273573134835712  Congratulations to Evan Lysacek for being nominated SI spo…\n3 x29014512646       I was on The View this morning. We talked about The Appren…\n4 x7483813542232064  Tomorrow night's episode of The Apprentice delivers excite…\n5 x5775731054        Donald Trump Partners with TV1 on New Reality Series Entit…\n6 x14785576859340800 I'll be appearing on Larry King Live for his final show, T…\n\n\n\n\n\n\n\n\n\nPlease note that although tweets are perhaps overused as a source of scientific information, we use them here because they nicely exemplify issues around non-textual elements such as hyperlinks. See Chapter 12 for information on how to use the Twitter and other APIs to collect your own data.\n\n10.2.1 Removing stopwords\nA first step in cleaning a DTM is often stop word removal. Words such as “a” and “the” are often called stop words, i.e. words that do not tell us much about the content. Both quanteda and scikit-learninclude built-in lists of stop words, making it very easy to remove the most common words. Example 10.8 shows the result of specifying “English” stop words to be removed for both packages.\n\n\n\n\n\n\n\nExample 10.8 Simple stop word removal\n\nPython codeR code\n\n\n\ncv = CountVectorizer(\n    stop_words=stopwords.words(\"english\"), tokenizer=mytokenizer.tokenize\n)\nd = cv.fit_transform(tweets.text)\nwc = wordcloud(d, cv, background_color=\"white\")\nplt.imshow(wc)\nplt.axis(\"off\")\n\n\n\n\nd = corpus(tweets) %>% \n  tokens(remove_punct=T) %>% \n  dfm() %>%\n  dfm_remove(stopwords(\"english\"))\ntextplot_wordcloud(d, max_words=100)\n\n\n\n\n\n\n\n\n\n\n\nNote, however, that it might seem easy to list words like “a” and “and”, but as it turns out there is no single well-defined list of stop words, and (as always) the best choice depends on your data and your research question.\nLinguistically, stop words are generally function words or closed word classes such as determiner or pronoun, with closed classes meaning that while you can coin new nouns, you can’t simply invent new determiners or prepositions. However, there are many different stop word lists around which make different choices and are compatible with different kinds of preprocessing. The Python word cloud in Example 10.8 shows a nice example of the importance of matching stopwords with the used tokenization: a central “word” in the cloud is the contraction ’s. We are using the NLTK tokenizer, which splits ’s from the word it was attached to, but the scikit-learnstop word list does not include that term. So, it is important to make sure that the words created by the tokenization match the way that words appear in the stop word list.\nAs an example of the substantive choices inherent in using a stop word lists, consider the word “will”. As an auxiliary verb, this is probably indeed a stop word: for most substantive questions, there is no difference whether you will do something or simply do it. However, “will” can also be a noun (a testament) and a name (e.g. Will Smith). Simply dropping such words from the corpus can be problematic; see Section 10.3.4 for ways of telling nouns and verbs apart for more fine-grained filtering.\nMoreover, some research questions might actually be interested in certain stop words. If you are interested in references to the future or specific modalities, the word might actually be a key indicator. Similarly, if you are studying self-expression on Internet forums, social identity theory, or populist rhetoric, words like “I”, “us” and “them” can actually be very informative.\nFor this reason, it is always a good idea to understand and inspect what stop word list you are using, and use a different one or customize it as needed (see also Nothman, Qin, and Yurchak 2018). Example 10.9 shows how you can inspect and customize stop word lists. For more details on which lists are available and what choices these lists make, see the package documentation for the stopwords package in Python (part of NLTK) and R (part of quanteda)\n\n\n\n\n\n\n\nExample 10.9 Inspecting and Customizing stop word lists\n\nPython codeR code\n\n\n\nmystopwords = [\"go\", \"to\"] + stopwords.words(\"english\")\nprint(f\"{len(mystopwords)} stopwords:\" f\"{', '.join(mystopwords[:5])}...\")\n\n181 stopwords:go, to, i, me, my...\n\n\n\n\n\nmystopwords = stopwords(\"english\", \n                        source=\"snowball\")\nmystopwords = c(\"go\", \"one\", mystopwords)\nglue(\"Now {length(mystopwords)} stopwords:\")\n\nNow 177 stopwords:\n\nmystopwords[1:5]\n\n[1] \"go\"  \"one\" \"i\"   \"me\"  \"my\" \n\n\n\n\n\n\n\n\n\n\n\n10.2.2 Removing Punctuation and Noise\nNext to stop words, text often contains punctuation and other things that can be considered “noise” for most research questions. For example, it could contain emoticons or emoji, Twitter hashtags or at-mentions, or HTML tags or other annotations.\nIn both Python and R, we can use regular expressions to remove (parts of) words. As explained above in Section 9.2, regular expressions are a powerful way to specify (sequences of) characters which are to be kept or removed. You can use this, for example, to remove things like punctuation, emoji, or HTML tags. This can be done either before or after tokenizing (splitting the text into words): in other words, we can clean the raw texts or the individual words (tokens).\nIn general, if you only want to keep or remove certain words, it is often easiest to do so after tokenization using a regular expression to select the words to keep or remove. If you want to remove parts of words (e.g. to remove the leading “#” in hashtags) it is easiest to do that before tokenization, that is, as a preprocessing step before the tokenization. Similarly, if you want to remove a term that would be split by the tokenization (such as hyperlinks), if can be better to remove them before the tokenization occurs.\nExample 10.10 shows how we can use regular expressions to remove noise in Python and R. For clarity, it shows the result of each step, it shows the result of each processing step on a single tweet that exemplifies many of the problems described above. To better understand the tokenization process, we print the tokens in that tweet separated by a vertical bar (|). As a first cleaning step, we will use a regular expression to remove hyperlinks and HTML entities like &amp; from the untokenized texts. Since both hyperlinks and HTML entities are split over multiple tokens, it would be hard to remove them after tokenization.\n\n\n\n\n\n\n\nExample 10.10 Cleaning a single tweet at the text and token level\n\nPython codeR code\n\n\n\nid = \"x263687274812813312\"\none_tweet = tweets.text.values[tweets.index == id][0]\nprint(f\"Raw:\\n{one_tweet}\")\n\nRaw:\nPart 1 of my @jimmyfallon interview discussing my $5M offer to Obama, ...\n\ntweet_tokens = mytokenizer.tokenize(one_tweet)\nprint(\"\\nTokenized:\")\n\n\nTokenized:\n\nprint(\" | \".join(tweet_tokens))\n\nPart | of | my | jimmyfallon | interview | discussing | my | 5M | offer | t...\n\none_tweet = re.sub(r\"\\bhttps?://\\S*|&\\w+;\", \"\", one_tweet)\ntweet_tokens = mytokenizer.tokenize(one_tweet)\nprint(\"After pre-processing:\")\n\nAfter pre-processing:\n\nprint(\" | \".join(tweet_tokens))\n\nPart | of | my | jimmyfallon | interview | discussing | my | 5M | offer | t...\n\ntweet_tokens = [\n    t.lower()\n    for t in tweet_tokens\n    if not (\n        t.lower() in stopwords.words(\"english\") or regex.match(r\"\\P{LETTER}\", t)\n    )\n]\nprint(\"After pruning tokens:\")\n\nAfter pruning tokens:\n\nprint(\" | \".join(tweet_tokens))\n\npart | jimmyfallon | interview | discussing | offer | obama | trump | tower...\n\n\n\n\n\nid=\"x263687274812813312\"\nsingle_tweet = tweets$text[tweets$status_id == id]\nprint(single_tweet)\n\n[1] \"Part 1 of my @jimmyfallon interview discussing my $5M offer to Obama, ...\"\n\ntweet_tokens = tokens(single_tweet)\nprint(\"After tokenizing:\")\n\n[1] \"After tokenizing:\"\n\nprint(paste(tweet_tokens, collapse=\" | \"))\n\n[1] \"Part | 1 | of | my | @jimmyfallon | interview | discussing | my | $ | ...\"\n\nsingle_tweet = single_tweet  %>% \n  str_remove_all(\"\\\\bhttps?://\\\\S*|&\\\\w+;\")\ntweet_tokens = tokens(single_tweet)\nprint(\"After pre-processing:\")\n\n[1] \"After pre-processing:\"\n\nprint(paste(tweet_tokens, collapse=\" | \"))\n\n[1] \"Part | 1 | of | my | @jimmyfallon | interview | discussing | my | $ | ...\"\n\ntweet_tokens = tweet_tokens %>%\n  tokens_tolower()  %>% \n  tokens_remove(stopwords(\"english\")) %>% \n  tokens_keep(\"^\\\\p{LETTER}\", valuetype=\"regex\")\nprint(\"After pruning:\")\n\n[1] \"After pruning:\"\n\nprint(paste(tweet_tokens, collapse=\" | \"))\n\n[1] \"part | interview | discussing | offer | obama | tower | atrium | tweet...\"\n\n\n\n\n\n\n\n\n\nRegular expressions are explained fully in Section 9.2, so we will keep the explanation short: the bar | splits the pattern in two parts, i.e. it will match if it finds either of the subpatterns. The first pattern looks for the literal text http, followed by an optional s and the sequence ://. Then, it takes all non-whitespace characters it finds, i.e. the pattern ends at the next whitespace or end of the text. The second pattern looks for an ampersand (&) followed by one or more letters (\\\\w+), followed by a semicolon (;). This matches HTML escapes like &amp; for an ampersand.\nIn the next step, we process the tokenized text to remove every token that is either a stopword or does not start with a letter. In Python, this is done by using a list comprehension ([process(item) for item in list]) for tokenizing each document; and a nested list comprehension for filtering each token in each document. In R this is not needed as the tokens_\\* functions are vectorized, that is, they directly run over all the tokens.\nComparing R and Python, we see that the different tokenization functions mean that #trump is removed in R (since it is a token that does not start with a letter), but in Python the tokenization splits the # from the name and the resulting token trump is kept. If we would have used a different tokenizer for Python (e.g. the WhitespaceTokenizer) this would have been different again. This underscores the importance of inspecting and understanding the results of the specific tokenizer used, and to make sure that subsequent steps match these tokenization choices. Concretely, with the TreebankWordtokenizer we would have had to also remove hashtags at the text level rather than the token level.\n\n\n\n\n\n\n\nExample 10.11 Cleaning the whole corpus and making a tag cloud\n\nPython codeR code\n\n\n\ndef do_nothing(x):\n    return x\n\ntokenized = [\n    WhitespaceTokenizer().tokenize(text) for text in tweets.text.values\n]\ntokens = [\n    [t.lower() for t in tokens if regex.match(\"#\", t)] for tokens in tokenized\n]\n\ncv = CountVectorizer(tokenizer=do_nothing, lowercase=False)\ndtm_emoji = cv.fit_transform(tokens)\nwc = wordcloud(dtm_emoji, cv, background_color=\"white\")\nplt.imshow(wc)\nplt.axis(\"off\")\n\n\n\n\ndfm_cleaned = tweets %>% \n  corpus() %>% \n  tokens()  %>% \n  tokens_keep(\"^#\", valuetype=\"regex\")  %>% \n  dfm()\ncolors = RColorBrewer::brewer.pal(8, \"Dark2\")\ntextplot_wordcloud(dfm_cleaned, max_words=100, \n    min_size = 1, max_size=4, random_order=TRUE,\n    random_color= TRUE, color=colors)\n\n\n\n\n\n\n\n\n\n\n\nAs a final example, Example 10.11 shows how to filter tokens for the whole corpus, but rather than removing hashtags it keeps only the hashtags to produce a tag cloud. In R, this is mostly a pipeline of quanteda functions to create the corpus, tokenize, keep only hashtags, and create a DFM. To spice up the output we use the RColorBrewer package to set random colors for the tags. In Python, you can see that we now have a nested list comprehension, where the outer loop iterates over the texts and the inner loop iterates over the tokens in each text. Next, we make a do_nothing function for the vectorizer since the results are already tokenized. Note that we need to disable lowercasing as otherwise it will try to call .lower() on the token lists.\n\n\n\n\n\n\nLambda functions in Python.\n\n\n\n\n\nSometimes, we need to define a function that is very simple and that we need only once. An example for such a throwaway function is do_nothing in Example 10.11. Instead of defining a reusable function with the def keyword and then to call it by its name when we need it later, we can therefore also directly define an unnamed function when we need it with the lambda keyword. The syntax is simple: lambda argument: returnvalue. A function that maps a value onto itself can therefore be written as lambda x: x. In Example 10.11, instead of defining a named function, we could therefore also simply write v = CountVectorizer(tokenizer=lambda x: x, lowercase=False). The advantages are that it saves you two lines of code here and you don’t clutter your environment with functions you do not intend to re-use anyway. The disadvantage is that it may be less clear what is happening, at least for people not familiar with lambda functions.\n\n\n\n\n\n10.2.3 Trimming a DTM\nThe techniques above both drop terms from the DTM based on specific choices or patterns. It can also be beneficial to trim a DTM by removing words that occur very infrequently or overly frequently. For the former, the reason is that if a word only occurs in a very small percentage of documents it is unlikely to be very informative. Overly frequent words, for example occurring in more than half or 75% of all documents, function basically like stopwords for this corpus. In many cases, this can be a result of the selection strategy. If we select all tweets containing “Trump”, the word Trump itself is no longer informative about their content. It can also be that some words are used as standard phrases, for example “fellow Americans” in state of the union speeches. If every president in the corpus uses those terms, they are no longer informative about differences between presidents.\n\n\n\n\n\n\n\nExample 10.12 Trimming a Document-Term Matrix\n\nPython codeR code\n\n\n\nprint(f\"# of words before trimming: {d.shape[1]}\")\n\n# of words before trimming: 45912\n\ncv_trim = CountVectorizer(\n    stop_words=stopwords.words(\"english\"),\n    tokenizer=mytokenizer.tokenize,\n    max_df=0.75,\n    min_df=0.005,\n)\nd_trim = cv_trim.fit_transform(tweets.text)\nprint(f\"  after trimming: {d_trim.shape[1]}\")\n\n  after trimming: 294\n\n\n\n\n\nglue(\"# of words before trimming: {ncol(d)}\")\n\n# of words before trimming: 44386\n\nd_trim = dfm_trim(d, min_docfreq = 0.005, \n                  max_docfreq = 0.75,\n                  docfreq_type = \"prop\")\nglue(\"# of word after trimming: {ncol(d_trim)}\")\n\n# of word after trimming: 301\n\n\n\n\n\n\n\n\n\nExample 10.12 shows how you can use the relative document frequency to trim a DTM in Python and R. We keep only words with a document frequency of between 0.5% and 75%.\nAlthough these are reasonable numbers every choice depends on the corpus and the research question, so it can be a good idea to check which words are dropped.\nNote that dropping words that occur almost never should normally not influence the results that much, since those words do not occur anyway. However, trimming a DTM to e.g. at least 1% document frequency often radically reduces the number of words (columns) in the DTM. Since many algorithms have to assign weights or parameters to each word, this can provide a significant improvement in computing speed or memory use.\n\n\n10.2.4 Weighting a DTM\nThe DTMs created above all use the raw frequencies as cell values. It can also be useful to weight the words so more informative words have a higher weight than less informative ones. A common technique for this is tf\\(\\cdot\\)idf weighting. This stands for term frequency \\(\\cdot\\) inverse document frequency and weights each occurrence by its raw frequency (term frequency) corrected for how often it occurs in all documents (inverse document frequency). In a formula, the most common implementation of this weight is given as follows:\n\\(tf\\cdot idf(t,d)=tf(t,d)\\cdot idf(t)=f_{t,d}\\cdot -\\log \\frac{n_t}{N}\\)\nWhere \\(f_{t,d}\\) is the frequency of term \\(t\\) in document \\(d\\), \\(N\\) is the total number of documents, and \\(n_t\\) is the number of documents in which term \\(t\\) occurs. In other words, the term frequency is weighted by the negative log of the fraction of documents in which that term occurs. Since \\(\\log(1)\\) is zero, terms that occur in every document are disregarded, and in general the less frequent a term is, the higher the weight will be.\n\n\n\n\n\n\n\nExample 10.13 Tf\\(\\cdot\\)Idf weighting\n\nPython codeR code\n\n\n\ntfidf_vectorizer = TfidfVectorizer(\n    tokenizer=mytokenizer.tokenize, sublinear_tf=True\n)\nd_w = tfidf_vectorizer.fit_transform(sotu[\"text\"])\nindices = [\n    tfidf_vectorizer.vocabulary_[x]\n    for x in [\"the\", \"for\", \"them\", \"submit\", \"sizes\"]\n]\nd_w[[[0], [25], [50], [75]], indices].todense()\n\nmatrix([[0.04443888, 0.03107326, 0.01989122, 0.        , 0.        ],\n        [0.05722359, 0.04138723, 0.02782201, 0.01429464, 0.        ],\n        [0.0548183 , 0.04440376, 0.02332654, 0.        , 0.        ],\n        [0.04866919, 0.03859479, 0.02234111, 0.        , 0.        ]])\n\n\n\n\n\nd_tf = corpus(sotu) %>% \n  tokens() %>% \n  dfm() %>% \n  dfm_tfidf(scheme_tf=\"prop\", smoothing=1)\nas.matrix(\n  d_tf[c(3, 25, 50, 75),\n       c(\"the\",\"first\",\"investment\",\"defrauded\")])\n\n                     features\ndocs                         the        first   investment defrauded\n  1946 Truman written 0.02084698 0.0002080106 1.103379e-04         0\n  1965 Johnson spoken 0.01746801 0.0008790725 0.000000e+00         0\n  1984 Reagan spoken  0.01132996 0.0004411759 6.825554e-05         0\n  2009 Obama spoken   0.01208793 0.0003657038 2.263162e-04         0\n\n\n\n\n\n\n\n\n\ntf\\(\\cdot\\)idf weighting is a fairly common technique and can improve the results of subsequent analyses such as supervised machine learning. As such, it is no surprise that it is easy to apply this in both Python and R, as shown in Example 10.13. This example uses the same data as Example 10.4 above, so you can compare the resulting weighted values with the results reported there. As you can see, the tf\\(\\cdot\\)idf weighting in both languages have roughly the same effect: very frequent terms such as the are made less important compared to less frequent words such as submit. For example, in the raw frequencies for the 1965 Johnson speech, the occurred 355 times compared to submit only once. In the weighted matrix, the weight for submit is four times as low as the weight for the.\nThere are two more things to note if you compare the examples from R and Python. First, to make the two cases somewhat comparable we have to use two options for R, namely to set the term frequency to proportional (scheme_tf='prop'), and to add smoothing to the document frequencies (smooth=1). Without those options, the counts for the first columns would all be zero (since they occur in all documents, and \\(\\log \\frac{85}{85}=0\\)), and the other counts would be greater than one since they would only be weighted, not normalized.\nEven with those options the results are still different (in details if not in proportions), mainly because R normalizes the frequencies before weighting, while Python normalizes after the weighting. Moreover, Python by default uses L2 normalization, meaning that the length of the document vectors will be one, while R uses L1 normalization, that is, the row sums are one (before weighting). Both R and Python have various parameters to control these choices which are explained in their respective help pages. However, although the differences in absolute values look large, the relative effect of making more frequent terms less important is the same, and the specific weighting scheme and options will probably not matter that much for the final results. However, it is always good to be aware of the specific options available and try out which work best for your specific research question."
  },
  {
    "objectID": "content/chapter10.html#sec-ngram",
    "href": "content/chapter10.html#sec-ngram",
    "title": "10  Text as data",
    "section": "10.3 Advanced Representation of Text",
    "text": "10.3 Advanced Representation of Text\nThe examples above all created document-term matrices where each column actually represents a word. There is more information in a text, however, than pure word counts. The phrases: the movie was not good, it was in fact quite bad and the movie was not bad, in fact it was quite good have exactly the same word frequencies, but are quite different in meaning. Similarly, the new kings of York and the kings of New York refer to very different people.\nOf course, in the end which aspect of the meaning of a text is important depends on your research question: if you want to know the sentiment about the movie, it is important to take a word like “not” into account; but if you are interested in the topic or genre of the review, or the extremity of the language used, this might not be relevant.\nThe core idea of this section is that in many cases this information can be captured in a DTM by having the columns represent different information than just words, for example word combinations or groups of related words. This is often called feature engineering, as we are using our domain expertise to find the right features (columns, independent variables) to capture the relevant meaning for our research question. If we are using other columns than words it is also technically more correct to use the name document-feature matrix, as quanteda does, but we will stick to the most common name here and simply continue using the name DTM.\n\n10.3.1 \\(n\\)-grams\nThe first feature we will discuss are n-grams. The simplest case is a bigram (or 2-gram), where each feature is a pair of adjacent words. The example used above, the movie was not bad, will yield the following bigrams: the-movie, movie-was, was-not, and not-bad. Each of those bigrams is then treated as a feature, that is, a DTM would contain one column for each word pair.\nAs you can see in this example, we can now see the difference between not-bad and not-good. The downside of using n-grams is that there are many more unique word pairs than unique words, so the resulting DTM will have many more columns. Moreover, there is a bigger data scarcity problem, as each of those pairs will be less frequent, making it more difficult to find sufficient examples of each to generalize over.\nAlthough bigrams are the most frequent use case, trigrams (3-grams) and (rarely) higher-order n-grams can also be used. As you can imagine, this will create even bigger DTMs and worse data scarcity problems, so even more attention must be paid to feature selection and/or trimming.\n\n\n\n\n\n\n\nExample 10.14 Generating n-grams\n\nPython codeR code\n\n\n\ncv = CountVectorizer(ngram_range=(1, 3), tokenizer=mytokenizer.tokenize)\ncv.fit_transform([\"This is a test\"])\n\n<1x9 sparse matrix of type '<class 'numpy.int64'>'\n    with 9 stored elements in Compressed Sparse Row format>\n\ncv.get_feature_names()\n\n['a', 'a test', 'is', 'is a', 'is a test', 'test', 'this', 'this is', 'this is a']\n\n\n\n\n\ntext = \"This is a test\"\ntokens(text) %>% \n  tokens_tolower() %>% \n  tokens_ngrams(1:3)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"this\"      \"is\"        \"a\"         \"test\"      \"this_is\"   \"is_a\"     \n[7] \"a_test\"    \"this_is_a\" \"is_a_test\"\n\n\n\n\n\n\n\n\n\nExample 10.14 shows how n-grams can be created and used in Python and R. In Python, you can pass the ngram_range=(n, m) option to the vectorizer, while R has a tokens_ngrams(n:m) function. Both will post-process the tokens to create all n-grams in the range of n to m. In this example, we are asking for unigrams (i.e., the words themselves), bigrams and trigrams of a simple example sentence. Both languages produce the same output, with R separating the words with an underscore while Python uses a simple space.\n\n\n\n\n\n\n\nExample 10.15 Words and bigrams containing “government”\n\nPython codeR code\n\n\n\ncv = CountVectorizer(\n    ngram_range=(1, 2), tokenizer=mytokenizer.tokenize, stop_words=\"english\"\n)\ndfm = cv.fit_transform(sotu.text.values)\nts = termstats(dfm, cv)\nts.filter(like=\"government\", axis=0).head(10)\n\n                     frequency  docfreq\ngovernment                1493       84\nfederal government         278       56\ngovernments                188       50\nlocal governments          104       28\ngovernment 's               79       27\ngovernmental                41       19\nlocal government            32       16\ngovernment spending         30       20\nself-government             26       20\ngovernment programs         23       17\n\n\n\n\n\nsotu_tokens = corpus(sotu) %>% \n  tokens(remove_punct=T)  %>%  \n  tokens_remove(stopwords(\"english\")) %>% \n  tokens_tolower()\ndfm_bigram = sotu_tokens %>% \n  tokens_ngrams(1:2) %>% \n  dfm()\ntextstat_frequency(dfm_bigram) %>% \n  filter(str_detect(feature, \"government\")) %>%\n  head(12)\n\n                 feature frequency rank docfreq group\n10            government      1424   10      84   all\n198   federal_government       265  198      56   all\n318          governments       188  318      50   all\n650    local_governments       104  648      28   all\n976         government's        71  972      25   all\n1204     government_must        55 1195      28   all\n1449      government_can        44 1433      26   all\n1552        governmental        41 1537      19   all\n1947    local_government        32 1919      16   all\n2183 government_spending        28 2135      19   all\n2271     self-government        26 2259      20   all\n2645 government_programs        22 2589      17   all\n\n\n\n\n\n\n\n\n\nExample 10.15 shows how you can generate n-grams for a whole corpus. In this case, we create a DTM of the state of the union matrix with all bigrams included. A glance at the frequency table for all words containing government shows that, besides the word itself and its plural and possessive forms, the bigrams include compound words (federal and local government), phrases with the government as subject (the government can and must), and nouns for which the government is an adjective (government spending and government programs).\nYou can imagine that including all these words as features will add many possibilities for analysis of the DTM which would not be possible in a normal bag-of-words approach. The terms local and federal government can be quite important to understand policy positions, but for e.g. sentiment analysis a bigram like not good would also be insightful (but make sure “not” is not on your stop word list!).\n\n\n10.3.2 Collocations\nA special case of n-grams are collocations. In the strict corpus linguistic sense of the word, collocations are pairs of words that occur more frequently than expected based on their underlying occurrence. For example, the phrase crystal clear presumably occurs much more often than would be expected by chance given how often crystal and clear occur separately. Collocations are important for text analysis since they often have a specific meaning, for example because they refer to names such as New York or disambiguate a term like sound in sound asleep, a sound proposal, or loud sound.\nExample 10.16 shows how to identify the most “surprising” collocations using R and Python. For Python, we use the gensim package which we will also use for topic modeling in Section 11.5. This package has a Phrases class which can identify the bigrams in a list of tokens. In R, we use the textstat_collocations function from quanteda. These packages each use a different implementation: gensim uses pointwise mutual information, i.e. how much information about finding the second word does seeing the first word give you? Quanteda estimates an interaction parameter in a loglinear model. Nonetheless, both methods give very similar results, with Saddam Hussein, the Iron Curtain, Al Qaida, and red tape topping the list for each.\n\n\n\n\n\n\n\nExample 10.16 Identifying and applying collocations in the US State of the Union.\n\nPython codeR code\n\n\n\ntokenized_texts = [mytokenizer.tokenize(t) for t in sotu.text]\ntokens = [\n    [t.lower() for t in tokens if not regex.search(\"\\P{letter}\", t)]\n    for tokens in tokenized_texts\n]\nphrases_model = Phrases(tokens, min_count=10, scoring=\"npmi\", threshold=0.5)\nscore_dict = phrases_model.export_phrases()\nscores = pd.DataFrame(score_dict.items(), columns=[\"phrase\", \"score\"])\n\n\nscores.sort_values(\"score\", ascending=False).head()\n\n             phrase     score\n232    iron_curtain  0.977816\n423  saddam_hussein  0.975395\n438        al_qaida  0.963029\n358        red_tape  0.952860\n410    persian_gulf  0.951335\n\n\n\nphraser = Phraser(phrases_model)\ntokens_phrases = [phraser[doc] for doc in tokens]\ncv = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\ndtm = cv.fit_transform(tokens_phrases)\n\n\ntermstats(dtm, cv).filter(like=\"hussein\", axis=0)\n\n                frequency  docfreq\nsaddam_hussein         29        5\n\n\n\n\n\nsotu_tokens = corpus(sotu)  %>% \n  tokens(remove_punct=T) %>% \n  tokens_tolower()\n\ncolloc = sotu_tokens %>% \n  textstat_collocations(min_count=10) %>% \n  as_tibble()\n\ncolloc %>% arrange(-lambda)  %>% head()\n\n# A tibble: 6 × 6\n  collocation    count count_nested length lambda     z\n  <chr>          <int>        <int>  <dbl>  <dbl> <dbl>\n1 saddam hussein    26            0      2   15.2 10.3 \n2 iron curtain      11            0      2   15.2  9.85\n3 al qaida          37            0      2   14.6 10.1 \n4 red tape          22            0      2   13.5 15.1 \n5 persian gulf      31            0      2   12.9 18.5 \n6 line-item veto    10            0      2   12.9  8.81\n\ncollocations = colloc  %>% \n  filter(lambda > 8)  %>%  \n  pull(collocation)  %>%  \n  phrase()\ndfm = sotu_tokens %>% \n  tokens_compound(collocations) %>% \n  dfm()\ntextstat_frequency(dfm) %>% \n  filter(str_detect(feature, \"hussein\"))\n\n            feature frequency rank docfreq group\n2186 saddam_hussein        26 2120       5   all\n8529      hussein's         3 7341       2   all\n\n\n\n\n\n\n\n\n\nThe next block demonstrates how to use these collocations in further processing. In R, we filter the collocations list on \\(lambda>8\\) and use the tokens_compound function to compound bigrams from that list. As you can see in the term frequencies filtered on “Hussein”, the regular terms (apart from the possessive) are removed and the compounded term now has 26 occurrences. For Python, we use the PhraseTransformer class, which is an adaptation of the Phrases class to the scikit-learnmethodology. After setting a standard threshold of 0.7, we can use fit_transform to change the tokens. The term statistics again show how the individual terms are now replaced by their compound.\n\n\n10.3.3 Word Embeddings\nA recent addition to the text analysis toolbox are word embeddings. Although it is beyond the scope of this book to give a full explanation of the algorithms behind word embeddings, they are relatively easy to understand and use at an intuitive level.\nThe first core idea behind word embeddings is that the meaning of a word can be expressed using a relatively small embedding vector, generally consisting of around 300 numbers which can be interpreted as dimensions of meaning. The second core idea is that these embedding vectors can be derived by scanning the context of each word in millions and millions of documents.\nThese embedding vectors can then be used as features or DTM columns for further analysis. Using embedding vectors instead of word frequencies has the advantages of strongly reducing the dimensionality of the DTM: instead of (tens of) thousands of columns for each unique word we only need hundreds of columns for the embedding vectors. This means that further processing can be more efficient as fewer parameters need to be fit, or conversely that more complicated models can be used without blowing up the parameter space. Another advantage is that a model can also give a result for words it never saw before, as these words most likely will have an embedding vector and so can be fed into the model. Finally, since words with similar meanings should have similar vectors, a model fit on embedding vectors gets a “head start” since the vectors for words like “great” and “fantastic” will already be relatively close to each other, while all columns in a normal DTM are treated independently.\nThe assumption that words with similar meanings have similar vectors can also be used directly to extract synonyms. This can be very useful, for example for (semi-)automatically expanding a dictionary for a concept. Example 10.17 shows how to download and use pre-trained embedding vectors to extract synonyms. First, we download a very small subset of the pre-trained Glove embedding vectors1, wrapping the download call in a condition to only download it when needed.\nThen, for Python, we use the excellent support from the gensim package to load the embeddings into a KeyedVectors object. Although not needed for the rest of the example, we create a Pandas data frame from the internal embedding values so the internal structure becomes clear: each row is a word, and the columns (in this case 50) are the different (semantic) dimensions that characterize that word according to the embeddings model. This data frame is sorted on the first dimension, which shows that negative values on that dimension are related to various sports. Next, we switch back to the KeyedVectors object to get the most similar words to the word fraud, which is apparently related to similar words like bribery and corruption but also to words like charges and alleged. These similarities are a good way to (semi-)automatically expand a dictionary: start from a small list of words, find all words that are similar to those words, and if needed manually curate that list. Finally, we use the embeddings to solve the “analogies” that famously showcase the geometric nature of these vectors: if you take the vector for king, subtract the vector for man and add that for woman, the closest word to the resulting vector is queen. Amusingly, it turns out that soccer is a female form of football, probably showing the American cultural origin of the source material.\nFor R, there was less support from existing packages so we decided to use the opportunity to show both the conceptual simplicity of embeddings vectors and the power of matrix manipulation in R. Thus, we directly read in the word vector file which has a head line and then on each line a word followed by its 50 values. This is converted to a matrix with the row names showing the word, which we normalize to (Euclidean) length of one for each vector for easier processing. To determine similarity, we take the cosine distance between the vector representing a word with all other words in the matrix. As you might remember from algebra, the cosine distance is the dot product between the vectors normalized to have length one (just like Pearson’s product–moment correlation is the dot product between the vectors normalized to z-scores per dimension). Thus, we can simply multiply the normalized target vector with the normalized matrix to get the similarity scores. These are then sorted, renamed, and the top values are taken using the basic functions from Chapter 6. Finally, analogies are solved by simply adding and subtracting the vectors as explained above, and then listing the closest words to the resulting vector (excluding the words in the analogy itself).\n\n\n\n\n\n\n\nExample 10.17 Using word embeddings for finding similar and analogous words.\n\nPython codeR code\n\n\n\n# Download the model if needed\nglove_fn = \"glove.6B.50d.10k.w2v.txt\"\nurl = f\"https://cssbook.net/d/{glove_fn}\"\nif not os.path.exists(glove_fn):\n    urllib.request.urlretrieve(url, glove_fn)\n\n('glove.6B.50d.10k.w2v.txt', <http.client.HTTPMessage object at 0x7fc7c4566710>)\n\n\n\n# Load the vectors\nwv = KeyedVectors.load_word2vec_format(glove_fn)\nwvdf = pd.DataFrame(wv.vectors, index=wv.index_to_key)\n\n\nwvdf.sort_values(0, ascending=False).head()\n# Find similar terms\n\n                0        1        2   ...        47        48        49\nairbus      2.5966 -0.53562  0.41422  ...  1.505000 -0.066982  0.133350\nspacecraft  2.5187  0.74418  1.66480  ... -0.065039 -0.235400 -0.401210\nfiat        2.2865 -1.14970  0.48850  ...  0.173990  0.398950  0.042601\nnaples      2.2656 -0.10631 -1.27220  ... -0.857650  0.211240 -0.279690\ndi          2.2441 -0.60324 -1.46890  ... -1.229200  0.805580  0.775450\n\n[5 rows x 50 columns]\n\nfor term, similarity in wv.most_similar(\"fraud\")[:5]:\n    print(f\"{term}: {similarity:.2f}\")\n# Find analogous terms\n\ncharges: 0.86\nbribery: 0.86\nalleged: 0.84\ncorruption: 0.83\nallegations: 0.82\n\ndef analogy(a, b, c):\n    result = wv.most_similar(positive=[b, c], negative=[a])\n    return result[0][0]\n\nwords = [\"king\", \"boy\", \"father\", \"pete\", \"football\"]\nfor x in words:\n    y = analogy(\"man\", x, \"woman\")\n    print(f\"Man is to {x} as woman is to {y}\")\n\nMan is to king as woman is to queen\nMan is to boy as woman is to girl\nMan is to father as woman is to mother\nMan is to pete as woman is to barbara\nMan is to football as woman is to soccer\n\n\n\n\n\nglove_fn = \"glove.6B.50d.10k.w2v.txt\"\nurl = glue(\"https://cssbook.net/d/{glove_fn}\")\nif (!file.exists(glove_fn)) \n    download.file(url, glove_fn)\nwv_tibble = read_delim(glove_fn, skip=1,\n                       delim=\" \", quote=\"\", \n    col_names = c(\"word\", paste0(\"d\", 1:50)))\nwv = as.matrix(wv_tibble[-1])\nrownames(wv) = wv_tibble$word\nwv = wv / sqrt(rowSums(wv^2))\nwv[order(wv[,1])[1:5], 1:5]\n\n                   d1         d2          d3         d4           d5\n20003      -0.4402265 0.07209431 -0.02397687 0.18428984  0.001802660\nbasketball -0.4234652 0.23817458 -0.09346347 0.17270343 -0.001520135\ncollegiate -0.4232457 0.23873925 -0.28741579 0.02797958 -0.066008001\nvolleyball -0.4217268 0.18378662 -0.26229465 0.31409226 -0.124286069\nncaa       -0.4131240 0.14502199 -0.06088206 0.17017979 -0.157397324\n\nwvector = function(wv, word) wv[word,,drop=F]\nwv_similar = function(wv, target, n=5) {\n  similarities = wv %*% t(target)\n  similarities %>% \n    as_tibble(rownames = \"word\") %>% \n    rename(similarity=2) %>% \n    arrange(-similarity) %>% \n    head(n=n)  \n}\nwv_similar(wv, wvector(wv, \"fraud\"))\n\n# A tibble: 5 × 2\n  word       similarity\n  <chr>           <dbl>\n1 fraud           1    \n2 charges         0.859\n3 bribery         0.856\n4 alleged         0.842\n5 corruption      0.830\n\nwv_analogy = function(wv, a, b, c) {\n  result = (wvector(wv, b) \n            + wvector(wv, c) \n            - wvector(wv, a))\n  matches = wv_similar(wv, result) %>% \n    filter(!word %in% c(a,b,c))\n  matches$word[1]\n}\nwords=c(\"king\",\"boy\",\"father\",\"pete\",\"football\")\nfor (x in words) {\n  y = wv_analogy(wv, \"man\", x, \"woman\")\n  print(glue(\"Man is to {x} as woman is to: {y}\"))\n}\n\nMan is to king as woman is to: queen\nMan is to boy as woman is to: girl\nMan is to father as woman is to: mother\nMan is to pete as woman is to: barbara\nMan is to football as woman is to: soccer\n\n\n\n\n\n\n\n\n\n\n\n10.3.4 Linguistic Preprocessing\nA final technique to be discussed here is the use of linguistic preprocessing steps to enrich and filter a DTM. So far, all techniques discussed here are language independent. However, there are also many language-specific tools for automatically enriching text developed by computational linguistics communities around the world. Two techniques will be discussed here as they are relatively widely available for many languages and easy and quick to apply: Part-of-speech tagging and lemmatizing.\nIn part-of-speech tagging or POS-tagging, each word is enriched with information on its function in the sentence: verb, noun, determiner etc. For most languages, this can be determined with very high accuracy, although sometimes text can be ambiguous: in one famous example, the word flies in fruit flies is generally a noun (fruit flies are a type of fly), but it can also be a verb (if fruit could fly). Although there are different sets of POS tags used by different tools, there is broad agreement on the core set of tags listed in Table 10.1.\n\n\nTable 10.1: Overview of part-of-speech (POS) tags.\n\n\nPart of speech\nExample\nUDPipe/Spacy Tag\nPenn Treebank Tag\n\n\n\n\nNoun\napple\nNOUN\nNN, NNS\n\n\nProper Name\nCarlos\nPROPN\nNNP\n\n\nVerb\nwrite\nVERB\nVB, VBD, VBP, ..\n\n\nAuxiliary verb\nbe, have\nAUX\n(same as verb)\n\n\nAdjective\nquick\nADJ\nJJ, JJR, JJS\n\n\nAdverb\nquickly\nADV\nRB\n\n\nPronoun\nI, him\nPRON\nPRP\n\n\nAdposition\nof, in\nADP\nIN\n\n\nDeterminer\nthe, a\nDET\nDT\n\n\n\n\nPOS tags are useful since they allow us for example to analyze only the nouns if we care about the things that are discussed, only the verbs if we care about actions that are described, or only the adjectives if we care about the characteristics given to a noun. Moreover, knowing the POS tag of a word can help disambiguate it. For example, like as a verb (I like books) is generally positive, but like as a preposition (a day like no other) has no clear sentiment attached.\nLemmatizing is a technique for reducing each word to its root or lemma (plural: lemmata). For example, the lemma of the verb reads is (to) read and the lemma of the noun books is book. Lemmatizing is useful since for most of our research questions we do not care about these different conjugations of the same word. By lemmatizing the texts, we do not need to include all conjugations in a dictionary, and it reduces the dimensionality of the DTM – and thus also the data scarcity.\nNote that lemmatizing is related to a technique called stemming, which removes known suffixes (endings) from words. For example, for English it will remove the “s” from both reads and books. Stemming is much less sophisticated than lemmatizing, however, and will trip over irregular conjugations (e.g. are as a form of to be) and regular word endings that look like conjugations (e.g. virus will be stemmed to viru). English has relatively simple conjugations and stemming can produce adequate results. For morphologically richer languages such as German or French, however, it is strongly advised to use lemmatizing instead of stemming. Even for English we would generally advise lemmatization since it is so easy nowadays and will yield better results than stemming.\nFor Example 10.18, we use the UDPipe natural language processing toolkit (Straka and Straková 2017), a “Pipeline” that parses text into “Universal Dependencies”, a representation of the syntactic structure of the text. For R, we can immediately call the udpipe function from the package of the same name. This parses the given text and returns the result as a data frame with one token (word) per row, and the various features in the columns. For Python, we need to take some more steps ourselves. First, we download the English models if they aren’t present. Second, we load the model and create a pipeline with all default settings, and use that to parse the same sentence. Finally, we use the conllu package to read the results into a form that can be turned into a data frame.\nIn both cases, the resulting tokens clearly show some of the potential advantages of linguistic processing: the lemma column shows that it correctly deals with irregular verbs and plural forms. Looking at the upos (universal part-of-speech) column, John is recognized as a proper name (PROPN), bought as a verb, and knives as a noun. Finally, the head_token_id and dep_rel columns represent the syntactic information in the sentence: “Bought” (token 2) is the root of the sentence, and “John” is the subject (nsubj) while “knives” is the object of the buying.\n\n\n\n\n\n\n\n\n('english-ewt-ud-2.4-190531.udpipe', <http.client.HTTPMessage object at 0x7fc7c4a117e0>)\n\n\n\nExample 10.18 Using UDPipe to analyze a sentence\n\nPython codeR code\n\n\n\nudpipe_model = \"english-ewt-ud-2.4-190531.udpipe\"\nm = Model.load(udpipe_model)\npipeline = Pipeline(m, \"tokenize\", Pipeline.DEFAULT, Pipeline.DEFAULT, \"conllu\")\ntext = \"John bought new knives\"\ntokenlist = conllu.parse(pipeline.process(text))\npd.DataFrame(tokenlist[0])\n\n   id    form  lemma   upos  ... head deprel  deps                   misc\n0   1    John   John  PROPN  ...    2  nsubj  None                   None\n1   2  bought    buy   VERB  ...    0   root  None                   None\n2   3     new    new    ADJ  ...    4   amod  None                   None\n3   4  knives  knife   NOUN  ...    2    obj  None  {'SpacesAfter': '\\n'}\n\n[4 rows x 10 columns]\n\n\n\n\n\nudpipe(\"John bought new knives\", \"english\") %>% \n  select(token_id:upos, head_token_id:dep_rel)\n\n  token_id  token lemma  upos head_token_id dep_rel\n1        1   John  John PROPN             2   nsubj\n2        2 bought   buy  VERB             0    root\n3        3    new   new   ADJ             4    amod\n4        4 knives knife  NOUN             2     obj\n\n\n\n\n\n\n\n\n\nThe syntactic relations can be useful if you need to differentiate between who is doing something and whom it was done to. For example, one of the authors of this book used syntactic relations to analyze conflict coverage, where there is an important difference between attacking and getting attacked (Van Atteveldt et al. 2017). However, in most cases you probably don’t need this information and analyzing dependency graphs is relatively complex. We would advise you to almost always consider lemmatizing and tagging your texts, as lemmatizing is simply so much better than stemming (especially for languages other than English), and the part-of-speech can be very useful for analyzing different aspects of a text.\nIf you only need the lemmatizer and tagger, you can speed up processing by setting udpipe(.., parser='none') (R) or setting the third argument to Pipeline (the parser) to Pipeline.NONE (Python). Example 10.19 shows how this can be used to extract only the nouns from the most recent state of the union speeches, create a DTM with these nouns, and then visualize them as a word cloud. As you can see, these words (such as student, hero, childcare, healthcare, and terrorism), are much more indicative of the topic of a text than the general words used earlier. In the next chapter we will show how you can further analyze these data, for example by analyzing usage patterns per person or over time, or using an unsupervised topic model to cluster words into topics.\n\n\n\n\n\n\n\nExample 10.19 Nouns used in the most recent State of the Union addresses\n\nPython codeR code\n\n\n\ndef get_nouns(text):\n    result = conllu.parse(pipeline.process(text))\n    for sentence in result:\n        for token in sentence:\n            if token[\"upos\"] == \"NOUN\":\n                yield token[\"lemma\"]\n\nparser = Pipeline.NONE\npipeline = Pipeline(m, \"tokenize\", Pipeline.DEFAULT, Pipeline.NONE, \"conllu\")\n\ntokens = [list(get_nouns(text)) for text in sotu.text[-5:]]\ncv = CountVectorizer(tokenizer=lambda x: x, lowercase=False, max_df=0.7)\ndtm_verbs = cv.fit_transform(tokens)\nwc = wordcloud(dtm_verbs, cv, background_color=\"white\")\nplt.imshow(wc)\nplt.axis(\"off\")\n\n\n\n\ntokens = sotu %>% \n  top_n(5, Date) %>% \n  udpipe(\"english\", parser=\"none\")\nnouns = tokens %>% \n  filter(upos == \"NOUN\") %>% \n  group_by(doc_id)  %>% \n  summarize(text=paste(lemma, collapse=\" \"))\nnouns %>% \n  corpus() %>% \n  tokens() %>%\n  dfm() %>% \n  dfm_trim(max_docfreq=0.7,docfreq_type=\"prop\")%>%\n  textplot_wordcloud(max_words=50)\n\n\n\n\n\n\n\n\n\n\n\nAs an alternative to UDPipe, you can also use Spacy, which is another free and popular natural language toolkit. It is written in Python, but the spacyr package offers an easy way to use it from R. For R users, installation of spacyr on MacOS and Linux is easy, but note that on Windows there are some additional steps, see cran.r-project.org/web/packages/spacyr/readme/README.html for more details.\nExample 10.20 shows how you can use Spacy to analyze the proverb “all roads lead to Rome” in Spanish. In the first block, the Spanish language model is downloaded (this is only needed once). The second block loads the language model and parses the sentence. You can see that the output is quite similar to UDPipe, but one additional feature is the inclusion of Named Entity Recognition: Spacy can automatically identify persons, locations, organizations and other entities. In this example, it identifies “Rome” as a location. This can be very useful to extract e.g. all persons from a newspaper corpus automatically. Note that in R, you can use the quanteda function as.tokens to directly use the Spacy output in quanteda.\n\n\n\n\n\n\n\nExample 10.20 Using Spacy to analyze a Spanish sentence.\n\nPython codeR code\n\n\nFirst, download the model using the command below and restart python: (in jupyter, try !python or !python3 instead of plain python)\n\npython -m spacy download es_core_news_sm\n\n\n\n\n# Only needed once\nspacy_install()\n# Only needed for languages other than English:\nspacy_download_langmodel(\"es_core_news_sm\")\n\n\n\n\n\nPython codeR code\n\n\n\nnlp = spacy.load(\"es_core_news_sm\")\ntokens = nlp(\"Todos los caminos llevan a Roma\")\npd.DataFrame(\n    [\n        dict(\n            i=t.i,\n            word=t.text,\n            lemma=t.lemma_,\n            head=t.head,\n            dep=t.dep_,\n            ner=t.ent_type_,\n        )\n        for t in tokens\n    ]\n)\n\n   i     word   lemma     head    dep  ner\n0  0    Todos    todo      los    det     \n1  1      los      el  caminos    det     \n2  2  caminos  camino   llevan  nsubj     \n3  3   llevan  llevar   llevan   ROOT     \n4  4        a       a     Roma   case     \n5  5     Roma    Roma   llevan    obj  LOC\n\n\n\n\n\n# I couldn't get this to work properly in the renv environment\nspacy_initialize(\"es_core_news_sm\")\nspacy_parse(\"Todos los caminos llevan a Roma\")\n# To close spacy (or switch languages), use:\nspacy_finalize()\n\n\n\n\n\n\n\n\nAs you can see, nowadays there are a number of good and relatively easy to use linguistic toolkits that can be used. Especially Stanza (Qi et al. 2020) is also a very good and flexible toolkit with support for multiple (human) languages and good integration especially with Python. If you want to learn more about natural language processing, the book Speech and Language Processing by Jurafsky and Martin is a very good starting point [Jurafsky and Martin (2009)]2."
  },
  {
    "objectID": "content/chapter10.html#which-preprocessing-to-use",
    "href": "content/chapter10.html#which-preprocessing-to-use",
    "title": "10  Text as data",
    "section": "10.4 Which Preprocessing to Use?",
    "text": "10.4 Which Preprocessing to Use?\nThis chapter has shown how to create a DTM and especially introduced a number of different steps that can be used to clean and preprocess the DTM before analysis. All of these steps are used by text analysis practitioners and in the relevant literature. However, no study ever uses all of these steps on top of each other. This of courses raises the question of how to know which preprocessing steps to use for your research question.\nFirst, there are a number of things that you should (almost) always do. If your data contains noise such as boilerplate language, HTML artifacts, etc., you should generally strip these out before proceeding. Second, text almost always has an abundance of uninformative (stop) words and a very long tail of very rare words. Thus, it is almost always a good idea to use a combination of stop word removal, trimming based on document frequency, and/or tf.idf weighting. Note that when using a stop word list, you should always manually inspect and/or fine-tune the word list to make sure it matches your domain and research question.\nThe other steps such as n-grams, collocations, and tagging and lemmatization are more optional but can be quite important depending on the specific research. For this (and for choosing a specific combination of trimming and weighting), it is always good to know your domain well, look at the results, and think whether you think they make sense. Using the example given above, bigrams can make more sense for sentiment analysis (since not good is quite different from good), but for analyzing the topic of texts it may be less important.\nUltimately, however, many of these questions have no good theoretical answer, and the only way to find a good preprocessing “pipeline” for your research question is to try many different options and see which works best. This might feel like “cheating” from a social science perspective, since it is generally frowned upon to just test many different statistical models and report on what works best. There is a difference, however, between substantive statistical modeling where you actually want to understand the mechanisms, and technical processing steps where you just want the best possible measurement of an underlying variable (presumably to be used in a subsequent substantive model). Lin (2015) uses the analogy of the mouse trap and the human condition: in engineering you want to make the best possible mouse trap, while in social science we want to understand the human condition. For the mouse trap, it is OK if it is a black box for which we have no understanding of how it works, as long as we are sure that it does work. For the social science model, this is not the case as it is exactly the inner workings we are interested in.\nTechnical (pre)processing steps such as those reviewed in this chapter are primarily engineering devices: we don’t really care how something like tfc.idf works, as long as it produces the best possible measurement of the variables we need for our analysis. In other words, it is an engineering challenge, not a social science research question. As a consequence, the key criterion by which to judge these steps is validity, not explainability. Thus, it is fine to try out different options, as long as you validate the results properly. If you have many different choices to evaluate against some metric such as performance on a subsequent prediction task, using the split-half or cross-validation techniques discussed in chapter Chapter 8 are also relevant here to avoid biasing the evaluation.\n\n\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nJurafsky, Daniel, and James H Martin. 2009. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition (2nd Ed.). Prentice Hall.\n\n\nLin, Jimmy. 2015. “On Building Better Mousetraps and Understanding the Human Condition: Reflections on Big Data in the Social Sciences.” The ANNALS of the American Academy of Political and Social Science 659 (1): 33–47.\n\n\nNothman, Joel, Hanmin Qin, and Roman Yurchak. 2018. “Stop Word Lists in Free Open-Source Software Packages.” In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), 7–12.\n\n\nQi, Peng, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. “Stanza: A Python Natural Language Processing Toolkit for Many Human Languages.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. https://nlp.stanford.edu/pubs/qi2020stanza.pdf.\n\n\nStraka, Milan, and Jana Straková. 2017. “Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe.” In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, 88–99. Vancouver, Canada: Association for Computational Linguistics. http://www.aclweb.org/anthology/K/K17/K17-3009.pdf.\n\n\nVan Atteveldt, W., T. Sheafer, S. R. Shenhav, and Y. Fogel-Dror. 2017. “Clause Analysis: Using Syntactic Information to Automatically Extract Source, Subject, and Predicate from Texts with an Application to the 2008–2009 Gaza War.” Political Analysis 25 (2): 207–22."
  },
  {
    "objectID": "content/chapter11.html#sec-deciding",
    "href": "content/chapter11.html#sec-deciding",
    "title": "11  Automatic analysis of text",
    "section": "11.1 Deciding on the Right Method",
    "text": "11.1 Deciding on the Right Method\nWhen thinking about the computational analysis of texts, it is important to realize that there is no method that is the one to do so. While there are good choices and bad choices, we also cannot say that one method is necessarily and always superior to another. Some methods are more fashionable than others. For instance, there has been a growing interest in topic models (see Section 11.5) in the past few years. There are indeed very good applications for such models, they are also sometimes applied to research questions and/or data where they make much less sense. As always, the choice of method should follow the research question and not the other way round. We therefore caution you about reading Chapter 11 selectively because you want, for instance, to learn about supervised machine learning or about unsupervised topic models. Instead, you should be aware of very different approaches to make an informed decision on what to use when.\nBoumans and Trilling (2016) provide useful guidelines for this. They place automatic text analysis approaches on a continuum from deductive (or top-down) to inductive (or bottom-up). At the deductive end of the spectrum, they place dictionary approaches (Section 11.3). Here, the researcher has strong a priori (theoretical) assumptions (for instance, which topics exist in a news data set; or which words are positive or negative) and can compile lists of words or rules based on these assumptions. The computer then only needs to execute these rules. At the inductive end of the spectrum, in contrast, lie approaches such as topic models (Section 11.5) where little or no a priori assumptions are made, and where we exploratively look for patterns in the data. Here, we typically do not know which topics exist in advance. Supervised approaches (Section 11.4) can be placed in between: here, we do define categories a priori (we do know which topics exist, and given an article, we know to which topic it belongs), but we do not have any set of rules: we do not know which words to look for or which exact rules to follow. These rules are to be “learned” by the computer from the data.\nBefore we get into the details and implementations, let us discuss some use cases of the three main approaches for the computational analysis of text: dictionary (or rule-based) approaches, supervised machine learning, and unsupervised machine learning.\nDictionary approaches excel under three conditions. First, the variable we want to code is manifest and concrete rather than latent and abstract: names of actors, specific physical objects, specific phrases, etc., rather than feelings, frames, or topics. Second, all synonyms to be included must be known beforehand. And third, the dictionary entries must not have multiple meanings. For instance, coding for how often gun control is mentioned in political speeches fits these criteria. There are only so many ways to talk about it, and it is rather unlikely that speeches about other topics contain a phrase like “gun control”. Similarly, if we want to find references to Angela Merkel, Donald Trump, or any other well-known politician, we can just directly search for their names – even though problems arise when people have very common surnames and are referred to by their surnames only.\nSadly, most interesting concepts are more complex to code. Take a seemingly straightforward problem: distinguishing whether a news article is about the economy or not. This is really easy to do for humans: there may be some edge cases, but in general, people rarely need longer than a few seconds to grasp whether an article is about the economy rather than about sports, culture, etc. Yet, many of these articles won’t directly state that they are about the economy by explicitly using the word “economy”.\nWe may think of extending our dictionary not only with econom.+ (a regular expression that includes economists, economic, and so on), but also come up with other words like “stock exchange”, “market”, “company.” Unfortunately, we will quickly run into a problem that we also faced when we discussed the precision-recall trade-off in ?sec-validation: the more terms we add to our dictionary, the more false positives we will get: articles about the geographical space called “market”, about some celebrity being seen in “company” of someone else, and so on.\nFrom this example, we can conclude that often (1) it is easy for humans to decide to which class a text belongs, but (2) it is very hard for humans to come up with a list of words (or rules) on which their judgment is based. Such a situation is perfect for applying supervised machine learning: after all, it won’t take us much time to annotate, say, 1000 articles based on whether they are about the economy or not (probably this takes less time than thoroughly fine tuning a list of words to include or exclude); and the difficult part, deciding on the exact rules underlying the decision to classify an article as economic is done by the computer in seconds. Supervised machine learning, therefore, has replaced dictionary approaches in many areas.\nBoth dictionary (or rule-based) approaches and supervised machine learning assume that you know in advance which categories (positive versus negative; sports versus economy versus politics; …) exist. The big strength of unsupervised approaches such as topic models is that you can also apply them without this knowledge. They therefore allow you to find patterns in data that you did not expect and can generate new insights. This makes them particularly suitable for explorative research questions. Using them for confirmatory tests, in contrast, is less defensible: after all, if we are interested in knowing whether, say, news site A published more about the economy than news site B, then it would be a bit weird to pretend not to know that the topic “economy” exists. Also practically, mapping the resulting topics that the topic model produces onto such a priori existing categories can be challenging.\nDespite all differences, all approaches share one requirement: you need to “Validate. Validate. Validate” (Grimmer and Stewart 2013). Though it has been done in the past, simply applying a dictionary without comparing the performance to manual coding of the same concepts is not acceptable; neither is using a supervised machine learning classifier without doing the same; or blindly trusting a topic model without at least manually checking whether the scores the model assigns to documents really capture what the documents are about."
  },
  {
    "objectID": "content/chapter11.html#sec-reviewdataset",
    "href": "content/chapter11.html#sec-reviewdataset",
    "title": "11  Automatic analysis of text",
    "section": "11.2 Obtaining a Review Dataset",
    "text": "11.2 Obtaining a Review Dataset\nFor the sections on dictionary and supervised approaches we will use a dataset of movie reviews from the IMDB database (Maas et al. 2011). This dataset is published as a compressed set of folders, with separate folders for the train and test datasets and subfolders for positive and negative reviews. Lots of other review datasets are available online, for example for Amazon review data (jmcauley.ucsd.edu/data/amazon/).\nThe IMDB dataset we will use is a relatively large file and it requires bit of processing, so it is smart to cache the data rather than downloading and processing it every time you need it. This is done in Example 11.1, which also serves as a nice example of how to download and process files. Both R and Python follow the same basic pattern. First, we check whether the cached file exists, and if it does we read the data from that file. For R, we use the standard RDS format, while for Python we use a compressed pickle file. The format of the data is also slightly different, following the convention for each language: In R we use the data frame returned by readtext, which can read files from a folder or zip archive and return a data frame containing one text per row. In Python, we have separate lists for the train and test datasets and for the full texts and labels: text_train are the training texts and y_train are the corresponding labels.\n\n\n\n\n\n\n\nExample 11.1 Downloading and caching IMDB review data.\n\nPython codeR code\n\n\n\nfilename = \"reviewdata.pickle.bz2\"\nif os.path.exists(filename):\n    print(f\"Using cached file {filename}\")\n    with bz2.BZ2File(filename, \"r\") as zipfile:\n        data = pickle.load(zipfile)\n    text_train, text_test, y_train, y_test = data\nelse:\n    url = \"https://cssbook.net/d/aclImdb_v1.tar.gz\"\n    print(f\"Downloading from {url}\")\n    fn, _headers = urllib.request.urlretrieve(url, filename=None)\n    t = tarfile.open(fn, mode=\"r:gz\")\n    text_train, text_test = [], []\n    y_train, y_test = [], []\n    for f in t.getmembers():\n        m = re.match(\"aclImdb/(\\w+)/(pos|neg)/\", f.name)\n        if not m:\n            # skip folder names, other categories\n            continue\n        dataset, label = m.groups()\n        text = t.extractfile(f).read().decode(\"utf-8\")\n        if dataset == \"train\":\n            text_train.append(text)\n            y_train.append(label)\n        elif dataset == \"test\":\n            text_test.append(text)\n            y_test.append(label)\n    data = text_train, text_test, y_train, y_test\n    print(f\"Saving to {filename}\")\n    with bz2.BZ2File(filename, \"w\") as zipfile:\n        pickle.dump(data, zipfile)\n\nDownloading from https://cssbook.net/d/aclImdb_v1.tar.gz\nSaving to reviewdata.pickle.bz2\n\n\n\n\n\nfilename = \"reviewdata.rds\"\nif (file.exists(filename)) {\n    print(\"Using cached data\")\n    reviewdata= readRDS(filename)\n} else {\n    print(\"Downloading data\")\n    fn = \"aclImdb_v1.tar.gz\"\n    url = glue(\"https://cssbook.net/d/{fn}\")\n    download.file(url, fn)\n    untar(fn)\n    reviewdata = readtext(\n      file.path(\"aclImdb\", \"*\", \"*\", \"*.txt\"), \n      docvarsfrom = \"filepaths\", dvsep=\"[/\\\\]\",\n      docvarnames=c(\"i\",\"dataset\",\"label\",\"fn\"))\n    unlink(c(\"aclImdb\", fn), recursive=TRUE)\n    reviewdata = reviewdata %>% \n      filter(label %in% c(\"pos\", \"neg\")) %>% \n      select(-i) %>% \n      corpus()\n    saveRDS(reviewdata, filename)\n}\n\n[1] \"Downloading data\"\n\n\n\n\n\n\n\n\n\nIf the cached data file does not exist yet, the file is downloaded from the Internet. In R, we then extract the file and call readtext on the resulting folder. This automatically creates columns for the subfolders, so in this case for the dataset and label. After this, we remove the download file and the extracted folder, clean up the reviewdata, and save it to the reviewdata.rds file. In Python, we can extract files from the downloaded file directly, so we do not need to explicitly extract it. We loop over all files in the archive, and use a regular expression to select only text files and extract the label and dataset name (see Section 9.2 for more information about regular expressions). Then, we extract the text from the archive, and add the text and the label to the appropriate list. Finally, the data is saved as a compressed pickle file, so the next time we run this cell it does not need to download the file again."
  },
  {
    "objectID": "content/chapter11.html#sec-dictionary",
    "href": "content/chapter11.html#sec-dictionary",
    "title": "11  Automatic analysis of text",
    "section": "11.3 Dictionary Approaches to Text Analysis",
    "text": "11.3 Dictionary Approaches to Text Analysis\nA straightforward way to automatically analyze text is to compile a list of terms you are interested in and simply count how often they occur in each document. For example, if you are interested in finding out whether mentions of political parties in news articles change over the years, you only need to compile a list of all party names and write a small script to count them.\nHistorically, this is how sentiment analysis was done. Example 11.2 shows how to do a simple sentiment analysis based on a list of positive and negative words. The logic is straightforward: you count how often each positive word occurs in a text, you do the same for the negative words, and then determine which occur more often.\n\n\n\n\n\n\n\nExample 11.2 Different approaches to a simple dictionary-based sentiment analysis: counting and summing all words using a for-loop over all reviews (Python) versus constructing a term-document matrix and looking up the words in there (R). Note that both approaches would be possible in either language.\n\nPython codeR code\n\n\n\nposwords = \"https://cssbook.net/d/positive.txt\"\nnegwords = \"https://cssbook.net/d/negative.txt\"\npos = set(requests.get(poswords).text.split(\"\\n\"))\nneg = set(requests.get(negwords).text.split(\"\\n\"))\nsentimentdict = {word: +1 for word in pos}\nsentimentdict.update({word: -1 for word in neg})\n\nscores = []\nmytokenizer = TreebankWordTokenizer()\n# For speed, we only take the first 100 reviews\nfor review in text_train[:100]:\n    words = mytokenizer.tokenize(review)\n    # we look up each word in the sentiment dict\n    # and assign its value (with default 0)\n    scores.append(sum(sentimentdict.get(word, 0) for word in words))\nprint(scores)\n\n[-3, -4, 1, 3, -2, -7, -6, 9, 7, 7, 10, 5, -1, 2, 7, -4, 2, 21, 1, -1, 2, -...\n\n\n\n\n\nposwords = \"https://cssbook.net/d/positive.txt\"\nnegwords = \"https://cssbook.net/d/negative.txt\"\npos = scan(poswords, what=\"list\")\nneg = scan(negwords, what=\"list\")\nsentimentdict = dictionary(list(pos=pos, neg=neg))\n\n# For speed, we only take the first 100 reviews\nscores = corpus_sample(reviewdata, 100)  %>% \n  tokens() %>%\n  dfm() %>% \n  dfm_lookup(sentimentdict) %>% \n  convert(to=\"data.frame\")  %>% \n  mutate(sent = pos - neg)\nhead(scores)\n\n                             doc_id pos neg sent\n1    test/neg/4769_1.txt/4769_1.txt   5  11   -6\n2    test/neg/4645_2.txt/4645_2.txt  13   5    8\n3 train/pos/9595_10.txt/9595_10.txt   6   5    1\n4 train/neg/11045_1.txt/11045_1.txt   2   4   -2\n5   train/neg/7411_4.txt/7411_4.txt   9   7    2\n6   train/neg/4225_2.txt/4225_2.txt   5   4    1\n\n\n\n\n\n\n\n\n\nAs you may already realize, there are a lot of downsides to this approach. Most notably, our bag-of-words approach does not allow us to account for negation: “not good” will be counted as positive. Relatedly, we cannot handle modifiers such as “very good”. Also, all words are either positive or negative, while “great” should be more positive than “good”. More advanced dictionary-based sentiment analysis packages like Vader (Hutto and Gilbert 2014) or SentiStrength (Thelwall, Buckley, and Paltoglou 2012) include such functionalities. Yet, as we will discuss in Section 11.4, also these off-the-shelf packages perform very poorly in many sentiment analysis tasks, especially outside of the domains they were developed for. Dictionary-based sentiment analysis has been shown to be problematic when analyzing news content [e.g. Gonzalez-Bailon and Paltoglou (2015);@ Boukes2019]. They are problematic when accuracy at the sentence level is important, but may be satisfactory with longer texts for comparatively easy tasks such as movie review classification (Reagan et al. 2017), where there is clear ground truth data and the genre convention implies that the whole text is evaluative and evaluates one object (the film).\nStill, there are many use cases where dictionary approaches work very well. Because your list of words can contain anything, not just positive or negative words, dictionary approaches have been used, for instance, to measure the use of racist words or swearwords in online fora (e.g., Tulkens et al. 2016). Dictionary approaches are simple to understand and straightforward, which can be a good argument for using them when it is important that the method is no black-box but fully transparent even without technical knowledge. Especially when the dictionary already exists or is easy to create, it is also a very cheap method. However, this is at the expense of their limitation to only performing well when measuring easy to operationalize concepts. To put it bluntly: it’s great for measuring the visibility of parties or organizations in the news, but it’s not good for measuring concepts such as emotions or frames.\nWhat gave dictionary approaches a bit of a bad name is that many researchers applied them without validating them. This is especially problematic when a dictionary is applied in a slightly different domain than that for which it was originally made.\nIf you want to use a dictionary-based approach, we advise the following procedure:\n\nConstruct a dictionary based on theoretical considerations and by closely reading a sample of example texts.\n\nCode some articles manually and compare with the automated coding.\nImprove your dictionary and check again.\nManually code a validation dataset of sufficient size. The required size depends a bit on how balanced your data is – if one code occurs very infrequently, you will need more data.\nCalculate the agreement. You could use standard intercoder reliability measures used in manual content analysis, but we would also advise you to calculate precision and recall (see Section ?sec-validation).\n\n\nVery extensive dictionaries will have a high recall (it becomes increasingly unlikely that you “miss” a relevant document), but often suffer from low precision (more documents will contain one of the words even though they are irrelevant). Vice versa, a very short dictionary will often be very precise, but miss a lot of documents. It depends on your research question where the right balance lies, but to substantially interpret your results, you need to be able to quantify the performance of your dictionary-based approach.\n\n\n\n\n\n\nHow many documents do you need to calculate agreement with human annotators?\n\n\n\n\n\nTo determine the number of documents one needs to determine the agreement between a human and a machine, one can follow the same standards that are recommended for traditional manual content analysis.\nFor instance, Krippendorff (2004) provides a convenience table to look up the required sample sizes for determining the agreement between two human coders (p. 240). Riffe et al. (2019) provide similar suggestions (p. 114). In short, the sample size depends on the level of statistical significance the researcher deems acceptable as well as on the distribution of the data. In an extreme case, if only 5 out of 100 items are to be coded as \\(x\\), then in a sample of 20 items, such an item may not even occur. In order to determine agreement between the automated method and a human, we suggest that sample sizes that one would also use for the calculation of agreement between human coders are used. For specific calculations, we refer to content analysis books such as the two referenced here. To give a very rough ballpark figure (that shouldn’t replace a careful calculation!), roughly 100 to 200 items will cover many scenarios (assuming a small amount of reasonably balanced classes)."
  },
  {
    "objectID": "content/chapter11.html#sec-supervised",
    "href": "content/chapter11.html#sec-supervised",
    "title": "11  Automatic analysis of text",
    "section": "11.4 Supervised Text Analysis: Automatic Classification and Sentiment Analysis",
    "text": "11.4 Supervised Text Analysis: Automatic Classification and Sentiment Analysis\nFor many applications, there are good reasons to use the dictionary approach presented in the previous section. First, it is intuitively understandable and results can – in principle – even be verified by hand, which can be an advantage when transparency or communicability is of high importance. Second, it is very easy to use. But as we have discussed in Section 11.1, dictionary approaches in general perform less well the more abstract, non-manifest, or complex a concept becomes. In the next section, we will make the case that topics, but also sentiment, in fact, are quite a complex concepts that are often hard to capture with dictionaries (or at least, crafting a custom dictionary would be difficult). For instance, while “positive” and “negative” seem straightforward categories at first sight, the more we think about it, the more apparent it becomes how context-dependent it actually is: in a dataset about the economy and stock market returns, “increasing” may indicate something positive, in a dataset about unemployment rates the same word would be something negative. Thus, machine learning can be a more appropriate technique for such tasks.\n\n11.4.1 Putting Together a Workflow\nWith the knowledge we gained in previous chapters, it is not difficult to set up a supervised machine learning classifier to automatically determine, for instance, the topic of a news article.\nLet us recap the building blocks that we need. In Chapter 8, you learned how to use different classifiers, how to evaluate them, and how to choose the best settings. However, in these examples, we used numerical data as features; now, we have text. In Chapter 10, you learned how to turn text into numerical features. And that’s all we need to get started!\nTypical examples for supervised machine learning in the analysis of communication include the classification of topics (e.g., Scharkow 2011), frames (e.g., Burscher et al. 2014), user characteristics such as gender or ideology, or sentiment.\nLet us consider the case of sentiment analysis in more detail. Classical sentiment analysis is done with a dictionary approach: you take a list of positive words, a list of negative words, and count which occur more frequently. Additionally, one may attach a weight to each word, such that “perfect” gets a higher weight than “good”, for instance. An obvious drawback is that these pure bag-of-words approaches cannot cope with negation (“not good”) and intensifiers (“very good”), which is why extensions have been developed that take these (and other features, such as punctuation) into account (Thelwall, Buckley, and Paltoglou 2012; Hutto and Gilbert 2014; De Smedt, Daelemans, and Smedt 2012).\nBut while available off-the-shelf packages that implement these extended dictionary-based methods are very easy to use (in fact, they spit out a sentiment score with one single line of code), it is questionable how well they work in practice. After all, “sentiment” is not exactly a clear, manifest concept for which we can enumerate a list of words. It has been shown that results obtained with multiple of these packages correlate very poorly with each other and with human annotations (Boukes et al. 2019; Chan et al. in press).\nConsequently, it has been suggested that it is better to use supervised machine learning to automatically code the sentiment of texts (Gonzalez-Bailon and Paltoglou 2015; Vermeer et al. 2019). However, you may need to annotate documents from your own dataset: training a classifier on, for instance, movie reviews and then using it to predict sentiment in political texts violates the assumption that training set, test set, and the unlabeled data that are to be classified are (at least in principle and approximately) drawn from the same population.\nTo illustrate the workflow, we will use the ACL IMDB dataset, a large dataset that consists of a training dataset of 25000 movie reviews (of which 12500 are positive and 12500 are negative) and an equally sized test dataset (Maas et al. 2011). It can be downloaded at ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\nThese data do not come in one file, but rather in a set of text files that are sorted in different folders named after the dataset to which they belong (test or train) and their label (pos and neg). This means that we cannot simply use a pre-defined function to read them, but we need to think of a way of reading the content into a data structure that we can use. This data was loaded in Example 11.1 above.\n\n\n\n\n\n\nSparse versus dense matrices in Python and R\n\n\n\n\n\nIn a document-term matrix, you would typically find a lot of zeros: most words do not appear in any given document. For instance, the reviews in the IMDB dataset contain more than 100000 unique words. Hence, the matrix has more than 100000 columns. Yet, most reviews only consist of a couple of hundred words. As a consequence, more than 99% of the cells in the table contain a zero. In a sparse matrix, we do not store all these zeros, but only store the values for cells that actually contain a value. This drastically reduces the memory needed. But even if you have a huge amount of memory, this does not solve the issue: in R, the number of cells in a matrix is limited to 2147483647. It is therefore impossible to store a matrix with 100000 features and 25000 documents as a dense matrix. Unfortunately, many models that you can run via caret in R will convert your sparse document-term matrix to a dense matrix, and hence are effectively only usable for very small datasets. An alternative is using the quanteda package, which does use sparse matrices throughout. However, at the time of writing this book, quanteda only provides a very limited number of models. As all of these problems do not arise in scikit-learn, you may want to consider using Python for many text classification tasks.\n\n\n\nLet us now train our first classifier. We choose a Naïve Bayes classifier with a simple count vectorizer (Example 11.3). In the Python example, pay attention to the fitting of the vectorizer: we fit on the training data and transform the training data with it, but we only transform the test data without re-fitting the vectorizer. Fitting, here, includes the decision about which words to include (by definition, words that are not present in the training data are not included; but we could also choose additional constraints, such as excluding very rare or very common words), but also assigning an (internally used) identifier (variable name) to each word. If we fit the classifier again, these would not be compatible any more. In R, the same is achieved in a slightly different way: two term-document matrices are created independently, before they are matched in such a way that only the features that are present in the training matrix are retained in the test matrix.\n\n\n\n\n\n\nNote\n\n\n\n\n\nA word that is not present in the training data, but is present in the test data, is thus ignored. If you want to use the information such out-of-vocabulary words can entail (e.g., they may be synonyms), consider using a word embedding approach (see Section 10.3.3)\n\n\n\nWe do not necessarily expect this first model to be the best classifier we can come up with, but it provides us with a reasonable baseline. In fact, even without any further adjustments, it works reasonably well: precision is higher for positive reviews and recall is higher for negative reviews (classifying a positive review as negative happens twice as much as the reverse), but none of the values is concerningly low.\n\n\n\n\n\n\n\nExample 11.3 Training a Naïve Bayes classifier with simple word counts as features\n\nPython codeR code\n\n\n\nvectorizer = CountVectorizer(stop_words=\"english\")\nX_train = vectorizer.fit_transform(text_train)\nX_test = vectorizer.transform(text_test)\n\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\n\nMultinomialNB()\n\ny_pred = nb.predict(X_test)\n\nrep = metrics.classification_report(y_test, y_pred)\nprint(rep)\n\n              precision    recall  f1-score   support\n\n         neg       0.79      0.88      0.83     12500\n         pos       0.86      0.76      0.81     12500\n\n    accuracy                           0.82     25000\n   macro avg       0.82      0.82      0.82     25000\nweighted avg       0.82      0.82      0.82     25000\n\n\n\n\n\ndfm_train = reviewdata %>% \n  corpus_subset(dataset == \"train\") %>% \n  tokens() %>%\n  dfm() %>% \n  dfm_trim(min_docfreq=0.01, docfreq_type=\"prop\")\n\ndfm_test = reviewdata %>% \n  corpus_subset(dataset == \"test\") %>% \n  tokens() %>%\n  dfm() %>% \n  dfm_match(featnames(dfm_train))\n\nmyclassifier = textmodel_nb(dfm_train, \n                  docvars(dfm_train, \"label\"))\n\npredicted = predict(myclassifier,newdata=dfm_test)\nactual = docvars(dfm_test, \"label\")\n\nresults = list()\nfor (label in c(\"pos\", \"neg\")) {\n  results[[label]] = tibble(\n    Precision=Precision(actual, predicted, label),\n    Recall=Recall(actual, predicted, label),\n    F1=F1_Score(actual, predicted, label))\n}\nbind_rows(results, .id=\"label\")\n\n# A tibble: 2 × 4\n  label Precision Recall    F1\n  <chr>     <dbl>  <dbl> <dbl>\n1 pos       0.825  0.794 0.809\n2 neg       0.801  0.832 0.816\n\n\n\n\n\n\n\n\n\n\n\n11.4.2 Finding the Best Classifier\nLet us start by comparing the two simple classifiers we know (Naïve Bayes and Logistic Regression (see Section 8.3) and the two vectorizers that transform our texts into two numerical representations that we know: word counts and tf.idf scores (see Chapter 10).\nWe can also tune some things in the vectorizer, such as filtering out stopwords, or specifying a minimum number (or proportion) of documents in which a word needs to occur in order to be included, or the maximum number (or proportion) of documents in which it is allowed to occur. For instance, it could make sense to say that a word that occurs in less than \\(n=5\\) documents is probably a spelling mistake or so unusual that it just unnecessarily bloats our feature matrix; and on the other hand, a word that is so common that it occurs in more than 50% of all documents is so common that it does not help us to distinguish between different classes.\nWe can try all of these things out by hand by just re-running the code from Example 11.3 and only changing the line in which the vectorizer is specified and the line in which the classifier is specified. However, copy-pasting essentially the same code is generally not a good idea, as it makes your code unnecessary long and increases the likelihood of errors creeping in when you, for instance, need to apply the same changes to multiple copies of the code. A more elegant approach is outlined in Example 11.4: We define a function that gives us a short summary of only the output we are interested in, and then use a for-loop to iterate over all configurations we want to evaluate, fit them and call the function we defined before. In fact, with 23 lines of code, we manage to compare four different models, while we already needed 15 lines (in Example 11.3) to evaluate only one model.\n\n\n\n\n\n\n\nExample 11.4 An example of a custom function to give a brief overview of the performance of four simple vectorizer-classifier combinations.\n\ndef short_classification_report(y_test, y_pred):\n    print(\"    \\tPrecision\\tRecall\")\n    for label in set(y_pred):\n        pr = metrics.precision_score(y_test, y_pred, pos_label=label)\n        re = metrics.recall_score(y_test, y_pred, pos_label=label)\n        print(f\"{label}:\\t{pr:0.2f}\\t\\t{re:0.2f}\")\n\n\nconfigs = [\n    (\"NB-count\", CountVectorizer(min_df=5, max_df=0.5), MultinomialNB()),\n    (\"NB-TfIdf\", TfidfVectorizer(min_df=5, max_df=0.5), MultinomialNB()),\n    (\n        \"LR-Count\",\n        CountVectorizer(min_df=5, max_df=0.5),\n        LogisticRegression(solver=\"liblinear\"),\n    ),\n    (\n        \"LR-TfIdf\",\n        TfidfVectorizer(min_df=5, max_df=0.5),\n        LogisticRegression(solver=\"liblinear\"),\n    ),\n]\n\nfor name, vectorizer, classifier in configs:\n    print(name)\n    X_train = vectorizer.fit_transform(text_train)\n    X_test = vectorizer.transform(text_test)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    short_classification_report(y_test, y_pred)\n    print(\"\\n\")\n\nNB-count\nMultinomialNB()\n        Precision   Recall\nneg:    0.79        0.88\npos:    0.87        0.77\n\n\nNB-TfIdf\nMultinomialNB()\n        Precision   Recall\nneg:    0.80        0.88\npos:    0.87        0.78\n\n\nLR-Count\nLogisticRegression(solver='liblinear')\n        Precision   Recall\nneg:    0.85        0.87\npos:    0.87        0.85\n\n\nLR-TfIdf\nLogisticRegression(solver='liblinear')\n        Precision   Recall\nneg:    0.88        0.89\npos:    0.89        0.88\n\n\n\n\n\n\nThe output of this little example already gives us quite a bit of insight into how to tackle our specific classification tasks: first, we see that a \\(tf\\cdot idf\\) classifier seems to be slightly but consistently superior to a count classifier (this is often, but not always the case). Second, we see that the logistic regression performs better than the Naïve Bayes classifier (again, this is often, but not always, the case). In particular, in our case, the logistic regression improved on the excessive misclassification of positive reviews as negative, and achieves a very balanced performance.\nThere may be instances where one nevertheless may want to use a Count Vectorizer with a Naïve Bayes classifier instead (especially if it is too computationally expensive to estimate the other model), but for now, we may settle on the best performing combination, logistic regression with a tf.idf vectorizer. You could also try fitting a Support Vector Machine instead, but we have little reason to believe that our data isn’t linearly separable, which means that there is little reason to believe that the SVM will perform better. Given the good performance we already achieved, we decide to stick to the logistic regression for now.\nWe can now go as far as we like, include more models, use crossvalidation and gridsearch (see Section 8.3.3), etc. However, our workflow now consists of two steps: fitting/transforming our input data using a vectorizer, and fitting a classifier. To make things easier, in scikit-learn, both steps can be combined into a so-called pipe. Example 11.5 shows how the loop in Example 11.4 can be re-written using pipes (the result stays the same).\n\n\n\n\n\n\n\nExample 11.5 Instead of fitting vectorizer and classifier separately, they can be combined in a pipeline.\n\nfor name, vectorizer, classifier in configs:\n    print(name)\n    pipe = make_pipeline(vectorizer, classifier)\n    pipe.fit(text_train, y_train)\n    y_pred = pipe.predict(text_test)\n    short_classification_report(y_test, y_pred)\n    print(\"\\n\")\n\nNB-count\nPipeline(steps=[('countvectorizer', CountVectorizer(max_df=0.5, min_df=5)),\n                ('multinomialnb', MultinomialNB())])\n        Precision   Recall\nneg:    0.79        0.88\npos:    0.87        0.77\n\n\nNB-TfIdf\nPipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_df=0.5, min_df=5)),\n                ('multinomialnb', MultinomialNB())])\n        Precision   Recall\nneg:    0.80        0.88\npos:    0.87        0.78\n\n\nLR-Count\nPipeline(steps=[('countvectorizer', CountVectorizer(max_df=0.5, min_df=5)),\n                ('logisticregression', LogisticRegression(solver='liblinear'))])\n        Precision   Recall\nneg:    0.85        0.87\npos:    0.87        0.85\n\n\nLR-TfIdf\nPipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_df=0.5, min_df=5)),\n                ('logisticregression', LogisticRegression(solver='liblinear'))])\n        Precision   Recall\nneg:    0.88        0.89\npos:    0.89        0.88\n\n\n\n\n\n\nSuch a pipeline lends itself very well to performing a gridsearch. Example 11.6 gives you an example. With LogisticRegression? and TfIdfVectorizer?, we can get a list of all possible hyperparameters that we may want to tune. For instance, these could be the minimum and maximum frequency for words to be included or whether we want to use only unigrams (single words) or also bigrams (combinations of two words, see Section 10.3). For the Logistic Regression, it may be the regularization hyperparameter C, which applies a penalty for too complex models. We can put all values for these parameters that we want to consider in a dictionary, with a descriptive key (i.e., a string with the step of the pipeline followed by two underscores and the name of the hyperparameter) and a list of all values we want to consider as the corresponding value.\nThe gridsearch procedure will then estimate all combinations of all values, using cross-validation (see ?sec-validation). In our example, we have \\(2 x 2 x 2 x 2 x 3 = 24\\) different models, and \\(24 models x 5 folds = 120\\) models to estimate. Hence, it may take you some time to run the code.\n\n\n\n\n\n\n\nExample 11.6 A gridsearch to find the best hyperparameters for a pipeline consisting of a vectorizer and a classifier. Note that we can tune any parameter that either the vectorizer or the classifier accepts as an input, not only the four hyperparameters we chose in this example.\n\npipeline = Pipeline(\n    steps=[\n        (\"vectorizer\", TfidfVectorizer()),\n        (\"classifier\", LogisticRegression(solver=\"liblinear\")),\n    ]\n)\ngrid = {\n    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n    \"vectorizer__max_df\": [0.5, 1.0],\n    \"vectorizer__min_df\": [0, 5],\n    \"classifier__C\": [0.01, 1, 100],\n}\nsearch = GridSearchCV(\n    estimator=pipeline, n_jobs=-1, param_grid=grid, scoring=\"accuracy\", cv=5\n)\nsearch.fit(text_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n                                       ('classifier',\n                                        LogisticRegression(solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid={'classifier__C': [0.01, 1, 100],\n                         'vectorizer__max_df': [0.5, 1.0],\n                         'vectorizer__min_df': [0, 5],\n                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},\n             scoring='accuracy')\n\nprint(f\"Best parameters: {search.best_params_}\")\n\nBest parameters: {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectori...\n\npred = search.predict(text_test)\nprint(short_classification_report(y_test, pred))\n\n        Precision   Recall\nneg:    0.90        0.90\npos:    0.90        0.90\nNone\n\n\n\n\n\n\nWe see that we could further improve our model to precision and recall values of 0.90, by excluding extremely infrequent and extremely frequent words, including both unigrams and bigrams (which, we may speculate, help us to account for the “not good” versus “not”, “good” problem), and changing the default penalty of \\(C=1\\) to \\(C=100\\).\nLet us, just for the sake of it, compare the performance of our model with an off-the-shelf sentiment analysis package, in this case Vader (Hutto and Gilbert 2014). For any text, it will directly estimate sentiment scores (more specifically, a positivity score, a negativity score, a neutrality score, and a compound measure that combines them), without any need to have training data. However, as Example 11.7 shows, such a method is clearly inferior to a supervised machine learning approach. While in almost all cases (except for \\(n=11\\) cases), Vader was able to make a choice (getting scores of 0 is a notorious problem in very short texts), precision and recall are clearly worse than even the simple baseline model we started with, and much worse than those of the final model we finished with. In fact, we miss half (!) of the negative reviews. There are probably very few applications in the analysis of communication in which we would find this acceptable. It is important to highlight that this is not because the off-the-shelf package we chose is a particularly bad one (on the contrary, it is actually comparatively good), but because of the inherent limitations of dictionary-based sentiment analysis.\n\n\n\n\n\n\n\nExample 11.7 For the sake of comparison, we calculate how an off-the-shelf sentiment analysis package would have performed in this task\n\nnltk.download(\"vader_lexicon\")\n\nTrue\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/runner/nltk_data...\n\nanalyzer = SentimentIntensityAnalyzer()\npred = []\nfor review in text_test:\n    sentiment = analyzer.polarity_scores(review)\n    if sentiment[\"compound\"] > 0:\n        pred.append(\"pos\")\n    elif sentiment[\"compound\"] < 0:\n        pred.append(\"neg\")\n    else:\n        pred.append(\"dont know\")\n\nprint(metrics.confusion_matrix(y_test, pred))\n\n[[    0     0     0]\n [    6  6706  5788]\n [    5  1748 10747]]\n\nprint(metrics.classification_report(y_test, pred))\n\n              precision    recall  f1-score   support\n\n   dont know       0.00      0.00      0.00         0\n         neg       0.79      0.54      0.64     12500\n         pos       0.65      0.86      0.74     12500\n\n    accuracy                           0.70     25000\n   macro avg       0.48      0.47      0.46     25000\nweighted avg       0.72      0.70      0.69     25000\n\n\n\n\n\n\nWe need to keep in mind, though, that with this dataset, we chose one of the easiest sentiment analysis tasks: a set of long, rather formal texts (compared to informal short social media messages), that evaluate exactly one entity (one film), and that are not ambiguous at all. Many applications that communication scientists are interested in are much less straightforward. Therefore, however tempting it may be to use an off-the-shelf package, doing so requires a thorough test based on at least some human-annotated data.\n\n\n11.4.3 Using the Model\nSo far, we have focused on training and evaluating models, almost forgetting why we were doing this in the first place: to use them to predict the label for new data that we did not annotate.\nOf course, we could always re-train the model when we need to use it – but that has two downsides: first, as you may have seen, it may actually take considerable time to train it, and second, you need to have the training data available, which may be a problem both in terms of storage space and of copyright and/or privacy if you want to share your classifier with others.\nTherefore, it makes sense to save both our classifier and our vectorizer to a file, so that we can reload them later (Example 11.8). Keep in mind that you have to re-use both – after all, the columns of your feature matrix will be different (and hence, completely useless for the classifier) when fitting a new vectorizer. Therefore, as you see, you do not do any fitting any longer, and only use the .transform() method of the (already fitted) vectorizer and the .predict() method of the (already fitted) classifier.\nIn R, you have no vectorizer you could save – but because in contrast to Python, both your DTM and your classifier include the feature names, it suffices to save the classifier only (using saveRDS(myclassifier, \"myclassifier.rds\")) and using on a new DTM later on. You do need to remember, though, how you constructed the DTM (e.g., which preprocessing steps you took), to make sure that the features are comparable.\n\n\n\n\n\n\n\nExample 11.8 Saving and loading a vectorizer and a classifier\n\n# Make a vectorizer and train a classifier\nvectorizer = TfidfVectorizer(min_df=5, max_df=0.5)\nclassifier = LogisticRegression(solver=\"liblinear\")\nX_train = vectorizer.fit_transform(text_train)\nclassifier.fit(X_train, y_train)\n\n# Save them to disk\n\nLogisticRegression(solver='liblinear')\n\nwith open(\"myvectorizer.pkl\", mode=\"wb\") as f:\n    pickle.dump(vectorizer, f)\nwith open(\"myclassifier.pkl\", mode=\"wb\") as f:\n    joblib.dump(classifier, f)\n\n# Later on, re-load this classifier and apply:\nnew_texts = [\"This is a great movie\", \"I hated this one.\", \"What an awful fail\"]\n\nwith open(\"myvectorizer.pkl\", mode=\"rb\") as f:\n    myvectorizer = pickle.load(f)\nwith open(\"myclassifier.pkl\", mode=\"rb\") as f:\n    myclassifier = joblib.load(f)\n\nnew_features = myvectorizer.transform(new_texts)\npred = myclassifier.predict(new_features)\n\nfor review, label in zip(new_texts, pred):\n    print(f\"'{review}' is probably '{label}'.\")\n\n'This is a great movie' is probably 'pos'.\n'I hated this one.' is probably 'neg'.\n'What an awful fail' is probably 'neg'.\n\n\n\n\n\n\nAnother thing that we might want to do is to get a better idea of the features that the model uses to arrive at its prediction; in our example, what actually characterizes the best and the worst reviews. Example 11.9 shows how this can be done in one line of code using eli5 – a package that aims to “explain [the model] like I’m 5 years old”. Here, we re-use the pipe we constructed earlier to provide both the vectorizer and the classifier to eli5 – if we had only provided the classifier, then the feature names would have been internal identifiers (which are meaningless to us) rather than human-readable words.\n\n\n\n\n\n\n\nExample 11.9 Using eli5 to get the most predictive features\n\npipe = make_pipeline(\n    TfidfVectorizer(min_df=5, max_df=0.5),\n    LogisticRegression(solver=\"liblinear\"),\n)\npipe.fit(text_train, y_train)\n\nPipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_df=0.5, min_df=5)),\n                ('logisticregression', LogisticRegression(solver='liblinear'))])\n\nprint(eli5.format_as_text(eli5.explain_weights(pipe)))\n\nExplained as: linear model\n\nFeatures with largest coefficients.\nCaveats:\n1. Be careful with features which are not\n   independent - weights don't show their importance.\n2. If scale of input features is different then scale of coefficients\n   will also be different, making direct comparison between coefficient values\n   incorrect.\n3. Depending on regularization, rare features sometimes may have high\n   coefficients; this doesn't mean they contribute much to the\n   classification result for most examples.\n\ny='pos' top features\nWeight  Feature  \n------  ---------\n+7.173  great    \n+6.101  excellent\n+5.055  best     \n+4.791  perfect  \n+4.698  wonderful\n+4.181  amazing  \n+4.170  well     \n… 13660 more positive …\n… 13567 more negative …\n-4.050  dull     \n-4.070  no       \n-4.176  horrible \n-4.247  poorly   \n-4.406  worse    \n-4.599  nothing  \n-4.695  terrible \n-5.337  poor     \n-5.733  boring   \n-6.315  waste    \n-6.349  awful    \n-7.347  bad      \n-9.059  worst    \n\n\n\n\n\n\nWe can also use eli5 to explain how the classifier arrived at a prediction for a specific document, by using different shades of green and red to explain how much different features contributed to the classification, and in which direction (Example 11.10).\n\n\n\n\n\n\n\nExample 11.10 Using eli5 to explain a prediction ## Python code\n\n# WvA: This doesn't work outside a notebook, should probably call other functions \n# eli5.show_prediction(classifier, text_test[0], vec=vectorizer, targets=[\"pos\"])\n\n\n\n\n\n\n\n11.4.4 Deep Learning\nDeep learning models were introduced in Section 11.4.4 as a (relatively) new class of models in supervised machine learning. Using the Python keras package you can define various model architectures such as Convolutional or Recurrent Neural Networks. Although it is beyond the scope of this chapter to give a detailed treatment of building and training deep learning models, in this section we do give an example of using a Convolutional Neural Network using pre-trained word embeddings. We would urge anyone who is interested in machine learning for text analysis to continue studying deep learning, probably starting with the excellent book by Goldberg (2017).\nImpressively, in R you can now also use the keras package to train deep learning models, as shown in the example. Similar to how spacyr works (Section 10.3.4), the R package actually installs and calls Python behind the screens using the reticulate package. Although the resulting models are relatively similar, it is less easy to build and debug the models in R because most of the documentation and community examples are written in Python. Thus in the end, we probably recommend people who want to dive into deep learning should choose Python rather than R.\n\n\n\n\n\n\n\nExample 11.11 Loading Dutch Sentiment Data (from Van Atteveldt, Van der Velden, and Boukes 2021)\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/dutch_sentiment.csv\"\nh = pd.read_csv(url)\nh.head()\n\n      id  value                                            lemmata\n0  10007      0  Rabobank voorspellen flink stijging hypotheekr...\n1  10027      0  D66 willen reserve provincie aanspreken voor g...\n2  10037      1                             UWV dit jaar veel baan\n3  10059      1                  proost op geslaagd beursgang bols\n4  10099      0         helft werknemer gaan na 65ste met pensioen\n\n\n\n\n\nurl=\"https://cssbook.net/d/dutch_sentiment.csv\"\nd = read_csv(url)\nhead(d)\n\n# A tibble: 6 × 3\n     id value lemmata                                             \n  <dbl> <dbl> <chr>                                               \n1 10007     0 Rabobank voorspellen flink stijging hypotheekrente  \n2 10027     0 D66 willen reserve provincie aanspreken voor groei  \n3 10037     1 UWV dit jaar veel baan                              \n4 10059     1 proost op geslaagd beursgang bols                   \n5 10099     0 helft werknemer gaan na 65ste met pensioen          \n6 10101     1 Europa groeien voorzichtig dankzij laag energieprijs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 11.12 Deep Learning: Defining a Recursive Neural Network\n\nPython codeR code\n\n\n\n# Tokenize texts\ntokenizer = Tokenizer(num_words=9999)\ntokenizer.fit_on_texts(h.lemmata)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(h.lemmata)\ntokens = pad_sequences(sequences, maxlen=1000)\n\n# Prepare embeddings layer\nfn = \"w2v_320d_trimmed\"\nif not os.path.exists(fn):\n    url = f\"https://cssbook.net/d/{fn}\"\n    print(f\"Downloading embeddings from {url}\")\n    urllib.request.urlretrieve(url, fn)\n\nDownloading embeddings from https://cssbook.net/d/w2v_320d_trimmed\n('w2v_320d_trimmed', <http.client.HTTPMessage object at 0x7f8914d18820>)\n\nembeddings = KeyedVectors.load_word2vec_format(fn)\nemb_matrix = np.zeros((len(tokenizer.word_index) + 1, embeddings.vector_size))\nfor word, i in tokenizer.word_index.items():\n    if word in embeddings:\n        emb_matrix[i] = embeddings[word]\nembedding_layer = Embedding(\n    emb_matrix.shape[0],\n    emb_matrix.shape[1],\n    input_length=tokens.shape[1],\n    trainable=True,\n    weights=[emb_matrix],\n)\n\n# Configure the CNN model\nsequence_input = Input(shape=(tokens.shape[1],), dtype=\"int32\")\nseq = embedding_layer(sequence_input)\nm = Conv1D(filters=128, kernel_size=3, activation=\"relu\")(seq)\nm = GlobalMaxPooling1D()(m)\nm = Dense(64, activation=\"relu\")(m)\npreds = Dense(1, activation=\"tanh\")(m)\nm = Model(sequence_input, preds)\nm.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 1000)]            0         \n                                                                 \n embedding (Embedding)       (None, 1000, 320)         2176640   \n                                                                 \n conv1d (Conv1D)             (None, 998, 128)          123008    \n                                                                 \n global_max_pooling1d (Globa  (None, 128)              0         \n lMaxPooling1D)                                                  \n                                                                 \n dense (Dense)               (None, 64)                8256      \n                                                                 \n dense_1 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 2,307,969\nTrainable params: 2,307,969\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n\ntext_vectorization = layer_text_vectorization(\n    max_tokens=10000, output_sequence_length=50)\nadapt(text_vectorization, d$lemmata)\n\ninput = layer_input(shape=1, dtype = \"string\")\noutput = input %>% \n  text_vectorization() %>% \n  layer_embedding(input_dim = 10000 + 1, \n                  output_dim = 16) %>%\n  layer_conv_1d(filters=128, kernel_size=3, \n                activation=\"relu\") %>%\n  layer_global_max_pooling_1d() %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"tanh\")\n\nmodel = keras_model(input, output)\nmodel\n\nModel: \"model_1\"\n________________________________________________________________________________\n Layer (type)                  Output Shape               Param #    Trainable  \n================================================================================\n input_2 (InputLayer)          [(None, 1)]                0          Y          \n text_vectorization (TextVecto  (None, 50)                0          Y          \n rization)                                                                      \n embedding_1 (Embedding)       (None, 50, 16)             160016     Y          \n conv1d_1 (Conv1D)             (None, 48, 128)            6272       Y          \n global_max_pooling1d_1 (Globa  (None, 128)               0          Y          \n lMaxPooling1D)                                                                 \n dense_3 (Dense)               (None, 64)                 8256       Y          \n dense_2 (Dense)               (None, 1)                  65         Y          \n================================================================================\nTotal params: 174,609\nTrainable params: 174,609\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 11.13 Deep Learning: Training and Testing the model\n\nPython codeR code\n\n\n\n# Split data into train and test\ntrain_data = tokens[:4000]\ntest_data = tokens[4000:]\ntrain_labels = h.value[:4000]\ntest_labels = h.value[4000:]\n\n# Train model (note: remove 'verbose=0' to see progress)\nm.compile(loss=\"mean_absolute_error\", optimizer=RMSprop(learning_rate=0.004))\nlabels = np.asarray([[x] for x in train_labels])\nm.fit(train_data, labels, epochs=5, batch_size=128, verbose=0)\n\n<keras.callbacks.History object at 0x7f8911c38f40>\n\noutput = m.predict(test_data, verbose=0)\n\n# Bin output into -1, 0, 1\npred = [1 if x[0] > 0.3 else (0 if x[0] > -0.3 else -1) for x in output]\ncorrect = [x == y for (x, y) in zip(pred, test_labels)]\nacc = sum(correct) / len(pred)\nprint(f\"Accuracy: {acc}\")\n\nAccuracy: 0.5775193798449613\n\n\n\n\n\n# Split data into train and test\nd_train = d %>% slice_sample(n=4000)\nd_test = d %>% anti_join(d_train)\n\n# Train model\ncompile(model, loss = \"binary_crossentropy\",\n        optimizer = \"adam\", metrics = \"accuracy\")\nfit(model, d_train$lemmata, d_train$value,\n  epochs = 10, batch_size = 512, \n  validation_split = 0.2)\n# Validate against test data\neval=evaluate(model, d_test$lemmata, d_test$value)\nprint(glue(\"Accuracy: {eval['accuracy']}\"))\n\nAccuracy: 0.447028428316116\n\n\n\n\n\n\n\n\n\nFirst, Example 11.11 loads a dataset described by Van Atteveldt, Van der Velden, and Boukes (2021), which consists of Dutch economic news headlines with a sentiment value. Next, in Example 11.12 a model is defined consisting of several layers, corresponding roughly to ?fig-cnn. First, an embedding layer transforms the textual input into a semantic vector for each word. Next, the convolutional layer defines features (filters) over windows of vectors, which are then pooled in the max-pooling layer. This results in a vector of detected features for each document, which are then used in a regular (hidden) dense layer followed by an output layer.\nFinally, in Example 11.13 we train the model on 4000 documents and test it against the remaining documents. The Python model, which uses pre-trained word embeddings (the w2v_320d file downloaded at the top), achieves a mediocre accuracy of about 56% (probably due to the low number of training sentences). The R model, which trains the embedding layer as part of the model, performs more poorly at 44% as this model is even more dependent on large training data to properly estimate the embedding layer."
  },
  {
    "objectID": "content/chapter11.html#sec-unsupervised",
    "href": "content/chapter11.html#sec-unsupervised",
    "title": "11  Automatic analysis of text",
    "section": "11.5 Unsupervised Text Analysis: Topic Modeling",
    "text": "11.5 Unsupervised Text Analysis: Topic Modeling\nIn Section 7.3, we discussed how clustering techniques can be used to find patterns in data, such as which cases or respondents are most similar. Similarly, especially in survey research it is common to use factor analysis to discover (or confirm) variables that form a scale.\nIn essence, the idea behind these techniques is similar: by understanding the regularities in the data (which cases or variables behave similarly), you can describe the relevant information in the data with fewer data points. Moreover, assuming that the regularities capture interesting information and the deviations from these regularities are mostly uninteresting noise, these clusters of cases or variables can actually be substantively informative.\nSince a document-term matrix (DTM) is “just” a matrix, you can also apply these clustering techniques to the DTM to find groups of words or documents. You can therefore use any of the techniques we described in Chapter 7, and in particular clustering techniques such as \\(k\\)-means clustering (see Section 7.3) to group documents that use similar words together.\nIt can be very instructive to do this, and we encourage you to play around with such techniques. However, in recent years, a set of models called topic models have become especially popular for the unsupervised analysis of texts. Very much what like what you would do with other unsupervised techniques, also in topic modeling, you group words and documents into “topics”, consisting of words and documents that co-vary. If you see the word “agriculture” in a news article, there is a good chance you might find words such as “farm” or “cattle”, and there is a lower chance you will find a word like “soldier”. In other words, the words “agriculture” and “farm” generally occur in the same kind of documents, so they can be said to be part of the same topic. Similarly, two documents that share a lot of words are probably about the same topic, and if you know what topic a document is on (e.g., an agricultural topic), you are better able to guess what words might occur in that document (e.g., “cattle”).\nThus, we can formulate the goal of topic modeling as: given a corpus, find a set of \\(n\\) topics, consisting of specific words and/or documents, that minimize the mistakes we would make if we try to reconstruct the corpus from the topics. This is similar to regression where we try to find a line that minimizes the prediction error.\nIn early research on document clustering, a technique called Latent Semantic Analysis (LSA) essentially used a factor analysis technique called Singular Value Decomposition (see Section 7.3.3) on the DTM. This has yielded promising results in information retrieval (i.e., document search) and studying human memory and language use. However, it has a number of drawbacks including factor loadings that can be difficult to interpret substantively and is not a good way of dealing with words that can have multiple meanings (Landauer et al. 2013).\n\n11.5.1 Latent Dirichlet Allocation (LDA)\nThe most widely used technique for topic modeling is Latent Dirichlet Allocation (LDA, D. M. Blei, Ng, and Jordan 2003). Although the goal of LDA is the same as for clustering techniques, it starts from the other end with what is called a generative model. A generative model is a (simplified) formal model of how the data is assumed to have been generated. For example, if we would have a standard regression model predicting income based on age and education level, the implicit generative model is that to determine someone’s income, you take their age and education level, multiply them both by their regression parameters, and then add the intercept and some random error. Of course, we know that’s not actually how most companies determine wages, but it can be a useful starting point to analyze, e.g., labor market discrimination.\nThe generative model behind LDA works as follows. Assume that you are a journalist writing a 500 word news item. First, you would choose one or more topics to write about, for example 70% healthcare and 30% economy. Next, for each word in the item, you randomly pick one of these topics based on their respective weight. Finally, you pick a random word from the words associated with that topic, where again each word has a certain probability for that topic. For example, “hospital” might have a high probability for healthcare while “effectiveness” might have a lower probability but could still occur.\nAs said, we know (or at least strongly suspect) that this is not how journalists actually write their stories. However, this generative model helps understand the substantive interpretation of topics. Moreover, LDA is a mixture model, meaning it allows for each document to be about multiple topics, and for each word to occur in multiple topics. This matches with the fact that in many cases, our documents are in fact about multiple topics, from a news article about the economic effects of the COVID virus to an open survey answer containing multiple reasons for supporting a certain candidate. Additionally, since topic assignment is based on what other words occur in a document, the word “pupil” could be assigned either to a “biology” topic or to an “education” topic, depending on whether the document talks about eyes and lenses or about teachers and classrooms.\n\n\n\nFigure 11.1: Latent Dirichlet Allocation in ``Plate Model’’ notation (source: Blei et al, 2003)\n\n\nFigure 11.1 is a more formal notation of the same generative model. Starting from the left, for each document you pick a set of topics \\(\\Theta\\). This set of topics is drawn from a Dirichlet distribution which itself has a parameter \\(\\alpha\\) (see note). Next, for each word you select a single topic \\(z\\) from the topics in that document. Finally, you pick an actual word \\(w\\) from the words in that topic, again controlled by a parameter \\(\\beta\\).\nNow, if we know which words and documents are in which topics, we can start generating the documents in the corpus. In reality, of course, we have the reverse situation: we know the documents, and we want to know the topics. Thus, the task of LDA is to find the parameters that have the highest chance of generating these documents. Since only the word frequencies are observed, this is a latent variable model where we want to find the most likely values for the (latent) topic \\(z\\) for each word in each document.\nUnfortunately, there is no simple analytic solution to calculate these topic assignments like there is for OLS regression. Thus, like other more complicated statistical models such as multilevel regression, we need to use an iterative estimation that progressively optimizes the assignment to improve the fit until it converges.\nAn estimation method that is often used for LDA is Gibbs sampling. Simply put, this starts with a random assignment of topics to words. Then, in each iteration, it reconsiders each word and recomputes what likely topics for that word are given the other topics in that document and the topics in which that word occurs in other documents. Thus, if a document already contains a number of words placed in a certain topic, a new word is more likely to be placed in that topic as well. After enough iterations, this converges to a solution.\n\n\n\n\n\n\nThe Dirichlet Distribution and its Hyperparameters\n\n\n\n\n\nThe Dirichlet distribution can be seen as a distribution over multinomial distributions, that is, every draw from a Dirichlet distribution results in a multinomial distribution. An easy way to visualize this is to see the Dirichlet distribution as a bag of dice. You draw a die from the bag, and each die is a distribution over the numbers one to six.\nThis distribution is controlled by a parameter called alpha (\\(\\alpha\\)), which is often called a hyperparameter because it is a parameter that controls how other parameters (the actual topic distributions) are estimated, similar to, e.g., the learning speed in many machine learning models. This alpha hyperparameter controls what kind of dice there are in the bag. A high alpha means that the dice are generally fair, i.e., give a uniform multinomial distribution. For topic models, this means that documents will in general contain an even spread of multiple topics. A low alpha means that each die is unfair in the sense of having a strong preference for some number(s), as if these numbers are weighted. You can then draw a die that prefers ones, or a die that prefers sixes. For topic models this means that each document tends to have one or two dominant topics. Finally, alpha can be symmetric (meaning dice are unfair, but randomly, so in the end each topic has the same chance) or asymmetric (they are still unfair, and now also favor some topics more than others). This would correspond to some topics being more likely to occur in all documents.\nIn our experience, most documents actually do have one or two dominant topics, and some topics are actually more prevalent across many documents then others – especially if you consider that procedural words and boilerplate also need to be fit into a topic unless they are filtered out beforehand. Thus, we would generally recommend a relatively low and asymmetric alpha, and in fact gensim provides an algorithm to find, based on the data, an alpha that corresponds to this recommendation (by specifying alpha='auto'). In R, we would recommend picking a lower alpha than the default value, probably around \\(\\alpha=5/K\\), and optionally try using an asymmetric alpha if you find some words that occur across multiple topics.\nTo get a more intuitive understanding of the effects of alpha, please see cssbook.net/lda for additional material and visualizations.\n\n\n\n\n\n11.5.2 Fitting an LDA Model\n\n\n\n\n\n\n\nExample 11.14 LDA Topic Model of Obama’s State of the Union speeches.\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/sotu.csv\"\nsotu = pd.read_csv(url)\np_obama = sotu[sotu.President == \"Obama\"].text.str.split(\"\\n\\n\").explode()\n\ncv = CountVectorizer(min_df=0.01, stop_words=\"english\")\ndtm = cv.fit_transform(p_obama)\ndtm\ncorpus = matutils.Sparse2Corpus(dtm, documents_columns=False)\nvocab = dict(enumerate(cv.get_feature_names()))\nlda = LdaModel(\n    corpus, id2word=vocab, num_topics=10, random_state=123, alpha=\"asymmetric\"\n)\n\n\npd.DataFrame(\n    {\n        f\"Topic {n}\": [w for (w, tw) in words]\n        for (n, words) in lda.show_topics(formatted=False)\n    }\n)\n\n    Topic 0    Topic 1    Topic 2  ...    Topic 7     Topic 8     Topic 9\n0   america   american       year  ...    america     america         new\n1  american       jobs  americans  ...  americans         tax        jobs\n2    people      years       know  ...       hard        work        just\n3       new       like     people  ...     people      states        help\n4     right        let        new  ...      years   americans     country\n5     world         ve   american  ...     nation  businesses    families\n6      help  americans      years  ...       work         god        make\n7   economy     people       like  ...         ve       bless       world\n8       let       year         ve  ...      world      united  government\n9      know       home       make  ...       time      people        fact\n\n[10 rows x 10 columns]\n\n\n\n\n\nurl = \"https://cssbook.net/d/sotu.csv\"\nsotu = read_csv(url) \np_obama = sotu %>% \n  filter(President == \"Obama\") %>% \n  corpus() %>% \n  corpus_reshape(\"paragraphs\")\ndfm = p_obama %>% \n  tokens(remove_punct=T) %>%\n  dfm() %>% \n  dfm_remove(stopwords(\"english\")) %>%\n  dfm_trim(min_docfreq=.01,docfreq_type = \"prop\")\ndfm\n\nDocument-feature matrix of: 738 documents, 746 features (97.02% sparse) and 6 docvars.\n         features\ndocs      speaker mr vice president members congress first united around come\n  text1.1       1  1    1         1       1        1     1      1      1    0\n  text1.2       0  0    0         0       0        0     0      0      0    1\n  text1.3       0  0    0         0       0        0     0      0      0    0\n  text1.4       0  0    0         0       0        0     0      1      0    0\n  text1.5       0  0    0         0       0        0     0      0      0    0\n  text1.6       0  0    0         0       0        0     0      0      0    0\n[ reached max_ndoc ... 732 more documents, reached max_nfeat ... 736 more features ]\n\nlda = dfm %>% \n  convert(to = \"topicmodels\") %>%\n  LDA(k=10,control=list(seed=123, alpha = 1/1:10))\n\nterms(lda, 10)\n\n      Topic 1     Topic 2     Topic 3     Topic 4       Topic 5     Topic 6   \n [1,] \"college\"   \"care\"      \"us\"        \"can\"         \"years\"     \"energy\"  \n [2,] \"new\"       \"health\"    \"nation\"    \"make\"        \"first\"     \"change\"  \n [3,] \"workers\"   \"still\"     \"people\"    \"congress\"    \"economy\"   \"new\"     \n [4,] \"education\" \"families\"  \"can\"       \"republicans\" \"back\"      \"clean\"   \n [5,] \"help\"      \"americans\" \"states\"    \"democrats\"   \"time\"      \"world\"   \n [6,] \"job\"       \"like\"      \"one\"       \"work\"        \"home\"      \"power\"   \n [7,] \"every\"     \"new\"       \"america\"   \"take\"        \"crisis\"    \"america\" \n [8,] \"kids\"      \"need\"      \"together\"  \"pay\"         \"financial\" \"can\"     \n [9,] \"small\"     \"must\"      \"united\"    \"cuts\"        \"two\"       \"economy\" \n[10,] \"schools\"   \"job\"       \"president\" \"banks\"       \"plan\"      \"research\"\n      Topic 7      Topic 8    Topic 9    Topic 10    \n [1,] \"people\"     \"country\"  \"world\"    \"jobs\"      \n [2,] \"get\"        \"time\"     \"american\" \"year\"      \n [3,] \"day\"        \"future\"   \"war\"      \"years\"     \n [4,] \"now\"        \"american\" \"security\" \"new\"       \n [5,] \"know\"       \"america\"  \"people\"   \"last\"      \n [6,] \"see\"        \"people\"   \"troops\"   \"tax\"       \n [7,] \"tax\"        \"done\"     \"us\"       \"million\"   \n [8,] \"americans\"  \"work\"     \"america\"  \"$\"         \n [9,] \"government\" \"get\"      \"new\"      \"businesses\"\n[10,] \"american\"   \"now\"      \"nations\"  \"america\"   \n\n\n\n\n\n\n\n\n\nExample 11.14 shows how you can fit an LDA model in Python or R. As example data, we use Obama’s State of the Union Speeches using the corpus introduced in Chapter 10. Since such a speech generally touches on many different topics, we choose to first split by paragraph as these will be more semantically coherent (for Obama, at least). In R, we use the corpus_reshape function to split the paragraphs, while in Python we use pandas’ str.split, which creates a list or paragraphs for each text, which we then convert into a paragraph per row using explode. Converting this to a DTM we get a reasonably sized matrix of 738 paragraphs and 746 unique words.\nNext, we fit the actual LDA model using the package gensim (Python) and topicmodels (R). Before we can do this, we need to convert the DTM format into a format accepted by that package. For Python, this is done using the Sparse2Corpus helper function while in R this is done with the quanteda convert function. Then, we fit the model, asking for 10 topics to be identified in these paragraphs. There are three things to note in this line. First, we specify a random seed of 123 to make sure the analysis is replicable. Second, we specify an “asymmetric” of 1/1:10, meaning the first topic has alpha 1, the second 0.5, etc. (in R). In Python, instead of using the default of alpha='symmetric', we set alpha='asymmetric', which uses the formula \\(\\frac{1}{topic\\_index + \\sqrt{num\\_topics}}\\) to determine the priors. At the cost of a longer estimation time, we can even specify alpha='auto', which will learn an asymmetric prior from the data. See the note on hyperparameters for more information. Third, for Python we also need to specify the vocabulary names since these are not included in the DTM.\nThe final line generates a data frame of top words per topic for first inspection (which in Python requires separating the words from their weights in a list comprehension and converting it to a data frame for easy viewing). As you can see, most topics are interpretable and somewhat coherent: for example, topic 1 seems to be about education and jobs, while topic 2 is health care. You also see that the word “job” occurs in multiple topics (presumably because unemployment was a pervasive concern during Obama’s tenure). Also, some topics like topic 3 are relatively more difficult to interpret from this table. A possible reason for this is that not every paragraph actually has policy content. For example, the first paragraph of his first State of the Union was: Madam Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States – she’s around here somewhere. None of these words really fit a “topic” in the normal meaning of that term, but all of these words need to be assigned a topic in LDA. Thus, you often see “procedural” or “boilerplate” topics such as topic 3 occurring in LDA outputs.\nFinally, note that we showed the R results here. As gensim uses a different estimation algorithm (and scikit-learnuses a different tokenizer and stopword list), results will not be identical, but should be mostly similar.\n\n\n11.5.3 Analyzing Topic Model Results\n\n\n\n\n\n\n\nExample 11.15 Analyzing and inspecting LDA results.\n\nPython codeR code\n\n\n\ntopics = pd.DataFrame(\n    [\n        dict(lda.get_document_topics(doc, minimum_probability=0.0))\n        for doc in corpus\n    ]\n)\nmeta = sotu.iloc[p_obama.index].drop(columns=[\"text\"]).reset_index(drop=True)\ntpd = pd.concat([meta, topics], axis=1)\ntpd.head()\n\n  FirstName President        Date  ...         7         8         9\n0    Barack     Obama  2009-02-24  ...  0.007036  0.006406  0.005879\n1    Barack     Obama  2009-02-24  ...  0.005277  0.004805  0.004410\n2    Barack     Obama  2009-02-24  ...  0.002436  0.002218  0.002035\n3    Barack     Obama  2009-02-24  ...  0.004222  0.003844  0.003528\n4    Barack     Obama  2009-02-24  ...  0.002639  0.002402  0.002205\n\n[5 rows x 16 columns]\n\nfor docid in [622, 11, 322]:\n    print(f\"{docid}: {list(p_obama)[docid]}\")\n\n622: I intend to protect a free and open Internet, extend its reach to ever...\n11: Because of this plan, there are teachers who can now keep their jobs an...\n322: I want every American looking for work to have the same opportunity as...\n\n\n\n\n\ntopics = posterior(lda)$topics %>% \n  as_tibble() %>% \n  rename_all(~paste0(\"Topic_\", .))\nmeta = docvars(p_obama) %>% \n  select(President:Date) %>%\n  add_column(doc_id=docnames(p_obama),.before=1)\ntpd = bind_cols(meta, topics) \nhead(tpd)\n\n   doc_id President       Date     Topic_1     Topic_2     Topic_3     Topic_4\n1 text1.1     Obama 2009-02-24 0.013920249 0.013920249 0.681016299 0.013958435\n2 text1.2     Obama 2009-02-24 0.010817618 0.529660710 0.383715945 0.010826304\n3 text1.3     Obama 2009-02-24 0.225158932 0.004360997 0.004365404 0.004358067\n4 text1.4     Obama 2009-02-24 0.008349675 0.008345439 0.439462311 0.008356367\n5 text1.5     Obama 2009-02-24 0.089106811 0.098659225 0.330131825 0.004954681\n6 text1.6     Obama 2009-02-24 0.012717996 0.012718835 0.506051395 0.012766016\n      Topic_5     Topic_6     Topic_7     Topic_8     Topic_9    Topic_10\n1 0.109552840 0.013940244 0.013928915 0.013938857 0.111895624 0.013928289\n2 0.010817139 0.010816612 0.010825601 0.010849458 0.010849549 0.010821065\n3 0.387834833 0.004366069 0.356468915 0.004375268 0.004354753 0.004356762\n4 0.008367663 0.008355544 0.008349396 0.493717715 0.008349213 0.008346677\n5 0.060645762 0.004977742 0.060033550 0.185271671 0.112512468 0.053706265\n6 0.392025793 0.012724883 0.012792718 0.012739463 0.012741326 0.012721575\n\nfor (id in c(\"text7.73\", \"text5.1\", \"text2.12\")) {\n  text = as.character(p_obama)[id]\n  print(glue(\"{id}: {text}\"))\n}\n\n[1] \"text7.73: So the question for those of us here tonight is how we, all ...\"\n[1] \"text5.1: Mr. Speaker, Mr. Vice President, members of Congress, fellow ...\"\n[1] \"text2.12: And tonight, I'd like to talk about how together, we can del...\"\n\n\n\n\n\n\n\n\n\nExample 11.15 shows how you can combine the LDA results (topics per document) with the original document metadata. This could be your starting point for substantive analyses of the results, for example to investigate relations between topics or between, e.g., time or partisanship and topic use.\nYou can also use this to find specific documents for reading. For example, we noted above that topic 3 is difficult to interpret. As you can see in the table in Example 11.15 (which is sorted by value of topic 3), most of the high scoring documents are the first paragraph in each speech, which do indeed contain the “Madam speaker” boilerplate noted above. The other three documents are all calls for bipartisanship and support. As you can see from this example, carefully inspecting the top documents for each topic is very helpful for making sense of the results.\n\n\n11.5.4 Validating and Inspecting Topic Models\nAs we saw in the previous subsection, running a topic model is relatively easy. However, that doesn’t mean that the resulting topic model will always be useful. As with all text analysis techniques, validation is the key to good analysis: are you measuring what you want to measure? And how do you know?\nFor topic modeling (and arguably for all text analysis), the first step after fitting a model is inspecting the results and establishing face validity. Top words per topic such as those listed above are a good place to start, but we would really encourage you to also look at the top documents per topic to better understand how words are used in context. Also, it is good to inspect the relationships between topics and look at documents that load high on multiple topics to understand the relationship.\nIf the only goal is to get an explorative understanding of the corpus, for example as a first step before doing a dictionary analysis or manual coding, just face validity is probably sufficient. For a more formal validation, however, it depends on the reason for using topic modeling.\nIf you are using topic modeling in a true unsupervised sense, i.e., without a predefined analytic schema in mind, it is difficult to assess whether the model measures what you want to measure – because the whole point is that you don’t know what you want to measure. That said, however, you can have the general criteria that the model needs to achieve coherence and interpretability, meaning that words and documents that share a topic are also similar semantically.\nIn their excellent paper on the topic, Chang et al. (2009) propose two formal tasks to judge this using manual (or crowd) coding: in word intrusion, a coder is asked to pick the “odd one out” from a list where one other word is mixed in a group of topic words. In topic intrusion, the coder is presented with a document and a set of topics that occur in the document, and is asked to spot the one topic that was not present according to the model. In both tasks, if the coder is unable to identify the intruding word or topic, apparently the model does not fit our intuitive notion of “aboutness” or semantic similarity. Perhaps their most interesting finding is that goodness-of-fit measures like perplexity1 are actually not good predictors of the interpretability of the resulting models.\nIf you are using topic models in a more confirmatory manner, that is, if you wish the topics to match some sort of predefined categorization, you should use regular gold standard techniques for validation: code a sufficiently large random sample of documents with your predefined categories, and test whether the LDA topics match those categories. In general, however, in such cases it is a better idea to use a dictionary or supervised analysis technique as topic models often do not exactly capture our categories. After all, unsupervised techniques mainly excel in bottom-up and explorative analyses (Section 11.1).\n\n\n11.5.5 Beyond LDA\nThis chapter focused on regular or “vanilla” LDA topic modeling. Since the seminal publication, however, a large amount of variations and extensions on LDA have been proposed. These include, for example, Dynamic Topic Models (which incorporate time; D. M. Blei and Lafferty (2006)) and Correlated Topic Models (which explicitly model correlation between topics; D. Blei and Lafferty (2006)). Although it is beyond the scope of this book to describe these models in detail, the interested reader is encouraged to learn more about these models.\nEspecially noteworthy are Structural Topic Models (R package stm; Roberts et al. (2014)), which allow you to model covariates as topic or word predictors. This allows you, for example, to model topic shifts over time or different words for the same topic based on, e.g., Republican or Democrat presidents.\nPython users should check out Hierarchical Topic Modeling (Griffiths et al. 2004). In hierarchical topic modeling, rather than the researcher specifying a fixed number of topics, the model returns a hierarchy of topics from few general topics to a large number of specific topics, allowing for a more flexible exploration and analysis of the data.\n\n\n\n\n\n\nHow many topics?\n\n\n\n\n\nWith topic modeling, the most important researcher choices are the number of topics and the value of alpha. These choices are called hyperparameters, since they determine how the model parameters (e.g. words per topic) are found.\nThere is no good theoretical solution to determine the “right” number of topics for a given corpus and research question. Thus, a sensible approach can be to ask the computer to try many models, and see which works best. Unfortunately, because this is an unsupervised (inductive) method, there is no single metric that determines how good a topic model is.\nThere are a number of such metrics proposed in the literature, of which we will introduce two. Perplexity is a score of how well the LDA model can fit (predict) the actual word distribution (or in other words: how “perplexed” the model is seeing the corpus). Coherence is a measure of how semantically coherent the topics are by checking how often the top token co-occurs in documents in each topic (Mimno et al. 2011).\nThe code below shows how these can be calculated for a range of topic counts, and the same code could be used for trying different values of alpha. For both measures, lower values are better, and both essentially keep decreasing as you add more topics. What you are looking for is the inflection point (or “elbow point”) where it goes from a steep decrease to a more gradual decrease. For coherence, this seems to be at 10 topics, while for perplexity this is at 20 topics.\nThere are two very important caveats to make here, however. First, these metrics are no substitute for human validation and the best model according to these metrics is not always the most interpretable or coherent model. In our experience, most metrics give a higher topic count that would be optimal from an interpretability perspective, but of course that also depends on how we operationalize interpretability. Nonetheless, these topic numbers are probably more indicative of a range of counts that should be inspected manually, rather than giving a definitive answer.\nSecond, the code below was written so it is easy to understand and quick to run. For real use in a research project, it is advised to include a broader range of topic counts and also vary the \\(\\alpha\\). Moreover, it is smart to run each count multiple times so you get an indication of the variance as well as a single point (it is quite likely that the local minimum for coherence at \\(k=10\\) is an outlier that will disappear if more runs are averaged). Finally, especially for a goodness-of-fit measure like perplexity it is better to split the data into a training and test set (see Section 11.4.1 for more details).\n\nPython codeR code\n\n\n\nresult = []\nfor k in [5, 10, 15, 20, 25, 30]:\n    m = LdaModel(\n        corpus,\n        num_topics=k,\n        id2word=vocab,\n        random_state=123,\n        alpha=\"asymmetric\",\n    )\n    perplexity = m.log_perplexity(corpus)\n    coherence = CoherenceModel(\n        model=m, corpus=corpus, coherence=\"u_mass\"\n    ).get_coherence()\n    result.append(dict(k=k, perplexity=perplexity, coherence=coherence))\n\nresult = pd.DataFrame(result)\nresult.plot(x=\"k\", y=[\"perplexity\", \"coherence\"])\nplt.show()\n\n\n\n\n\n\n\nresults = list()\ndtm = convert(dfm, to=\"topicmodels\")\nfor (k in c(5, 10, 15, 20, 25, 30)) {\n    alpha = 1/((1:k)+sqrt(k))\n    lda = LDA(dtm,k=k,control=list(seed=99,alpha=alpha))\n    results[[as.character(k)]] = data.frame(\n      perplexity=perplexity(lda),\n      coherence=mean(topic_coherence(lda,dtm)))\n}\nbind_rows(results, .id=\"k\") %>% \n  mutate(k=as.numeric(k)) %>%\n  pivot_longer(-k) %>% \n  ggplot() + \n  geom_line(aes(x=k, y=value)) + \n  xlab(\"Number of topics\") + \n  facet_grid(name ~ ., scales=\"free\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlei, David M, and John D Lafferty. 2006. “Dynamic Topic Models.” In Proceedings of the 23rd International Conference on Machine Learning, 113–20.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022.\n\n\nBlei, David, and John Lafferty. 2006. “Correlated Topic Models.” Advances in Neural Information Processing Systems 18: 147.\n\n\nBoukes, Mark, Bob van de Velde, Theo Araujo, and Rens Vliegenthart. 2019. “What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and Agreement Between Off-the-Shelf Sentiment Analysis Tools.” Communication Methods and Measures 00 (00): 1–22. https://doi.org/10.1080/19312458.2019.1671966.\n\n\nBoumans, Jelle W., and Damian Trilling. 2016. “Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars.” Digital Journalism 4 (1): 8–23. https://doi.org/10.1080/21670811.2015.1096598.\n\n\nBurscher, Björn, Daan Odijk, Rens Vliegenthart, Maarten de Rijke, and Claes H. de Vreese. 2014. “Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis.” Communication Methods and Measures 8 (3): 190–206. https://doi.org/10.1080/19312458.2014.937527.\n\n\nChan, Chung-hong, Joseph Bajjalieh, Loretta Auvil, Hartmut Wessler, Scott Althaus, Kasper Welbers, Wouter van Atteveldt, and Marc Jungblut. in press. “Four Best Practices for Measuring News Sentiment Using ‘Off-the-Shelf’ Dictionaries: A Large-Scale p-Hacking Experiment.” Computational Communication Research, in press.\n\n\nChang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. 2009. “Reading Tea Leaves: How Humans Interpret Topic Models.” In Advances in Neural Information Processing Systems, 288–96.\n\n\nDe Smedt, Tom, W Daelemans, and Tom De Smedt. 2012. “Pattern for Python.” The Journal of Machine Learning Research 13: 2063–67. http://dl.acm.org/citation.cfm?id=2343710.\n\n\nGoldberg, Yoav. 2017. Neural Network Models for Natural Language Processing. Morgan & Claypool.\n\n\nGonzalez-Bailon, S., and G. Paltoglou. 2015. “Signals of Public Opinion in Online Communication: A Comparison of Methods and Data Sources.” The ANNALS of the American Academy of Political and Social Science 659 (1): 95–107. https://doi.org/10.1177/0002716215569192.\n\n\nGriffiths, Thomas L, Michael I Jordan, Joshua B Tenenbaum, and David M Blei. 2004. “Hierarchical Topic Models and the Nested Chinese Restaurant Process.” In Advances in Neural Information Processing Systems, 17–24.\n\n\nGrimmer, J., and B. M. Stewart. 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.” Political Analysis 21 (3): 267–97. https://doi.org/10.1093/pan/mps028.\n\n\nHutto, Clayton J, and Eric Gilbert. 2014. “Vader: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text.” In Eighth International AAAI Conference on Weblogs and Social Media.\n\n\nKrippendorff, Klaus. 2004. Content Analysis: An Introduction to Its Methodology. 2nd ed. Thousand Oaks, CA: SAGE.\n\n\nLandauer, Thomas K, Danielle S McNamara, Simon Dennis, and Walter Kintsch. 2013. Handbook of Latent Semantic Analysis. Psychology Press.\n\n\nMaas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. “Learning Word Vectors for Sentiment Analysis.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 142–50. Portland, Oregon, USA: Association for Computational Linguistics. http://www.aclweb.org/anthology/P11-1015.\n\n\nMimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. “Optimizing Semantic Coherence in Topic Models.” In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 262–72.\n\n\nReagan, Andrew J., Christopher M. Danforth, Brian Tivnan, Jake Ryland Williams, and Peter Sheridan Dodds. 2017. “Sentiment analysis methods for understanding large-scale texts: a case for using continuum-scored words and word shift graphs.” EPJ Data Science 6 (1). https://doi.org/10.1140/epjds/s13688-017-0121-9.\n\n\nRiffe, Daniel, Stephen Lacy, Brendan R. Watson, and Frederick Fico. 2019. Analyzing Media Messages: Using Quantitative Content Analysis in Research. 4th ed. New York, NY: Routledge.\n\n\nRoberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G Rand. 2014. “Structural Topic Models for Open-Ended Survey Responses.” American Journal of Political Science 58 (4): 1064–82.\n\n\nScharkow, Michael. 2011. “Thematic content analysis using supervised machine learning: An empirical evaluation using German online news.” Quality & Quantity 47 (2): 761–73. https://doi.org/10.1007/s11135-011-9545-7.\n\n\nThelwall, Mike, Kevan Buckley, and Georgios Paltoglou. 2012. “Sentiment Strength Detection for the Social Web.” Journal of the American Society for Information Science and Technology 63 (1): 163–73. https://doi.org/10.1002/asi.21662.\n\n\nTulkens, Stéphan, Lisa Hilte, Elise Lodewyckx, Ben Verhoeven, and Walter Daelemans. 2016. “A Dictionary-based Approach to Racism Detection in Dutch Social Media.” Proceedings of the Workshop on Text Analytics for Cybersecurity and Online Safety (TA-COS 2016), 11–17. http://www.clips.ua.ac.be/bibliography/a-dictionary-based-approach-to-racism-detection-in-dutch-social-media.\n\n\nVan Atteveldt, Wouter, Mariken ACG Van der Velden, and Mark Boukes. 2021. “The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms.” Communication Methods and Measures 15 (2): 121–40.\n\n\nVermeer, Susan A. M., Theo Araujo, Stefan F. Bernritter, and Guda van Noort. 2019. “Seeing the wood for the trees: How machine learning can help firms in identifying relevant electronic word-of-mouth in social media.” International Journal of Research in Marketing 36 (3): 492–508. https://doi.org/10.1016/j.ijresmar.2019.01.010."
  },
  {
    "objectID": "content/chapter12.html#sec-apis",
    "href": "content/chapter12.html#sec-apis",
    "title": "12  Scraping online data",
    "section": "12.1 Using Web APIs: From Open Resources to Twitter",
    "text": "12.1 Using Web APIs: From Open Resources to Twitter\nLet’s assume we want to retrieve data from some online service. This could be some social media platform, but could also be a government website, some open data platform or initiative, or sometimes a commercial organization that provides some online service. Of course, we could surf to their website, enter a search query, and somehow save the result. This would result in a lot of impracticalities, though. Most notably, websites are designed such that they are perfectly readable and understandable for humans, but the cues that are used often have no “meaning” for a computer program. As humans, we have no problem understanding which parts of a web page refer to the author of some item on a web page, what the numbers “2006” and “2008” mean, and on. But it is not trivial to think of a way to explain to a computer program how to identify variables like author, title, or year on a web page. We will learn how to do exactly that in Section 12.2. Writing such a parser is often necessary, but it is also error-prone and a detour, as we are trying to bring some information that has been optimized for human reading back to a more structured data structure.\nLuckily, however, many online services not only have web interfaces optimized for human reading, but also offer another possibility to access the data they provide: an API (Application Programming Interface). The vast amount of contemporary web APIs work like this: you send a request to some URL, and you get back a JSON object. As you learned in Section 5.2, JSON is a nested data structure, very much like a Python dictionary or R named list (and, in fact, JSON data are typically represented as such in Python and R). In other words: APIs directly gives us machine-readable data that we can work with without any need to develop a custom parser.\nDiscussing specific APIs in a book can be a bit tricky, as there is a chance that it will be outdated: after all, the API provider may change it at any time. We therefore decided not to include a chapter on very specific applications such as “How to use the Twitter API” or similar – given the popularity of such APIs, a quick online search will produce enough up-to-date (and out-of-date) tutorials on these. Instead, we discuss the generic principles of APIs that should easily translate to examples other than ours.\nIn its simplest form, using an API is nothing more than visiting a specific URL. The first part of the URL specifies the so-called API endpoint: the address of the specific API you want to use. This address is then followed by a ? and one or more key-value pairs with an equal sign like this: key=value. Multiple key-value pairs are separated with a \\&.\nFor instance, at the time of the writing of this book, Google offers an API endpoint, https://www.googleapis.com/books/v1/volumes, to search for books on Google Books. If you want to search for books about Python, you can supply a key q (which stands for query) with the value “python” (Example 12.1). We do not need any specific software for this – we could, in fact, use a web browser as well. Popular packages that allow us to do it programatically are httr in combination with jsonlite (R) and requests (Python).\nBut how do we know which parameters (i.e., which key-value pairs) we can use? We need to look it up in the documentation of the API we are interested in (in this example developers.google.com/books/docs/v1/using). There is no other way of knowing that the key to submit a query is called q, and which other parameters can be specified.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn our example, we used a simple value to include in the request: the string “python”. But what if we want to submit a string that contains, let’s say, a space, or a character like & or ? which, as we have seen, have a special meaning in the request? In these cases, you need to “encode” your URL using a mechanism called URL encoding or percent encoding. You may have seen this earlier: a space, for instance, is represented by \\%20\n\n\n\n\n\n\n\n\n\n\nExample 12.1 Retrieving JSON data from the Google Books API.\n\nPython codeR code\n\n\n\nr = requests.get(\"https://www.googleapis.com//books/v1/volumes?q=python\")\ndata = r.json()\nprint(data.keys())  # \"items\" seems most promising\n\ndict_keys(['kind', 'totalItems', 'items'])\n\npprint(data[\"items\"][0])  # let's print the 1st one\n\n{\n  \"kind\": \"books#volume\",\n  \"id\": \"RQ6xDwAAQBAJ\",\n  \"etag\": \"4Qy7pYqxKdU\",\n  \"selfLink\": \"https://www.googleapis.com/books/v1/volumes/RQ6xDwAAQBAJ\",\n  \"volumeInfo\": {\n    \"title\": \"Automate the Boring Stuff with Python, 2nd Edition\",\n    \"subtitle\": \"Practical Programming for Total Beginners\",\n    \"authors\": [\n      \"Al Sweigart\"\n...\n\n\n\n\n\nurl = str_c(\"https://www.googleapis.com/books/v1/volumes?q=python\")\nr = GET(url)\ndata = content(r, as=\"parsed\")\nprint(names(data))\n\n[1] \"kind\"       \"totalItems\" \"items\"     \n\nprint(data$items[[1]])\n\n$kind\n[1] \"books#volume\"\n\n$id\n[1] \"RQ6xDwAAQBAJ\"\n\n$etag\n[1] \"d8YbgQl37Uc\"\n\n...\n\n\n\n\n\n\n\n\n\nThe data our request returns are nested data, and hence, they do not really “fit” in a tabular data frame. We could keep the data as they are (and then, for instance, just extract the key-value pairs that we are interested in), but – for the sake of getting a quick overview – let’s flatten the data so that they can be represented in a data frame (Example 12.2). This works quite well here, but may be more problematic when the items have a widely varying structure. If that is the case, we probably would want to write a loop to iterate over the different items and extract the information we are interested in.\n\n\n\n\n\n\n\nExample 12.2 Transforming the data into a data frame.\n\nPython codeR code\n\n\n\nd = json_normalize(data[\"items\"])\nd.head()\n\n           kind  ...                        accessInfo.pdf.acsTokenLink\n0  books#volume  ...                                                NaN\n1  books#volume  ...  http://books.google.com/books/download/Python_...\n2  books#volume  ...  http://books.google.com/books/download/Effecti...\n3  books#volume  ...  http://books.google.com/books/download/Python_...\n4  books#volume  ...                                                NaN\n\n[5 rows x 50 columns]\n\n\n\n\n\nr_text = content(r, \"text\")\n#| cache: true\ndata_json = fromJSON(r_text, flatten=T)\nd = as_tibble(data_json)\nhead(d)\n\n# A tibble: 6 × 3\n  kind          totalItems items$k…¹ $id   $etag $self…² $volu…³ $volu…⁴ $volu…⁵\n  <chr>              <int> <chr>     <chr> <chr> <chr>   <chr>   <chr>   <list> \n1 books#volumes       1566 books#vo… RQ6x… d8Yb… https:… Automa… Practi… <chr>  \n2 books#volumes       1566 books#vo… Lqma… +5ZE… https:… Python… <NA>    <chr>  \n3 books#volumes       1566 books#vo… bTUF… 113u… https:… Effect… 59 Spe… <chr>  \n4 books#volumes       1566 books#vo… aJQI… FDE+… https:… Python… An Int… <chr>  \n5 books#volumes       1566 books#vo… H9em… zkaN… https:… Progra… A Comp… <chr>  \n6 books#volumes       1566 books#vo… Chr1… Ato+… https:… Python… <NA>    <chr>  \n# … with 43 more variables: items$volumeInfo.publisher <chr>,\n#   $volumeInfo.publishedDate <chr>, $volumeInfo.description <chr>,\n#   $volumeInfo.industryIdentifiers <list>, $volumeInfo.pageCount <int>,\n#   $volumeInfo.printType <chr>, $volumeInfo.categories <list>,\n#   $volumeInfo.averageRating <dbl>, $volumeInfo.ratingsCount <int>,\n#   $volumeInfo.maturityRating <chr>, $volumeInfo.allowAnonLogging <lgl>,\n#   $volumeInfo.contentVersion <chr>, $volumeInfo.language <chr>, …\n\n\n\n\n\n\n\n\n\nYou may have realized that you did not get all results. This protects you from accidentally downloading a huge dataset (you may have underestimated the number of Python books available on the market), and saves the provider of the API a lot of bandwidth. This does not mean that you cannot get more data. In fact, many APIs work with pagination: you first get the first “page” of results, then the next, and so on. Sometimes, the API response contains a specific key-value pair (sometimes called a “continuation key”) that you can use to get the next results; sometimes, you can just say at which result you want to start (say, result number 11) and then get the next “page”. You can then write a loop to retrieve as many results as you need (Example 12.3) – just make sure that you do not get stuck in an eternal loop. When you start playing around with APIs, make sure you do not cause unnecessary traffic, but limit the number of calls that are made (see also Section 12.4).\n\n\n\n\n\n\n\nExample 12.3 Full script including pagination.\n\nPython codeR code\n\n\n\nallitems = []\ni = 0\nwhile True:\n    r = requests.get(\n        \"https://www.googleapis.com/\"\n        \"books/v1/volumes?q=python&maxResults=\"\n        f\"40&startIndex={i}\"\n    )\n    data = r.json()\n    if not \"items\" in data:\n        print(f\"Retrieved {len(allitems)},\" \"it seems like that's it\")\n        break\n    allitems.extend(data[\"items\"])\n    i += 40\n\nRetrieved 584,it seems like that's it\n\nd = json_normalize(allitems)\n\n\n\n\ni = 0\nj = 1\nurl = str_c(\"https://www.googleapis.com/books/\",\n            \"v1/volumes?q=python&maxResults=40\",\n            \"&startIndex={i}\")\nalldata = list()\nwhile (TRUE) {\n    r = GET(glue(url))\n    r_text = content(r, \"text\")\n    data_json = fromJSON(r_text, flatten=T)\n    if (length(data_json$items)==0) {break}\n    alldata[[j]] = as.data.frame(data_json)\n    i = i + 40\n    j = j + 1} \nd = rbindlist(alldata, fill=TRUE)\n\n\n\n\n\n\n\n\nMany APIs work very much like the example we discussed, and you can adapt the logic above to many APIs once you have read their documentation. You would usually start by playing around with single requests, and then try to automate the process by means of a loop.\nHowever, many APIs have restrictions regarding who can use them, how many requests can be made, and so on. For instance, you may need to limit the number of requests per minute by calling a sleep function within your loop to delay the execution of the next call. Or, you may need to authenticate yourself. In the example of the Google Books API, this will allow you to request more data (such as whether you own an (electronic) copy of the books you retrieved). In this case, the documentation outlines that you can simply pass an authentication token as a parameter with the URL. However, many APIs use more advanced authentication methods such as OAuth (see Section 12.3).\nLastly, for many APIs that are very popular with social scientists, specific wrapper packages exist (such as tweepy (Python) or rtweet (R) for downloading twitter messages) which are a bit more user-friendly and handle things like authentication, pagination, respecting rate-limits, etc., for you."
  },
  {
    "objectID": "content/chapter12.html#sec-webpages",
    "href": "content/chapter12.html#sec-webpages",
    "title": "12  Scraping online data",
    "section": "12.2 Retrieving and Parsing Web Pages",
    "text": "12.2 Retrieving and Parsing Web Pages\nUnfortunately, not all online services we may be interested in offer an API – in fact, it has even been suggested that computational researchers have arrived in an “post-API age” (Freelon 2018), as API access for researchers has become increasingly restricted.\nIf data cannot be collected using an API (or a similar service, such as RSS feeds), we need to resort to web scraping. Before you start a web scraping project, make sure to ask the appropriate authorities for ethical and legal advice (see also Section 12.4).\nWeb scraping (sometimes also referred to as harvesting), in essence, boils down to automatically downloading web pages aimed at a human audience, and extracting meaningful information out of them. One could also say that we are reverse-engineering the way the information was published on the web. For instance, a news site may always use a specific formatting to denote the title of an article – and we would then use this to extract the title. This process is called “parsing”, which in this context is just a fancy term for “extracting meaningful information”.\nWhen scraping data from the web, we can distinguish two different tasks: (1) downloading a (possibly large) number of webpages, and (2) parsing the content of the webpages. Often, both go hand in hand. For instance, the URL of the next page to be downloaded might actually be parsed from the content of the current page; or some overview page may contain the links and thus has to be parsed first in order to download subsequent pages.\nWe will first discuss how to parse a single HTML page (say, the page containing one specific product review, or one specific news article), and then describe how to “scale up” and repeat the process in a loop (to scrape, let’s say, all reviews for the product; or all articles in a specific time frame).\n\n12.2.1 Retrieving and Parsing an HTML Page\nIn order to parse an HTML file, you need to have a basic understanding of the structure of an HTML file. Open your web browser, visit a website of your choice (we suggest to use a simple page, such as css-book.net/d/restaurants/index.html), and inspect its underlying HTML code (almost all browsers have a function called something like “view source”, which enables you to do so).\nYou will see that there are some regular patterns in there. For example, you may see that each paragraph is enclosed with the tags <p> and </p>. Thinking back to Section 9.2, you may figure out that you could, for instance, use a regular expression to extract the text of the first paragraph. In fact, packages like beautifulsoup under the hood use regular expressions to do exactly that.\nWriting your own set of regular expressions to parse an HTML page is usually not a good idea (but it can be a last resort when everything else fails). Chances are high that you will make a mistake or not handle some edge case correctly; and besides, it would be a bit like re-inventing the wheel. Packages like rvest (R), beautifulsoup, and lxml (both Python) already do this for you.\nIn order to use them, though, you need to have a basic understanding of what an HTML page looks like. Here is a simplified example:\n<html>\n<body>\n<h1>This is a title</h1>\n<div id=\"main\">\n<p> Some text with one <a href=\"test.html\">link </a> </p>\n<img src = \"plaatje.jpg\">an image </img>\n</div>\n<div id=\"body\">\n<p class=\"lead\"> Some more text </p>\n<p> Even more... </p>\n<p> And more. </p>\n</div>\n</body>\n</html>\nFor now, it is not too important to understand the function of each specific tag (although it might help, for instance, to realize that a denotes a link, h1 a first-level heading, p a paragraph and div some kind of section).\nWhat is important, though, is to realize that each tag is opened and closed (e.g., <p> is closed by </p>). Because tags can be nested, we can actually draw the code as a tree. In our example, this would look like this:\n\n\nhtml\n\n\n\nbody\n\n\n\nh1\n\n\ndiv#main\n\n\n\np\n\n\n\na\n\n\n\nimg\n\n\n\ndiv\n\n\n\np.lead\n\n\np\n\n\np\n\n\n\n\n\nAdditionally, tags can have attributes. For instance, the makers of a page with customer reviews may use attributes to specify what a section contains. For example, they may have written <p class=\"lead\"> ... </div> to mark the lead paragraph of an article, and <a href=test.html\"> ...</a> to specify the target of a hyperlink. Especially important here are the id and class attributes, which are often used by webpages to control the formatting. id (indicated with the hash sign # above) gives a unique ID to a single element, while class (indicated with a period) assigns a class label to one or more elements. This enables web sites to specify their layout and formatting using a technique called Cascading Style Sheets (CSS). For example, the web page could set the lead paragraph to be bold. The nice thing is that we can exploit this information to tell our parser where to find the elements we are interested in.\n\n\n\nTable 12.1: Overview of CSSSelect and XPath syntax\n\n\n\n\n\n\n\nExample\nCSS Select\nXPath\n\n\n\n\nBasic tree navigation\n\n\n\n\nh1anywhere in document\nh1\n//h1\n\n\nh1inside a body\nbody h1\n//body//h1\n\n\nh1directly insidediv\ndiv > h1\n//div/h1\n\n\nAny node directly insidediv\ndiv *\n//div/*\n\n\npnext to ah1\nh1   p\n//h1/following-sibling::p\n\n\npnext to ah1\nh1 + p\n//h1/following-sibling::p[1]\n\n\nNode attributes\n\n\n\n\n<div id='x1'>\ndiv#x1\n//div[@id='x1']\n\n\nany node withid x1\n#x1\n//*[@id='x1']\n\n\n<div class='row'>\ndiv.row\n//div[@class='row']\n\n\nany node withclass row\n.row\n//*[@class='row']\n\n\nawithhref=\"#\"\na[href=\"#\"]\n//a[@href=\"#\"]\n\n\nAdvanced tree navigation\n\n\n\n\nain adivwith class ‘meta’ directly in the main\n#main > div.meta a\n//*[@id='main'] /div[@class='meta']//a\n\n\nFirstpin adiv\ndiv p:first-of-type\n//div/p[1]\n\n\nFirst child of adiv\ndiv :first-child\n//div/*[1]\n\n\nSecondpin adiv\ndiv p:nth-of-type(2)\n//div/p[2]\n\n\nSecondpin adiv\ndiv p:nth-of-type(2)\n//div/p[2]\n\n\nparent of thedivwith idx1\n(not possible)\n//div[@id='x1']/parent::*\n\n\n\n\nCSS Selectors. The easiest way to specify our parser to look for a specific element is to use a CSS Selector, which might be familiar to you if you have created web pages. For example, to find the lead paragraph(s) we specify p.lead. To find the node with id=\"body\", we can specify \\#body. You can also use this to specify relations between nodes. For example, to find all paragraphs within the body element we would write \\#body p.\nTable 12.1 gives an overview of the possibilities of CSS Select. In general, a CSS selector is a set of node specifiers (like h1, .lead or div\\#body), optionally with relation specifiers between them. So, \\#body p finds a p anywhere inside the id=body element, while \\#body > p requires the p to be directly contained inside the body (with no other nodes in between).\nXPath. An alternative to CSS Selectors is XPath. Where CSS Selectors are directly based on HTML and CSS styling, XPath is a general way to describe nodes in XML (and HTML) documents. The general form of XPath is similar to CSS Select: a sequence of node descriptors (such as h1 or \\*[@id='body']). Contrary to CSS Select, you always have to specify the relationship, where // means any direct or indirect descendant and / means a direct child. If the relationship is not a child or descendant relationship (but for example a sibling or parent), you specify the axis with e.g. //a/parent::p meaning an a anywhere in the document (//a) which has a direct parent (/parent::) that is a p.\nA second difference with CSS Selectors is that the class and id attributes are not given special treatment, but can be used with the general [@attribute='value'] pattern. Thus, to get the lead paragraph you would specify //p[@class='lead'].\nThe advantage of XPath is that it is a very powerful tool. Everything that you can describe with a CSS Selector can also be described with an XPath pattern, but there are some things that CSS Selectors cannot describe, such as parents. On the other hand, XPath patterns can be a bit harder to write, read, and debug. You can choose to use either tool, and you can even mix and match them in a single script, but our general recommendation is to use CSS Selectors unless you need to use the specific abilities of XPath.\nExample 12.4 shows how to use XPATHs and CSS selectors to parse an HTML page. To fully understand it, open cssbook.net/d/restaurants/index.html in a browser and look at its source code (all modern browsers have a function “View page source” or similar), or – more comfortable – right-click on an element you are interested in (such as a restaurant name) and select “Inspect element” or similar. This will give you a user-friendly view of the HTML code.\n\n\n\n\n\n\n\nExample 12.4 Parsing websites using XPATHs or CSS selectors\n\nPython codeR code\n\n\n\ntree = parse(urlopen(\"https://cssbook.net/d/eat/index.html\"))\n\n# get the restaurant names via XPATH\nprint([e.text_content().strip() for e in tree.xpath(\"//h3\")])\n\n# get the restaurant names via CSS Selector\n\n['Pizzeria Roma', 'Trattoria Napoli', 'Curry King']\n\nprint([e.text_content().strip() for e in tree.getroot().cssselect(\"h3\")])\n\n['Pizzeria Roma', 'Trattoria Napoli', 'Curry King']\n\n\n\n\n\nurl = \"https://cssbook.net/d/eat/index.html\"\npage = read_html(url)\n\n# get the restaurant names via XPATH\npage %>% html_nodes(xpath=\"//h3\") %>% html_text()\n\n[1] \" Pizzeria Roma \"    \" Trattoria Napoli \" \" Curry King \"      \n\n# get the restaurant names via CSS Selector\npage %>% html_nodes(\"h3\") %>% html_text() \n\n[1] \" Pizzeria Roma \"    \" Trattoria Napoli \" \" Curry King \"      \n\n\n\n\n\n\n\n\n\nOf course, Example 12.4 only parses one possible element of interest: the restaurant names. Try to retrieve other elements as well!\nNotably, you may want to parse links. In HTML, links use a specific tag, a. These tags have an attribute, href, which contains the link itself. Example 12.5 shows how, after selecting the a tags, we can access these attributes.\n\n\n\n\n\n\n\nExample 12.5 Parsing link texts and links\n\nPython codeR code\n\n\n\nlinkelements = tree.xpath(\"//a\")\nlinktexts = [e.text for e in linkelements]\nlinks = [e.attrib[\"href\"] for e in linkelements]\n\nprint(linktexts)\n\n['here', 'here', 'here']\n\nprint(links)\n\n['review0001.html', 'review0002.html', 'review0003.html']\n\n\n\n\n\npage %>% \n  html_nodes(xpath=\"//a\") %>% \n  html_text() \n\n[1] \"here\" \"here\" \"here\"\n\npage %>% \n  html_nodes(xpath=\"//a\") %>% \n  html_attr(\"href\")\n\n[1] \"review0001.html\" \"review0002.html\" \"review0003.html\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo you care about the children?\n\n\n\n\n\nRegardless of whether you use XPATHS or CSS Selectors to specify which part of the page you are interested in, it is often the case that there are other elements within it. Depending on whether you want to also retrieve the text of these elements or not, you have to use different approaches. The code examples below shows some of these differences\nAppending /text() to the XPATH gives you exactly the text that is in the element itself, including line-breaks that happen to be in the source code. In python, the same information is also present in the .text property of the elements (but without the line-breaks):\n\nPython codeR code\n\n\n\nprint(tree.xpath(\"//div[@class='restaurant']/text()\"))\n\n[' ', '\\n      ', '\\n      ', '\\n    ', ' ', '\\n      ', '\\n      ', '\\n   ...\n\nprint([e.text for e in tree.xpath(\"//div[@class='restaurant']\")])\n\n[' ', ' ', ' ']\n\n\n\n\n\npage %>% html_nodes(xpath=\"//div[@class='restaurant']/text()\")\n\n{xml_nodeset (12)}\n [1]  \n [2] \\n      \n [3] \\n      \n [4] \\n    \n [5]  \n [6] \\n      \n [7] \\n      \n [8] \\n    \n [9]  \n[10] \\n      \n[11] \\n      \n[12] \\n    \n\n\n\n\n\nYou can also use the .text_content property (in Python) or the html_text function (in R) to accces the full text of an element, including children:\n\nPython codeR code\n\n\n\nprint([e.text_content() for e in tree.xpath(\"//div[@class='restaurant']\")])\n\n['  Pizzeria Roma \\n       Here you can get ... ... \\n       Read the full ...\n\nprint([e.text_content() for e in tree.getroot().cssselect(\".restaurant\")])\n\n['  Pizzeria Roma \\n       Here you can get ... ... \\n       Read the full ...\n\n\n\n\n\npage %>% html_nodes(xpath=\"//div[@class='restaurant']\") %>% html_text()\n\n[1] \"  Pizzeria Roma \\n       Here you can get ... ... \\n       Read the full review here\\n    \"     \n[2] \"  Trattoria Napoli \\n       Another restaurant ... ... \\n       Read the full review here\\n    \"\n[3] \"  Curry King \\n       Some description. \\n       Read the full review here\\n    \"               \n\npage %>% html_nodes(\".restaurant\") %>% html_text()\n\n[1] \"  Pizzeria Roma \\n       Here you can get ... ... \\n       Read the full review here\\n    \"     \n[2] \"  Trattoria Napoli \\n       Another restaurant ... ... \\n       Read the full review here\\n    \"\n[3] \"  Curry King \\n       Some description. \\n       Read the full review here\\n    \"               \n\n\n\n\n\nAnd you can do the same but using CSS rather than XPATH:\n\nPython codeR code\n\n\n\nprint([e.text_content() for e in tree.getroot().cssselect(\".restaurant\")])\n\n['  Pizzeria Roma \\n       Here you can get ... ... \\n       Read the full ...\n\n\n\n\n\npage %>% html_nodes(\".restaurant\") %>% html_text()\n\n[1] \"  Pizzeria Roma \\n       Here you can get ... ... \\n       Read the full review here\\n    \"     \n[2] \"  Trattoria Napoli \\n       Another restaurant ... ... \\n       Read the full review here\\n    \"\n[3] \"  Curry King \\n       Some description. \\n       Read the full review here\\n    \"               \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPretending to be a specific browser.\n\n\n\n\n\nWhen lxml, rvest, or your web browser download an HTML page, they send a so-called HTTP request. This request contains the URL, but also some meta-data, such as a so-called user-agent string. This string specifies the name and version of the browser. Some sites may block specific user agents (such as, for instance, the ones that lxml or rvest use); and sometimes, they deliver different content for different browsers. By using a more powerful module for downloading the HTML code (such as requests or httr) before parsing it, you can specify your own user-agent string and thus pretend to be a specific browser. If you do a web search, you will quickly find long lists with popular strings. In the code below, we rewrote Example 12.4 such that a custom user-agent can be specified:\n\nPython codeR code\n\n\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows \"\n    \"NT 10.0; Win64; x64; rv:60.0) \"\n    \"Gecko/20100101 Firefox/60.0\"\n}\n\nhtmlsource = requests.get(\n    \"https://cssbook.net/d/eat/index.html\", headers=headers\n).text\ntree = fromstring(htmlsource)\nprint([e.text_content().strip() for e in tree.xpath(\"//h3\")])\n\n['Pizzeria Roma', 'Trattoria Napoli', 'Curry King']\n\n\n\n\n\nr = GET(\"https://cssbook.net/d/eat/index.html\",\n    user_agent=str_c(\"Mozilla/5.0 (Windows NT \",\n    \"10.0; Win64; x64; rv:60.0) Gecko/20100101 \",\n    \"Firefox/60.0\"))\npage = read_html(r)\npage %>% html_nodes(xpath=\"//h3\") %>% html_text()\n\n[1] \" Pizzeria Roma \"    \" Trattoria Napoli \" \" Curry King \"      \n\n\n\n\n\n\n\n\n\n\n12.2.2 Crawling Websites\nOnce we have mastered parsing a single HTML page, it is time to scale up. Only rarely are we interested in parsing a single page. In most cases, we want to use an HTML page as a starting point, parse it, follow a link to some other interesting page, parse it as well, and so on. There are some dedicated frameworks for this such as scrapy, but in our experience, it may be more of a burden to learn that framework than to just implement your crawler yourself.\nStaying with the example of a restaurant review website, we might be interested in retrieving all restaurants from a specific city, and for all of these restaurants, all available reviews.\nOur approach, thus, could look as follows:\n\nRetrieve the overview page.\n\nParse the names of the restaurants and the corresponding links.\nLoop over all the links, retrieve the corresponding pages.\nOn each of these pages, parse the interesting content (i.e., the reviews, ratings, and so on).\n\n\nSo, what if there are multiple overview pages (or multiple pages with reviews)? Basically, there are two possibilities: the first possibility is to look for the link to the next page, parse it, download the next page, and so on. The second possibility exploits the fact that often, URLs are very systematic: for instance, the first page of restaurants might have a URL such as myreviewsite.com/amsterdam/restaurants.html?page=1. If this is the case, we can simply construct a list with all possible URLs (Example 12.6)\n\n\n\n\n\n\n\nExample 12.6 Generating a list of URLs that follow the same pattern.\n\nPython codeR code\n\n\n\nbaseurl = \"https://reviews.com/?page=\"\ntenpages = [f\"{baseurl}{i+1}\" for i in range(10)]\nprint(tenpages)\n\n['https://reviews.com/?page=1', 'https://reviews.com/?page=2', 'https://rev...\n\n\n\n\n\nbaseurl=\"https://reviews.com/?page=\"\ntenpages=glue(\"{baseurl}{1:10}\")\nprint(tenpages)\n\nhttps://reviews.com/?page=1\nhttps://reviews.com/?page=2\nhttps://reviews.com/?page=3\nhttps://reviews.com/?page=4\nhttps://reviews.com/?page=5\nhttps://reviews.com/?page=6\nhttps://reviews.com/?page=7\nhttps://reviews.com/?page=8\nhttps://reviews.com/?page=9\nhttps://reviews.com/?page=10\n\n\n\n\n\n\n\n\n\nAfterwards, we would just loop over this list and retrieve all the pages (a bit like how we approached Example 12.3 in Section 12.1).\nHowever, often, things are not as straightforward, and we need to find the correct links on a page that we have been parsing – that’s why we crawl through the website.\nWriting a good crawler can take some time, and they will look very differently for different pages. The best advice is to build them up step-by-step. Carefully inspect the website you are interested in. Take a sheet of paper, draw its structure, and try to find out which pages you need to parse, and how you can get from one page to the next. Also think about how the data that you want to extract should be organized.\nWe will illustrate this process using our mock-up review website cssbook.net/d/restaurants/. First, have a look at the site and try to understand its structure.\nYou will see that it has an overview page, index.html, with the names of all restaurants and, per restaurant, a link to a page with reviews. Click on these links, and note your observations, such as: - the pages have different numbers of reviews; - each review consists of an author name, a review text, and a rating; - some, but not all, pages have a link saying “Get older reviews” - …\nIf you combine what you just learned about extracting text and links from HTML pages with your knowledge about control structures like loops and conditional statements (Section 3.2), you can now write your own crawler.\nWriting a scraper is a craft, and there are several ways of achieving your goal. You probably want to develop your scraper in steps: first write a function to parse the overview page, then a function to parse the review pages, then try to combine all elements into one script. Before you read on, try to write such a scraper.\nTo show you one possible solution, we implemented a scraper in Python that crawls and parses all reviews for all restaurants (Example 12.7), which we describe in detail below.\n\n\n\n\n\n\n\nExample 12.7 Crawling a website in Python\n\nBASEURL = \"https://cssbook.net/d/eat/\"\n\ndef get_restaurants(url):\n    \"\"\"takes the URL of an overview page as input\n    returns a list of (name, link) tuples\"\"\"\n    tree = parse(urlopen(url))\n    names = [\n        e.text.strip() for e in tree.xpath(\"//div[@class='restaurant']/h3\")\n    ]\n    links = [\n        e.attrib[\"href\"] for e in tree.xpath(\"//div[@class='restaurant']//a\")\n    ]\n    return list(zip(names, links))\n\ndef get_reviews(url):\n    \"\"\"yields reviews on the specified page\"\"\"\n    while True:\n        print(f\"Downloading {url}...\")\n        tree = parse(urlopen(url))\n        names = [\n            e.text.strip() for e in tree.xpath(\"//div[@class='review']/h3\")\n        ]\n        texts = [e.text.strip() for e in tree.xpath(\"//div[@class='review']/p\")]\n        ratings = [e.text.strip() for e in tree.xpath(\"//div[@class='rating']\")]\n        for u, txt, rating in zip(names, texts, ratings):\n            review = {}\n            review[\"username\"] = u.replace(\"wrote:\", \"\")\n            review[\"reviewtext\"] = txt\n            review[\"rating\"] = rating\n            yield review\n        bb = tree.xpath(\"//span[@class='backbutton']/a\")\n        if bb:\n            print(\"Processing next page\")\n            url = BASEURL + bb[0].attrib[\"href\"]\n        else:\n            print(\"No more pages found.\")\n            break\n\nprint(\"Retrieving all restaurants...\")\n\nRetrieving all restaurants...\n\nlinks = get_restaurants(BASEURL + \"index.html\")\nprint(links)\n\n[('Pizzeria Roma', 'review0001.html'), ('Trattoria Napoli', 'review0002.htm...\n\nwith open(\"reviews.json\", mode=\"w\") as f:\n    for restaurant, link in links:\n        print(f\"Processing {restaurant}...\")\n        for r in get_reviews(BASEURL + link):\n            r[\"restaurant\"] = restaurant\n            f.write(json.dumps(r))\n            f.write(\"\\n\")\n\n# You can process the results with pandas\n# (using lines=True since it\"s one json per line)\n\nProcessing Pizzeria Roma...\nDownloading https://cssbook.net/d/eat/review0001.html...\n177\n1\n120\n1\nNo more pages found.\nProcessing Trattoria Napoli...\nDownloading https://cssbook.net/d/eat/review0002.html...\n140\n1\n118\n1\nNo more pages found.\nProcessing Curry King...\nDownloading https://cssbook.net/d/eat/review0003.html...\n96\n1\n98\n1\n122\n1\n113\n1\nProcessing next page\nDownloading https://cssbook.net/d/eat/review0003-1.html...\n120\n1\n123\n1\n102\n1\n105\n1\nProcessing next page\nDownloading https://cssbook.net/d/eat/review0003-2.html...\n130\n1\n118\n1\nNo more pages found.\n\ndf = pd.read_json(\"reviews.json\", lines=True)\nprint(df)\n\n          username  ...        restaurant\n0     gourmet2536   ...     Pizzeria Roma\n1        foodie12   ...     Pizzeria Roma\n2    mrsdiningout   ...  Trattoria Napoli\n3        foodie12   ...  Trattoria Napoli\n4           smith   ...        Curry King\n5        foodie12   ...        Curry King\n6      dontlikeit   ...        Curry King\n7        otherguy   ...        Curry King\n8           tasty   ...        Curry King\n9            anna   ...        Curry King\n10           hans   ...        Curry King\n11        bee1983   ...        Curry King\n12         rhebjf   ...        Curry King\n13  foodcritic555   ...        Curry King\n\n[14 rows x 4 columns]\n\n\n\n\n\n\nFirst, we need to get a list of all restaurants and the links to their reviews. That’s what is done in the function get_restaurants. This is actually the first thing we do (see line 32).\nWe now want to loop over these links and retrieve the reviews. We decided to use a generator (Section 3.2): instead of writing a function that collects all reviews in a list first, we let the function yield each review immediately – and then append that review to a file. This has a big advantage: if our scraper fails (for instance, due to a time out, a block, or a programming error), then we have already saved the reviews we got so far.\nWe loop over the links to the restaurants (line 36) and call the function get_reviews (line 38). Each review it returns (the review is a dict) gets the name of the restaurant as an extra key, and then gets written to a file which contains one JSON-object per line (also known as a jsonlines-file).\nThe function get_reviews takes a link to a review page as input and yields reviews. If we knew all pages with reviews already, then we would not need the while loop statement in line 12 and the lines 24–29. However, as we have seen, some review pages contain a link to older reviews. We therefore use a loop that runs forever (that is what while True: does), unless it encounters a break statement (line 29). An inspection of the HTML code shows that these links have a span tag with the attribute class=\"backbutton\". We therefore check if such a button exists (line 24), and if so, we get its href attribute (i.e., the link itself), overwrite the url variable with it, and then go back to line 16, the beginning of the loop, so that we can download and parse this next URL. This goes on until such a link is no longer found.\n\n\n12.2.3 Dynamic Web Pages\nYou may have realized that all our scraping efforts until now proceeded in two steps: we retrieved (downloaded) the HTML source of a web page and then parsed it. However, modern websites more and more frequently are dynamic rather than static. For example, after being loaded, they load additional content, or what is displayed changes based on what the user does. Frequently, some JavaScript is run within the user’s browser to do that. However, we do not have a browser here. The HTML code we downloaded may contain some instructions for the browser that some code needs to be run, but in the absence of a browser, our Python or R script cannot do this.\nAs a first test to check out whether this is a concern, you can simply check whether the HTML code in your browser is the same as that you would get if you downloaded it with R or Python. After having retrieved the page (Example 12.4), you simply dump it to a file (Example 12.8) and open this file in your browser to verify that you indeed downloaded what you intended to download (and not, for instance, a login page, a cookie wall, or an error message).\n\n\n\n\n\n\n\nExample 12.8 Dumping the HTML source to a file\n\nPython codeR code\n\n\n\nwith open(\"test.html\", mode=\"w\") as fo:\n    fo.write(htmlsource)\n\n\n\n\nfileConn<-file(\"test.html\")\nwriteLines(content(r, as = \"text\"), fileConn)\nclose(fileConn)\n\n\n\n\n\n\n\n\nIf this test shows that the data you are interested in is indeed not part of the HTML code you can retrieve with R or Python, and use the following checklist to find\n\nDoes using a different user-agent string (see above) solve the issue?\nIs the issue due to some cookie that needs to be accepted or requires you to log in (see below)?\nIs a different page delivered for different browsers, devices, display settings, etc.?\n\nIf all of this does not help, or if you already know for sure that the content you are interested in is dynamically fetched via JavaScript or similar, you can use Selenium to literally start a browser and extract the content you are interested in from there. Selenium has been designed for testing web sites and allows you to automate clicks in a browser window, and also supports CSS selectors and Xpaths to specify parts of the web page.\nUsing Selenium may require some additional setup on your computer, which may depend on your operating system and the software versions you are using – check out the usual online sources for guidance if needed. It is possible to use Selenium through R using Rselenium. However, doing so can be quite a hassle and requires, running a separate Selenium server, for instance, using Docker. If you opt to use Selenium for web scraping, your safest bet is probably to follow an online tutorial and/or to dive into the documentation. To give you a first impression of the general working, Example 12.9 shows you how to (at the time of writing of this book) open Firefox, surf to Google, google for Tintin by entering that string and pressing the return key, click on the first link containing that string, and take a screenshot of the result.\n\n\n\n\n\n\n\nExample 12.9 Using Selenium to literally open a browser, input text, click on a link, and take a screenshot.\n\ndriver = webdriver.Firefox()\ndriver.implicitly_wait(10)\ndriver.get(\"https://www.duckduckgo.com\")\nelement = driver.find_element(\"name\", \"q\")\nelement.send_keys(\"TinTin\")\nelement.send_keys(Keys.RETURN)\ntry:\n    driver.find_element(\"css selector\", \"#links a\").click()\n    # let\"s be cautious and wait 10 seconds\n    # so that everything is loaded\n    time.sleep(10)\n    driver.save_screenshot(\"screenshotTinTin.png\")\nfinally:\n    # whatever happens, close the browser\n    driver.quit()\n\n\n\n\n\n\n\n\n\n\n\nLosing your head\n\n\n\n\n\nIf you want to run long-lasting scraping processes using Selenium in the background (or on a server without a graphical user interface), you may want to look into what is called a “headless” browser. For instance, Selenium can start Firefox in “headless” mode, which means that it will run without making any connection to a graphical interface. Of course, that also means that you cannot watch Selenium scrape, which may make debugging more difficult. You could opt for developing your scraper first using a normal browser, and then changing it to use a headless browser once everything works."
  },
  {
    "objectID": "content/chapter12.html#sec-authentication",
    "href": "content/chapter12.html#sec-authentication",
    "title": "12  Scraping online data",
    "section": "12.3 Authentication, Cookies, and Sessions",
    "text": "12.3 Authentication, Cookies, and Sessions\n\n12.3.1 Authentication and APIs\nWhen we introduced APIs in Section 12.1, we used the example of an API where you did not need to authenticate yourself. As we have seen, using such an API is as simple as sending an HTTP request to an endpoint and getting a response (usually, a JSON object) back. And indeed, there are plenty of interesting APIs (think for instance of open government APIs) that work this way.\nWhile this has obvious advantages for you, it also has some serious downsides from the perspective of the API provider as well as from a security and privacy standpoint. The more confidential the data is, the more likely it is that the API provider needs to know who you are in order to determine which data you are allowed to retrieve; and even if the data are not confidential, authentication may be used to limit the number of requests that an individual can make in a given time frame.\nIn its most simple form, you just need to provide a unique key that identifies you as a user. For instance, Example 12.10 shows how such a key can be passed along as an HTTP header, essentially as additional information next to the URL that you want to retrieve (see also Section 12.3.2). The example shows a call to an endpoint of a commercial API for natural language processing to inform how many requests we have made today.\n\n\n\n\n\n\n\nExample 12.10 Passing a key as HTTP request header to authenticate at an API endpoint\n\nPython codeR code\n\n\n\nrequests.get(\n    \"https://api.textrazor.com/account/\", headers={\"x-textrazor-key\": \"SECRET\"}\n).json()\n\n{'ok': False, 'time': 0, 'error': 'Your TextRazor API Key was invalid.'}\n\n\n\n\n\nr = GET(\"https://api.textrazor.com/account/\", \n  add_headers(\"x-textrazor-key\"=\"SECRET\"))\ncat(content(r, \"text\"))\n\n{\"ok\":false, \"time\":0, \"error\":\"Your TextRazor API Key was invalid.\"}\n\n\n\n\n\n\n\n\n\nAs you see, using an API that requires authentication by passing a key as an HTTP header is hardly more complicated than using APIs that do not require authentication such as outlined in Section 12.1. However, many APIs use more complex protocols for authentication.\nThe most popular one is called OAuth, and it is used by many APIs provided by major players such as Google, Facebook, Twitter, GitHub, LinkedIn, etc. Here, you have a client ID and a client secret (sometimes also called consumer key and consumer secret, or API key and API secret) and an access token with associated access token secret. The first pair authenticates you as a user, the second pair authenticates the specific “app” (i.e., your script). Once authenticated, your script can then interact with the API. While it is possible to directly work with OAuth HTTP requests using requests_oauthlib (Python) or httr (R), chances are relatively low that you have to do so, unless you plan on really developing your own app or even your own API: for all popular API’s, so-called wrappers, packages that provide a simpler interface to the API, are available on pypi and CRAN. Still, all of these require to have at least a consumer key and a consumer secret. The access token sometimes is generated via a web interface where you manage your account (e.g., in the case of Twitter), or can be acquired by your script itself, which then will redirect the user to a website in which they are asked to authenticate the app. The nice thing about this is that it only needs to happen once: once your app is authenticated, it can keep making requests.\n\n\n12.3.2 Authentication and Webpages\nIn this section, we briefly discuss different approaches for dealing with websites where you need to log on, accept something (e.g., a so-called cookie wall), or have to otherwise authenticate yourself. One approach can be the use of a web testing framework like Selenium (see Section 12.2.3): you let your script literally open a browser and, for instance, fill in your login information.\nHowever, sometimes that’s not necessary and we can still use simpler and more efficient webscraping without invoking a browser. As we have already seen in Section 12.2.1, when making an HTTP request, we can transmit additional information, such as the so-called user-agent string. In a similar way, we can pass other information, such as cookies.\nIn the developer tools of your browser (which we already used to determine XPATHs and CSS selectors), you can look up which cookies a specific website has placed. For instance, you could inspect all cookies before you logged on (or passed a cookie wall) and again inspect them afterwards to determine what has changed. With this kind of reverse-engineering, you can find out what cookies you need to manually set.\nIn Example 12.11, we illustrate this for a specific page (at the time of writing of our book). Here, by inspecting the cookies in Firefox, we found out that clicking “Accept” on the cookie wall landing page caused a cookie with the name cpc and the value 10 to be set. To set those cookies in our scraper, the easiest way is to retrieve that page first and store the cookies sent by the server. In Example 12.11, we therefore start a session and try to download the page. We know that this will only show us the cookie wall – but it will also generate the necessary cookies. We then store these cookies, and add the cookie that we want to be set (cpc=10) to this cookie jar. Now, we have all cookies that we need for future requests. They will stay there for the whole session.\nIf we only want to get a single page, we may not need to start a session to remember all the cookies, and we can just directly pass the single cookie we care about to a request instead (Example 12.12).\n\n\n\n\n\n\n\nExample 12.11 Explicitly setting a cookie to circumvent a cookie wall\n\nPython codeR code\n\n\n\nURL = \"https://www.geenstijl.nl/5160019/page\"\n\n\n# circumvent cookie wall by setting a specific\n# cookie: the key-value pair (cpc: 10)\nclient = requests.session()\nr = client.get(URL)\n\ncookies = client.cookies.items()\ncookies.append((\"cpc\", \"10\"))\nresponse = client.get(URL, cookies=dict(cookies))\n# end circumvention\n\ntree = fromstring(response.text)\nallcomments = [e.text_content().strip() for e in tree.cssselect(\".cmt-content\")]\nprint(f\"There are {len(allcomments)} comments.\")\n\nThere are 318 comments.\n\n\n\n\n\nURL = \"https://www.geenstijl.nl/5160019/page/\"\n\n# circumvent cookie wall by setting a specific\n# cookie: the key-value pair (cpc: 10)\nr = GET(URL)\ncookies = setNames(cookies(r)$value, \n                   cookies(r)$name)\ncookies = c(cookies, cpc=10)\nr = GET(URL, set_cookies(cookies))\n# end circumvention\n\nallcomments = r %>% \n  read_html() %>%\n  html_nodes(\".cmt-content\") %>% \n  html_text()\n\nglue(\"There are {length(allcomments)} comments.\")\n\nThere are 318 comments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 12.12 Shorter version of for single requests\n\nPython codeR code\n\n\n\nr = requests.get(URL, cookies={\"cpc\": \"10\"})\ntree = fromstring(r.text)\nallcomments = [e.text_content().strip() for e in tree.cssselect(\".cmt-content\")]\nprint(f\"There are {len(allcomments)} comments.\")\n\nThere are 318 comments.\n\n\n\n\n\nr = GET(URL, set_cookies(cpc=10))\nallcomments = r %>% \n  read_html() %>%\n  html_nodes(\".cmt-content\") %>% \n  html_text()\nglue(\"There are {length(allcomments)} comments.\")\n\nThere are 318 comments."
  },
  {
    "objectID": "content/chapter12.html#sec-ethicallegalpractical",
    "href": "content/chapter12.html#sec-ethicallegalpractical",
    "title": "12  Scraping online data",
    "section": "12.4 Ethical, Legal, and Practical Considerations",
    "text": "12.4 Ethical, Legal, and Practical Considerations\nWeb scraping is a powerful tool, but it needs to be handled responsibly. Between the white area of sites that explicitly consented to creating a copy of their data (for instance, by using a creative commons license) and the black area of an exact copy of copyrighted material and redistributing it as it is, there is a large gray area where it is less clear what is acceptable and what is not.\nThere is a tension between legitimate interests of the operators of web sites and the producers of content on the one hand, and the societal interest of studying online communication on the other hand. Which interest prevails may differ on a case-to-case basis. For instance, when using APIs as described in Section 12.1, in most cases, you have to consent to the terms of service (TOS) of the API provider.\nFor example, Twitter’s TOS allow you to redistribute the numerical tweet ids, but not the tweets themselves, and therefore, it is common to share such lists of ids with fellow researchers instead of the “real” Twitter datasets. Of course, this is not optimal from a reproducibility point of view: if another researcher has to retrieve the tweets again based on their ids, then this is not only cumbersome, but most likely also leads to a slightly different dataset, because tweets may have been deleted in the meantime. At the same time, it is a compromise most people can live with.\nOther social media platforms have closed their APIs or tightened the restrictions a lot, making it impossible to study many pressing research questions. Therefore, some have even called researchers to neglect these TOS, because “in some circumstances the benefits to society from breaching the terms of service outweigh the detriments to the platform itself” (Bruns 2019, ~1561). Others acknowledge the problem, but doubt that this is a good solution (Puschmann 2019). In general, one needs to distinguish between the act of collecting the data and sharing the data. For instance, in many jurisdictions, there are legal exemptions for collecting data for scientific purposes, but that does not mean that they can be re-distributed as they are (Van Atteveldt et al. 2019).\nThis chapter can by no means replace the consultation of a legal expert and/or an ethics board, but we would like to offer some strategies to minimize potential problems.\nBe nice. Of course, you could send hundreds of requests per minute (or second) to a website and try to download everything that they have ever published. However, this causes unnecessary load on their servers (and you would probably get blocked). If, on the other hand, you carefully think about what you really need to download, and include a lot of waiting times (for instance, using sys.sleep (R) or time.sleep (Python) so that your script essentially does the same as could be done by hiring a couple of student assistants to copy-paste the data manually, then problems are much less likely to arise.\nCollaborate. Another way to minimize traffic and server load is to collaborate more. A concerted effort with multiple researchers may lead to less duplicate data and in the end probably an even better, re-usable dataset.\nBe extra careful with personal data. Both from an ethical and a legal point of view, the situation changes drastically as soon as personal data are involved. Especially since the General Data Protection Regulation (GDPR) regulations took effect in the European Union, collecting and processing such data requires a lot of additional precaution and is usually subject to explicit consent. It is clearly infeasible to ask every Twitter user for consent to process their tweet and doing so is probably covered by research exceptions, the general advice is to store as little personal data as possible and only what is absolutely needed. Most likely, you need to have a data management plan in place, and should get appropriate advice from your legal department. Therefore, think carefully whether you really need, for instance, the user names of the authors of reviews you are going to scrape, or whether the text alone suffices.\nOnce all ethical and legal concerns are sorted out and you have made sure that you have written a scraper in such a way that it does not cause unnecessary traffic and load on the servers from which you are scraping, and after doing some test runs, it is time to think about how to actually run it on a larger scale. You may already have figured that you probably do not want to run your scraper from a Jupyter Notebook that is constantly open in your browser on your personal laptop. Also here, we would like to offer some suggestions.\nConsider using a database. Imagine the following scenario: your scraper visits hundreds of websites, collects its results in a list or in a data frame, and after hours of running suddenly crashes – maybe because some element that you were sure must exist on each page, exists only on 999 out of 1000 pages, because a connection timed out, or any other error. Your data is lost, you need to start again (not only annoying, but also undesirable from a traffic minimization point of view). A better strategy may be to immediately write the data for each page to a file. But then, you need to handle a potentially huge number of files later on. A much better approach, especially if you plan to run your scraper repeatedly over a long period of time, is to consider the use of a database in which you dump the results immediately after a page has been scraped (see Section 15.1).\nRun your script from the command line. Store your scraper as a .py or .R script and run it from your terminal (your command line) by typing python myscript.py or R myscript.R rather than using an IDE such as Spyder or R Studio or a Jupyter Notebook. You may want to have your script print a lot of status information (for instance, which page it is currently scraping), so that you can watch what it is doing. If you want to, you can have your computer run this script in regular intervals (e.g., once an hour). On Linux and MacOS, for instance, you can use a so-called cron job to automate this.\nRun your script on a server. If your scraper runs for longer than a couple of hours, you may not want to run it on your laptop, especially if your Internet connection is not stable. Instead, you may consider using a server. As we will explain in Section 15.2, it is quite affordable to set up a Linux VM on a cloud computing platform (and next to commercial services, in some countries and institutions there are free services for academics). You can then use tools like nohup or screen to run your script on the background, even if you are no longer connected to the server (see Section 15.2).\n\n\n\n\n\n\n\nBruns, Axel. 2019. “After the ‘APIcalypse’: social media platforms and their fight against critical scholarly research.” Information, Communication & Society 22 (11): 1544–66. https://doi.org/10.1080/1369118X.2019.1637447.\n\n\nFreelon, Deen. 2018. “Computational Research in the Post-API Age.” Political Communication 35 (4): 665–68. https://doi.org/10.1080/10584609.2018.1477506.\n\n\nPuschmann, Cornelius. 2019. “An end to the wild west of social media research: a response to Axel Bruns.” Information, Communication & Society 22 (11): 1582–89. https://doi.org/10.1080/1369118X.2019.1646300.\n\n\nVan Atteveldt, Wouter, Joanna Strycharz, Damian Trilling, and Kasper Welbers. 2019. “Toward Open Computational Communication Science : A Practical Road Map for Reusable Data and Code University of Amsterdam , the Netherlands.” International Journal of Communication 13: 3935–54."
  },
  {
    "objectID": "content/chapter13.html#sec-graph",
    "href": "content/chapter13.html#sec-graph",
    "title": "13  Network Data",
    "section": "13.1 Representing and Visualizing Networks",
    "text": "13.1 Representing and Visualizing Networks\nHow can networks help us to understand and represent social problems? How can we use social media as a source for small and large-scale network analysis? In the computational analysis of communication these questions become highly relevant given the huge amount of social media data produced every minute on the Internet. In fact, although graph theory and SNA were already being used during the last two decades of the 20th century, we can say that the widespread adoption of the Internet and especially social networking services such as Twitter and Facebook really unleashed their potential. Firstly, computers made it easier to compute graph measures and visualize their general and communal structures. Secondly, the emergence of a big spectrum of social media network sites (i.e. Facebook, Twitter, Sina Weibo, Instagram, Linkedin, etc.) produced an unprecedented number of online social interactions, which still is certainly an excellent arena to apply this framework. Thus, the use of social media as a source for network analysis has become one of the most exciting and promising areas in the field of computational social science.\nThis section presents a brief overview of graph structures (nodes and edges) and types (directed, weighted, etc.), together with their representations in R and Python. We also include visual representations and basic graph analysis.\nA graph is a structure derived from a set of elements and their relationships. The element could be a neuron, a person, an organization, a street, or even a message, and the relationship could be a synapse, a trip, a commercial agreement, a drive connection or a content transmission. This is a different way to represent, model and analyze the world: instead of having rows and columns as in a typical data frame, in a graph we have nodes (components) and edges (relations). The mathematical representation of a graph \\(G=(V,E)\\) is based on a set of nodes (also called vertices): \\(\\{v_{1}, v_{2},\\ldots v_{n}\\}\\) and the edges or pair of nodes: \\(\\{(v_{1}, v_{2}), (v_{1}, v_{3}), (v_{2},v_{3}) \\ldots (v_{m}, v_{n}) \\in E\\}\\) As you may imagine, it is a very versatile procedure to represent many kinds of situations that include social, media, or political interactions. In fact, if we go back to 1934 we can see how graph theory (originally established in the 18th century) was first applied to the representation of social interactions (Moreno 1934) in order to measure the attraction and repulsion of individuals of a social group1.\nThe network approach in social sciences has an enormous potential to model and predict social actions. There is empirical evidence that we can successfully apply this framework to explain distinct phenomena such as political opinions, obesity, and happiness, given the influence of our friends (or even of the friends of our friends) over our behavior (Christakis and Fowler 2009). The network created by this sophisticated structure of human and social connections is an ideal scenario to understand how close we are to each other in terms of degrees of separation (Watts 2004) in small (e.g., a school) and large-scale (e.g., a global pandemic) social dynamics. Moreover, the network approach can help us to track the propagation either of a virus in epidemiology, or a fake news story in political and social sciences, such as in the work by Vosoughi, Roy, and Aral (2018).\nNow, let us show you how to create and visualize network structures in R and Python. As we mentioned above, the structure of a graph is based on nodes and edges, which are the fundamental components of any network. Suppose that we want to model the social network of five American politicians in 2017 (Donald Trump, Bernie Sanders, Hillary Clinton, Barack Obama and John McCain), based on their imaginary connections on Facebook (friending) and Twitter (following)2. Technically, the base of any graph is a list of edges (written as pair of nodes that indicate the relationships) and a list of nodes (some nodes might be isolated without any connection!). For instance, the friendship on Facebook between two politicians would normally be expressed as two strings separated by comma (e.g., “Hillary Clinton”, “Donald Trump”). In Example 13.1 we use libraries igraph (R)3 and networkx (Python) to create from scratch a simple graph with five nodes and four edges, using the above-mentioned structure of pairs of nodes (notice that we only include the edges while the vertices are automatically generated).\n\n\n\n\n\n\n\nExample 13.1 Imaginary Facebook network of 5 American politicians\n\nPython codeR code\n\n\n\nedges = [\n    (\"Hillary Clinton\", \"Donald Trump\"),\n    (\"Bernie Sanders\", \"Hillary Clinton\"),\n    (\"Hillary Clinton\", \"Barack Obama\"),\n    (\"John McCain\", \"Donald Trump\"),\n    (\"Barack Obama\", \"Bernie Sanders\"),\n]\ng1 = nx.Graph()\ng1.add_edges_from(edges)\nprint(\"Nodes:\", g1.number_of_nodes(), \"Edges: \", g1.number_of_edges())\n\nNodes: 5 Edges:  5\n\nprint(g1.edges)\n\n[('Hillary Clinton', 'Donald Trump'), ('Hillary Clinton', 'Bernie Sanders')...\n\n\n\n\n\nedges=c(\"Hillary Clinton\", \"Donald Trump\", \n        \"Bernie Sanders\",\"Hillary Clinton\", \n        \"Hillary Clinton\",  \"Barack Obama\", \n        \"John McCain\", \"Donald Trump\", \n        \"Barack Obama\", \"Bernie Sanders\")\ng1 = make_graph(edges, directed = FALSE)\ng1\n\nIGRAPH cb241ef UN-- 5 5 -- \n+ attr: name (v/c)\n+ edges from cb241ef (vertex names):\n[1] Hillary Clinton--Donald Trump   Hillary Clinton--Bernie Sanders\n[3] Hillary Clinton--Barack Obama   Donald Trump   --John McCain   \n[5] Bernie Sanders --Barack Obama  \n\n\n\n\n\n\n\n\n\nIn both cases we generated a graph object g1 which contains the structure of the network and different attributes (such as number_of_nodes() in networkx). You can add/remove nodes and edges to/from this initial graph, or even modify the names of the vertices. One of the most useful functions is the visualization of the network (plot in igraph and draw or draw_networkx in networkx). Example 13.2 shows a basic visualization of the imaginary network of friendships of five American politicians on Facebook.\n\n\n\n\n\n\n\nExample 13.2 Visualization of a simple graph.\n\nPython codeR code\n\n\n\nnx.draw_networkx(g1)\n\npos = nx.shell_layout(g1)\nx_values, y_values = zip(*pos.values())\nx_max = max(x_values)\nx_min = min(x_values)\nx_margin = (x_max - x_min) * 0.40\nplt.xlim(x_min - x_margin, x_max + x_margin)\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\nplot(g1)\n\n\n\n\n\n\n\n\n\n\n\nUsing network terminology, either nodes or edges can be adjacent or not. In the figure we can say that nodes representing Donald Trump and John McCain are adjacent because they are connected by an edge that depicts their friendship. Moreover, the edges representing the friendships between John McCain and Donald Trump, and Hillary Clinton and Donald Trump, are also adjacent because they share one node (Donald Trump).\nNow that you know the relevant terminology and basics of working with graphs, you might be wondering: what if I want to do the same with Twitter? Can I represent the relationships between users in the very same way as Facebook? Well, when you model networks it is extremely important that you have a clear definition of what you mean with nodes and edges, in order to maintain a coherent interpretation of the graph. In both, Facebook and Twitter, the nodes represent the users, but the edges might not be the same. In Facebook, an edge represents the friendship between two users and this link has no direction (once a user accepts a friend request, both users become friends). In the case of Twitter, an edge could represent various relationships. For example, it could mean that two users follow each other, or that one user is following another user, but not the other way around! In the latter case, the edge has a direction, which you can establish in the graph. When you give directions to the edges you are creating a directed graph. In Example 13.3 the directions are declared with the order of the pair of nodes: the first position is for the “from” and the second for the “to”. In igraph (R) we set the argument directed of the function make_graph to TRUE. In networkx (Python), you use the class DiGraph instead of Graph to create the object g2.\n\n\n\n\n\n\n\nExample 13.3 Creating a directed graph\n\nPython codeR code\n\n\n\nedges += [\n    (\"Hillary Clinton\", \"Bernie Sanders\"),\n    (\"Barack Obama\", \"Hillary Clinton\"),\n]\ng2 = nx.DiGraph()\ng2.add_edges_from(edges)\nprint(\"Nodes:\", g2.number_of_nodes(), \"Edges: \", g2.number_of_edges())\n\nNodes: 5 Edges:  7\n\nprint(g2.edges)\n\n[('Hillary Clinton', 'Donald Trump'), ('Hillary Clinton', 'Barack Obama'), ...\n\n\n\n\n\nedges = c(edges, \n          \"Hillary Clinton\", \"Bernie Sanders\",\n          \"Barack Obama\",\"Hillary Clinton\")\ng2 = make_graph(edges, directed = TRUE)\nprint(g2)\n\nIGRAPH 6471a68 DN-- 5 7 -- \n+ attr: name (v/c)\n+ edges from 6471a68 (vertex names):\n[1] Hillary Clinton->Donald Trump    Bernie Sanders ->Hillary Clinton\n[3] Hillary Clinton->Barack Obama    John McCain    ->Donald Trump   \n[5] Barack Obama   ->Bernie Sanders  Hillary Clinton->Bernie Sanders \n[7] Barack Obama   ->Hillary Clinton\n\n\n\n\n\n\n\n\n\nIn the new graph the edges represent the action of following a user on Twitter. The first declared edge indicates that Hillary Clinton follows Donald Trump, but does not indicate the opposite. In order to provide the directed graph with more arrows we included in g2 two new edges (Obama following Clinton and Clinton following Sanders), so we can have a couple of reciprocal relationships besides the unidirectional ones. You can visualize the directed graph in Example 13.4 and see how the edges now contain useful arrows.\n\n\n\n\n\n\n\nExample 13.4 Visualization of a directed graph.\n\nPython codeR code\n\n\n\nnx.draw_networkx(g2)\n\npos = nx.shell_layout(g2)\nx_values, y_values = zip(*pos.values())\nx_max = max(x_values)\nx_min = min(x_values)\nx_margin = (x_max - x_min) * 0.40\nplt.xlim(x_min - x_margin, x_max + x_margin)\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\nplot(g2)\n\n\n\n\n\n\n\n\n\n\n\nThe edges and nodes of our graph can also have weights and features or attributes. When the edges have specific values that depict a feature of every pair of nodes (i.e., the distance between two cities) we say that we have a weighted graph. This type of graph is extremely useful for creating a more accurate representation of a network. For example, in our hypothetical network of American politicians on Twitter (g2) we can assign weights to the edges by including the number of likes that each politician has given to the followed user. This value can serve as a measure of the distance between the nodes (i.e., the higher the number of likes the shorter the social distance). In Example 13.5 we include the weights for each edge: Clinton has given five likes to Trumps’ tweets, Sanders 20 to Clinton’s messages, and so on. In the plot you can see how the sizes of the lines between the nodes change as a function of the weights.\n\n\n\n\n\n\n\nExample 13.5 Visualization of a weighted graph\n\nPython codeR code\n\n\n\nedges_w = [\n    (\"Hillary Clinton\", \"Donald Trump\", 5),\n    (\"Bernie Sanders\", \"Hillary Clinton\", 20),\n    (\"Hillary Clinton\", \"Barack Obama\", 30),\n    (\"John McCain\", \"Donald Trump\", 40),\n    (\"Barack Obama\", \"Hillary Clinton\", 50),\n    (\"Hillary Clinton\", \"Bernie Sanders\", 10),\n    (\"Barack Obama\", \"Bernie Sanders\", 15),\n]\ng2 = nx.DiGraph()\ng2.add_weighted_edges_from(edges_w)\n\nedge_labels = dict(\n    [\n        (\n            (\n                u,\n                v,\n            ),\n            d[\"weight\"],\n        )\n        for u, v, d in g2.edges(data=True)\n    ]\n)\n\nnx.draw_networkx_edge_labels(g2, pos, edge_labels=edge_labels)\nnx.draw_networkx(g2, pos)\n\npos = nx.spring_layout(g2)\nx_values, y_values = zip(*pos.values())\nx_max = max(x_values)\nx_min = min(x_values)\nx_margin = (x_max - x_min) * 0.40\nplt.xlim(x_min - x_margin, x_max + x_margin)\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\nE(g2)$weight = c(5, 20, 30, 40, 50, 10, 15)\nplot(g2, edge.label = E(g2)$weight)\n\n\n\n\n\n\n\n\n\n\n\nYou can include more properties of the components of your graph. Imagine you want to use the number of followers of each politician to determine the size of the nodes, or the gender of the user to establish a color. In Example 13.6 we added the variable followers to each of the nodes and asked the packages to plot the network using this value as the size parameter (in fact we multiplied the values by 0.001 to make it realistic on the screen, but you could also normalize these values when needed). We also included the variable party that was later recoded in a new one called color in order to represent Republicans with red and Democrats with blue. You may need to add other features to the nodes or edges, but with this example you have an overview of what you can do.\n\n\n\n\n\n\n\nExample 13.6 Visualization of a weighted graph including vertex sizes.\n\nPython codeR code\n\n\n\nattrs = {\n    \"Hillary Clinton\": {\"followers\": 100000, \"party\": \"Democrat\"},\n    \"Donald Trump\": {\"followers\": 200000, \"party\": \"Republican\"},\n    \"Bernie Sanders\": {\"followers\": 50000, \"party\": \"Democrat\"},\n    \"Barack Obama\": {\"followers\": 500000, \"party\": \"Democrat\"},\n    \"John McCain\": {\"followers\": 40000, \"party\": \"Republican\"},\n}\nnx.set_node_attributes(g2, attrs)\nsize = nx.get_node_attributes(g2, \"followers\")\nsize = list(size.values())\n\ncolors = nx.get_node_attributes(g2, \"party\")\ncolors = list(colors.values())\ncolors = [w.replace(\"Democrat\", \"blue\") for w in colors]\ncolors = [w.replace(\"Republican\", \"red\") for w in colors]\n\nnx.draw_networkx_edge_labels(g2, pos, edge_labels=edge_labels)\nnx.draw_networkx(\n    g2, pos, node_size=[x * 0.002 for x in size], node_color=colors\n)\n\npos = nx.spring_layout(g2)\nx_values, y_values = zip(*pos.values())\nx_max = max(x_values)\nx_min = min(x_values)\nx_margin = (x_max - x_min) * 0.40\nplt.xlim(x_min - x_margin, x_max + x_margin)\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\nV(g2)$followers = c(100000, 200000,\n                    50000,500000, 40000)\nV(g2)$party = c(\"Democrat\", \"Republican\",\n            \"Democrat\", \"Democrat\", \"Republican\")\nV(g2)$color = V(g2)$party\nV(g2)$color = gsub(\"Democrat\", \"blue\", \n                   V(g2)$color)\nV(g2)$color = gsub(\"Republican\", \"red\",\n                   V(g2)$color)\nplot(g2, edge.label = E(g2)$weight,\n     vertex.size = V(g2)$followers*0.0001)\n\n\n\n\n\n\n\n\n\n\n\nWe can mention a third type of graphs: the induced subgraphs, which are in fact subsets of nodes and edges of a bigger graph. We can represent these subsets as \\(G' = V', E'\\). In Example 13.7 we extract two induced subgraphs from our original network of American politicians on Facebook (g1): the first (g3) is built with the edges that contain only Democrat nodes, and the second (g4) with edges formed by Republican nodes. There is also a special case of an induced subgraph, called a clique, which is an independent or complete subset of an undirected graph (each node of the clique must be connected to the rest of the nodes of the subgraph).\n\n\n\n\n\n\n\nExample 13.7 Induced subgraphs for Democrats and Republicans\n\nPython codeR code\n\n\n\n# Democrats:\ng3 = g1.subgraph([\"Hillary Clinton\", \"Bernie Sanders\", \"Barack Obama\"])\nprint(\"Nodes:\", g3.number_of_nodes(), \"Edges: \", g3.number_of_edges())\n\nNodes: 3 Edges:  3\n\nprint(g3.edges)\n\n# Republicans:\n\n[('Hillary Clinton', 'Bernie Sanders'), ('Hillary Clinton', 'Barack Obama')...\n\ng4 = g1.subgraph([\"Donald Trump\", \"John McCain\"])\nprint(\"Nodes:\", g4.number_of_nodes(), \"Edges: \", g4.number_of_edges())\n\nNodes: 2 Edges:  1\n\nprint(g4.edges)\n\n[('John McCain', 'Donald Trump')]\n\n\n\n\n\n# Democrats:\ng3 = induced_subgraph(g1, c(1,3,4))\nprint(g3)\n\nIGRAPH 9abc2c8 UN-- 3 3 -- \n+ attr: name (v/c)\n+ edges from 9abc2c8 (vertex names):\n[1] Hillary Clinton--Bernie Sanders Hillary Clinton--Barack Obama  \n[3] Bernie Sanders --Barack Obama  \n\n# Republicans:)\ng4 = induced_subgraph(g1, c(2,5))\nprint(g4)\n\nIGRAPH 9ac0459 UN-- 2 1 -- \n+ attr: name (v/c)\n+ edge from 9ac0459 (vertex names):\n[1] Donald Trump--John McCain\n\n\n\n\n\n\n\n\n\nKeep in mind that in network visualization you can always configure the size, shape and color of your nodes or edges. It is out of the scope of this book to go into more technical details, but you can always check the online documentation of the recommended libraries.\nSo far we have created networks from scratch, but most of the time you will have to create a graph from an existing data file. This means that you will need an input data file with the graph structure, and some functions to load them as objects onto your workspace in R or Python. You can import graph data from different specific formats (e.g., Graph Modeling Language (GML), GraphML, JSON, etc.), but one popular and standardized procedure is to obtain the data from a text file containing a list of edges or a matrix. In Example 13.8 we illustrate how to read graph data in igraph and networkx using a simple adjacency list that corresponds to our original imaginary Twitter network of American politicians (g2).\n\n\n\n\n\n\n\nExample 13.8 Reading a graph from a file\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/poltwit.csv\"\nfn, _headers = urllib.request.urlretrieve(url)\ng2 = nx.read_adjlist(fn, create_using=nx.DiGraph, delimiter=\",\")\nprint(\"Nodes:\", g2.number_of_nodes(), \"Edges: \", g2.number_of_edges())\n\nNodes: 5 Edges:  7\n\n\n\n\n\nedges = read_csv(\n\"https://cssbook.net/d/poltwit.csv\",\n    col_names=FALSE)\ng2 = graph_from_data_frame(d=edges) \nglue(\"Nodes: \", gorder(g2),\n     \"  Edges: \", gsize(g2))\n\nNodes: 5  Edges: 7\n\nplot(g2)"
  },
  {
    "objectID": "content/chapter13.html#sec-sna",
    "href": "content/chapter13.html#sec-sna",
    "title": "13  Network Data",
    "section": "13.2 Social Network Analysis",
    "text": "13.2 Social Network Analysis\nThis section gives an overview of the existing measures to conduct Social Network Analysis (SNA). Among other functions, we explain how to examine paths and reachability, how to calculate centrality measures (degree, closeness, betweenness, eigenvector) to quantify the importance of a node in a graph, and how to detect communities in the graph using clustering.\n\n13.2.1 Paths and Reachability\nThe first idea that comes to mind when analyzing a graph is to understand how their nodes are connected. When multiple edges create a network we can observe how the vertices constitute one or many paths that can be described. In this sense, a sequence between node x and node y is a path where each node is adjacent to the previous. In the imaginary social network of friendship of American politicians contained in the undirected graph g1, we can determine the sequences or simple paths between any pair of politicians. As shown in Example 13.9 we can use the function all_simple_paths contained in both igraph (R) and networkx (Python), to obtain the two possible routes between Barack Obama and John McCain. The shortest path includes the nodes Hillary Clinton and Donald Trump; and the longer includes Sanders, Clinton, and Trump.\n\n\n\n\n\n\n\nExample 13.9 Possible paths between two nodes in the imaginary Facebook network of American politicians\n\nPython codeR code\n\n\n\nfor path in nx.all_simple_paths(\n    g1, source=\"Barack Obama\", target=\"John McCain\"\n):\n    print(path)\n\n['Barack Obama', 'Hillary Clinton', 'Donald Trump', 'John McCain']\n['Barack Obama', 'Bernie Sanders', 'Hillary Clinton', 'Donald Trump', 'John...\n\n\n\n\n\nall_simple_paths(g1, \"Barack Obama\",\"John McCain\",\n                 mode = c(\"all\"))\n\n[[1]]\n+ 4/5 vertices, named, from cb241ef:\n[1] Barack Obama    Hillary Clinton Donald Trump    John McCain    \n\n[[2]]\n+ 5/5 vertices, named, from cb241ef:\n[1] Barack Obama    Bernie Sanders  Hillary Clinton Donald Trump   \n[5] John McCain    \n\n\n\n\n\n\n\n\n\nOne specific type of path is the one in which the initial node is the same than the final node. This closed path is called a circuit. To understand this concept let us recover the inducted subgraph of Democrat politicians (g3) in which we only have three nodes. If you plot this graph, as we do in Example 13.10, you can clearly visualize how a circuit works.\n\n\n\n\n\n\n\nExample 13.10 Visualization of a circuit.\n\nPython codeR code\n\n\n\nnx.draw_networkx(g3)\npos = nx.shell_layout(g3)\nx_values, y_values = zip(*pos.values())\nx_max = max(x_values)\nx_min = min(x_values)\nx_margin = (x_max - x_min) * 0.40\nplt.xlim(x_min - x_margin, x_max + x_margin)\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\nplot(g3)\n\n\n\n\n\n\n\n\n\n\n\nIn SNA it is extremely important to be able to describe the possible paths since they help us to estimate the reachability of the vertices. For instance, if we go back to our original graph of American politicians on Facebook (g1) visualized in Example 13.2, we can see that Sanders is reachable from McCain because there is a path between them (McCain–Trump–Clinton–Sanders). Moreover, we observe that this social network is fully connected because you can reach any given node from any other node in the graph. But it might not always be that way. Imagine that we remove the friendship of Clinton and Trump by deleting that specific edge. As you can observe in Example 13.11, when we create and visualize the graph g6 without this edge we can see that the network is no longer fully connected and it has two components. Technically speaking, we would say for example that the subgraph of Republicans is a connected component of the network of American politicians, given that this connected subgraph is part of the bigger graph while not connected to it.\n\n\n\n\n\n\n\nExample 13.11 Visualization of connected components.\n\nPython codeR code\n\n\n\n# Remove the friendship between Clinton and Trump\ng6 = g1.copy()\ng6.remove_edge(\"Hillary Clinton\", \"Donald Trump\")\nnx.draw_networkx(g6)\npos = nx.shell_layout(g6)\nx_values, y_values = zip(*pos.values())\nx_max = max(x_values)\nx_min = min(x_values)\nx_margin = (x_max - x_min) * 0.40\nplt.xlim(x_min - x_margin, x_max + x_margin)\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\n#Remove the friendship between Clinton and Trump\ng6 = delete.edges(g1, E(g1, P=\n            c(\"Hillary Clinton\",\"Donald Trump\")))\nplot(g6)\n\n\n\n\n\n\n\n\n\n\n\nWhen analyzing paths and reachability you may be interested in knowing the distances in your graph. One common question is what is the average path length of a social network, or in other words, what is the average of the shortest distance between each pair of vertices in the graph? This mean distance can tell you a lot about how close the nodes in the network are: the shorter the distance the closer the nodes are. Moreover, you can estimate the specific distance (shortest path) between two specific nodes. As shown in Example 13.12 we can estimate the average path length (1.7) in our imaginary Facebook network of American politicians using the functions mean_distance in igraph and average_shortest_path_length in networkx. In this example we also estimate the specific distance in the network between Obama and McCain (3) using the function distances in igraph and estimating the length (len) of the shortest path (first result of shortest_simple_paths minus 1) in networkx.\n\n\n\n\n\n\n\nExample 13.12 Estimating distances in the network\n\nPython codeR code\n\n\n\nprint(\n    \"Average path length in Facebook network: \",\n    nx.average_shortest_path_length(g1),\n)\n\nAverage path length in Facebook network:  1.7\n\npaths = list(nx.shortest_simple_paths(g1, \"Barack Obama\", \"John McCain\"))\nprint(\"Distance between Obama and McCain\", len(paths[0]) - 1)\n\nDistance between Obama and McCain 3\n\n\n\n\n\nglue(\"Average path length in Facebook network: \",\n     mean_distance(g1, directed = T))\n\nAverage path length in Facebook network: 1.7\n\nglue(\"Distance between Obama and McCain\",\n     \"in Facebook network: \", \n     distances(g1, v=\"Barack Obama\", \n               to=\"John McCain\", weights=NA))\n\nDistance between Obama and McCainin Facebook network: 3\n\n\n\n\n\n\n\n\n\nIn terms of distance, we can also wonder what the edges or nodes that share a border with any given vertex are. In the first case, we can identify the incident edges that go out or into one vertex. As shown in Example 13.13, by using the functions incident in igraph and edges in networkx we can easily get incident edges of John McCain in the Facebook Network (g1), which is just one single edge that joins Trump with McCain. In the second case, we can also identify its adjacent nodes, or in other words its neighbors. In the very same example, we use neighbors (same function in igraph and networkx) to obtain all the nodes one step away from McCain (in this case only Trump).\n\n\n\n\n\n\n\nExample 13.13 Incident edges and neighbors of J. McCain the imaginary Facebook Network\n\nPython codeR code\n\n\n\nprint(\"Incident edges of John McCain:\", g1.edges(\"John McCain\"))\n\nIncident edges of John McCain: [('John McCain', 'Donald Trump')]\n\nprint(\"Neighbors of John McCain\", [n for n in g1.neighbors(\"John McCain\")])\n\nNeighbors of John McCain ['Donald Trump']\n\n\n\n\n\n#mode: all, out, in\nglue(\"Incident edges of John McCain in\",\n     \"Facebook Network:\")\n\nIncident edges of John McCain inFacebook Network:\n\nincident(g1, V(g1)[\"John McCain\"], mode=\"all\")\n\n+ 1/5 edge from cb241ef (vertex names):\n[1] Donald Trump--John McCain\n\nglue(\"Neighbors of John McCain in\",\n     \"Facebook Network:\")\n\nNeighbors of John McCain inFacebook Network:\n\nneighbors(g1, V(g1)[\"John McCain\"], mode=\"all\")\n\n+ 1/5 vertex, named, from cb241ef:\n[1] Donald Trump\n\n\n\n\n\n\n\n\n\nThere are some other interesting descriptors of social networks. One of the most common measures is the density of the graph, which accounts for the proportion of edges relative to all possible ties in the network. In simpler words, the density tells us from 0 to 1 how much connected the nodes of a graph are. This can be estimated for both undirected and directed graphs. Using the functions edge_density in igraph and density in networkx we obtain a density of 0.5 (middle level) in the imaginary Facebook network of American politicians (undirected graph) and 0.35 in the Twitter network (directed graph).\nIn undirected graphs we can also measure transitivity (also known as clustering coefficient) and diameter. The first is a key property of social networks that refers to the ratio of triangles over the total amount of connected triples. It is to say that we wonder how likely it is that two nodes are connected if they share a mutual neighbor. Applying the function transitivity (included in igraph and networkx) to g1 we can see that this tendency is of 0.5 in the Facebook network (there is a 50% probability that two politicians are friends when they have a common contact). The second descriptor, the diameter, depicts the length of the network in terms of the longest geodesic distance4. We use the function diameter (included in igraph and networkx) in the Facebook network and get a diameter of 3, which you can also check if you go back to the visualization of g1 in Example 13.2.\nAdditionally, in directed graphs we can calculate the reciprocity, which is just the proportion of reciprocal ties in a social network and can be computed with the function reciprocity (included in igraph and networkx). For the imaginary Twitter network (directed graph) we get a reciprocity of 0.57 (which is not bad for a Twitter graph where important people usually have much more followers than follows!).\nIn Example 13.14 we show how to estimate these four measures in R and Python. Notice that in some of the network descriptors you have to decide whether or not to include the edge weights for computation (in the provided examples we did not take these weights into account).\n\n\n\n\n\n\n\nExample 13.14 Estimations of density, transitivity, diameter and reciprocity\n\nPython codeR code\n\n\n\n# Density in Facebook network:\nnx.density(g1)\n\n0.5\n\n\n\n# Density in Twitter network:\nnx.density(g2)\n\n0.35\n\n\n\n# Transitivity in Facebook network:\nnx.transitivity(g1)\n\n0.5\n\n\n\n# Diameter in Facebook network\nnx.diameter(g1, e=None, usebounds=False)\n\n3\n\n\n\n# Reciprocity in Twitter network:\nnx.reciprocity(g2)\n\n0.5714285714285714\n\n\n\n\n\n# Density in Facebook network:\nedge_density(g1)\n\n[1] 0.5\n\n# Density in Twitter network:\necount(g2)/(vcount(g2)*(vcount(g2)-1))\n\n[1] 0.35\n\n# Transitivity in Facebook network:\ntransitivity(g1, type=\"global\")\n\n[1] 0.5\n\n# Diameter in Facebook network\ndiameter(g1, directed = F, weights = NA)\n\n[1] 3\n\n# Reciprocity in Twitter network:\nreciprocity(g2)\n\n[1] 0.5714286\n\n\n\n\n\n\n\n\n\n\n\n13.2.2 Centrality Measures\nNow let us move to centrality measures. Centrality is probably the most common, popular, or known measure in the analysis of social networks because it gives you a clear idea of the importance of any of the nodes within a graph. Using its measures you can pose many questions such as which is the most central person in a network of friends on Facebook, who can be considered an opinion leader on Twitter or who is an influencer on Instagram. Moreover, knowing the specific importance of every node of the network can help us to visualize or label only certain vertices that overpass a previously determined threshold, or to use the color or size to distinguish the most central nodes from the others. There are four typical centrality measures: degree, closeness, eigenvector and betweenness.\nThe degree of a node refers to the number of ties of that vertex, or in other words, to the number of edges that are incident to that node. This definition is constant for undirected graphs in which the directions of the links are not declared. In the case of directed graphs, you will have three options to measure the degree. First, you can think of the number of edges pointing into a node, which we call indegree; second, we have the number of edges pointing out of a node, or outdegree. In addition, we could also have the total number of edges pointing (in and out) any node. Degree, as well as other measures of centrality mentioned below, can be expressed in absolute numbers, but we can also normalize5 these measures for better interpretation and comparison. We will prefer this latter approach in our examples, which is also the default option in many SNA packages.\nWe can then estimate the degree of two of our example networks. In Example 13.15, we first estimate the degree of each of the five American politicians in the imaginary Facebook network, which is an undirected graph; and then the total degree in the Twitter network, which is a directed graph. For both cases, we use the functions degree in igraph (R) and degree_centrality in networkx (Python). We later compute the in and out degree for the Twitter network. Using igraph we again used the function degree but now adjust the parameter mode to in or out, respectively. Using networkx, we employ the functions in_degree_centrality and out_degree_centrality.\n\n\n\n\n\n\n\nExample 13.15 Computing degree centralities in undirected and directed graphs\n\nPython codeR code\n\n\n\n# Degree centrality of Facebook network (undirected):\nprint(nx.degree_centrality(g1))\n\n{'Hillary Clinton': 0.75, 'Donald Trump': 0.5, 'Bernie Sanders': 0.5, 'Bara...\n\n\n\n# Degree centrality of Twitter network (directed):\nprint(nx.degree_centrality(g2))\n\n{'Hillary Clinton': 1.25, 'Donald Trump': 0.5, 'Bernie Sanders': 0.75, 'Bar...\n\n\n\n# In degree centrality of Twitter network (directed):\nprint(nx.in_degree_centrality(g2))\n\n{'Hillary Clinton': 0.5, 'Donald Trump': 0.5, 'Bernie Sanders': 0.5, 'Barac...\n\n\n\n# Out degree centrality of Twitter network (directed):\nprint(nx.out_degree_centrality(g2))\n\n{'Hillary Clinton': 0.75, 'Donald Trump': 0.0, 'Bernie Sanders': 0.25, 'Bar...\n\n\n\n\n\n# Degree centrality of Facebook network (undirected):\nprint(degree(g1, normalized = T))\n\nHillary Clinton    Donald Trump  Bernie Sanders    Barack Obama     John McCain \n           0.75            0.50            0.50            0.50            0.25 \n\n# Degree centrality of Twitter network (directed):\nprint(degree(g2, normalized = T, mode=\"all\"))\n\nHillary Clinton  Bernie Sanders     John McCain    Barack Obama    Donald Trump \n           1.25            0.75            0.25            0.75            0.50 \n\n# In degree centrality of Twitter network (directed):\nprint(degree(g2, normalized = T, mode=\"in\"))\n\nHillary Clinton  Bernie Sanders     John McCain    Barack Obama    Donald Trump \n           0.50            0.50            0.00            0.25            0.50 \n\n# Out degree centrality of Twitter network (directed):\nprint(degree(g2, normalized = T, mode=\"out\"))\n\nHillary Clinton  Bernie Sanders     John McCain    Barack Obama    Donald Trump \n           0.75            0.25            0.25            0.50            0.00 \n\n\n\n\n\n\n\n\n\nThere are three other types of centrality measures. Closeness centrality refers to the geodesic distance of a node to the rest of nodes in the graph. Specifically, it indicates how close a node is to the others by taking the length of the shortest paths between the vertices. Eigenvector centrality takes into account the importance of the surrounding nodes and computes the centrality of a vertex based on the centrality of its neighbors. In technical words, the measure is proportional to the sum of connection centralities. Finally, betweenness centrality indicates to what extent the node is in the paths that connect many other nodes. Mathematically it is computed as the sum of the fraction of every pair of (shortest) paths that go through the analyzed node.\nAs shown in Example 13.16, we can obtain these three measures from undirected graphs using the functions closeness, eigen_centrality and betweenness in igraph, and closeness_centrality, eigenvector_centrality and betweenness_centrality in networkx. If we take a look to the centrality measures for every politician of the imaginary Facebook network we see that Clinton seems to be a very important and central node of the graph, just coinciding with the above-mentioned findings based on the degree. It is not a rule that we obtain the very same trend in each of the centrality measures but it is likely that they have similar results although they are looking for different dimensions of the same construct.\n\n\n\n\n\n\n\nExample 13.16 Estimations of closeness, eigenvector and betweenness centralities\n\nPython codeR code\n\n\n\n# Closeness centrality of Facebook network (undirected):\nprint(nx.closeness_centrality(g1))\n\n{'Hillary Clinton': 0.8, 'Donald Trump': 0.6666666666666666, 'Bernie Sander...\n\n\n\n# Eigenvector centrality of Facebook network (undirected):\nprint(nx.eigenvector_centrality(g1))\n\n{'Hillary Clinton': 0.6037035301706528, 'Donald Trump': 0.34248744909850964...\n\n\n\n# Betweenness centrality of Facebook network (undirected):\nprint(nx.betweenness_centrality(g1))\n\n{'Hillary Clinton': 0.6666666666666666, 'Donald Trump': 0.5, 'Bernie Sander...\n\n\n\n\n\n# Closeness centrality of Facebook network (undirected):\nprint(closeness(g1, normalized = T))\n\nHillary Clinton    Donald Trump  Bernie Sanders    Barack Obama     John McCain \n      0.8000000       0.6666667       0.5714286       0.5714286       0.4444444 \n\n# Eigenvector centrality of Facebook network (undirected):\nprint(eigen_centrality(g1, scale=F)$vector)\n\nHillary Clinton    Donald Trump  Bernie Sanders    Barack Obama     John McCain \n      0.6037035       0.3424853       0.4971537       0.4971537       0.1546684 \n\n# Betweenness centrality of Facebook network (undirected):\nprint(betweenness(g1, normalized = T))\n\nHillary Clinton    Donald Trump  Bernie Sanders    Barack Obama     John McCain \n      0.6666667       0.5000000       0.0000000       0.0000000       0.0000000 \n\n\n\n\n\n\n\n\n\nWe can use these centrality measures in many ways. For example, you can take the degree centrality as a parameter of the node size and labeling when plotting the network. This may be of great utility since the reader can visually identify the most important nodes of the network while minimizing the visual impact of those that are less central. In Example 13.17 we decided to specify the size of the nodes (parameters vertex.size in igraph and node_size in networkx) with the degree centrality of each of the American politicians in the Twitter network (directed graph) contained in g2. We also used the degree centrality to filter the labels in the graph, and then included only those that overpassed a threshold of 0.5 (parameters vertex.label in igraph and labels in networkx). These two simple parameters of the plot give you a fair image of the potential of the centrality measures to describe and understand your social network.\n\n\n\n\n\n\n\nExample 13.17 Using the degree centrality to change the size and labels of the nodes\n\nPython codeR code\n\n\n\nsize = list(nx.degree_centrality(g2).values())\nsize = [x * 1000 for x in size]\nlabels_filtered = {k: v for k, v in nx.degree_centrality(g2).items() if v > 0.5}\nlabels = {}\nfor k, v in labels_filtered.items():\n    labels[k] = k\n\nnx.draw_networkx(g2, node_size=size, labels=labels)\n\npos = nx.shell_layout(g2)\nx_values, y_values = zip(*pos.values())\nx_max = max(x_values)\nx_min = min(x_values)\nx_margin = (x_max - x_min) * 0.40\nplt.xlim(x_min - x_margin, x_max + x_margin)\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\nplot(g2, vertex.label.cex = 2, \n     vertex.size= degree(g2, normalized = T)*40,\n     vertex.label = ifelse(degree(g2, \n        normalized = T) > 0.5, V(g2)$name, NA))\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.3 Clustering and Community Detection\nOne of the greatest potentials of SNA is the ability to identify how nodes are interconnected and thus define communities within a graph. This is to say that most of the time the nodes and edges in our network are not distributed homogeneously, but they tend to form clusters that can later be interpreted. In a social network you can think for example of the principle of homophily, which is the tendency of human beings to associate and interact with similar individuals; or you can think of extrinsic factors (e.g., economic or legal) that may generate the cohesion of small groups of citizens that belong to a wider social structure. While it is of course difficult to make strong claims regarding the underlying causes, we can use different computational approaches to model and detect possible communities that emerge from social networks and even interpret and label those groups. The creation of clusters as an unsupervised machine learning technique was introduced in Section 7.3 for structured data and in Section 11.5 for text analysis (topic modeling). We will use some similar unsupervised approaches for community detection in social networks.\nMany social and communication questions may arise when clustering a network. The identification of subgroups can tell us how diverse and fragmented a network is, or how the behavior of a specific community relates to other groups and to the entire graph. Moreover, the concentration of edges in some nodes of the graph would let us know about the social structure of the networks which in turn would mean a better understanding of its inner dynamic. It is true that the computational analyst will need more than the provided algorithms when labeling the groups to understand the communities, which means that you must become familiar with the way the graph has been built and what the nodes, edges or weights represent.\nA first step towards an analysis of subgroups within a network is to find the available complete subgraphs in an undirected graph. As we briefly explained at the end of Section 13.1, these independent subgraphs are called cliques and refer to subgroups where every vertex is connected to every other vertex. We can find the maximal cliques (a clique is maximal when it cannot be extended to a bigger clique) in the imaginary undirected graph of American politicians on Facebook (g1) by using the functions max_cliques in igraph (Eppstein, Löffler, and Strash 2010) and max_cliques in networkx (Cazals and Karande 2008). As you can see in Example 13.18, we obtain a total of three subgraphs, one representing the Democrats, another the Republicans, and one more the connector of the two parties (Clinton–Trump).\n\n\n\n\n\n\n\nExample 13.18 Finding all the maximal cliques in an undirected graph\n\nPython codeR code\n\n\n\nprint(f\"Number of cliques: {nx.graph_number_of_cliques(g1)}\")\n\nNumber of cliques: 3\n\nprint(f\"Cliques: {list(nx.find_cliques(g1))}\")\n\nCliques: [['Hillary Clinton', 'Bernie Sanders', 'Barack Obama'], ['Hillary ...\n\n\n\n\n\nglue(\"Number of cliques: {clique_num(g1)}\")\n\nNumber of cliques: 3\n\nmax_cliques(g1)\n\n[[1]]\n+ 2/5 vertices, named, from cb241ef:\n[1] Donald Trump John McCain \n\n[[2]]\n+ 2/5 vertices, named, from cb241ef:\n[1] Donald Trump    Hillary Clinton\n\n[[3]]\n+ 3/5 vertices, named, from cb241ef:\n[1] Hillary Clinton Bernie Sanders  Barack Obama   \n\n\n\n\n\n\n\n\n\nNow, in order to properly detect communities we will apply some common algorithms to obtain the most likely subgroups in a social network. The first of these models is the so called edge-between or Girvan–Newman algorithm (Newman and Girvan 2004). This algorithm is based on divisive hierarchical clustering (explained in Section 7.3) by breaking down the graph into pieces and iteratively removing edges from the original one. Specifically, the Girvan–Newman approach uses the betweenness centrality measure to remove the most central edge at each iteration. You can easily visualize this splitting process in a dendogram, as we do in Example 13.19, where we estimated cl_girvan to detect possible communities in the Facebook network. We used the functions cluster_edge_betweenness in igraph and girvan_newman in networkx.\n\n\n\n\n\n\n\nExample 13.19 Clustering with Girvan–Newman.\n\nPython codeR code\n\n\n\ncl_girvan = nxcom.girvan_newman(g1)\n# Note: see R output for a nice visualization\n\n\n\n\ncl_girvan = cluster_edge_betweenness(g1) \ndendPlot(cl_girvan, mode=\"hclust\")\n\n\n\n\n\n\n\n\n\n\n\nWhen you look at the figure you will notice that the final leaves correspond to the nodes (the politicians) and then you have different partition levels (1 to 4), which in fact are different cluster possibilities. In edge-betweenness clustering, the big question is which partition level to choose, or in other words, which of the community division is better. The concept of modularity arises as a good measure (\\(-1\\) to 1) to evaluate how good the division is (technically it’s measured as the fraction of edges that fall within any given groups, let’s say group 1 and group 2, minus the expected number of edges within those groups distributed at random). Thus, we can choose which of the four proposed divisions is the best based on the highest value of their modularities: the higher the modularity the more dense the connections within the community and the more sparse the connections across communities. In the case of cluster_edge_betweenness in igraph it automatically estimates that the best division (on modularity) is the first one with two communities.\nWith community detection algorithms we can then estimate the length (number of suggested clusters), membership (to which cluster belongs each node) and modularity (how good is the clustering). In the case of igraph in R we apply the functions length (base), membership and modularity over the produced clustering object (i.e., cl_girvan). In the case of networkx in Python we first have to specify that we want to use the first component of the divisions (out of four) using the function next. Then, we can apply the functions len (base) and modularity to get the descriptors, and print the first division (stored as communities1) to obtain the membership.\nThese functions are demonstrated in Example 13.20. Note that since we will be showing these properties for multiple clustering algorithms below, we create a convenience function summarize_clustering to display them.\n\n\n\n\n\n\n\nExample 13.20 Community detection with Girvan–Newman\n\nPython codeR code\n\n\n\ndef summarize_clustering(graph, clustering):\n    print(f\"Length {len(clustering)}\")\n    print(f\"Modularity: {nxcom.modularity(graph, clustering):.2f}\")\n    print(\"Membership:\")\n    for cluster in clustering:\n        print(f\" - {cluster}\")\n\nc1 = next(cl_girvan)\nsummarize_clustering(g1, c1)\n\nLength 2\nModularity: 0.22\nMembership:\n - {'Hillary Clinton', 'Bernie Sanders', 'Barack Obama'}\n - {'John McCain', 'Donald Trump'}\n\n\n\n\n\nsummarize_clustering = function(clustering) {\n    print(glue(\"Length {length(clustering)}\"))\n    print(glue(\"Modularity: {modularity(clustering)}\"))\n    print(glue(\"Membership:\"))\n    membership_list = membership(clustering)\n    for (cluster in unique(membership_list)) {\n        members = names(membership_list)[membership_list==cluster]\n        print(glue(\" - {paste(members, collapse=', ')}\"))\n    }\n}\nsummarize_clustering(cl_girvan)\n\nLength 2\nModularity: 0.22\nMembership:\n - Hillary Clinton, Bernie Sanders, Barack Obama\n - Donald Trump, John McCain\n\n\n\n\n\n\n\n\n\nWe can estimate the communities for our network using many other more clustering algorithms, such as the Louvain algorithm, the Propagating Label algorithm, and Greedy Optimization, among others. Similar to Girvan–Newman, the Louvain algorithm uses the measure of modularity to obtain a multi-level optimization (Blondel et al. 2008) and its goal is to obtain optimized clusters which minimize the number of edges between the communities and maximize the number of edges within the same community. For its part, the Greedy Optimization algorithm is also based on the modularity indicator (Clauset, Newman, and Moore 2004). It does not consider the edges’ weights and works by initially setting each vertex in its own community and then joining two communities to increase modularity until obtaining the maximum modularity. Finally, the Propagating Label algorithm – which takes into account edges’ weights – initializes each node with a unique label and then iteratively each vertex adopts the label of its neighbors until all nodes have the most common label of their neighbors (Raghavan, Albert, and Kumara 2007). The process can be conducted asynchronously (as done in our example), synchronously or semi-synchronously (it might produce different results).\nIn Example 13.21 we use cluster_louvain, cluster_fast_greedy and cluster_label_prop in igrapgh (R) and best_partition, greedy_modularity_communities and asyn_lpa_communities in networkx (Python). You can see that the results are quite similar6 and it is pretty clear that there are two communities in the Facebook network: Democrats and Republicans!\n\n\n\n\n\n\n\nExample 13.21 Community detection with Louvain Propagating Label and Greedy Optimization\n\nPython codeR code\n\n\n\n# Louvain:\n# (Note that the Louvain output is a dict of {member: cluster} rather than a \n#  list of clusters, so we convert the output first)\ncl_louvain = community_louvain.best_partition(g1)\ncl_louvain= [{k for (k,v) in cl_louvain.items() if v == cluster} \n             for cluster in set(cl_louvain.values())]\nsummarize_clustering(g1, cl_louvain)\n\nLength 2\nModularity: 0.22\nMembership:\n - {'Hillary Clinton', 'Bernie Sanders', 'Barack Obama'}\n - {'John McCain', 'Donald Trump'}\n\n\n\n# Greedy optimization:\n# (Note that the nxcom output is a generator, sorting it turns it into a list)\ncl_greedy = nxcom.greedy_modularity_communities(g1)\ncl_greedy = sorted(cl_greedy, key=len, reverse=True)\nsummarize_clustering(g1, cl_greedy)\n\nLength 2\nModularity: 0.22\nMembership:\n - frozenset({'Hillary Clinton', 'Bernie Sanders', 'Barack Obama'})\n - frozenset({'John McCain', 'Donald Trump'})\n\n\n\n# Propagating label:\ncl_propagation = nxcom.asyn_lpa_communities(g1)\n# (Note that the nxcom output is a generator, sorting it turns it into a list)\ncl_propagation = sorted(cl_propagation, key=len, reverse=True)\nsummarize_clustering(g1, cl_propagation)\n\nLength 1\nModularity: 0.00\nMembership:\n - {'Hillary Clinton', 'Bernie Sanders', 'Barack Obama', 'John McCain', 'Do...\n\n\n\n\n\n# Louvain:\ncl_louvain = cluster_louvain(g1) \nsummarize_clustering(cl_louvain)\n\nLength 2\nModularity: 0.22\nMembership:\n - Hillary Clinton, Bernie Sanders, Barack Obama\n - Donald Trump, John McCain\n\n# Greedy optimization:\ncl_greedy = cluster_fast_greedy(g1) \nsummarize_clustering(cl_greedy)\n\nLength 2\nModularity: 0.22\nMembership:\n - Hillary Clinton, Bernie Sanders, Barack Obama\n - Donald Trump, John McCain\n\n# Label propagation:\ncl_propagation = cluster_label_prop(g1) \nsummarize_clustering(cl_propagation)\n\nLength 1\nModularity: 0\nMembership:\n[1] \" - Hillary Clinton, Donald Trump, Bernie Sanders, Barack Obama, John M...\"\n\n\n\n\n\n\n\n\n\nWe can plot each of those clusters for better visualization of the communities. In Example 13.22 we generate the plots with the Greedy Optimization algorithm in R and the Louvain algorithm in Python, and we get two identical results.\n\n\n\n\n\n\n\nExample 13.22 Plotting clusters with Greedy optimization in R and Louvain in Python\n\nPython codeR code\n\n\n\n# From which cluster does each node originate?\n# (note that for Louvain clustering, this actually reverses the preprocessing we did above\n#  so we could also have used the original community_louvain(.) output directly. \n#  For the networkx community functions this conversion is needed in any case)\nclusters = [next(i for (i, members) in enumerate(cl_louvain) if x in members) \n            for x in g1.nodes]\n\npos = nx.spring_layout(g1)\nplt.figure(figsize=(6, 3))\nplt.axis(\"off\")\nnx.draw_networkx_labels(g1, pos, font_size=6)\nnx.draw_networkx_nodes(g1, pos, cmap=plt.cm.RdYlBu, node_color=clusters)\nnx.draw_networkx_edges(g1, pos, alpha=0.3)\nplt.show(g1)\n\n\n\n\n\n\n\nplot(cl_louvain, g1)\n\n\n\n\n\n\n\n\n\n\n\nThere are more ways to obtain subgraphs of your network (such as the K-core decomposition) or to evaluate the homophily of your graph (using the indicator of assortativity that measures the degree to which the nodes associate to similar vertices). In fact, there are many other measures and techniques you can use to conduct SNA that we have deliberately omitted in this section for reasons of space, but we have covered the most important aspects and procedures you need to know to initiate yourself in the computational analysis of networks.\nSo far we have seen how to conduct SNA over “artificial” graphs for the sake of simplicity. However, the representation and analysis of “real world” networks will normally be more challenging because of their size or their complexity. To conclude this chapter we will show you how to apply some of the explained concepts to real data.\nUsing the Twitter API (see Section 12.1), we retrieved the names of the first 100 followers of the five most important politicians in Spain by 2017 (Mariano Rajoy, Pedro Sánchez, Albert Rivera, Alberto Garzón and Pablo Iglesias). With this information we produced an undirected graph7 of the “friends” of these Spanish politicians in order to understand how these leaders where connected through their followers. In Example 13.23 we load the data into a graph object g_friends that contains the 500 edges of the network. As we may imagine the five mentioned politicians were normally the most central nodes, but if we look at the degree, betweenness and closeness centralities we can easily get some of the relevant nodes of the Twitter network: CEARefugio, elenballesteros or Unidadpopular. These accounts deserve special attention since they contribute to the connection of the main leaders of that country. In fact, if we conduct clustering analysis using Louvain algorithm we will find a high modularity (0.77, which indicates that the clusters are well separated) and not surprisingly five clusters.\n\n\n\n\n\n\n\nExample 13.23 Loading and analyzing a real network of Spanish politicians and their followers on Twitter\n\nPython codeR code\n\n\n\nurl = \"https://cssbook.net/d/friends3.csv\"\nfn, _headers = urllib.request.urlretrieve(url)\ng_friends = nx.read_adjlist(fn, create_using=nx.Graph, delimiter=\";\")\nprint(f\"Nodes: {g_friends.number_of_nodes()}, \"\n      f\"Edges: {g_friends.number_of_edges()}\")\n\nNodes: 491, Edges: 500\n\n\n\n# Degree centrality:\ndef sort_by_value(dict):\n    return sorted(dict.items(), key=lambda item: item[1], reverse=True)\nprint(sort_by_value(nx.degree_centrality(g_friends)))\n\n[('Pablo_Iglesias_', 0.20612244897959187), ('Albert_Rivera', 0.204081632653...\n\n\n\n# Betweenness centrality: \nprint(sort_by_value(nx.betweenness_centrality(g_friends)))\n\n[('sanchezcastejon', 0.4847681328312369), ('agarzon', 0.44044055921356695),...\n\n\n\n# Closeness centrality: \nprint(sort_by_value(nx.closeness_centrality(g_friends)))\n\n[('sanchezcastejon', 0.3592375366568915), ('agarzon', 0.33653846153846156),...\n\n\n\n# Clustering with Louvain: \ncluster5 = community_louvain.best_partition(g_friends)\nprint(\"Length: \", len(set(cluster5.values())))\n\nLength:  5\n\nprint(\"Modularity: \" f\"{community_louvain.modularity(cluster5,g_friends):.2f}\")\n\nModularity: 0.77\n\n\n\n\n\nedges = read_delim(\"https://cssbook.net/d/friends3.csv\",\n                   col_names=FALSE, delim=\";\")\ng_friends = graph_from_data_frame(d=edges, directed = FALSE)\nglue(\"Nodes: {gorder(g_friends)}, edges: {gsize(g_friends)}\")\n\nNodes: 491, edges: 500\n\n# Degree centrality:\nprint(sort(degree(g_friends, normalized = T), decreasing = TRUE)[1:10])\n\nPablo_Iglesias_   Albert_Rivera         agarzon sanchezcastejon    marianorajoy \n    0.206122449     0.204081633     0.204081633     0.204081633     0.204081633 \n     CEARefugio  VictorLapuente javierfernandez        mas_demo       kanciller \n    0.004081633     0.004081633     0.004081633     0.004081633     0.004081633 \n\n# Betweenness centrality:\nprint(sort(betweenness(g_friends, normalized = T), decreasing = TRUE)[1:10])\n\nsanchezcastejon         agarzon   Albert_Rivera    marianorajoy Pablo_Iglesias_ \n     0.48476813      0.44044056      0.42263277      0.38465757      0.35788117 \nelenballesteros Unidadpopular__       kanciller JuanfranGuevara      CEARefugio \n     0.19595160      0.13749254      0.08787927      0.08787927      0.06942327 \n\n# Closeness centrality:\nprint(sort(closeness(g_friends, normalized = T), decreasing = TRUE)[1:10])\n\nsanchezcastejon         agarzon Pablo_Iglesias_ Unidadpopular__      CEARefugio \n      0.3592375       0.3365385       0.3356164       0.3353867       0.3349282 \n VictorLapuente javierfernandez        mas_demo elenballesteros   Albert_Rivera \n      0.3349282       0.3349282       0.3349282       0.3151125       0.3149100 \n\n# Clustering with Louvain:\ncluster5 = cluster_louvain(g_friends) \nprint(glue(\"Length: {length(cluster5)}\"))\n\nLength: 5\n\nprint(glue(\"Modularity: {modularity(cluster5)}\"))\n\nModularity: 0.771998\n\n\n\n\n\n\n\n\n\nWhen we visualize the clusters in the network (Example 13.24) using the degree centrality for the size of the node, we can locate the five politicians in the center of the clusters (depicted with different colors). More interesting, we can see that even when some users follow two of the political leaders, they are just assigned to one of the clusters. This the case of the node joining Garzón and Sánchez who is assigned to the Sánchez’s cluster, or the node joining Garzón and Rajoy who is assigned to Rajoy’s cluster. In the plot you can also see two more interesting facts. First, we can see a triangle that groups Sánchez, Garzón and Iglesias, which are leaders of the left-wing parties in Spain. Second, some pair of politicians (such as Iglesias–Garzón or Sánchez–Rivera) share more friends than the other possible pairs.\n\n\n\n\n\n\n\nExample 13.24 Visualizing the network of Spanish politicians and their followers on Twitter and plotting its clusters\n\nPython codeR code\n\n\n\npos = nx.spring_layout(g_friends)\nplt.figure(figsize=(10, 10))\nplt.axis(\"off\")\nsize = list(nx.degree_centrality(g_friends).values())\nsize = [x * 7000 for x in size]\nlabels_filtered = {\n    k: v for k, v in nx.degree_centrality(g_friends).items() if v > 0.005\n}\nlabels = {}\nfor k, v in labels_filtered.items():\n    labels[k] = k\n\nnx.draw_networkx_labels(g_friends, pos, font_size=10, labels=labels)\nnx.draw_networkx_nodes(\n    g_friends,\n    pos,\n    node_size=size,\n    cmap=plt.cm.RdYlBu,\n    node_color=list(cluster5.values()),\n)\nnx.draw_networkx_edges(g_friends, pos, alpha=0.5)\nplt.show(g_friends)\n\n\n\n\n\n\n\nplot(cluster5, g_friends, vertex.label.cex = 2,\n   vertex.size=degree(g_friends, normalized=T)*40,\n   vertex.label = ifelse(degree(g_friends,\n                         normalized=T) > 0.005,\n                         V(g_friends)$name, NA))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlondel, Vincent D, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. “Fast Unfolding of Communities in Large Networks.” Journal of Statistical Mechanics: Theory and Experiment 2008 (10): P10008.\n\n\nCazals, Frédéric, and Chinmay Karande. 2008. “A Note on the Problem of Reporting Maximal Cliques.” Theoretical Computer Science 407 (1-3): 564–68.\n\n\nChristakis, Nicholas A, and James H Fowler. 2009. Connected: The Surprising Power of Our Social Networks and How They Shape Our Lives. Little, Brown Spark.\n\n\nClauset, Aaron, Mark EJ Newman, and Cristopher Moore. 2004. “Finding Community Structure in Very Large Networks.” Physical Review E 70 (6): 066111.\n\n\nEppstein, David, Maarten Löffler, and Darren Strash. 2010. “Listing All Maximal Cliques in Sparse Graphs in Near-Optimal Time.” In International Symposium on Algorithms and Computation, 403–14. Springer.\n\n\nMoreno, Jacob Levy. 1934. Who Shall Survive? A New Approach to the Problem of Human Interrelations. Nervous; mental disease publishing co.\n\n\nNewman, Mark EJ, and Michelle Girvan. 2004. “Finding and Evaluating Community Structure in Networks.” Physical Review E 69 (2): 026113.\n\n\nRaghavan, Usha Nandini, Réka Albert, and Soundar Kumara. 2007. “Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks.” Physical Review E 76 (3): 036106.\n\n\nVosoughi, Soroush, Deb Roy, and Sinan Aral. 2018. “The Spread of True and False News Online.” Science 359 (6380): 1146–51.\n\n\nWatts, Duncan J. 2004. Six Degrees: The Science of a Connected Age. WW Norton & Company."
  },
  {
    "objectID": "content/chapter14.html#sec-beyond",
    "href": "content/chapter14.html#sec-beyond",
    "title": "14  Multimedia data",
    "section": "14.1 Beyond Text Analysis: Images, Audio and Video",
    "text": "14.1 Beyond Text Analysis: Images, Audio and Video\nA book about the computational analysis of communication would be incomplete without a chapter dedicated to analyzing visual data. In fact, if you think of the possible contents derived from social, cultural and political dynamics in the current digital landscape, you will realize that written content is only a limited slice of the bigger cake. Humans produce much more oral content than text messages, and are more agile in deciphering sounds and visual content. Digitalization of social and political life, as well as the explosion of self-generated digital content in the web and social media, have provoked an unprecedented amount of multimedia content that deserve to be included in many types of research.\nJust imagine a collection of digital recorded radio stations, or the enormous amount of pictures produced every day on Instagram, or even the millions of videos of social interest uploaded on Youtube. These are definitely goldmines for social researchers who traditionally used manual techniques to analyze just a very small portion of this multimedia content. However, it is also true that computational techniques to analyze audio, images or video are still little developed in social sciences given the difficulty of application for non-computational practitioners and the novelty of the discoveries in fields such as computer vision.\nThis section gives a brief overview of different formats of multimedia files. We explain how to generate useful inputs into our pipeline to perform computational analysis.\nYou are probably already familiar with digital formats of images (.jpg, .bmp, .gif, etc.), audio (.mp3, .wav, .wma, flac, etc.) or video (.avi, .mov, .wmv, .flv, etc.), which is the very first step to use these contents as input. However, similar to the case of texts you will need to do some preprocessing to put these formats into good shape and get a proper mathematical representation of the content.\nIn the case of audio, there are many useful computational approaches to do research over these contents: from voice recognition, audio sentiment analysis or sound classification, to automatic generation of music. Recent advances in the field of artificial intelligence have created a prosperous and diversified field with multiple academic and commercial applications. Nevertheless, computational social scientists can obtain great insights just by using specific applications such as speech-to-text transformation and then apply text analytics (already explained in chapters 9, 10, and 11) to the results. As you will see in Section 14.2, there are some useful libraries in R and Python to use pre-trained models to transcribe voice in different languages.\nEven when this approach is quite limited (just a small portion of the audio analytics world) and constrained (we will not address how to create the models), it will show how a specific, simple and powerful application of the automatic analysis of audio inputs can help answering many social questions (e.g., what are the topics of a natural conversation, what are the sentiments expressed in the scripts of radio news pieces, or which actors are named in oral speeches of any political party). In fact, automated analysis of audio can enable new research questions, different from those typically applied to text analysis. This is the case of the research by Knox and Lucas (2021), who used a computational approach over audio data from the Supreme Court Oral Arguments (407 arguments and 153 hours of audio, comprising over 66000 justice utterances and 44 million moments) to demonstrate that some crucial information such as the skepticism of legal arguments was transmitted by vocal delivery (e.g., speech tone), something indecipherable to text analysis. Or we could also mention the work by Dietrich, Hayes, and O’BRIEN (2019) who computationally analyzed the vocal pitch of more than 70000 Congressional floor audio speeches and found that female members of the Congress spoke with greater emotional intensity when talking about women.\nOn the other hand, applying computational methods to video input is probably the most challenging task in spite of the recent and promising advances in computer vision. For the sake of space, we will not cover specific video analytics in this chapter, but it is important to let you know that most of the computational analysis of video is based on the inspection of image and audio contents. With this standard approach you need to specify which key frames you are going to extract from the video (for example take a still image every 1000 frames) and then apply computer vision techniques (such as object detection) to those independent images. Check for example version 3 of the object detection architecture You Only Look Once Take (YOLOv3)1 created by Redmon and Farhadi (2018), which uses a pre-trained Convolutional Neural Network (CNN) (see Section 14.4) to locate objects within the video (Figure 14.1). To answer many social science questions you might complement this frame-to-frame image analysis with an analysis of audio features. In any case, this approach will not cover some interesting aspects of the video such as the camera frame shots and movements, or the editing techniques, which certainly give more content information.\n\n\n\nFigure 14.1: A screen shot of a real-time video analyzed by YOLOv3 on its website https://pjreddie.com/darknet/yolo/"
  },
  {
    "objectID": "content/chapter14.html#sec-apivisions",
    "href": "content/chapter14.html#sec-apivisions",
    "title": "14  Multimedia data",
    "section": "14.2 Using Existing Libraries and APIs",
    "text": "14.2 Using Existing Libraries and APIs\nIn the following sections we will show you how to deal with multimedia contents from scratch, with special attention to image classification using state-of-the-art libraries. However, it might be a good idea to begin by using existing libraries that directly implement multimedia analyses or by connecting to commercial services to deploy classification tasks remotely using their APIs. There is a vast variety of available libraries and APIs, which we cannot cover in this book, but we will briefly mention some of them that may be useful in the computational analysis of communication.\nOne example in the field of visual analytics is the optical character recognition (OCR). It is true that you can train your own models to deploy multi-class classification and predict every letter, number or symbol in an image, but it will be a task that will take you a lot of effort. Instead, there are specialized libraries in both R and Python such as tesseract that deploy this task in seconds with high accuracy. It is still possible that you will have to apply some pre-processing to the input images in order to get them in good shape. This means that you may need to use packages such as PIL or Magick to improve the quality of the image by cropping it or by reducing the background noise. In the case of PDF files you will have to convert them first into images and then apply OCR.\nIn the case of more complex audio and image documents you can use more sophisticated services provided by private companies (e.g., Google, Amazon, Microsoft, etc.). These commercial services have already deployed their own machine learning models with very good results. Sometimes you can even customize some of their models, but as a rule their internal features and configuration are not transparent to the user. Moreover, these services offer friendly APIs and, usually, a free quota to deploy your first exercises.\nTo work with audio files, many social researchers might need to convert long conversations, radio programs, or interviews to plain text. For this propose, Google Cloud offer the service Speech-to-Text2 that remotely transcribes the audio to a text format supporting multiple languages (more than 125!). With this service you can remotely use the advanced deep learning models created by Google Platform from your own local computer (you must have an account and connect with the proper packages such as googleLanguageR or google-cloud-language in Python).\nIf you apply either OCR to images or Speech-to-Text recognition to audio content you will have juicy plain text to conduct NLP, sentiment analysis, topic modelling, among other techniques (see Chapter 11). Thus, it is very likely that you will have to combine different libraries and services to perform a complete computational pipeline, even jumping from R to Python, and vice versa!\nFinally, we would like to mention the existence of the commercial services of autotaggers, such as Google’s Cloud Vision, Microsoft’s Computer Vision or Amazon’s Recognition. For example, if you connect to the services of Amazon’s Recognition you can not only detect and classify images, but also conduct sentiment analysis over faces or predict sensitive contents within the images. As in the case of Google Cloud, you will have to obtain commercially sold credentials to be able to connect to Amazon’s Recognition API (although you get a free initial “quota” of API access calls before you are required to pay for usage). This approach has two main advantages. The first is the access to a very well trained and validated model (continuously re-trained) over millions of images and with the participation of thousands of coders. The second is the scalability because you can store and analyze images at scale at a very good speed using cloud computing services.\n\n\n\nFigure 14.2: A photograph of refugees on a lifeboat, used as an input for Amazon’s Recognition API. The commercial service detects in the pictures classes such as clothing, apparel, human, person, life jacket or vest.\n\n\nAs an example, you can use Amazon’s Recognition to detect objects in a news photograph of refugees in a lifeboat (Figure 14.2) and you will obtain a set of accurate labels: Clothing (99.95%), Apparel (99.95%), Human (99.70%), Person (99.70%), Life jacket (99.43%) and Vest (99.43%). With a lower confidence you will also find labels such as Coat (67.39%) and People (66.78%). This example also highlights the need for validation, and the difficulty of grasping complex concepts in automated analyses: while all of these labels are arguably correct, it is safe to say that they fail to actually grasp the essence of the picture and the social context. One may even go as far as saying that – knowing the picture is about refugees – some of these labels, were they given by a human to describe the picture, would sound pretty cynical.\nIn Section 14.4 we will use this very same image (stored as myimg2_RGB) to detect objects using a classification model trained with an open-access database of images (ImageNet). You will find that there are some different predictions in both methods, but especially that the time to conduct the classification is shorter in the commercial service, since we don’t have to train or choose a model. As you may imagine, you can neither modify the commercial models nor have access to their internal details, which is a strong limitation if you want to build your own customized classification system."
  },
  {
    "objectID": "content/chapter14.html#sec-storing",
    "href": "content/chapter14.html#sec-storing",
    "title": "14  Multimedia data",
    "section": "14.3 Storing, Representing, and Converting Images",
    "text": "14.3 Storing, Representing, and Converting Images\nIn this section we will focus on learning how to store, represent, and convert images for further computational analysis. For a more exhaustive discussion of the computational analysis of images, see Williams, Casas, and Wilkerson (2020).\nTo perform basic image manipulation we have to: (i) load images and transform their shape when it is necessary (by cropping or resizing), and (ii) create a mathematical representation of the image (normally derived from its size, colors and pixel intensity) such as a three-dimensional matrix (x, y, color channel) or a flattened vector. You have some useful libraries in Python and R (pil and imagemagik, respectively) to conduct research in these initial stages, but you will also find that more advanced libraries in computer vision will include functions or modules for pre-processing images. At this point you can work either locally or remotely, but keep in mind that images can be heavy files and if you are working with thousands of files you will probably need to store or process them in the cloud (see Section 15.2).\nYou can load any image as an object into your workspace as we show in Example 14.1. In this case we load two pictures of refugees published by mainstream media in Europe (see Amores, Calderón, and Stanek (2019)), one is a JPG and the other is a PNG file. For this basic loading step we used the open function of the Image module in pil and image_read function in imagemagik. The JPG image file is a \\(805\\times 453\\) picture with the color model RGB and the PNG is a \\(1540\\times 978\\) picture with the color model RGBA. As you may notice the two objects have different formats, sizes and color models, which means that there is little analysis you can do if you don’t create a standard mathematical representation of both.\n\n\n\n\n\n\n\nExample 14.1 Loading JPG and PNG pictures as objects\n\nPython codeR code\n\n\n\nmyimg1 = Image.open(\n    requests.get(\"https://cssbook.net/d/259_3_32_15.jpg\", stream=True).raw\n)\nmyimg2 = Image.open(\n    requests.get(\"https://cssbook.net/d/298_5_52_15.png\", stream=True).raw\n)\nprint(myimg1)\n\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=805x453 at 0x7FCA51B...\n\nprint(myimg2)\n\n<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1540x978 at 0x7FCA51B...\n\n\n\n\n\nmyimg1 = image_read(\n    \"https://cssbook.net/d/259_3_32_15.jpg\")\nmyimg2 = image_read(\n    \"https://cssbook.net/d/298_5_52_15.png\")\nrbind(image_info(myimg1), image_info(myimg2))\n\n# A tibble: 2 × 7\n  format width height colorspace matte filesize density\n  <chr>  <int>  <int> <chr>      <lgl>    <int> <chr>  \n1 JPEG     805    453 sRGB       FALSE    75275 72x72  \n2 PNG     1540    978 sRGB       TRUE   2752059 57x57  \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.3: Representation of the matrix data structure of a RGB image in which each pixel contains information for the intensity of each color component.\n\n\nThe good news when working with digital images is that the concept of pixel (picture element) will help you to understand the basic mathematical representation behind computational analysis of images. A rectangular grid of pixels is represented by a dot matrix which in turn generates a bitmap image or raster graphic. The dot matrix data structure is a basic but powerful representation of the images since we can conduct multiple simple and advanced operations with the matrices. Specifically, each dot in the matrix is a number that contains information about the intensity of each pixel (that commonly ranges from 0 to 255) also known as bit or color depth (figure 14.3). This means that the numerical representation of a pixel can have 256 different values, 0 being the darkest tone of a given color and 255 the lightest. Keep in mind that if you divide the pixel values by 255 you will have a 0–1 scale to represent the intensity.\nIn a black-and-white picture we will only have one color (gray-scale), with the darker points representing the black and the lighter ones the white. The mathematical representation will be a single matrix or a two-dimensional array in which the number of rows and columns will correspond to the dimensions of the image. For instance in a \\(224 \\times 224\\) black-and-white picture we will have 50176 integers (0–255 scales) representing each pixel intensity.\nIn Example 14.2 we convert our original JPG picture to gray-scale and then create an object with the mathematical representation (a \\(453 \\times 805\\) matrix).\n\n\n\n\n\n\n\nExample 14.2 Converting images to gray-scale and creating a two-dimensional array\n\nPython codeR code\n\n\n\nmyimg1_L = myimg1.convert(\"L\")\nprint(type(myimg1_L))\n\n<class 'PIL.Image.Image'>\n\nmyimg1_L_array = np.array(myimg1_L)\nprint(type(myimg1_L_array))\n\n<class 'numpy.ndarray'>\n\nprint(myimg1_L_array.shape)\n\n(453, 805)\n\n\n\n\n\nmyimg1_L = image_convert(myimg1, colorspace = \"gray\")\nprint(class(myimg1_L))\n\n[1] \"magick-image\"\n\nmyimg1_L_array = as.integer(myimg1_L[[1]])\nprint(class(myimg1_L_array))\n\n[1] \"array\"\n\nprint(dim(myimg1_L_array))\n\n[1] 453 805   1\n\n\n\n\n\n\n\n\n\nBy contrast, color images will have multiple color channels that depend on the color model you chose. One standard color model is the three-channel RGB (red, green and blue), but you can find other variations in the chosen colors and the number of channels such as: RYB (red, yellow and blue), RGBA (red, green, blue and alpha3 ) or CMYK (cyan, magenta, yellow and key4). Importantly, while schemes used for printing such as CMYK are substractive (setting all colors to their highest value results in black, setting them to their lowest value results in white), schemes used for computer and television screens (such as RGB) are additive: setting all of the colors to their maximal value results in white (pretty much the opposite as what you got with your paintbox in primary school).\nWe will mostly use RGB in this book since it is the most used representation in the state-of-the-art literature in computer vision given that normally these color channels yield more accurate models. RGB’s mathematical representation will be a three-dimensional matrix or a collection of three two-dimensional arrays (one for each color) as we showed in figure 14.3. Then an RGB \\(224 \\times 224\\) picture will have 50176 pixel intensities for each of the three colors, or in other words a total of 150528 integers!\nNow, in Example 14.3 we convert our original JPG file to a RGB object and then create a new object with the mathematical representation (a \\(453 \\times 805 \\times 3\\) matrix).\n\n\n\n\n\n\n\nExample 14.3 Converting images to RGB color model and creating three two-dimensional arrays\n\nPython codeR code\n\n\n\n\nmyimg1_RGB = myimg1.convert(\"RGB\")\nprint(type(myimg1_RGB))\n\n<class 'PIL.Image.Image'>\n\nmyimg1_RGB_array = np.array(myimg1_RGB)\nprint(type(myimg1_RGB_array))\n\n<class 'numpy.ndarray'>\n\nprint(myimg1_RGB_array.shape)\n\n(453, 805, 3)\n\n\n\n\n\nmyimg1_RGB = image_convert(myimg1, colorspace = \"RGB\")\nprint(class(myimg1_RGB))\n\n[1] \"magick-image\"\n\nmyimg1_RGB_array = as.integer(myimg1_RGB[[1]])\nprint(class(myimg1_RGB_array))\n\n[1] \"array\"\n\nprint(dim(myimg1_RGB_array))\n\n[1] 453 805   3\n\n\n\n\n\n\n\n\n\nInstead of pixels, there are other ways to store digital images. One of them is the vector graphics, with formats such as .ai, .eps, .svg or .drw. Differently to bitmap images, they don’t have a grid of dots but a set of paths (lines, triangles, square, curvy shapes, etc.) that have a start and end point, so simple and complex images are created with paths. The great advantage of this format is that images do not get “pixelated” when you enlarge them because the paths can easily be transformed while remaining smooth. However, to obtain the standard mathematical representation of images you can convert the vector graphics to raster graphics (the way back is a bit more difficult and often only possible by approximation).\nSometimes you need to convert your image to a specific size. For example, in the case of image classification this is a very important step since all the input images of the model must have the same size. For this reason, one of the most common tasks in the preprocessing stage is to change the dimensions of the image in order to adjust width and height to a specific size. In Example 14.4 we use the resize method provided by pil and the image_scale function in imagemagik to reduce the first of our original pictures in RGB (myimg1_RGB) to 25% . Notice that we first obtain the original dimensions of the photograph (i.e. myimg1_RGB.width or image_info(myimg1_RGB)['width'][[1]]) and then multiply it by 0.25 in order to obtain the new size which is the argument required by the functions.\n\n\n\n\n\n\n\nExample 14.4 Resize to 25% and visualize a picture\n\nPython codeR code\n\n\n\n# Resize and visalize myimg1. Reduce to 25%\nwidth = int(myimg1_RGB.width * 0.25)\nheight = int(myimg1_RGB.height * 0.25)\nmyimg1_RGB_25 = myimg1_RGB.resize((width, height))\nplt.imshow(myimg1_RGB_25)\n\n\n\n\n\n\n\n#Resize and visalize myimg1. Reduce to 25%\nmyimg1_RGB_25 = image_scale(myimg1_RGB,\n        image_info(myimg1_RGB)[\"width\"][[1]]*0.25)\nplot(myimg1_RGB_25)\n\n\n\n\n\n\n\n\n\n\n\nNow, using the same functions of the latter example, we specify in Example 14.5 how to resize the same picture to \\(224 \\times 244\\), which is one of the standard dimensions in computer vision.\n\n\n\n\n\n\n\nExample 14.5 Resize to \\(224 \\times 224\\) and visualize a picture\n\nPython codeR code\n\n\n\n# Resize to 224 x 224\nmyimg1_RGB_224 = myimg1_RGB.resize((224, 224))\nplt.imshow(myimg1_RGB_224)\n\n\n\n\n\n\n\n# Resize and visalize myimg1. Resize to 224 x 224\n# The ! is used to specify an exact width and height\nmyimg1_RGB_224 = image_scale(myimg1_RGB,\n                             \"!224x!224\")\nplot(myimg1_RGB_224)\n\n\n\n\n\n\n\n\n\n\n\nYou may have noticed that the new image has now the correct width and height but that it looks deformed. The reason is that the original picture was not squared and our order was to force it to fit into a \\(224 \\times 224\\) square, losing its original aspect. There are different alternatives to solving this issue, but probably the most extended is to crop the original image to create a squared picture. As you can see in Example 14.6 we can create a function that first determines the orientation of the picture (vertical versus horizontal) and then cut the margins (up and down if it is vertical; and left and right if it is horizontal) to create a square. After applying this ad hoc function crop to the original image we can resize again to obtain a non-distorted \\(224 \\times 224\\) image.\nOf course you are now losing part of the picture information, so you may think of other alternatives such as filling a couple of sides with blank pixels (or padding) in order to create the square by adding information instead of removing it.\n\n\n\n\n\n\n\nExample 14.6 Function to crop the image to create a square and the resize the picture\n\nPython codeR code\n\n\n\n# Crop and resize to 224 x 224\n\n# Adapted from Webb, Casas & Wilkerson (2020)\ndef crop(img):\n    height = img.height\n    width = img.width\n    hw_dif = abs(height - width)\n    hw_halfdif = hw_dif / 2\n    crop_leftright = width > height\n    if crop_leftright:\n        y0 = 0\n        y1 = height\n        x0 = 0 + hw_halfdif\n        x1 = width - hw_halfdif\n    else:\n        y0 = 0 + hw_halfdif\n        y1 = height - hw_halfdif\n        x0 = 0\n        x1 = width\n    return img.crop((x0, y0, x1, y1))\n\nmyimg1_RGB_crop = crop(myimg1_RGB)\nmyimg1_RGB_crop_224 = myimg1_RGB_crop.resize((224, 224))\nplt.imshow(myimg1_RGB_crop_224)\n\n\n\n\n\n\n\n#Crop and resize to 224 x 224\n#Create function\ncrop = function(img) {\n    width = image_info(img)[\"width\"][[1]]\n    height = image_info(img)[\"height\"][[1]]\n    if (width > height) {\n        return (image_crop(img, \n                sprintf(\"%dx%d+%d\", height,\n                    height, (width-height)/2)))\n    }   else  {\n        return (image_crop(img,\n                sprintf(\"%sx%s+%s+%s\", width,\n        width, (width-width), (height-width)/2)))\n        }\n    }\n\nmyimg1_RGB_crop = crop(myimg1_RGB)\nmyimg1_RGB_crop_224 = image_scale(myimg1_RGB_crop, \"!224x!224\")\nplot(myimg1_RGB_crop_224)\n\n\n\n\n\n\n\n\n\n\n\nYou can also adjust the orientation of the image, flip it, or change its background, among other commands. These techniques might be useful for creating extra images in order to enlarge the training set in image classification (see Section 14.4). This is called data augmentation and consists of duplicating the initial examples on which the model was trained and altering them so that the algorithm can be more robust and generalize better. In Example 14.7 we used the rotate method in pil and image_rotate function in imagemagik to rotate 45 degrees the above resized image myimg1_RGB_224 to see how easily we can get an alternative picture with similar information to include in an augmented training set.\n\n\n\n\n\n\n\nExample 14.7 Rotating a picture 45 degrees\n\nPython codeR code\n\n\n\n# Rotate 45 degrees\nmyimg1_RGB_224_rot = myimg1_RGB_224.rotate(-45)\nplt.imshow(myimg1_RGB_224_rot)\n\n\n\n\n\n\n\n#Rotate 45 degrees\n#| cache: true\nmyimg1_RGB_224_rot = image_rotate(\n    myimg1_RGB_224, 45)\nplot(myimg1_RGB_224_rot)\n\n\n\n\n\n\n\n\n\n\n\nFinally, the numerical representation of visual content can help us to compare pictures in order to find similar or even duplicate images. Let’s take the case of RGB images which in Example 14.3 we showed how to transform to a three two-dimensional array. If we now convert the three-dimensional matrix of the image into a flattened vector we can use this simpler numerical representation to estimate similarities. Specifically, as we do in Example 14.8, we can take the vectors of two flattened images of resized \\(15 \\times 15\\) images to ease computation (img_vect1 and img_vect2) and use the cosine similarity to estimate how akin those images are. We stacked the two vectors in a matrix and then used the cosine_similarity function of the metrics module of the sklearn package in Python and the cosine function of the lsa package in R.\n\n\n\n\n\n\n\nExample 14.8 Comparing two flattened vectors to detect similarities between images\n\nPython codeR code\n\n\n\n# Create two 15x15 small images to compare\n\n# image1\nmyimg1_RGB_crop_15 = myimg1_RGB_crop_224.resize((15, 15))\n# image2\nmyimg2_RGB = myimg2.convert(\"RGB\")\nmyimg2_RGB_array = np.array(myimg2_RGB)\nmyimg2_RGB_crop = crop(myimg2_RGB)\nmyimg2_RGB_crop_224 = myimg2_RGB_crop.resize((224, 224))\nmyimg2_RGB_crop_15 = myimg2_RGB_crop_224.resize((15, 15))\n\nimg_vect1 = np.array(myimg1_RGB_crop_15).flatten()\nimg_vect2 = np.array(myimg2_RGB_crop_15).flatten()\n\nmatrix = np.row_stack((img_vect1, img_vect2))\n\nsim_mat = cosine_similarity(matrix)\nsim_mat\n\narray([[1.        , 0.86455477],\n       [0.86455477, 1.        ]])\n\n\n\n\n\n#Create two 15x15 small images to compare\n\n#image1\nmyimg1_RGB_crop_15 = image_scale(myimg1_RGB_crop_224, 15)\nimg_vect1 = as.integer(myimg1_RGB_crop_15[[1]])\nimg_vect1 = as.vector(img_vect1)\n\n#image2\nmyimg2_RGB = image_convert(myimg2, colorspace = \"RGB\")\nmyimg2_RGB_crop = crop(myimg2_RGB)\nmyimg2_RGB_crop_15 = image_scale(myimg2_RGB_crop, 15)\nimg_vect2 = as.integer(myimg2_RGB_crop_15[[1]])\n#drop the extra channel for comparision\nimg_vect2 = img_vect2[,,-4] \nimg_vect2 = as.vector(img_vect2)\n\nmatrix = cbind(img_vect1, img_vect2)\n\ncosine(img_vect1, img_vect2)\n\n          [,1]\n[1,] 0.8993252\n\ncosine(matrix)\n\n          img_vect1 img_vect2\nimg_vect1 1.0000000 0.8993252\nimg_vect2 0.8993252 1.0000000\n\n\n\n\n\n\n\n\n\nAs you can see in the resulting matrix when the images are compared with themselves (that would be the case of an exact duplicate) they obtain a value of 1. Similar images would obtain values under 1 but still close to it, while dissimilar images would obtain low values."
  },
  {
    "objectID": "content/chapter14.html#sec-cnn",
    "href": "content/chapter14.html#sec-cnn",
    "title": "14  Multimedia data",
    "section": "14.4 Image Classification",
    "text": "14.4 Image Classification\nThe implementation of computational image classification can help to answer many scientific questions, from testing some traditional hypotheses to opening new fields of interest in social science research. Just think about the potential of detecting at scale who appears in news photographs or what are the facial emotions expressed in the profiles of a social network. Moreover, imagine you can automatically label whether an image contains a certain action or not. For example, this is the case of Williams, Casas, and Wilkerson (2020) who conducted a binary classification of pictures related to the Black Lives Matter movement in order to model if a picture was a protest or not, which can help to understand the extent to which the media covered a relevant social and political issue.\nThere are many other excellent examples of how you can adopt image classification tasks to answer specific research questions in social sciences such as those of Horiuchi, Komatsu, and Nakaya (2012) who detected smiles in images of politicians to estimate the effects of facial appearance on election outcomes; or the work by Peng (2018) who used automated recognition of facial traits in American politicians to investigating the bias of media portrayals.\nIn this section, we will learn how to conduct computational image classification which is probably the most extended computer vision application in communication and social sciences (see Table 14.1 for some terminology). We will first discuss how to apply a shallow algorithm and then a deep-learning approach, given a labelled data set.\n\n\nTable 14.1: Some computer vision concepts used in computational analysis of communication\n\n\n\n\n\n\nComputer vision lingo\nDefinition\n\n\n\n\nbitmap\nFormat to store digital images using a rectangular grid of points of colors. Also called “raster image”.\n\n\npixel\nStands for “picture element” and is the smallest point of a bitmap image\n\n\ncolor model\nMathematical representation of colors in a picture. The standard in computer vision is RGB, but there are others such as RYB, RGBA or CMYK.\n\n\nvector graphic\nFormat to store digital images using lines and curves formed by points.\n\n\ndata augmentation\nTechnique to increase the training set of images by creating new ones base on the modification of some of the originals (cropping, rotating, etc.)\n\n\nimage classification\nMachine learning task to predict a class of an image based on a model. State-of-the-art image classification is conducted with Convolutional Neural Networks (CNN). Related tasks are object detection and image segmentation.\n\n\nactivation function\nParameter of a CNN that defines the output of a layer given the inputs of the previous layer. Some usual activation functions in image classification are sigmoid, softmax, or RELU.\n\n\nloss function\nParameter of a CNN which accounts for the difference between the prediction and the target variable (confidence in the prediction). A common one in image classification is the cross entropy loss.\n\n\noptimization\nParameter of a CNN that updates weights and biases in order to reduce the error. Some common optimizers in image classification are Stochastic Gradient Descent and ADAM.\n\n\ntransfer learning\nUsing trained layers of other CNN architectures to fine tune a new model investing less resources (e.g. training data).\n\n\n\n\nTechnically, in an image classification task we train a model with examples (e.g., a corpus of pictures with labels) in order to predict the category of any given new sample. It is the same logic used in supervised text classification explained in Section 11.4 but using images instead of texts. For example, if we show many pictures of cats and houses the algorithm would learn the constant features in each and will tell you with some degree of confidence if a new picture contains either a cat or a house. It is the same with letters, numbers, objects or faces, and you can apply either binary or multi-class classification. Just think when your vehicle registration plate is recognized by a camera or when your face is automatically labelled in pictures posted on Facebook.\nBeyond image classification we have other specific tasks in computer vision such as object detection or semantic segmentation (Figure 14.4). To conduct object detection we first have to locate all the possible objects contained in a picture by predicting a bounding box (i.e., the four points corresponding to the vertical and horizontal coordinates of the center of the object), which is normally a regression task. Once the bounding boxes are placed around the objects, we must apply multi-class classification as explained earlier. In the case of semantic segmentation, instead of classifying objects, we classify each pixel of the image according to the class of the object the pixel belongs to, which means that different objects of the same class might not be distinguished. See Géron (2019) for a more detailed explanation and graphical examples of object detection versus image segmentation.\n\n\n\nFigure 14.4: Object detection (left) versus semantic segmentation (right). Source: Géron (2019)\n\n\nIt is beyond the scope of this book to address the implementation of object detection or semantic segmentation, but we will focus on how to conduct basic image classification in state-of-the-art libraries in R and Python. As you may have imagined we will need some already-labelled images to have a proper training set. It is also out of the scope of this chapter to collect and annotate the images, which is the reason why we will mostly rely on pre-existing image databases (i.e., MINST or Fashion MINST) and pre-trained models (i.e., CNN architectures).\n\n14.4.1 Basic Classification with Shallow Algorithms\nIn Chapter 8 we introduced you to the exciting world of machine learning and in Section 11.4 we introduced the supervised approach to classify texts. Most of the discussed models used so-called shallow algorithms such as Naïve Bayes or Support Vector Machines rather than the various large neural network models called deep learning. As we will see in the next section, deep neural networks are nowadays the best option for complex tasks in image classification. However, we will now explain how to conduct simple multi-class classification of images that contain numbers with a shallow algorithm.\nLet us begin by training a model to recognize numbers using 70000 small images of digits handwritten from the Modified National Institute of Standards and Technology (MNIST) dataset ((LeCun et al. 1998)). This popular training corpus contains gray-scale examples of numbers written by American students and workers and it is usually employed to test machine learning models (60000 for training and 10000 for testing). The image sizes are \\(28 \\times 28\\), which generates 784 features for each image, with pixels values from white to black represented by a 0–255 scales. In Figure 14.5 you can observe the first 10 handwritten numbers used in both training and test set.\n\n\n\nFigure 14.5: First 10 handwritten digits from the training and test set of the MNIST.\n\n\nYou can download the MNIST images from its project web page5, but many libraries also offer this dataset. In Example 14.9 we use the read_mnist function from the dslabs package (Data Science Labs) in R and the fetch_openml function from the sklearn package (datasets module) in Python to read and load a mnist object into our workspace. We then create the four necessary objects (X_train, X_test, y_train, y_test) to generate a ML model and print the first numbers in training and test sets and check they coincide with those in 14.5.\n\n\n\n\n\n\n\nExample 14.9 Loading MNIST dataset and preparing training and test sets\n\nPython codeR code\n\n\n\n\n\n\nmnist = fetch_openml(\"mnist_784\", version=1)\nX, y = mnist[\"data\"], mnist[\"target\"]\ny = y.astype(np.uint8)\nX_train, X_test = X[:60000], X[60000:]\ny_train, y_test = y[:60000], y[60000:]\n\n\nprint(\"Numbers in training set= \", y_train[0:10])\n\nNumbers in training set=  0    5\n1    0\n2    4\n3    1\n4    9\n5    2\n6    1\n7    3\n8    1\n9    4\nName: class, dtype: uint8\n\nprint(\"Numbers in test set= \", y_test[0:10])\n\nNumbers in test set=  60000    7\n60001    2\n60002    1\n60003    0\n60004    4\n60005    1\n60006    4\n60007    9\n60008    5\n60009    9\nName: class, dtype: uint8\n\n\n\n\n\nmnist = read_mnist()\n\nX_train = mnist$train$images\ny_train = factor(mnist$train$labels)\nX_test = mnist$test$images\ny_test = factor(mnist$test$labels)\n\nprint(\"Numbers in training set = \")\n\n[1] \"Numbers in training set = \"\n\nprint(factor(y_train[1:10]), max.levels = 0)\n\n [1] 5 0 4 1 9 2 1 3 1 4\n\nprint(\"Numbers in test set = \")\n\n[1] \"Numbers in test set = \"\n\nprint(factor(y_test[1:10]), max.levels = 0)\n\n [1] 7 2 1 0 4 1 4 9 5 9\n\n\n\n\n\n\n\n\n\nOnce we are ready to model the numbers we choose one of the shallow algorithms explained in Section 8.3 to deploy a binary or multi-class image classification task. In the case of binary, we should select a number of reference (for instance “3”) and then create the model of that number against all the others (to answer questions such as “What’s the probability of this digit of being number 3?”). On the other hand, if we choose multi-class classification our model can predict any of the ten numbers (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) included in our examples.\nNow, we used the basic concepts of the Random Forest algorithm (see ?sec-randomforest) to create and fit a model with 100 trees (forest_clf). In Example 14.10 we use again the randomForest package in R and sklearn package in Python to estimate a model for the ten classes using the corpus of 60000 images (classes were similarly balanced, $$9–11% each). As we do in the examples, you can check the predictions for the first ten images of the test set (X_test), which correctly correspond to the right digits, and also check the (predictions) for the whole test set and then get some metrics of the model. The accuracy is over 0.97 which means the classification task is performed very well.\n\n\n\n\n\n\n\nExample 14.10 Modeling the handwritten digits with RandomForest and predicting some outcomes\n\nPython codeR code\n\n\n\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)\n\nprint(forest_clf)\n\nRandomForestClassifier(random_state=42)\n\nprint(\n    \"Predict the first 10 numbers of our set:\", forest_clf.predict(X_test[:10])\n)\n\nPredict the first 10 numbers of our set: [7 2 1 0 4 1 4 9 5 9]\n\npredictions = forest_clf.predict(X_test)\nprint(\"Overall Accuracy: \", accuracy_score(y_test, predictions))\n\nOverall Accuracy:  0.9705\n\n\n\n\n\n#Multiclass classification with RandomForest\nrf_clf = randomForest(X_train, y_train, ntree=100)\nrf_clf\n\n\nCall:\n randomForest(x = X_train, y = y_train, ntree = 100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 28\n\n        OOB estimate of  error rate: 3.38%\nConfusion matrix:\n     0    1    2    3    4    5    6    7    8    9 class.error\n0 5847    1   10    5    7    5   12    0   33    3  0.01283134\n1    0 6647   36    9   11    8    4   11   10    6  0.01409077\n2   25   10 5762   31   26    5   17   41   38    3  0.03289695\n3   11   10   70 5821    3   73   10   51   56   26  0.05056271\n4   10   10   12    1 5644    1   31   10   18  105  0.03389250\n5   27    7   13   74    9 5175   47    5   39   25  0.04537908\n6   19   12    3    2    6   38 5819    0   19    0  0.01672862\n7    4   21   59   11   29    3    0 6046   15   77  0.03495611\n8   14   29   37   52   21   43   22    4 5559   70  0.04990600\n9   19   10   17   62   76   26    4   47   39 5649  0.05042864\n\npredict(rf_clf, X_test[1:10,])\n\n 1  2  3  4  5  6  7  8  9 10 \n 7  2  1  0  4  1  4  9  5  9 \nLevels: 0 1 2 3 4 5 6 7 8 9\n\npredictions = predict(rf_clf, X_test)\ncm = confusionMatrix(predictions, y_test)\nprint(cm$overall[\"Accuracy\"])\n\nAccuracy \n  0.9692 \n\n\n\n\n\n\n\n\n\nThis approach based on shallow algorithms seems to work pretty well for simple images, but has a lot of limitations for more complex images such as figures or real pictures. After all, the more complex the image and the more abstract the concept, the less likely it is that one can expect a direct relationship between a pixel color and the classification. In the next section we introduce the use of deep learning in image classification which is nowadays a more accurate approach for complex tasks.\n\n\n14.4.2 Deep Learning for Image Analysis\nEven though they require heavy computations, Deep Neural Networks (DNN) are nowadays the best way to conduct image classification because their performance is normally higher than shallow algorithms. The reason is that we broaden the learning process using intermediate hidden layers, so each of these layers can learn different patterns or aspects of the image with different levels of abstraction: e.g., from detecting lines or contours in the first layers to catching higher feature representation of an image (such as the color of skin, the shapes of the eyes or the noses) in the next layers. In Section ?sec-neural and Section 11.4.4 we introduced the general concepts of a DNN (such as perceptrons, layers, hidden layers, back or forward propagation, and output functions), and now we will cover some common architectures for image analysis.\nOne of the simplest DNNs architectures is the Multilayer Perceptron (MLP) which contains one input layer, one or many hidden layers, and one output layer (all of them fully connected and with bias neurons except for the output layer). Originally in a MLP the signals propagate from the inputs to the outputs (in one direction), which we call a feedforward neural network (FNN), but using Gradient Decent as an optimizer we can apply backpropagation (automatically computing the gradients of the network’s errors in two stages: one forward and one backward) and then obtain a more efficient training.\nWe can use MLPs for binary and multi-class classification. In the first case, we normally use a single output neuron with the sigmoid or logistic activation function (probability from 0 to 1) (see ?sec-logreg); and in the second case we will need one output neuron per class with the softmax activation function (probabilities from 0 to 1 for each class but they must add up to 1 if the classes are exclusive. This is the function used in multinomial logistic regression). To predict probabilities, in both cases we will need a loss function and the one that is normally recommended is the cross entropy loss or simply log loss.\nThe state-of-the-art library for neural networks in general and for computer vision in particular is TensorFlow6 (originally created by Google and later publicly released) and the high-level Deep Learning API Keras, although you can find other good implementation packages such as PyTorch (created by Facebook), which has many straightforward functionalities and has also become popular in recent years (see for example the image classification tasks for social sciences conducted in PyTorch by Williams, Casas, and Wilkerson (2020)). All these packages have current versions for both R and Python.\nNow, let’s train an MLP to build an image classifier to recognize fashion items using the Fashion MNIST dataset7. This dataset contains 70000 (60000 for training and 10000 for test) gray scale examples (\\(28\\times 28\\)) of ten different classes that include ankle boots, bags, coats, dresses, pullovers, sandals, shirts, sneakers, t-shirts/tops and trousers (Figure 14.6). If you compare this dataset with the MINST, you will find that figures of fashion items are more complex than handwritten digits, which normally generates a lower accuracy in supervised classification.\n\n\n\nFigure 14.6: Examples of Fashion MNIST items.\n\n\nYou can use Keras to load the Fashion MNIST. In Example 14.11 we load the complete dataset and create the necessary objects for modeling (X_train_full, y_train_full, X_test, y_test). In addition we rescaled all the input features from 0–255 to 0–1 by dividing them by 255 in order to apply Gradient Decent. Then, we obtained three sets with \\(28\\times 28\\) arrays: 60000 in the training, and 10000 in the test. We could also generate here a validation set (e.g., X_valid and y_valid) with a given amount of records extracted from the training set (e.g., 5000), but as you will later see Keras allows us to automatically generate the validation set as a proportion of the training set (e.g., 0.1, which would be 6000 records in our example) when fitting the model (check the importance to work with a validation set to avoid over-fitting, explained in Section 8.3.2).\n\n\n\n\n\n\n\nExample 14.11 Loading Fashion MNIST dataset and preparing training, test and validation sets\n\nPython codeR code\n\n\n\nfashion_mnist = keras.datasets.fashion_mnist\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n\n 8192/29515 [=======>......................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n\n    8192/26421880 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8396800/26421880 [========>.....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22634496/26421880 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n\n   8192/4422102 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n4422102/4422102 [==============================] - 0s 0us/step\n\nclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\",\n               \"Dress\", \"Coat\", \"Sandal\", \"Shirt\",\n               \"Sneaker\", \"Bag\", \"Ankle boot\"]\nX_train = X_train / 255.0\nX_test = X_test / 255.0\nprint(X_train.shape, X_test.shape)\n\n(60000, 28, 28) (10000, 28, 28)\n\n\n\n\n\nfashion_mnist <- dataset_fashion_mnist()\nc(X_train, y_train) %<-% fashion_mnist$train\nc(X_test, y_test) %<-% fashion_mnist$test\nclass_names = c(\"T-shirt/top\",\"Trouser\",\n    \"Pullover\",\"Dress\", \"Coat\", \"Sandal\",\"Shirt\",\n    \"Sneaker\", \"Bag\",\"Ankle boot\")\nX_train <- X_train / 255\ny_test <- y_test / 255\nprint(dim(X_train))\n\n[1] 60000    28    28\n\nprint(dim(X_test))\n\n[1] 10000    28    28\n\n\n\n\n\n\n\n\n\nThe next step is to design the architecture of our model. There are three ways to create the models in Keras (sequential, functional, or subclassing), but there are thousands of ways to configure a deep neural network. In the case of this MLP, we have to include first an input layer with the input_shape equal to the image dimension (\\(28\\times 28\\) for 784 neurons). At the top of the MLP you will need a output layer with 10 neurons (the number of possible outcomes in our multi-class classification task) and a softmax activation function for the final probabilities for each class.\nIn Example 14.12 we use the sequential model to design our MLP layer by layer including the above-mentioned input and output layers. In the middle, there are many options for the configuration of the hidden layers: number of layers, number of neurons, activation functions, etc. As we know that each hidden layer will help to model different patterns of the image, it would be fair to include at least two of them with different numbers of neurons (significantly reducing this number in the second one) and transmit its information using the relu activation function. What we actually do is create an object called model which saves the proposed architecture. We can use the method summary to obtain a clear representation of the created neural network and the number of parameters of the model (266610 in this case!).\n\n\n\n\n\n\n\nExample 14.12 Creating the architecture of the MLP with Keras\n\nPython codeR code\n\n\n\nmodel = keras.models.Sequential(\n    [\n        keras.layers.Flatten(input_shape=[28, 28]),\n        keras.layers.Dense(300, activation=\"relu\"),\n        keras.layers.Dense(100, activation=\"relu\"),\n        keras.layers.Dense(10, activation=\"softmax\"),\n    ]\n)\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 300)               235500    \n                                                                 \n dense_1 (Dense)             (None, 100)               30100     \n                                                                 \n dense_2 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n\nmodel = keras_model_sequential()\nmodel %>%\nlayer_flatten(input_shape = c(28, 28)) %>%\nlayer_dense(units=300, activation=\"relu\") %>%\nlayer_dense(units=100, activation=\"relu\") %>%\nlayer_dense(units=10, activation=\"softmax\")\nmodel\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n flatten_1 (Flatten)                (None, 784)                     0           \n dense_5 (Dense)                    (None, 300)                     235500      \n dense_4 (Dense)                    (None, 100)                     30100       \n dense_3 (Dense)                    (None, 10)                      1010        \n================================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\n\n\n\n\n\nThe next steps will be to compile, fit, and evaluate the model, similarly to what you have already done in previous exercises of this book. In Example 14.13 we first include the parameters (loss, optimizer, and metrics) of the compilation step and fit the model, which might take some minutes (or even hours depending on your dataset, the architecture of you DNN and, of course, your computer).\nWhen fitting the model you have to separate your training set into phases or epochs. A good rule of thumb to choose the optimal number of epochs is to stop a few iterations after the test loss stops improving8 (here we chose five epochs for the example). You will also have to set the proportion of the training set that will become the validation set (in this case 0.1). In addition, you can use the parameter verbose to choose whether to see the progress (1 for progress bar and 2 for one line per epoch) or not (0 for silent) of the training process. By using the method evaluate you can then obtain the final loss and accuracy, which in this case is 0.84 (but you can reach up 0.88 if you fit it with 25 epochs!).\n\n\n\n\n\n\n\nExample 14.13 Compiling fitting and evaluating the model for the MLP\n\nPython codeR code\n\n\n\nmodel.compile(\n    loss=\"sparse_categorical_crossentropy\",\n    optimizer=\"sgd\",\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(X_train, y_train, epochs=5, verbose=0, validation_split=0.1)\nprint(\"Evaluation: \")\n\nEvaluation: \n\nprint(model.evaluate(X_test, y_test, verbose=0))\n\n[0.433102548122406, 0.8460999727249146]\n\n\n\n\n\nmodel %>% compile(optimizer = \"sgd\", metrics = c(\"accuracy\"),\n                  loss = \"sparse_categorical_crossentropy\")\nhistory = model %>% fit(X_train, y_train,validation_split=0.1, epochs=5, verbose=0)\nprint(history$metrics)\n\n$loss\n[1] 0.7183684 0.4892329 0.4431078 0.4155466 0.3959438\n\n$accuracy\n[1] 0.7615926 0.8312593 0.8441296 0.8542778 0.8608889\n\n$val_loss\n[1] 0.5663074 0.4571685 0.4557612 0.4244801 0.4019246\n\n...\n\nscore = model %>% evaluate(X_test, y_test, verbose = 0)\nprint(\"Evaluation\")\n\n[1] \"Evaluation\"\n\nprint(score)\n\n     loss  accuracy \n1795.5438    0.0788 \n\n\n\n\n\n\n\n\n\nFinally, you can use the model to predict the classes of any new image (using predict_classes). In Example 14.14 we used the model to predict the classes of the first six elements of the test set. If you go back to 14.6 you can compare these predictions (“ankle boot”, “pullover”, “trouser”, “trouser”, “shirt”, and “tTrouser”) with the actual first six images of the test set, and see how accurate our model was.\n\n\n\n\n\n\n\nExample 14.14 Predicting classes using the MLP\n\nPython codeR code\n\n\n\nX_new = X_test[:6]\ny_pred = np.argmax(model.predict(X_new, verbose=0), axis=-1)\nclass_pred = [class_names[i] for i in y_pred]\nclass_pred\n\n['Ankle boot', 'Pullover', 'Trouser', 'Trouser', 'Shirt', 'Trouser']\n\n\n\n\n\nimg = X_test[1:6, , , drop = FALSE]\nclass_pred = model %>% predict(img, verbose=0) %>% k_argmax()\nclass_pred\n\ntf.Tensor([9 2 1 1 6 1], shape=(6), dtype=int64)\n\n\n\n\n\n\n\n\n\nUsing the above-described concepts and code you may try to train a new MLP using color images of ten classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck) using the CIFAR-10 and CIFAR-100 datasets9!\n\n\n14.4.3 Re-using an Open Source CNN\nTraining complex images such as photographs is normally a more sophisticated task if we compare it to the examples included in the last sections. On the one hand, it might not be a good idea to build a deep neural network from scratch as we did in section 14.4.2 to train a MLP. This means that you can re-use some lower layers of other DNNs and deploy transfer learning to save time with less training data. On the other hand, we should also move from traditional MLPs to other kinds of DNNs such as Convolutional Neural Networks (CNNs) which are nowadays the state-of-the-art approach in computer vision. Moreover, to get good results we should also build or explore different CNNs architectures that can produce more accurate predictions in image classification. In this section we will show how to re-use an open source CNN architecture and will suggest an example of how to fine-tune an existing CNN for a social science problem.\nAs explained in Section ?sec-cnnbasis a CNN is a specific type of DNN that has had great success in complex visual tasks (image classification, object detection or semantic segmentation) and voice recognition10. Instead of using fully connected layers like in a typical MLP, a CNN uses only partially connected layers inspired on how “real” neurons connect in the visual cortex: some neurons only react to stimuli located in a limited receptive field. In other words, in a CNN every neuron is connected to some neurons of the previous layer (and not to all of them), which significantly reduces the amount of information transmitted to the next layer and helps the DNN to detect complex patterns. Surprisingly, this reduction in the number of parameters and weights involved in the model works better for larger and more complex images, different from those shown in MNIST.\nBuilding a CNN is quite similar to a MLP, except for the fact that you will have to work with convolutional and pooling layers. The convolutional layers include a bias term and are the most important blocks of a CNN because they establish the specific connections among the neurons. In simpler words: a given neuron of a high-level layer is connected only to a rectangular group of neurons (the receptive field) of the low-level layer and not to all of them11. For more technical details of the basis of a CNN you can go to specific literature such as Géron (2019).\nInstead of building a CNN from scratch, there are many pre-trained and open-source architectures that have been optimized for image classification. Besides a stack of convolutional and pooling layers, these architectures normally include some fully connected layers and a regular output layer for prediction (just like in MLPs). We can mention here some of these architectures: LeNet-5, AlexNet, GoogLeNet, VGGNet, ResNet, Xception or SENet12. All these CNNs have been previously tested in image classification with promising results, but you still have to look at the internal composition of each of them and their metrics to choose the most appropriate for you. You can implement and train most of them from scratch either in keras or PyTorch, or you can just use them directly or even fine-tune the pre-trained model in order to save time.\nLet’s use the pre-trained model of a Residual Network (ResNet) with 50 layers, also known as ResNet50, to show you how to deploy a multi-class classifier over pictures. The ResNet architecture (also with 34, 101 and 152 layers) is based on residual learning and uses skip connections, which means that the input layer not only feeds the next layer but this signal is also added to the output of another high-level layer. This allows you to have a much deeper network and in the case of ResNet152 it has achieved a top-five error rate of 3.6%. As we do in Example 14.15, you can easily import into your workspace a ResNet50 architecture and include the pre-trained weights of a model trained with ImageNet (uncomment the second line of the code to visualize the complete model!).\n\n\n\n\n\n\n\nExample 14.15 Loading a visualizing the ResNet50 architecture\n\nPython codeR code\n\n\n\nmodel_rn50 = tf.keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n\n     8192/102967424 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   147456/102967424 [..............................] - ETA: 47s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   245760/102967424 [..............................] - ETA: 55s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   344064/102967424 [..............................] - ETA: 58s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   442368/102967424 [..............................] - ETA: 1:00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   540672/102967424 [..............................] - ETA: 1:00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   638976/102967424 [..............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   704512/102967424 [..............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   770048/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   868352/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n   966656/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1064960/102967424 [..............................] - ETA: 1:08\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1163264/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1261568/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1359872/102967424 [..............................] - ETA: 1:08\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1425408/102967424 [..............................] - ETA: 1:08\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1490944/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1556480/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1654784/102967424 [..............................] - ETA: 1:10\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1720320/102967424 [..............................] - ETA: 1:10\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1818624/102967424 [..............................] - ETA: 1:10\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  1916928/102967424 [..............................] - ETA: 1:10\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2015232/102967424 [..............................] - ETA: 1:10\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2113536/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2211840/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2310144/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2408448/102967424 [..............................] - ETA: 1:09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2506752/102967424 [..............................] - ETA: 1:08\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2605056/102967424 [..............................] - ETA: 1:08\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2703360/102967424 [..............................] - ETA: 1:08\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2801664/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2899968/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  2998272/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3096576/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3194880/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3260416/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3358720/102967424 [..............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3457024/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3555328/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3620864/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3719168/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3817472/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  3915776/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4014080/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4112384/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4210688/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4308992/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4407296/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4505600/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4603904/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4702208/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4800512/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4898816/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  4997120/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5062656/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5128192/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5201920/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5259264/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5357568/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5455872/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5554176/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5652480/102967424 [>.............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5718016/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5783552/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5849088/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5914624/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  5980160/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6045696/102967424 [>.............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6111232/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6209536/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6275072/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6340608/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6406144/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6504448/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6569984/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6668288/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6766592/102967424 [>.............................] - ETA: 1:07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6864896/102967424 [=>............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  6930432/102967424 [=>............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7028736/102967424 [=>............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7159808/102967424 [=>............................] - ETA: 1:06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7290880/102967424 [=>............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7389184/102967424 [=>............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7487488/102967424 [=>............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7585792/102967424 [=>............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7684096/102967424 [=>............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7782400/102967424 [=>............................] - ETA: 1:05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7880704/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  7979008/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8077312/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8175616/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8273920/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8372224/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8470528/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8536064/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8601600/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8667136/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8765440/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8863744/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  8962048/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9060352/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9125888/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9191424/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9256960/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9322496/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9388032/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9453568/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9551872/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9650176/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9748480/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9846784/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n  9945088/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10010624/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10108928/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10207232/102967424 [=>............................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10305536/102967424 [==>...........................] - ETA: 1:04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10436608/102967424 [==>...........................] - ETA: 1:03\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10534912/102967424 [==>...........................] - ETA: 1:03\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10633216/102967424 [==>...........................] - ETA: 1:03\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10731520/102967424 [==>...........................] - ETA: 1:03\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10829824/102967424 [==>...........................] - ETA: 1:03\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 10928128/102967424 [==>...........................] - ETA: 1:03\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11026432/102967424 [==>...........................] - ETA: 1:03\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11124736/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11223040/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11321344/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11419648/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11517952/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11616256/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11714560/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11780096/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11878400/102967424 [==>...........................] - ETA: 1:02\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 11976704/102967424 [==>...........................] - ETA: 1:01\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 12075008/102967424 [==>...........................] - ETA: 1:01\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 12173312/102967424 [==>...........................] - ETA: 1:01\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 12771328/102967424 [==>...........................] - ETA: 58s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 14336000/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 14794752/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 14860288/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 14958592/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15056896/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15122432/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15220736/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15286272/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15384576/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15482880/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15548416/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15613952/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15679488/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15745024/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15810560/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 15908864/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16007168/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16105472/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16171008/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16269312/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16367616/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16465920/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16564224/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16662528/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16793600/102967424 [===>..........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 16924672/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17022976/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17088512/102967424 [===>..........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17186816/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17285120/102967424 [====>.........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17350656/102967424 [====>.........................] - ETA: 51s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17448960/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17547264/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17645568/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17776640/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 17907712/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 18038784/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 18137088/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 18268160/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 18366464/102967424 [====>.........................] - ETA: 50s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 18497536/102967424 [====>.........................] - ETA: 49s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 18628608/102967424 [====>.........................] - ETA: 49s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 18825216/102967424 [====>.........................] - ETA: 49s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 19054592/102967424 [====>.........................] - ETA: 48s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 19251200/102967424 [====>.........................] - ETA: 48s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 19447808/102967424 [====>.........................] - ETA: 48s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 19644416/102967424 [====>.........................] - ETA: 47s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 19972096/102967424 [====>.........................] - ETA: 46s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 20234240/102967424 [====>.........................] - ETA: 46s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 20430848/102967424 [====>.........................] - ETA: 46s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 20594688/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 20725760/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 20824064/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 20922368/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21020672/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21151744/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21282816/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21381120/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21512192/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21626880/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21708800/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21807104/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 21905408/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22003712/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22102016/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22200320/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22298624/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22396928/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22495232/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22560768/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22659072/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22724608/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22790144/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22855680/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 22921216/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23019520/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23117824/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23216128/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23281664/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23347200/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23412736/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23478272/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23576576/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23674880/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23740416/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23805952/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23904256/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 23969792/102967424 [=====>........................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24035328/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24133632/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24231936/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24330240/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24428544/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24494080/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24592384/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24657920/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24756224/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24854528/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 24952832/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25051136/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25149440/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25247744/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25346048/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25444352/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25509888/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25608192/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25706496/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25772032/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25870336/102967424 [======>.......................] - ETA: 45s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 25968640/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26066944/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26165248/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26263552/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26361856/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26435584/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26525696/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26624000/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26722304/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26820608/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 26918912/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27017216/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27115520/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27213824/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27312128/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27410432/102967424 [======>.......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27508736/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27607040/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27705344/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27803648/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 27901952/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28000256/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28098560/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28164096/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28262400/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28360704/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28459008/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28557312/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28655616/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28721152/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28819456/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 28917760/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29016064/102967424 [=======>......................] - ETA: 44s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29097984/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29179904/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29278208/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29376512/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29474816/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29573120/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29671424/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29769728/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29868032/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 29966336/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30064640/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30162944/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30261248/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30359552/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30457856/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30556160/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30621696/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30720000/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30818304/102967424 [=======>......................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 30916608/102967424 [========>.....................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31014912/102967424 [========>.....................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31145984/102967424 [========>.....................] - ETA: 43s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31244288/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31375360/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31473664/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31571968/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31670272/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31768576/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31834112/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 31932416/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32030720/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32161792/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32292864/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32423936/102967424 [========>.....................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32587776/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32718848/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32849920/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 32948224/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33079296/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33210368/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33308672/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33406976/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33570816/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33669120/102967424 [========>.....................] - ETA: 41s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33767424/102967424 [========>.....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33865728/102967424 [========>.....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 33931264/102967424 [========>.....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34029568/102967424 [========>.....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34127872/102967424 [========>.....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34226176/102967424 [========>.....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34324480/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34390016/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34455552/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34521088/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34619392/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34717696/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34848768/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 34947072/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35045376/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35143680/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35209216/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35307520/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35405824/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35504128/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35569664/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 35667968/102967424 [=========>....................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 39682048/102967424 [==========>...................] - ETA: 34s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 46317568/102967424 [============>.................] - ETA: 26s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 51273728/102967424 [=============>................] - ETA: 21s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 55009280/102967424 [===============>..............] - ETA: 18s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 57196544/102967424 [===============>..............] - ETA: 17s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 63168512/102967424 [=================>............] - ETA: 13s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 69771264/102967424 [===================>..........] - ETA: 10s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 72769536/102967424 [====================>.........] - ETA: 9s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 79609856/102967424 [======================>.......] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 88186880/102967424 [========================>.....] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 94617600/102967424 [==========================>...] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n102948864/102967424 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n102967424/102967424 [==============================] - 22s 0us/step\n\n\n\n\n\nmodel_resnet50 = application_resnet50(weights=\"imagenet\")\n\n\n\n\n\n\n\n\nImageNet is a corpus of labelled images based on the WordNet hierarchy. ResNet uses a subset of ImageNet with  1000 examples for each of the 1000 classes for a total corpus of roughly 1350000 pictures (1200000 for training, 100000 for test, and 50000 for validation).\nIn Example 14.16 we crop a part of our second example picture of refugees arriving at the European coast (myimg2_RGB) in order to get just the sea landscape. With the created model_resnet50 we then ask for up to three predictions of the class of the photograph in Example 14.17.\n\n\n\n\n\n\n\nExample 14.16 Cropping an image to get a picture of a see landscape\n\nPython codeR code\n\n\n\ndef plot_color_image(image):\n    plt.imshow(image, interpolation=\"nearest\")\n    plt.axis(\"off\")\n\npicture1 = np.array(myimg2_RGB) / 255\npicture2 = np.array(myimg2_RGB) / 255\nimages = np.array([picture1, picture2])\nsee = [0, 0, 0.3, 0.3]\nrefugees = [0.1, 0.35, 0.8, 0.95]\ntf_images = tf.image.crop_and_resize(\n    images, [see, refugees], [0, 1], [224, 224]\n)\nplot_color_image(tf_images[0])\nplt.show()\n\n\n\n\n\n\n\npicture1 = image_crop(myimg2_RGB, \"224x224+50+50\")\nplot(picture1)\n\n\n\npicture1 = as.integer(picture1[[1]])\n#drop the extra channel for comparision\npicture1 = picture1[,,-4] \npicture1 = array_reshape(picture1, c(1, dim(picture1)))\npicture1 = imagenet_preprocess_input(picture1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 14.17 Predicting the class of the first image\n\nPython codeR code\n\n\n\ninputs = tf.keras.applications.resnet50.preprocess_input(tf_images * 255)\nY_proba = model_rn50.predict(inputs, verbose=0)\npreds = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n\n 8192/35363 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n35363/35363 [==============================] - 0s 0us/step\n\nprint(preds[0])\n\n[('n09421951', 'sandbar', 0.08357885), ('n09428293', 'seashore', 0.06147332...\n\n\n\n\n\npreds1 = model_resnet50 %>% predict(picture1)\nimagenet_decode_predictions(preds1, top = 3)[[1]]\n\n  class_name class_description      score\n1  n09421951           sandbar 0.07926155\n2  n04347754         submarine 0.04810233\n3  n02066245        grey_whale 0.04798760\n\n\n\n\n\n\n\n\n\nAs you can see in the Python and R outputs13, the best guess of the model is a sandbar, which is very close to the real picture that contains sea water, mountains and sky. However, it seems that the model is confusing sand with sea. Other results in the Python model are seashore and cliff, which are also very close to real sea landscape. Nevertheless, in the case of the R prediction the model detects a submarine and a gray whale, which revels that predictions are not 100% accurate yet.\nIf we do the same with another part of that original picture and focus only on the group of refugees in a lifeboat arriving at the European coast, we will get a different result! In Example 14.18 we crop again (myimg2_RGB) and get a new framed picture. Then in Example 14.19 we re-run the prediction task using the model ResNet50 trained with ImageNet and get a correct result: both predictions coincide to see a lifeboat, which is a good tag for the image we want to classify. Again, other lower-level predictions can seem accurate (speedboat) and totally inaccurate (volcano, gray whale or amphibian).\n\n\n\n\n\n\n\nExample 14.18 Cropping an image to get a picture of refugees in a lifeboat\n\nPython codeR code\n\n\n\nplot_color_image(tf_images[1])\nplt.show()\n\n\n\n\n\n\n\npicture2 = image_crop(myimg2_RGB, \"224x224+1000\")\nplot(picture2)\n\n\n\npicture2 = as.integer(picture2[[1]])\n#drop the extra channel for comparision\npicture2 = picture2[,,-4] \npicture2 = array_reshape(picture2, c(1, dim(picture2)))\npicture2 = imagenet_preprocess_input(picture2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 14.19 Predicting the class of the second image\n\nPython codeR code\n\n\n\nprint(preds[1])\n\n[('n03662601', 'lifeboat', 0.19698665), ('n09472597', 'volcano', 0.10091315...\n\n\n\n\n\npreds2 = model_resnet50 %>% predict(picture2)\nimagenet_decode_predictions(preds2, top = 3)[[1]]\n\n  class_name class_description      score\n1  n03662601          lifeboat 0.39761335\n2  n04273569         speedboat 0.11085796\n3  n02704792         amphibian 0.06916222\n\n\n\n\n\n\n\n\n\nThese examples show you how to use an open-source and pre-trained CNN that has 1000 classes and has been trained on images that we do not have control of. However, you may want to build your own classifier with your own training data, but using part of an existing architecture. This is called fine-tuning and you can follow a good example in social science in Williams, Casas, and Wilkerson (2020) in which the authors reuse RestNet18 to build binary and multi-class classifiers adding their own data examples over the pre-trained CNN14.\nSo far we have covered the main techniques, methods, and services to analyze multimedia data, specifically images. It is up to you to choose which library or service to use, and you will find most of them in R and Python, using the basic concepts explained in this chapter. If you are interested in deepening your understanding of multimedia analysis, we encourage you explore this emerging and exciting field of expertise given the enormous importance it will no doubt have in the near future.\n\n\n\n\nAmores, Javier J, Carlos Arcila Calderón, and Mikolaj Stanek. 2019. “Visual Frames of Migrants and Refugees in the Main Western European Media.” Economics & Sociology 12 (3): 147–61.\n\n\nDietrich, Bryce J, Matthew Hayes, and DIANA Z O’BRIEN. 2019. “Pitch Perfect: Vocal Pitch and the Emotional Intensity of Congressional Speech.” American Political Science Review 113 (4): 941–62.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.\n\n\nHoriuchi, Yusaku, Tadashi Komatsu, and Fumio Nakaya. 2012. “Should Candidates Smile to Win Elections? An Application of Automated Face Recognition Technology.” Political Psychology 33 (6): 925–33.\n\n\nKnox, Dean, and Christopher Lucas. 2021. “A Dynamic Model of Speech for the Social Sciences.” American Political Science Review 115 (2): 649–66.\n\n\nLeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324.\n\n\nPeng, Yilang. 2018. “Same Candidates, Different Faces: Uncovering Media Bias in Visual Portrayals of Presidential Candidates with Computer Vision.” Journal of Communication 68 (5): 920–41.\n\n\nRedmon, Joseph, and Ali Farhadi. 2018. “YOLOv3: An Incremental Improvement.” arXiv.\n\n\nWilliams, Nora Webb, Andreu Casas, and John D Wilkerson. 2020. “Images as Data for Social Science Research: An Introduction to Convolutional Neural Nets for Image Classification.” Elements in Quantitative and Computational Methods for the Social Sciences."
  },
  {
    "objectID": "content/chapter15.html#sec-databases",
    "href": "content/chapter15.html#sec-databases",
    "title": "15  Scaling up and distributing",
    "section": "15.1 Storing Data in SQL and noSQL Databases",
    "text": "15.1 Storing Data in SQL and noSQL Databases\n\n15.1.1 When to Use a Database\nIn this book, we have so far stored our data in files. In fact, before covering the wide range of methods for computational analysis, we discussed some basics of file handling (Chapter 5). Probably, you did not experience any major trouble here (apart from occasional struggles with non-standard encodings, or confusion about the delimiters in a csv file). On the other hand, the examples we used were still modest in size: usually, you were dealing with a handful of csv files; except for huge image classification datasets, the maximum you had to deal with were the 50000 text files from the IMDB movie review dataset.\nIn particular, when loading your data into a data frame, you copied all the data from your disk into memory1. But what if you want to scale up your analyses a bit Trilling and Jonkman (2018)? Maybe we want to build up a larger data collection, maybe even share it with multiple team members, search and filter our data, or collect it over a larger timespan? An example may illustrate the problems that can arise.\nImagine you do some web scraping (Chapter 12) that goes beyond a few thousand texts. Maybe you want to visit relevant news sites on a regular basis (say, once an hour) and retrieve everything that’s new. How do you store your data then? You could append everything to a huge csv file, but this file would quickly grow so large that you cannot load it into memory any more. Besides, you may run the risk of corrupting the file if something goes wrong in one of your attempts to extend the file. Or you could also write each article to a new, separate file. That’s maybe more failsafe, but you would need to design a good way to organize the data. In particular, devising a method to search and find relevant files would be a whole project in itself.\nLuckily, you can outsource all these problems to a database that you can install on your own computer or possibly on a server (in that case, make sure that it is properly secured!). In the example, the scraper, which is running once an hour, just sends the scraped data to the database instead of to a file, and the database will take care of storing it. Once you want to retrieve a subset of your articles for analysis, you can send a query to the database and read from it. Both Python and R offer integration for multiple commonly used databases. It is even possible to directly get the results of such a database query in the form of a data frame.\nWe can distinguish two main categories of databases that are most relevant to us (see also Günther, Trilling, and van de Velde 2018): relational databases (or SQL-databases) and noSQL-databases. Strictly speaking, SQL (“structured query language”) is a query language for databases, but it is so widespread that it is used almost synonymously for relational databases. Even though they have already been around for 50 years (Codd 1970), relational databases are still very powerful and very widely used. They consist of multiple tables that are linked by shared columns (keys). For instance, you could imagine a table with the orders placed in a webshop that has a column customer-id, and a different table with addresses, billing information, and names for each customer-id. Using filter and join operations (like in Chapter 6, but then on the database directly), one can then easily retrieve information on where the order has to be shipped. A big advantage of such a relational database is that, if a customer places 100 orders, we do not need to store their address 100 times, but only once, which is not only more efficient in terms of storage, but also prevents inconsistencies in the data.\nIn contrast to SQL databases, noSQL databases are not based on tables, but use concepts such as “documents” or key-value pairs, very much like Python dictionaries or JSON files. These types of databases are particularly interesting when your data are less well-structured. If not all of your cases have the same variables, or if the content is not well-defined (let’s say, you don’t know exactly in what format the date of publication on a news site will be written), or if the data structure may change over time, then it is hard or impossible to come up with a good table structure for an SQL database. Therefore, in many “big data” contexts, noSQL databases are used, as they – depending on your configuration – will happily accept almost any kind of content you dump in them. This comes, of course, at the expense of giving up advantages of SQL databases, such as the avoidance of inconsistencies. But often, you may want to store your data first and clean up later, rather than risking that data collection fails because you enforced a too strict structure. Also, there are many noSQL databases that are very fast in searching full text – something that SQL databases, in general, are not optimized for.\nDespite all of these differences, both SQL and noSQL databases can play the same role in the computational analysis of communication. They both help you to focus on data collection and data analysis without needing to devise an ingenious way to store your data. They both allow for much more efficient searching and filtering than you could design on your own. All of this becomes especially interesting when your dataset grows too large to fit in memory, but also when your data are continuously changed, for instance because new data are added while scraping.\n\n\n15.1.2 Choosing the Right Database\nChoosing the right database is not always easy, and has many consequences for the way you may conduct your analyses. As Günther, Trilling, and van de Velde (2018) explain, this is not a purely technical choice, but impacts your social-scientific workflow. Do you want to enforce a specific structure from the very start, or do you want to collect everything first and clean up later? What is your trade-off between avoiding any inconsistency and risking throwing away too much raw information?\nAcknowledging that there are often many different valid choices, and at the risk of oversimplifying matters, we will try to give some guidance in which databases to choose by offering some guiding questions.\nHow is your data structured?. Ask yourself: can I organize my data in a set of relational tables? For instance, think of television viewing data: there may be a table that gives information on when the television set was switched on and which channel was watched and by which user id. A second table can be used to associate personal characteristics such as age and gender with the user id. And a third table may be used to map the time stamps to details about a specific program aired at the time. If your data looks like this, ask yourself: can I determine the columns and the data types for each column in advance? If so, then a SQL database such as MySQL, PostgreSQL, or MariaDB is probably what you are looking for. If, on the other hand, you cannot determine such a structure a priori, if you believe that the structure of your information will change over time, or if it is very messy, then you may need a more flexible noSQL approach, such as MongoDB or ElasticSearch.\nHow important is full-text searching for you?. SQL databases can handle numeric datatypes as well as text datatypes, but they are usually not optimized for the latter. They handle short strings (such as usernames, addresses, and so on) just fine, but if you are interested in full-text searching, they are not the right tool for the job. This is in particular true if you want to be able to do fuzzy searches where, for instance, documents containing the plural of a word that you searched for as singular are also found. Databases of, for instance, news articles, tweets, transcripts of speeches, or other documents are much better accessed in a database such as ElasticSearch.\nHow flexible does it need to be?. In relational databases, it is relatively hard to change the structure afterwards. In contrast, a noSQL database has no problem whatsoever with adding a new document that contains keys that did not exist before. There is no assumption that all documents contain the same keys. Therefore, if it is hard to tell in advance which “columns” or “keys” may represent your data best, you should stay clear of SQL databases. In particular, if you think of gradually extending your data and use it on a long timeline for re-use, potentially even by multiple teams, the flexibility of a noSQL database may be a game changer.\n\n\n15.1.3 A Brief Example Using SQLite\nInstalling a database server such as mysql, mariadb (an open-source fork of mysql), MongoDB, or Elasticsearch is not really difficult (in fact, it may already be come pre-packaged with your operating system), but the exact configuration and setup may differ widely depending on your computer and your needs. Most importantly, especially if you store sensitive data in your database, you will need to think about authentication, roles, etc. — all beyond the scope of this book.\nLuckily, there is a compromise between storing your data in the files that you need to manage yourself and setting up a database server, locally or remotely. The library SQlite offers a self-contained database engine – essentially, it allows you to store a whole database in one file and interact with it using the SQL query language. Both R and Python offer multiple ways of directly interacting with sqlite files (Example 15.1). This gives you access to some great functionality straight away: after all, you can issue (almost) any SQL command now, including (and maybe most importantly) commands for filtering, joining, and aggregating data. Or you could consider immediately writing each datapoint you get from an API or a webscraper (Chapter 12) without risking losing any data if connections time out or scraping fails halfway.\n\n\n\n\n\n\n\nExample 15.1 SQLite offers you database functionality without setting up a database server such as mysql\n\nPython codeR code\n\n\n\nimport pandas as pd\nimport sqlite3\n\n# Load a dataframe\nurl = \"https://cssbook.net/d/gun-polls.csv\"\nd = pd.read_csv(url)\n\n# connecting  to a SQLite database\nconn = sqlite3.connect(\"mydb.db\")\n# store the df as table \"gunpolls\" in the database\nd.to_sql(\"gunpolls\", con=conn)\n\n# run a query on the SQLite database\n\n57\n\nsql = \"\"\"SELECT support, pollster \n         FROM gunpolls LIMIT 5;\"\"\"\nd2 = pd.read_sql_query(sql, conn)\n# close connection\nconn.close()\nd2\n\n   Support            Pollster\n0     72.0            CNN/SSRS\n1     82.0           NPR/Ipsos\n2     67.0           Rasmussen\n3     84.0  Harris Interactive\n4     78.0          Quinnipiac\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(RSQLite)\n\n# Load a dataframe\nurl = \"https://cssbook.net/d/gun-polls.csv\"\nd = read_csv(url)\n\n# connecting  to a SQLite database\nmydb = dbConnect(RSQLite::SQLite(), \"mydb.sqlite\")\n# store the df as table \"gunpolls\" in the database\ndbWriteTable(mydb, \"gunpolls\", d)\n\n# run a query on the SQLite database\nsql = \"SELECT support, pollster \n       FROM gunpolls LIMIT 5;\"\nd2 = dbGetQuery(mydb, sql)\nd2\n\n  Support           Pollster\n1      72           CNN/SSRS\n2      82          NPR/Ipsos\n3      67          Rasmussen\n4      84 Harris Interactive\n5      78         Quinnipiac\n\n# close connection\ndbDisconnect(mydb)\n\n\n\n\n\n\n\n\nOf course, SQlite cannot give you the same performance as a “real” mysql (or similar) installation could offer. Therefore, if your project grows bigger, or if you have a lot of read- or write-operations per second, then you may have to switch at some point. But as you can see in Example 15.1, Python and R do not really care about the back end: all you need to do is to change the connection conn such that it points to your new database instead of the sqlite file."
  },
  {
    "objectID": "content/chapter15.html#sec-cloudcomputing",
    "href": "content/chapter15.html#sec-cloudcomputing",
    "title": "15  Scaling up and distributing",
    "section": "15.2 Using Cloud Computing",
    "text": "15.2 Using Cloud Computing\nThroughout this book, we assumed that all tasks can actually be performed on your own computer. And often, that is indeed the best thing to do: you normally want to maintain a local copy of your data anyway, and it may be the safest bet for ethical and legal reasons – when working with sensitive data, you need to know what you are doing before transferring them somewhere else.\nHowever, once you scale up your project, problems may arise (see Trilling and Jonkman (2018)): - Multiple people need to work on the same data - Your dataset is too large to fit on your disk - You do not have enough RAM or processing power - Running a process simply takes too long (e.g., training a model for several days) or needs to be run in continuous intervals (e.g., scraping news articles once an hour) and you need your computer for other things.\nThis is the point where you need to start moving your project to some remote server instead. Broadly speaking, we can consider four scenarios: - A cloud service that just lets you run code. Here, you can just submit your code and have it run. You do not have full control, you cannot set up your own system, but you also do not have to do any administration. - A dedicated server. You (or your university) could buy a dedicated, physical server to run computational social science analyses. On the bright side, this gives you full control, but it is also not very flexible: after all, you make a larger investment once, and if it turns out that you need more (or less) resources, then it might be too late to change. - A virtual machine (VM) on a cloud computing platform. For most practical purposes, you can do the same as in the previous option, with the crucial difference that you rent the resources. If you need more, you just rent more; and when you are done, you just stop the machine. - A set of machines to run complex tasks using parallel computing. With large amounts of information (think about image or video data) and sophisticated modeling (such as deep learning) you may need to distribute the computation among several different computers at the same time.\nAn example for the first option is Google Colab. While it makes it easy to share and run notebooks, the free tier we used so far does not necessarily solve any of the scalability issues discussed. However, Google Colab also has a paid Pro version, in which additional hardware (such as GPUs, TPUs or extra memory) that you may not have on your own computer can be used. This makes it an attractive solution for enabling projects (e.g., involving resource-intensive neural networks) that otherwise would not be possible.\nHowever, this is often not enough. For instance, you may want to run a database (Section 15.1) or define a so-called cron job, which runs a specific script (e.g., a web scraper) at defined intervals. Here, options 2 and 3 come into play – most realistically for most beginners, option 3.\nThere are different providers for setting up VMs in the cloud, the most well-known probably being Amazon Web Services (AWS) and Microsoft Azure. Some universities or (national) research infrastructure providers provide high-performance computing in the cloud as well. While the specific way to set up a virtual machine of your own on such an infrastructure varies, the processes are roughly similar: you select the technical specifications such as the number of CPU cores and the amount of memory you need, attach some storage, and select a disk image with an operating system, virtually always some Linux distribution (Figure 15.1, Figure 15.2). After a couple of minutes, your machine is ready to use.\n\n\n\nFigure 15.1: Creating a Virtual Machine on Microsoft Azure\n\n\n\n\n\nFigure 15.2: Creating a Virtual Machine on a university cloud computing platform using OpenNebula\n\n\nWhile setting up such a machine is easy, some knowledge is required for the responsible and safe administration of the machine, in particular to prevent unauthorized access.\nImagine you have a script myscript.py that takes a couple of days to run. You can then use the tool scp to copy it to your new virtual machine, log on to your virtual machine using ssh, and then – now on your virtual machine! – run the script using a tool such as nohup or screen that will start your script and will keep running it (Figure 15.3). You can safely logout again, and your virtual machine in the cloud will keep on doing its work. The only thing you need to do is collect your results once your script is done, even if that’s a couple of weeks later. Or you may want to add your script to the crontab (Google it!), which will automatically run it at set intervals.\n\n\n\nFigure 15.3: Running a script on a virtual machine. Note that the first two commands are issued on the local machine (‘damian-thinkpad’) and the next command on the remote machine (‘packer-ubuntu-16’).\n\n\nYou may want to have some extra luxury, though. Popular things to set up are databases (Section 15.1) and JupyterHub, which allows users such as your colleagues to connect through their web browser with your server and run their own Jupyter Notebooks on the machine. Do not forget to properly encrypt all connections, for instance using letsencrypt.\nFinally, option 4 must be selected when the scale of your data and the complexity of the tasks cannot be deployed in a single server or virtual machine. For example, building a classification model by training a complex and deep convolutional neural network with millions of images and update this model constantly may require the use of different computers at the same time. Actually, in modern computers with multiple cores or processors you normally run parallel computing within a single machine. But when working at scale you will probably need to set a infrastructure of different computers such as that of a grid or a cluster.\nCloud services (e.g. AWS, Microsoft Azure, etc.) or scientific infrastructures (e.g. supercomputers) offer the possibility to set these architectures remotely. For instance, in a computer cluster you can configure a group of virtual computers, where one will act as a main and the others as workers. With this logic the main can distribute the storage and analysis of data among the slaves and then resume the results: see for example the MapReduce or the Resilient Distributed Dataset (RDD) approaches used by the open-source software Apache Hadoop and Apache Spark respectively. For a specific example of parallel computing in computational analysis of communication you can take a look at the implementation of distributed supervised sentiment analysis, in which one of the authors of this book deployed supervised text classification in Apache Spark and connected this infrastructure with real-time analysis of tweets using Apache Kafka in order to perform streaming analytics@calderon2019distributed.\nThese architectures for parallel processing will significantly increase your computation capacity for big data problems but the initial implementation will consume time and (most of the time) money, which is the reason why you must think in advance if there is a simpler solution (such as a single but powerful machine) before implementing a more complex infrastructure in your analysis."
  },
  {
    "objectID": "content/chapter15.html#sec-publishingsource",
    "href": "content/chapter15.html#sec-publishingsource",
    "title": "15  Scaling up and distributing",
    "section": "15.3 Publishing Your Source",
    "text": "15.3 Publishing Your Source\nAlready in Section 4.3, we briefly introduced the idea of version control protocols such as git, and the most well-known online git repository GitHub. There are others, such as Bitbucket and the question of which one you use is not really of importance for our argument here. Already for small projects, it is a good idea to use version control so that you can always go back to earlier versions, but as soon as you start working with multiple people on one project, it becomes indispensable.\nIn particular, it is possible to work on multiple branches, different versions of the code that can later be merged again. In this way, it is possible to develop new features without interfering with the main version of the code. There are plenty of git tutorials available online, and we highly recommended using git from the beginning of a specific project on – be it your bachelor, master or doctoral thesis, a paper, or a tool that you want to create.\nIn the computational analysis of communication, it is becoming more and more the norm to publish all your source code together with an article, even though it is important to keep in mind ethical and legal restrictions (Van Atteveldt et al. (2019)). Using a version control platform like GitHub from the beginning makes this easy: when publishing your paper, the only thing you have to do is to set access of your repository to “public” (in case it was private before), add a README.md file (in case you have not done so earlier), and preferably, get a persistent identifier, a doi for your code (see guides.github.com/activities/citable-code/). And don’t forget to add a license to your code, such as MIT, GPL, or Apache. All of these have specific implications on what others can or cannot do with your code (e.g., whether it can be used for commercial purposes or whether derivatives need to be published under the same license as well). Whatever you choose here, it is important that you make a choice, as otherwise, it may not be (legally) possible to use your code at all. If your code pertains to a specific paper, then we suggest you organize your repository as a so-called “research compendium”, integrating both your code and your data. Van Atteveldt et al. (2020) provide a template and tools for easily creating one 2.\nIn virtually all instances, your code will rely on libraries written by others, which are available free of charge. Therefore, it only seems fair to “give back” and make sure that any code that you wrote and that can be useful to others, is also available to them.\nJust like in the case of a research compendium for a specific paper, publishing source code for more generic reuse also begins with a GitHub repository. In fact, both R (with devtools) and Python (via pip) can install packages directly from GitHub. In order to make sure that your code can be installed as a package, you need to follow specific instructions on how to name files, how to structure your directory, and so on (see packaging.python.org/tutorials/packaging-projects/ and r-pkgs.had.co.nz/).\nRegardless of these specific technical instructions, you can make sure from the outset, though, that your code is easily re-usable. The checklist below can help making your code publishable from the outset.\n\nDo not hard-code values. Rather than using \"myoutputfile.csv\" or 50 within your script, create constants like OUTPUTFILE=\"myoutputfile\" and NUMBER_OF_TOPICS=50 at the beginning of your script and use these variables instead of the values later on. Even better, let the user provide these arguments as command line arguments or via a configuration file.\nUse functions. Rather than writing large scripts that are executed from the first line to the last in that order, structure the code in different functions that fulfill one specific task each, and can hence be reused. If you find yourself copy-pasting code, then most likely, you can write a function instead.\nDocument your code. Use docstrings (Python) or comments (R) to make clear what each function does."
  },
  {
    "objectID": "content/chapter15.html#sec-container",
    "href": "content/chapter15.html#sec-container",
    "title": "15  Scaling up and distributing",
    "section": "15.4 Distributing Your Software as Container",
    "text": "15.4 Distributing Your Software as Container\nWhen publishing your software, you can think of multiple user groups. Some may be interested in building on and further developing your code. Some may not care about your code at all and just want your software to run. And many others will be somewhere in between.\nOnly publishing your source code (Section 15.3) may be a burden for those who want your code to “just run” once your code becomes more complex and has more dependencies. Imagine a scenario where your software requires a specific version of Python or R and/or some very specific (or maybe incompatible) libraries that you do not want to force the user to install.\nAnd maybe your prospective user does not even know any R or Python.\nFor such cases, so-called containers are the solution, with as most prominent platform Docker. You can envision a container as a minimalistic virtual machine that includes everything to run your software. To the outside, none of that is visible – just a network port to connect to, or a command line to interact with, depending on your choices.\nSoftware that is containerized using Docker is distributed as a so-called Docker image. You can build such an image yourself, but it can also be distributed by pushing it to a so-called registry, such as the Docker Hub. If you publish your software this way, the end user has to do nothing other than installing Docker and running the command docker run nameofyourimage – it will even be downloaded automatically if necessary. There are also GUI versions of Docker available, which lowers the threshold for some end user groups even more.\nLet’s illustrate the typical workflow with a toy example. Imagine you wrote the following script, myscript.py:\n\nimport numpy as np\nfrom random import randint\na = randint(0,10)\nprint(f\"exp({a}) = {np.exp(a)}\")\n\nYou think that this is an awesome program (after all, it calculates \\(e\\) to the power of a random integer!), and others should be able to use it. And you don’t want to bother them with setting up Python, installing numpy, and then running the script. In fact, they do not even need to know that it’s a Python program. You could have written it as well in R, or any other language – for the user, that will make no difference at all.\nWhat would a Docker image that runs this code need to contain? Not much: first some basic operating system (usually, a tiny Linux distribution), Python, numpy, and the script itself.\nTo create such a Docker image, you create a file named Dockerfile in the same directory as your script with the following content:\nFROM python:3\nADD myscript.py /\nRUN pip install numpy\nCMD [ \"python\", \"./myscript.py\" ]\nThe first line tells Docker to build your new image by starting from an existing image that already contains an operating system and Python3. You could also start from scratch here, but this makes your life much easier. The next line adds your script to the image, and then we run pip install numpy within the image. The last line just specifies which command with which parameters needs to be executed when the image is run – in our case python ./myscript.py.\nTo create the image, you run docker build -t dockertest . (naming the image “dockertest”). After that, you can run it using docker run dockertest – and, if you want to, publish it.\nEasy, right?\nBut when does it make sense to use Docker? Not in our toy example, of course. While the original code is only a couple of bytes, it now got bloated to hundreds of megabytes. But there are plenty of scenarios where this makes a lot of sense.\n\nTo “abstract away” the inner workings of your code. Rather than giving potentially complicated instructions how to run your code, which dependencies to install, etc., you can just provide users with the Docker image, in which everything is already taken care of.\nTo ensure that users get the same results. Though it doesn’t form a huge problem on a daily basis for most computational scientists, different versions of different libraries on different systems may occasionally produce slightly different results. The container ensures that the code is run using the same software setup.\nTo avoid interfering with existing installations. Already our toy example had a dependency, numpy, but often, dependencies can be more complex and a program we write may need very specific libraries, or even some other software beyond Python or R libraries. Distributing the source code alone means forcing the user to also install these; and there are many good reasons why people may be reluctant to do so. It may be incompatible with other software on their computer, there may be security concerns, or it just may be too much work. But if it runs inside of the Docker container, many of these problems disappear.\n\nIn short, the Docker image is rarely the only way in which you distribute your source code. But already adding a Dockerfile to your GitHub repository so that users can build a Docker container can offer another and maybe better way of running your software to your audience.\n\n\n\n\n\n\n\nCodd, Edgar F. 1970. “A Relational Model of Data for Large Shared Data Banks.” Communications of the ACM 13 (6): 377–87. https://doi.org/10.1145/362384.362685.\n\n\nGünther, Elisabeth, Damian Trilling, and Bob van de Velde. 2018. “But How Do We Store It? Data Architecture in the Social-Scientific Research Process.” In Computational Social Science in the Age of Big Data. Concepts, Methodologies, Tools, and Applications, edited by C. M. Stuetzer, M. Welker, and M. Egger, 161–87. Herbert von Halem.\n\n\nTrilling, Damian, and Jeroen G. F. Jonkman. 2018. “Scaling up Content Analysis.” Communication Methods and Measures 12 (2-3): 158–74. https://doi.org/10.1080/19312458.2018.1447655.\n\n\nVan Atteveldt, Wouter, Anne Kroon, Felicia Loecherbach, Mickey Steijaert, Joanna Strycharz, Damian Trilling, Mariken Van der Velden, and Kasper Welbers. 2020. “Standardized Research Compendiums: Making Open and Transparent Science Fun and Easy.” Gold Coast, Australia (online due to Corona crisis).\n\n\nVan Atteveldt, Wouter, Joanna Strycharz, Damian Trilling, and Kasper Welbers. 2019. “Toward Open Computational Communication Science : A Practical Road Map for Reusable Data and Code University of Amsterdam , the Netherlands.” International Journal of Communication 13: 3935–54."
  },
  {
    "objectID": "content/chapter16.html#sec-howfarcome",
    "href": "content/chapter16.html#sec-howfarcome",
    "title": "16  Where to go next",
    "section": "16.1 How Far Have We Come?",
    "text": "16.1 How Far Have We Come?\nIn this book, we introduced you to the computational analysis of communication. In Chapter 1, we tried to convince you that the computational analysis of communication is a worthwhile endeavor – and we also highlighted that there is much more to the subject than this book can cover. So here we are now. Maybe you skipped some chapters, maybe you did some additional reading or followed some online tutorials, and maybe you completed your first small project that involved some of techniques we covered. Time to recap.\nYou now have some knowledge of programming. We hope that this has opened new doors for you, and allows you to use a wealth of libraries, tutorials, and tools that may make your life easier, your research more productive, and your analyses better.\nYou have learned how to handle new types of data. Not only traditional tabular datasets, but also textual data, semi-structured data, and to some extent network data and images.\nYou can apply machine-learning frameworks. You know about both unsupervised and supervised approaches, and can decide how they can be useful for finding answers to your research questions.\nFinally, you have got at least a first impression of some cool techniques like neural networks and services such as databases, containers, and cloud computing. We hope that being aware of them will help you to make an informed decision whether they may be good tools to dive into for your upcoming projects."
  },
  {
    "objectID": "content/chapter16.html#sec-wheretogo",
    "href": "content/chapter16.html#sec-wheretogo",
    "title": "16  Where to go next",
    "section": "16.2 Where To Go Next?",
    "text": "16.2 Where To Go Next?\nBut what should you learn next?\nMost importantly, we cannot stress enough that it should be the research question that is the guide, not the method. You shouldn’t use the newest neural network module just because it’s cool, when counting occurrences of a simple regular expression does the trick. But this also applies the other way around: if a new method performs much better than an old one, you should learn it! For too long, for instance, people have relied on simple bag-of-words sentiment analyses with off-the-shelf dictionaries, simply because they were easy to use – despite better alternatives being available.\nHaving said that, we will nevertheless try to give some general recommendations for what to learn next.\nBecome better at programming. In this book, we tried to find a compromise between teaching the programming concepts necessary to apply our methods on the one hand, and not getting overly technical on the other hand. After all, for many social scientists, programming is a means to an end, not a goal in itself. But as you progress, a deeper understanding of some programming concepts will make it easier for you to tailor everything according to your needs, and will – again – open new doors. There are countless books and online tutorials on “Programming in [Language of your choice]”. In fact, in this “bilingual” book we have shown you how to program with R and Python (the most used languages by data scientists), but there are other programming languages that might also deserve your attention (e.g. Java, Scala, Julia, etc.) if you become a computational scientist.\nLearn how to write libraries. A very specific yet widely applicable skill we’d encourage you to learn is writing your own packages (“modules” or “libraries”). One of the nice things about computational analyses is that they are very much compatible with an Open Science approach. Sharing what you have done is much easier if everything you did is already documented in some code that you can share. But you can go one step further: of course it is nice if people can exactly reproduce your analysis, but wouldn’t it be even nicer if they could also use your code to run analyses using their own data? If you thought about a great way to compute some statistic, why not make it easy for others to do the same? Consider writing (and documenting!) your code in a general way and then publishing it on CRAN or pypi so others can easily install and use it.\nGet inspiration for new types of studies. Try to think a bit out of the box and beyond classical surveys, experiments, and content analyses to design new studies. Books like Bit by Bit Salganik (2019) may help you with this. You can also take a look at other scientific disciplines such as computational biology that has reinvented its methods, questions and hypotheses. Keep in mind that computational methods have an impact on the theoretical and empirical discussions of communication processes, which in turn will call for novel types of studies. The emerging scientific fields such as Computational Communication Science, Computational Social Sciences and Digital Humanities show how theory and methods can develop hand in hand.\nGet a deeper understanding of deep learning. For many tasks in the computational analysis of communication, classical machine learning approaches (like regression or support vector machines) work just fine. In fact, there is no need to always jump on the latest band wagon of the newest technique. If a simple logistic regression achieves an F1-score of 88.1, and the most fancy neural network achieves an 88.5 – would it be worth the extra effort and the loss of explainability? It depends on your use case, but probably not. Nevertheless, by now, we can be fairly certain that neural networks and deep learning are here to stay. We could only give a limited introduction in this book, but state-of-the-art analysis of text and especially visual material cannot do without it any more. Even though you may not train such models yourself all the time, but may use, for instance, pre-trained word embeddings or use packages like spacy that have been trained using neural networks, it seems worthwhile to understand these techniques better. Also here, a lot of online tutorials exist for frameworks such as keras or tensorflow, but also thorough books that provide a sound understanding of the underlying models Goldberg (2017).\nLearn more about statistical models. Not everything in the computational analysis of communication is machine learning. We used the analogy of the mouse trap (where we only care about the performance, not the underlying mechanism) versus better prior understanding, and argued that often, we may use machine learning as a “mouse trap” to enrich our data – even if we are ultimately interested in explaining some other process. For instance, we may want to use machine learning as one step in a workflow to predict the topic of social media messages, and then use a conventional statistical approach to understand which factors explain how often the message has been shared. Such data, though, often have different characteristics than data that you may encounter in surveys or experiments. In this case, for instance, the number of shares is a so-called count variable: it can take only positive integers, and thus has a lower bound (0) but no upper bound. That’s very different than normally distributed data and requires regression models such as negative binomial regression. That’s not difficult to do, but worth reading up on. Similarly, multilevel modelling will often be appropriate for the data you work with. Being familiar with this and other techniques (such as mediation and moderation analysis, or even structural equation modeling) will allow you to make better choices. On a different note, you may want to familiarize yourself with Bayesian statistics – a framework that is very different from the so-called frequentist approach that you probably know from your statistics courses.\nAnd, last but not least: have fun! At least for us, that is one of the most important parts: don’t forget to enjoy the skills you gained, and create projects that you enjoy!"
  },
  {
    "objectID": "content/chapter16.html#sec-ethics",
    "href": "content/chapter16.html#sec-ethics",
    "title": "16  Where to go next",
    "section": "16.3 Open, Transparent, and Ethical Computational Science",
    "text": "16.3 Open, Transparent, and Ethical Computational Science\nWe started this book by reflecting on what we are actually doing when conducting computational analyses of communication. One of the things we highlighted in Chapter 1 was our use of open-source tools, in particular Python and R and the wealth of open-source libraries that extend them. Hopefully, you have also realized not only how much your work could therefore build on the work of others, but also how many of the resources you used were created as a community effort.\nNow that you acquired the knowledge it takes to conduct computational research on communication, it is time to reflect on how to give back to the community, and how to contribute to an open research environment. At the same time, it is not as simple as “just putting everything online” – after all, researchers often work with sensitive data. We therefore conclude this book with a short discussion on open, transparent, and ethical computational science.\nTransparent and Open Science. In the wake of the so-called reproducibility crisis, the call for transparent and open science has become louder and louder in the last years. The public, funders, and journals increasingly ask for access to data and analysis scripts that underly published research. Of course, publishing your data and code is not a panacea for all problems, but it is a step towards better science from at least two perspectives (Van Atteveldt et al. 2019): first, it allows others to reproduce your work, enhancing its credibility (and the credibility of the field as a whole). Second, it allows others to build on your work without reinventing the wheel.\nSo, how can you contribute to this? Most importantly, as we advised in Section 4.3: use a version control system and share your code on a site like github.com. We also discussed code-sharing possibilities in Section 15.3. Finally, you can find a template for organizing your code and data so that your research is easy to reproduce at github.com/ccs-amsterdam/compendium.\nThe privacy–transparency trade-off. While the sharing of code is not particularly controversial, the sharing of data sometimes is. In particular, you may deal with data that contain personally identifiable information. On the one hand, you should share your data to make sure that your work can be reproduced – on the other hand, it would be ethically (and depending on your jurisdiction, potentially also legally) wrong to share personal data about individuals. As boyd and Crawford (2012) write: “Just because it is accessible does not make it ethical.” Hence, the situation is not always black or white, and some techniques exist to find a balance between the two: you can remove (or hash) information such as usernames, you can aggregate your data, you can add artificial noise. Ideally, you should integrate legal, ethical, and technical considerations to make an informed decision on how to find a balance such that transparency is maximized while privacy risks are minimized. More and more literature explores different possibilities Breuer, Bishop, and Kinder-Kurlanda (2020).\nOther Ethical Challenges in Computational Analyses. Lastly, there are also other ethical challenges that go beyond the use of privacy-sensitive data. Many tools we use give us great power, and with that comes great responsibility. For instance, as we highlighted in Section 12.4, every time we scrape a website, we cause some costs somewhere. They may be neglectable for a single http request, but they may add up. Similarly, calculations on some cloud service cause environmental costs. Before starting a large-scale project, we should therefore make a trade-off between the costs or damage we cause, and the (scientific) gain that we achieve.\nIn the end, though, we firmly believe that as computational scientists, we are well-equipped to contribute to the move towards more ethical, open, and transparent science. Let’s do it!\n\n\n\n\nboyd, Danah, and Kate Crawford. 2012. “Critical questions for Big Data.” Information, Communication & Society 15 (5): 662–79. https://doi.org/10.1080/1369118X.2012.678878.\n\n\nBreuer, Johannes, Libby Bishop, and Katharina Kinder-Kurlanda. 2020. “The practical and ethical challenges in acquiring and sharing digital trace data: Negotiating public-private partnerships.” New Media & Society 22 (11): 2058–80. https://doi.org/10.1177/1461444820924622.\n\n\nGoldberg, Yoav. 2017. Neural Network Models for Natural Language Processing. Morgan & Claypool.\n\n\nSalganik, Matthew. 2019. Bit by Bit: Social Research in the Digital Age. Princeton University Press.\n\n\nVan Atteveldt, Wouter, Joanna Strycharz, Damian Trilling, and Kasper Welbers. 2019. “Toward Open Computational Communication Science : A Practical Road Map for Reusable Data and Code University of Amsterdam , the Netherlands.” International Journal of Communication 13: 3935–54."
  },
  {
    "objectID": "content/references.html",
    "href": "content/references.html",
    "title": "References",
    "section": "",
    "text": "Amores, Javier J, Carlos Arcila Calderón, and Mikolaj Stanek. 2019.\n“Visual Frames of Migrants and Refugees in the Main Western\nEuropean Media.” Economics & Sociology 12 (3):\n147–61.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package\nfor the Quantitative Analysis of Textual Data.” Journal of\nOpen Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBlei, David M, and John D Lafferty. 2006. “Dynamic Topic\nModels.” In Proceedings of the 23rd International Conference\non Machine Learning, 113–20.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent\nDirichlet Allocation.” Journal of Machine Learning\nResearch 3 (Jan): 993–1022.\n\n\nBlei, David, and John Lafferty. 2006. “Correlated Topic\nModels.” Advances in Neural Information Processing\nSystems 18: 147.\n\n\nBlondel, Vincent D, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne\nLefebvre. 2008. “Fast Unfolding of Communities in Large\nNetworks.” Journal of Statistical Mechanics: Theory and\nExperiment 2008 (10): P10008.\n\n\nBoukes, Mark, Bob van de Velde, Theo Araujo, and Rens Vliegenthart.\n2019. “What’s the Tone? Easy Doesn’t Do It:\nAnalyzing Performance and Agreement Between Off-the-Shelf Sentiment\nAnalysis Tools.” Communication Methods and\nMeasures 00 (00): 1–22. https://doi.org/10.1080/19312458.2019.1671966.\n\n\nBoumans, Jelle W., and Damian Trilling. 2016. “Taking stock of the toolkit: An overview of relevant\nautmated content analysis approaches and techniques for digital\njournalism scholars.” Digital Journalism 4 (1):\n8–23. https://doi.org/10.1080/21670811.2015.1096598.\n\n\nboyd, Danah, and Kate Crawford. 2012. “Critical questions for Big Data.”\nInformation, Communication & Society 15 (5):\n662–79. https://doi.org/10.1080/1369118X.2012.678878.\n\n\nBreiman, Leo. 2001. “Statistical modeling:\nThe two cultures.” Statistical Science 16 (3):\n199–215. https://doi.org/10.1214/ss/1009213726.\n\n\nBreuer, Johannes, Libby Bishop, and Katharina Kinder-Kurlanda. 2020.\n“The practical and ethical challenges in\nacquiring and sharing digital trace data: Negotiating public-private\npartnerships.” New Media &\nSociety 22 (11): 2058–80. https://doi.org/10.1177/1461444820924622.\n\n\nBruns, Axel. 2019. “After the ‘APIcalypse’:\nsocial media platforms and their fight against critical scholarly\nresearch.” Information, Communication\n& Society 22 (11): 1544–66. https://doi.org/10.1080/1369118X.2019.1637447.\n\n\nBryman, Alan. 2012. Social Research Methods. 4th edition. New\nYork, NY: Oxford University Press.\n\n\nBurscher, Björn, Daan Odijk, Rens Vliegenthart, Maarten de Rijke, and\nClaes H. de Vreese. 2014. “Teaching the\ncomputer to code frames in news: Comparing two supervised machine\nlearning approaches to frame analysis.” Communication\nMethods and Measures 8 (3): 190–206. https://doi.org/10.1080/19312458.2014.937527.\n\n\nCairo, Alberto. 2019. How Charts Lie. WW Norton & Company.\n\n\nCazals, Frédéric, and Chinmay Karande. 2008. “A Note on the\nProblem of Reporting Maximal Cliques.” Theoretical Computer\nScience 407 (1-3): 564–68.\n\n\nChan, Chung-hong, Joseph Bajjalieh, Loretta Auvil, Hartmut Wessler,\nScott Althaus, Kasper Welbers, Wouter van Atteveldt, and Marc Jungblut.\nin press. “Four Best Practices for Measuring News Sentiment Using\n‘Off-the-Shelf’ Dictionaries: A Large-Scale p-Hacking\nExperiment.” Computational Communication Research, in\npress.\n\n\nChang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and\nDavid M Blei. 2009. “Reading Tea Leaves: How Humans Interpret\nTopic Models.” In Advances in Neural Information Processing\nSystems, 288–96.\n\n\nChen, Lizi. 2017. “News-Processed-Dataset.” https://doi.org/10.6084/m9.figshare.5296357.v1.\n\n\nChristakis, Nicholas A, and James H Fowler. 2009. Connected: The\nSurprising Power of Our Social Networks and How They Shape Our\nLives. Little, Brown Spark.\n\n\nCioffi-Revilla, Claudio. 2014. Introduction to Computational Social\nScience: Principles and Applications. London, UK: Springer.\n\n\nClauset, Aaron, Mark EJ Newman, and Cristopher Moore. 2004.\n“Finding Community Structure in Very Large Networks.”\nPhysical Review E 70 (6): 066111.\n\n\nCodd, Edgar F. 1970. “A Relational Model of Data for Large Shared\nData Banks.” Communications of the ACM 13 (6): 377–87.\nhttps://doi.org/10.1145/362384.362685.\n\n\nCrawley, Michael J. 2012. The r Book. 2nd Edition. Wiley.\n\n\nDe Smedt, Tom, W Daelemans, and Tom De Smedt. 2012. “Pattern for Python.” The Journal of\nMachine Learning Research 13: 2063–67. http://dl.acm.org/citation.cfm?id=2343710.\n\n\nDietrich, Bryce J, Matthew Hayes, and DIANA Z O’BRIEN. 2019.\n“Pitch Perfect: Vocal Pitch and the Emotional Intensity of\nCongressional Speech.” American Political Science Review\n113 (4): 941–62.\n\n\nEppstein, David, Maarten Löffler, and Darren Strash. 2010.\n“Listing All Maximal Cliques in Sparse Graphs in Near-Optimal\nTime.” In International Symposium on Algorithms and\nComputation, 403–14. Springer.\n\n\nFreelon, Deen. 2018. “Computational Research\nin the Post-API Age.” Political Communication 35\n(4): 665–68. https://doi.org/10.1080/10584609.2018.1477506.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow: Concepts, Tools, and Techniques to Build\nIntelligent Systems. O’Reilly Media.\n\n\nGoldberg, Yoav. 2017. Neural Network Models for Natural Language\nProcessing. Morgan & Claypool.\n\n\nGonzalez-Bailon, S., and G. Paltoglou. 2015. “Signals of Public Opinion in Online Communication: A\nComparison of Methods and Data Sources.” The ANNALS of\nthe American Academy of Political and Social Science 659 (1):\n95–107. https://doi.org/10.1177/0002716215569192.\n\n\nGonzález-Bailón, Sandra. 2017. Decoding the\nsocial world: Data science and the unintended consequences of\ncommunication. Cambridge, MA: MIT.\n\n\nGriffiths, Thomas L, Michael I Jordan, Joshua B Tenenbaum, and David M\nBlei. 2004. “Hierarchical Topic Models and the Nested Chinese\nRestaurant Process.” In Advances in Neural Information\nProcessing Systems, 17–24.\n\n\nGrimmer, J., and B. M. Stewart. 2013. “Text as Data:\nThe Promise and Pitfalls of Automatic Content Analysis\nMethods for Political Texts.” Political Analysis 21 (3):\n267–97. https://doi.org/10.1093/pan/mps028.\n\n\nGünther, Elisabeth, Damian Trilling, and Bob van de Velde. 2018.\n“But How Do We Store It? Data Architecture in the\nSocial-Scientific Research Process.” In Computational Social\nScience in the Age of Big Data. Concepts,\nMethodologies, Tools, and Applications, edited by C. M. Stuetzer,\nM. Welker, and M. Egger, 161–87. Herbert von Halem.\n\n\nHoriuchi, Yusaku, Tadashi Komatsu, and Fumio Nakaya. 2012. “Should\nCandidates Smile to Win Elections? An Application of Automated Face\nRecognition Technology.” Political Psychology 33 (6):\n925–33.\n\n\nHorne, Benjamin D., William Dron, Sara Khedr, and Sibel Adali. 2018.\n“Sampling the News Producers: A Large News\nand Feature Data Set for the Study of the Complex Media\nLandscape.” In 12th International AAAI Conference on\nWeb and Social Media (ICWSM), 518–27. Icwsm. http://arxiv.org/abs/1803.10124.\n\n\nHutto, Clayton J, and Eric Gilbert. 2014. “Vader: A Parsimonious\nRule-Based Model for Sentiment Analysis of Social Media Text.” In\nEighth International AAAI Conference on Weblogs and Social\nMedia.\n\n\nJurafsky, Daniel, and James H Martin. 2009. Speech and Language\nProcessing: An Introduction to Natural Language Processing,\nComputational Linguistics, and Speech Recognition (2nd Ed.).\nPrentice Hall.\n\n\nKirk, Andy. 2016. Data Visualisation: A Handbook for Data Driven\nDesign. London, UK: SAGE.\n\n\nKitchin, Rob. 2014a. “Big Data, new\nepistemologies and paradigm shifts.” Big Data\n& Society 1 (1): 1–12. https://doi.org/10.1177/2053951714528481.\n\n\n———. 2014b. The Data Revolution: Big Data, Open Data, Data\nInfrastructures and Their Consequences. Sage.\n\n\nKnox, Dean, and Christopher Lucas. 2021. “A Dynamic Model of\nSpeech for the Social Sciences.” American Political Science\nReview 115 (2): 649–66.\n\n\nKrippendorff, Klaus. 2004. Content Analysis: An Introduction to Its\nMethodology. 2nd ed. Thousand Oaks, CA: SAGE.\n\n\nLandauer, Thomas K, Danielle S McNamara, Simon Dennis, and Walter\nKintsch. 2013. Handbook of Latent Semantic Analysis. Psychology\nPress.\n\n\nLeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998.\n“Gradient-Based Learning Applied to Document Recognition.”\nProceedings of the IEEE 86 (11): 2278–2324.\n\n\nLin, Jimmy. 2015. “On Building Better Mousetraps and Understanding\nthe Human Condition: Reflections on Big Data in the Social\nSciences.” The ANNALS of the American Academy of Political\nand Social Science 659 (1): 33–47.\n\n\nMaas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y.\nNg, and Christopher Potts. 2011. “Learning Word Vectors for\nSentiment Analysis.” In Proceedings of the 49th Annual\nMeeting of the Association for Computational Linguistics: Human Language\nTechnologies, 142–50. Portland, Oregon, USA: Association for\nComputational Linguistics. http://www.aclweb.org/anthology/P11-1015.\n\n\nMargolin, Drew B. 2019. “Computational Contributions: A Symbiotic\nApproach to Integrating Big, Observational Data Studies into the\nCommunication Field.” Communication Methods and Measures\n13 (4): 229–47.\n\n\nMayer-Schönberger, Viktor, and Kenneth Cukier. 2013. Big Data: A\nRevolution That Will Transform How We Live, Work, and Think. New\nYork, NY: Houghton Mifflin Harcourt.\n\n\nMimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew\nMcCallum. 2011. “Optimizing Semantic Coherence in Topic\nModels.” In Proceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing, 262–72.\n\n\nMoreno, Jacob Levy. 1934. Who Shall Survive? A New Approach to the\nProblem of Human Interrelations. Nervous; mental disease publishing\nco.\n\n\nNewman, Mark EJ, and Michelle Girvan. 2004. “Finding and\nEvaluating Community Structure in Networks.” Physical Review\nE 69 (2): 026113.\n\n\nNothman, Joel, Hanmin Qin, and Roman Yurchak. 2018. “Stop Word\nLists in Free Open-Source Software Packages.” In Proceedings\nof Workshop for NLP Open Source Software (NLP-OSS), 7–12.\n\n\nPeng, Yilang. 2018. “Same Candidates, Different Faces: Uncovering\nMedia Bias in Visual Portrayals of Presidential Candidates with Computer\nVision.” Journal of Communication 68 (5): 920–41.\n\n\nPiketty, Thomas. 2017. Capital in the Twenty-First Century.\nCambridge, MA: Harvard University Press.\n\n\nPuschmann, Cornelius. 2019. “An end to the\nwild west of social media research: a response to Axel\nBruns.” Information, Communication &\nSociety 22 (11): 1582–89. https://doi.org/10.1080/1369118X.2019.1646300.\n\n\nQi, Peng, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D.\nManning. 2020. “Stanza: A Python Natural Language\nProcessing Toolkit for Many Human Languages.” In Proceedings\nof the 58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations. https://nlp.stanford.edu/pubs/qi2020stanza.pdf.\n\n\nRaghavan, Usha Nandini, Réka Albert, and Soundar Kumara. 2007.\n“Near Linear Time Algorithm to Detect Community Structures in\nLarge-Scale Networks.” Physical Review E 76 (3): 036106.\n\n\nReagan, Andrew J., Christopher M. Danforth, Brian Tivnan, Jake Ryland\nWilliams, and Peter Sheridan Dodds. 2017. “Sentiment analysis methods for understanding large-scale\ntexts: a case for using continuum-scored words and word shift\ngraphs.” EPJ Data Science 6 (1). https://doi.org/10.1140/epjds/s13688-017-0121-9.\n\n\nRedmon, Joseph, and Ali Farhadi. 2018. “YOLOv3: An Incremental\nImprovement.” arXiv.\n\n\nRieder, Bernhard. 2017. “Scrutinizing an Algorithmic Technique:\nThe Bayes Classifier as Interested Reading of\nReality.” Information Communication and Society 20 (1):\n100–117. https://doi.org/10.1080/1369118X.2016.1181195.\n\n\nRiffe, Daniel, Stephen Lacy, Frederick Fico, and Brendan Watson. 2019.\nAnalyzing Media Messages. Using Quantitative Content\nAnalysis in Research. 4th edition. New York, NY: Routledge.\n\n\nRiffe, Daniel, Stephen Lacy, Brendan R. Watson, and Frederick Fico.\n2019. Analyzing Media Messages: Using Quantitative Content Analysis\nin Research. 4th ed. New York, NY: Routledge.\n\n\nRoberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher\nLucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and\nDavid G Rand. 2014. “Structural Topic Models for Open-Ended Survey\nResponses.” American Journal of Political Science 58\n(4): 1064–82.\n\n\nSalganik, Matthew. 2019. Bit by Bit: Social Research in the Digital\nAge. Princeton University Press.\n\n\nScharkow, Michael. 2011. “Thematic content\nanalysis using supervised machine learning: An empirical evaluation\nusing German online news.” Quality &\nQuantity 47 (2): 761–73. https://doi.org/10.1007/s11135-011-9545-7.\n\n\n———. 2017. “Content Analysis, Automatic.” In\nThe International Encyclopedia of Communication Research\nMethods, edited by Jörg Matthes, Christine S. Davis, and Robert F.\nPotter, 1–14. Hoboken, NJ: Wiley. https://doi.org/10.1002/9781118901731.iecrm0043.\n\n\nStraka, Milan, and Jana Straková. 2017. “Tokenizing, POS Tagging,\nLemmatizing and Parsing UD 2.0 with UDPipe.” In Proceedings\nof the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to\nUniversal Dependencies, 88–99. Vancouver, Canada: Association for\nComputational Linguistics. http://www.aclweb.org/anthology/K/K17/K17-3009.pdf.\n\n\nThelwall, Mike, Kevan Buckley, and Georgios Paltoglou. 2012.\n“Sentiment Strength Detection for the Social Web.”\nJournal of the American Society for Information Science and\nTechnology 63 (1): 163–73. https://doi.org/10.1002/asi.21662.\n\n\nTrilling, Damian. 2013. “Following the news:\nPatterns of online and offline news consumption.” {PhD}\nTheses, University of Amsterdam. https://hdl.handle.net/11245/1.394551.\n\n\n———. 2017. “Big Data, Analysis\nof.” In The International Encyclopedia of\nCommunication Research Methods, 1–20. Hoboken, NJ, USA: John Wiley\n& Sons, Inc. https://doi.org/10.1002/9781118901731.iecrm0014.\n\n\nTrilling, Damian, and Jeroen G. F. Jonkman. 2018. “Scaling up\nContent Analysis.” Communication Methods and Measures 12\n(2-3): 158–74. https://doi.org/10.1080/19312458.2018.1447655.\n\n\nTufte, Edward R. 2006. Beautiful Evidence. Vol. 1. Graphics\nPress Cheshire, CT.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. Vol. 2.\nReading, Mass.\n\n\nTulkens, Stéphan, Lisa Hilte, Elise Lodewyckx, Ben Verhoeven, and Walter\nDaelemans. 2016. “A Dictionary-based Approach\nto Racism Detection in Dutch Social Media.”\nProceedings of the Workshop on Text Analytics for Cybersecurity and\nOnline Safety (TA-COS 2016), 11–17. http://www.clips.ua.ac.be/bibliography/a-dictionary-based-approach-to-racism-detection-in-dutch-social-media.\n\n\nVan Atteveldt, Wouter, Anne Kroon, Felicia Loecherbach, Mickey\nSteijaert, Joanna Strycharz, Damian Trilling, Mariken Van der Velden,\nand Kasper Welbers. 2020. “Standardized Research Compendiums:\nMaking Open and Transparent Science Fun and Easy.” Gold Coast,\nAustralia (online due to Corona crisis).\n\n\nVan Atteveldt, Wouter, Joanna Strycharz, Damian Trilling, and Kasper\nWelbers. 2019. “Toward Open Computational\nCommunication Science : A Practical Road Map for Reusable Data and Code\nUniversity of Amsterdam , the Netherlands.”\nInternational Journal of Communication 13: 3935–54.\n\n\nVan Atteveldt, Wouter, Mariken ACG Van der Velden, and Mark Boukes.\n2021. “The Validity of Sentiment Analysis: Comparing Manual\nAnnotation, Crowd-Coding, Dictionary Approaches, and Machine Learning\nAlgorithms.” Communication Methods and Measures 15 (2):\n121–40.\n\n\nVan Atteveldt, W., T. Sheafer, S. R. Shenhav, and Y. Fogel-Dror. 2017.\n“Clause Analysis: Using Syntactic Information to Automatically\nExtract Source, Subject, and Predicate from Texts with an Application to\nthe 2008–2009 Gaza War.” Political\nAnalysis 25 (2): 207–22.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential\nTools for Working with Data. O’Reilly.\n\n\nVermeer, Susan A. M. 2018. “A supervised\nmachine learning method to classify Dutch-language news\nitems.” https://doi.org/10.6084/m9.figshare.7314896.v1.\n\n\nVermeer, Susan A. M., Theo Araujo, Stefan F. Bernritter, and Guda van\nNoort. 2019. “Seeing the wood for the trees:\nHow machine learning can help firms in identifying relevant electronic\nword-of-mouth in social media.” International Journal\nof Research in Marketing 36 (3): 492–508. https://doi.org/10.1016/j.ijresmar.2019.01.010.\n\n\nVosoughi, Soroush, Deb Roy, and Sinan Aral. 2018. “The Spread of\nTrue and False News Online.” Science 359 (6380):\n1146–51.\n\n\nWaldherr, Annie. 2014. “Emergence of News\nWaves: A Social Simulation Approach.” Journal of\nCommunication 64 (5): 852–73. https://doi.org/10.1111/jcom.12117.\n\n\nWatts, Duncan J. 2004. Six Degrees: The Science of a Connected\nAge. WW Norton & Company.\n\n\nWettstein, Martin. 2020. “Simulating hidden\ndynamics : Introducing Agent-Based Models as a tool for linkage\nanalysis.” Computational Communication Research 2\n(1): 1–33. https://doi.org/10.5117/CCR2020.1.001.WETT.\n\n\nWilliams, Nora Webb, Andreu Casas, and John D Wilkerson. 2020.\n“Images as Data for Social Science Research: An Introduction to\nConvolutional Neural Nets for Image Classification.” Elements\nin Quantitative and Computational Methods for the Social Sciences."
  }
]