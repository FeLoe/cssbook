<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>cssbook_quarto - 10&nbsp; Text as data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter11.html" rel="next">
<link href="./chapter09.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Text as data</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">cssbook_quarto</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Computational Analysis of Communication</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter01.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter02.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Getting started: Fun with data and visualizations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter03.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Programming concepts for data analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter04.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">How to write code</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter05.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">From file to data frame and back</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter06.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Wrangling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter07.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Exploratory data analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter08.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Statistical Modeling and Supervised Machine Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter09.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Processing text</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter10.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Text as data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter11.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic analysis of text</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter12.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Scraping online data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter13.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Network Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-dtm" id="toc-sec-dtm" class="nav-link active" data-scroll-target="#sec-dtm"><span class="toc-section-number">10.1</span>  The Bag of Words and the Term-Document Matrix</a>
  <ul class="collapse">
  <li><a href="#sec-tokenizations" id="toc-sec-tokenizations" class="nav-link" data-scroll-target="#sec-tokenizations"><span class="toc-section-number">10.1.1</span>  Tokenization</a></li>
  <li><a href="#sec-dtm" id="toc-sec-dtm" class="nav-link" data-scroll-target="#sec-dtm"><span class="toc-section-number">10.1.2</span>  The DTM as a Sparse Matrix</a></li>
  <li><a href="#sec-bagofwords" id="toc-sec-bagofwords" class="nav-link" data-scroll-target="#sec-bagofwords"><span class="toc-section-number">10.1.3</span>  The DTM as a “Bag of Words”</a></li>
  <li><a href="#sec-wordcloud" id="toc-sec-wordcloud" class="nav-link" data-scroll-target="#sec-wordcloud"><span class="toc-section-number">10.1.4</span>  The (Unavoidable) Word Cloud</a></li>
  </ul></li>
  <li><a href="#sec-dtmselect" id="toc-sec-dtmselect" class="nav-link" data-scroll-target="#sec-dtmselect"><span class="toc-section-number">10.2</span>  Weighting and Selecting Documents and Terms</a>
  <ul class="collapse">
  <li><a href="#sec-stopwords" id="toc-sec-stopwords" class="nav-link" data-scroll-target="#sec-stopwords"><span class="toc-section-number">10.2.1</span>  Removing stopwords</a></li>
  <li><a href="#sec-punctuation" id="toc-sec-punctuation" class="nav-link" data-scroll-target="#sec-punctuation"><span class="toc-section-number">10.2.2</span>  Removing Punctuation and Noise</a></li>
  <li><a href="#sec-trimdtm" id="toc-sec-trimdtm" class="nav-link" data-scroll-target="#sec-trimdtm"><span class="toc-section-number">10.2.3</span>  Trimming a DTM</a></li>
  <li><a href="#sec-dtmweight" id="toc-sec-dtmweight" class="nav-link" data-scroll-target="#sec-dtmweight"><span class="toc-section-number">10.2.4</span>  Weighting a DTM</a></li>
  </ul></li>
  <li><a href="#sec-ngram" id="toc-sec-ngram" class="nav-link" data-scroll-target="#sec-ngram"><span class="toc-section-number">10.3</span>  Advanced Representation of Text</a>
  <ul class="collapse">
  <li><a href="#sec-ngrams" id="toc-sec-ngrams" class="nav-link" data-scroll-target="#sec-ngrams"><span class="toc-section-number">10.3.1</span>  <span class="math inline">\(n\)</span>-grams</a></li>
  <li><a href="#sec-collocations" id="toc-sec-collocations" class="nav-link" data-scroll-target="#sec-collocations"><span class="toc-section-number">10.3.2</span>  Collocations</a></li>
  <li><a href="#sec-wordembeddings" id="toc-sec-wordembeddings" class="nav-link" data-scroll-target="#sec-wordembeddings"><span class="toc-section-number">10.3.3</span>  Word Embeddings</a></li>
  <li><a href="#sec-nlp" id="toc-sec-nlp" class="nav-link" data-scroll-target="#sec-nlp"><span class="toc-section-number">10.3.4</span>  Linguistic Preprocessing</a></li>
  </ul></li>
  <li><a href="#which-preprocessing-to-use" id="toc-which-preprocessing-to-use" class="nav-link" data-scroll-target="#which-preprocessing-to-use"><span class="toc-section-number">10.4</span>  Which Preprocessing to Use?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-chap-dtm" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Text as data</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell">

</div>
<div class="cell">

</div>
<p><strong>Abstract.</strong> This chapter shows how you can analyze texts that are stored as a data frame column or variable using functions from the package <em>quanteda</em> in R and the package <em>sklearn</em> in Python and R. Please see Chapter <a href="chapter09.html"><span>9</span></a> for more information on reading and cleaning text.</p>
<p><strong>Keywords.</strong> Text as Data, Document-Term Matrix</p>
<p><strong>Objectives:</strong></p>
<ul>
<li>Create a document-term matrix from text</li>
<li>Perform document and feature selection and weighting</li>
<li>Understand and use more advanced representations such as n-grams and embeddings</li>
</ul>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This chapter introduces the packages <em>quanteda</em> (R) and <em>sklearn</em> and <em>nltk</em> (Python) for converting text into a document-term matrix. It also introduces the <em>udpipe</em> package for natural language processing. You can install these packages with the code below if needed (see Section <a href="chapter01.html#sec-installing"><span>1.4</span></a> for more details):</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install ufal.udpipe spacy nltk scikit<span class="op">-</span>learn<span class="op">==</span><span class="fl">0.24.2</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install gensim<span class="op">==</span><span class="fl">4.0.1</span> wordcloud nagisa conllu tensorflow<span class="op">==</span><span class="fl">2.5.0</span> tensorflow<span class="op">-</span>estimator<span class="op">==</span><span class="fl">2.5.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(<span class="st">"glue"</span>,<span class="st">"tidyverse"</span>,<span class="st">"quanteda"</span>, </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"quanteda.textstats"</span>, <span class="st">"quanteda.textplots"</span>, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"udpipe"</span>, <span class="st">"spacyr"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>After installing, you need to import (activate) the packages every session:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard library and basic data wrangling</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> regex</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenization</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> TreebankWordTokenizer, WhitespaceTokenizer</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"stopwords"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True

[nltk_data] Downloading package stopwords to /home/runner/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nagisa</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># For plotting word clouds</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Natural language processing</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ufal.udpipe</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> KeyedVectors, Phrases</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.phrases <span class="im">import</span> Phraser</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ufal.udpipe <span class="im">import</span> Model, Pipeline</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> conllu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glue)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenization</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textstats)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textplots)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Natural language processing</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(udpipe)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(spacyr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<section id="sec-dtm" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="sec-dtm"><span class="header-section-number">10.1</span> The Bag of Words and the Term-Document Matrix</h2>
<p>Before you can conduct any computational analysis of text, you need to solve a problem: computations are usually done on numerical data – but you have text. Hence, you must find a way to <em>represent</em> the text by numbers. The document-term matrix (DTM, also called the term-document matrix or TDM) is one common numerical representation of text. It represents a <em>corpus</em> (or set of documents) as a matrix or table, where each row represents a document, each column represents a term (word), and the numbers in each cell show how often that word occurs in that document.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-dtm" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.1 </strong></span>Example document-term matrix</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell" data-hash="chapter10_cache/html/dtm-python_c1f6ffee45ec1a8da82e1c5a85ee7a12">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The caged bird sings with a fearful trill"</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"for the caged bird sings of freedom"</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> cv.fit_transform(texts)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dataframe of the word counts to inspect</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># - todense transforms the dtm into a dense matrix</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># - get_feature_names() gives a list words</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(d.todense(), columns<span class="op">=</span>cv.get_feature_names())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bird  caged  fearful  for  freedom  of  sings  the  trill  with
0     1      1        1    0        0   0      1    1      1     1
1     1      1        0    1        1   1      1    1      0     0</code></pre>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell" data-hash="chapter10_cache/html/dtm-r_10993886cfdae5fd6c268a0d4abd57dd">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>texts <span class="ot">=</span> <span class="fu">c</span>(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The caged bird sings with a fearful trill"</span>, </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"for the caged bird sings of freedom"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">tokens</span>(texts) <span class="sc">%&gt;%</span> <span class="fu">dfm</span>()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect by converting to a (dense) matrix</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="fu">convert</span>(d, <span class="st">"matrix"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       features
docs    the caged bird sings with a fearful trill for of freedom
  text1   1     1    1     1    1 1       1     1   0  0       0
  text2   1     1    1     1    0 0       0     0   1  1       1</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>As an example, Example <a href="#exm-dtm"><span>10.1</span></a> shows a DTM made from two lines from the famous poem by Mary Angelou. The resulting matrix has two rows, one for each line; and 11 columns, one for each unique term (word). In the columns you see the document frequencies of each term: the word “bird” occurs once in each line, but the word “with” occurs only in the first line (text1) and not in the second (text2).</p>
<p>In R, you can use the <code>dfm</code> function from the <em>quanteda</em> package <span class="citation" data-cites="quanteda">(<a href="references.html#ref-quanteda" role="doc-biblioref">Benoit et al. 2018</a>)</span>. This function can take a vector or column of texts and transforms it directly into a DTM (which quanteda actually calls a document-<em>feature</em> matrix, hence the function name <code>dfm</code>). In Python, you achieve the same by creating an object of the <code>CountVectorizer</code> class, which has a <code>fit_transform</code> function.</p>
<section id="sec-tokenizations" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="sec-tokenizations"><span class="header-section-number">10.1.1</span> Tokenization</h3>
<p>In order to turn a corpus into a matrix, each text needs to be <em>tokenized</em>, meaning that it must be split into a list (vector) of words. This seems trivial, as English (and most western) text generally uses spaces to demarcate words. However, even for English there are a number of edge cases. For example, should “haven’t” be seen as a single word, or two?</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-tokenize" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.2 </strong></span>Differences between tokenizers</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="cell" data-hash="chapter10_cache/html/tokenize-python_98ad3b11bb4bc863cd1ceff0c2564188">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"I haven't seen John's derring-do"</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> CountVectorizer().build_tokenizer()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['haven', 'seen', 'John', 'derring', 'do']</code></pre>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="cell" data-hash="chapter10_cache/html/tokenize-r_75a8a6951707057738fe077e9a4b3589">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>text <span class="ot">=</span> <span class="st">"I haven't seen John's derring-do"</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tokens</span>(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document.
text1 :
[1] "I"          "haven't"    "seen"       "John's"     "derring-do"</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Example <a href="#exm-tokenize"><span>10.2</span></a> shows how Python and R deal with the sentence “I haven’t seen John’s derring-do”. For Python, we first use <code>CountVectorizer.build_tokenizer</code> to access the built-in tokenizer. As you can see in the first line of input, this tokenizes “haven’t” to <code>haven</code>, which of course has a radically different meaning. Moreover, it silently drops all single-letter words, including the <code>'t</code>, <code>'s</code>, and <code>I</code>.</p>
<p>In the box “Tokenizing in Python” below, we therefore discuss some alternatives. For instance, the <code>TreebankWordTokenizer</code> included in the <em>nltk</em> package is a more reasonable tokenizer and splits “haven’t” into <code>have</code> and <code>n't</code>, which is a reasonable outcome. Unfortunately, this tokenizer assumes that text has already been split into sentences, and it also includes punctuation as tokens by default. To circumvent this, we can introduce a custom tokenizer based on the Treebank tokenizer, which splits text into sentences (using <code>nltk.sent_tokenize</code>) – see the box for more details.</p>
<p>For R, we simply call the <code>tokens</code> function from the <em>quanteda</em> package. This keeps <code>haven't</code> and <code>John's</code> as a single word, which is probably less desirable than splitting the words but at least better than outputting the word <code>haven</code>.</p>
<p>As this simple example shows, even a relatively simple sentence is tokenized differently by the tokenizers considered here (and see the box on tokenization in Python). Depending on the research question, these differences might or might not be important. However, it is always a good idea to check the output of this (and other) preprocessing steps so you understand what information is kept or discarded.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tokenization in Python
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>As you can see in the example, the built-in tokenizer in <code>scikit-learn</code>is not actually very good. For example, <em>haven’t</em> is tokenized to <em>haven</em>, which is an entirely different word. Fortunately, there are other tokenizers in the <em>nltk.tokenize</em> package that do better.</p>
<p>For example, the <code>TreebankTokenizer</code> uses the tokenization rules for the Penn Treebank to tokenize, which produces better results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"""I haven't seen John's derring-do. </span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="st">          Second sentence!"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TreebankWordTokenizer().tokenize(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['I', 'have', "n't", 'seen', 'John', "'s", 'derring-do.', 'Second', 'sentence', '!']</code></pre>
</div>
</div>
<p>Another example is the <code>WhitespaceTokenizer</code>, which simply uses whitespace to tokenize, which can be useful if your input has already been tokenized, and is used in Example <a href="#exm-tagcloud"><span>10.11</span></a> below for tweets to conserve hash tags.</p>
<div class="cell" data-hash="chapter10_cache/html/tokenizealt1-python_3ab00f82792819d15a4635126c7cdc0d">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(WhitespaceTokenizer().tokenize(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['I', "haven't", 'seen', "John's", 'derring-do.', 'Second', 'sentence!']</code></pre>
</div>
</div>
<p>You can also write your own tokenizer if needed. For example, the <code>TreebankTokenizer</code> assumes that text has already been split into sentences (which is why the period is attached to the word <em>derring-do.</em>). The code below shows how we can make our own tokenizer class, which uses <code>nltk.sent_tokenize</code> to first split the text into sentences, and then uses the <code>TreebankTokenizer</code> to tokenize each sentence, keeping only tokens that include at least one letter character. Although a bit more complicated, this approach can give you maximum flexibility.</p>
<div class="cell" data-hash="chapter10_cache/html/tokenizealt2-python_a9580f49d937d12435c4029bd0fd795c">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyTokenizer:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tokenize(<span class="va">self</span>, text):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        tokenizer <span class="op">=</span> TreebankWordTokenizer()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> []</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        word <span class="op">=</span> <span class="vs">r"\p</span><span class="sc">{letter}</span><span class="vs">"</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sent <span class="kw">in</span> nltk.sent_tokenize(text):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>            tokens <span class="op">=</span> tokenizer.tokenize(sent)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            tokens <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> tokens <span class="cf">if</span> regex.search(word, t)]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            result <span class="op">+=</span> tokens</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> result</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>mytokenizer <span class="op">=</span> MyTokenizer()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mytokenizer.tokenize(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['I', 'have', "n't", 'seen', 'John', "'s", 'derring-do', 'Second', 'sentence']</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-haiku" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.3 </strong></span>Tokenization of Japanese verse.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="cell" data-hash="chapter10_cache/html/haiku-python_db87bd312343e65e5cb8cfee9e523697">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this snippet uses the tokenizer created above</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (example "Tokenization with Python")</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>haiku <span class="op">=</span> (<span class="st">"</span><span class="ch">\u53e4\u6c60\u86d9</span><span class="st">"</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>         <span class="st">"</span><span class="ch">\u98db\u3073\u8fbc\u3080</span><span class="st">"</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>         <span class="st">"</span><span class="ch">\u6c34\u306e\u97f3</span><span class="st">"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Default: </span><span class="sc">{</span>mytokenizer<span class="sc">.</span>tokenize(haiku)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Default: ['古池蛙飛び込む水の音']</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Nagisa: </span><span class="sc">{</span>nagisa<span class="sc">.</span>tagging(haiku)<span class="sc">.</span>words<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Nagisa: ['古', '池蛙', '飛び込む', '水', 'の', '音']</code></pre>
</div>
</div>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<div class="cell" data-hash="chapter10_cache/html/haiku-r_7319e67db8344ce0ba97384837360aab">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>haiku <span class="ot">=</span> <span class="st">"\u53e4\u6c60\u86d9</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="st">         \u98db\u3073\u8fbc\u3080</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="st">         \u6c34\u306e\u97f3"</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">tokens</span>(haiku)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document.
text1 :
[1] "古池"     "蛙"       "飛び込む" "水"       "の"       "音"      </code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="panel-tabset">

</div>
</div>
</div>
</div>
</div>
<p>Note that for languages such as Chinese, Japanese, and Korean, which do not use spaces to delimit words, the story is more difficult. Although a full treatment is beyond the scope of this book, Example <a href="#exm-haiku"><span>10.3</span></a> shows a small example of tokenizing Japanese text, in this case the famous haiku “the sound of water” by Bashō. The default tokenizer in quanteda actually does a good job, in contrast to the default Python tokenizer that simply keeps the whole string as one word (which makes sense since this tokenizer only looks for whitespace or punctuation). For Python the best bet is to use a custom package for tokenizing Japanese, such as the <em>nagisa</em> package. This package contains a tokenizer which is able to tokenize the Japanese text, and we could use this in the <code>CountVectorizer</code> much like we used the <code>TreebankWordTokenizer</code> for English earlier. Similarly, with heavily inflected languages such as Hungarian or Arabic, it might be better to use preprocessing tools developed specifically for these languages, but treating those is beyond the scope of this book.</p>
</section>
<section id="sec-dtm" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="sec-dtm"><span class="header-section-number">10.1.2</span> The DTM as a Sparse Matrix</h3>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-sotu" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.4 </strong></span>Example document-term matrix</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-7-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-1" role="tab" aria-controls="tabset-7-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-7-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-2" role="tab" aria-controls="tabset-7-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-7-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-7-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this snippet uses the tokenizer created above</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (example "Tokenization with Python")</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>sotu <span class="op">=</span> pd.read_csv(<span class="st">"https://cssbook.net/d/sotu.csv"</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(tokenizer<span class="op">=</span>mytokenizer.tokenize)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> cv.fit_transform(sotu[<span class="st">"text"</span>])</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>d</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;85x17185 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 132900 stored elements in Compressed Sparse Row format&gt;</code></pre>
</div>
</div>
</div>
<div id="tabset-7-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-7-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>url <span class="ot">=</span> <span class="st">"https://cssbook.net/d/sotu.csv"</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>sotu <span class="ot">=</span> <span class="fu">read_csv</span>(url) <span class="sc">%&gt;%</span> </span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>       <span class="fu">mutate</span>(<span class="at">doc_id=</span><span class="fu">paste</span>(lubridate<span class="sc">::</span><span class="fu">year</span>(Date), </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>                           President, delivery))</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">corpus</span>(sotu) <span class="sc">%&gt;%</span> <span class="fu">tokens</span>() <span class="sc">%&gt;%</span> <span class="fu">dfm</span>()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>d</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 85 documents, 18,165 features (91.07% sparse) and 6 docvars.
                        features
docs                      to  the congress  :  in considering state   of union
  1945 Roosevelt written 247  642       14  6 236           1     5  376     2
  1945 Roosevelt spoken  110  238        8  3  90           0     1  137     0
  1946 Truman written    738 2141       74 17 669           4    24 1264     8
  1947 Truman spoken     227  473       27  7 132           1     5  292     7
  1948 Truman spoken     175  325       15  2  98           0     7  252     5
  1949 Truman spoken     139  239       17  2  69           1     1  150     1
                        features
docs                        ,
  1945 Roosevelt written  351
  1945 Roosevelt spoken   139
  1946 Truman written    1042
  1947 Truman spoken      236
  1948 Truman spoken      155
  1949 Truman spoken      148
[ reached max_ndoc ... 79 more documents, reached max_nfeat ... 18,155 more features ]</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Example <a href="#exm-sotu"><span>10.4</span></a> shows a more realistic example. It downloads all US “State of the Union” speeches and creates a document-term matrix from them. Since the matrix is now easily too large to print, both Python and R simply list the size of the matrix. R lists <span class="math inline">\(85\)</span> documents (rows) and <span class="math inline">\(17999\)</span> features (columns), and Python reports that its size is <span class="math inline">\(85\times17185\)</span>. Note the difference in the number of columns (unique terms) due to the differences in tokenization as discussed above.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-freq" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.5 </strong></span>A look inside the DTM.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-8-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-1" role="tab" aria-controls="tabset-8-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-8-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-2" role="tab" aria-controls="tabset-8-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-8-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-8-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> termstats(dfm, vectorizer):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Helper function to calculate term and</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co">    document frequency per term"""</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Frequencies are the column sums of the DFM</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    frequencies <span class="op">=</span> dfm.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>).tolist()[<span class="dv">0</span>]</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Document frequencies are the binned count</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># of the column indices of DFM entries</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    docfreqs <span class="op">=</span> np.bincount(dfm.indices)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    freq_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">dict</span>(frequency<span class="op">=</span>frequencies, docfreq<span class="op">=</span>docfreqs),</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        index<span class="op">=</span>vectorizer.get_feature_names(),</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> freq_df.sort_values(<span class="st">"frequency"</span>, ascending<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="chapter10_cache/html/freq-python2_c69dfaafa4824c7e4cf2770291949f0c">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>termstats(d, cv).iloc[[<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            frequency  docfreq
the             34996       85
is               5472       85
energy            707       68
scientific         73       28
escalate            2        2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"the"</span>, <span class="st">"is"</span>, <span class="st">"energy"</span>, <span class="st">"scientific"</span>, <span class="st">"escalate"</span>]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> [cv.vocabulary_[x] <span class="cf">for</span> x <span class="kw">in</span> words]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>d[[[<span class="dv">0</span>], [<span class="dv">25</span>], [<span class="dv">50</span>], [<span class="dv">75</span>]], indices].todense()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>matrix([[642,  78,   0,   0,   0],
        [355,  66,   1,   0,   0],
        [182,  45,   2,   0,   0],
        [326,  59,  15,   1,   0]])</code></pre>
</div>
</div>
</div>
<div id="tabset-8-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-8-2-tab">
<div class="cell" data-hash="chapter10_cache/html/freq-r_05d0e59bf48df8f3c7f178737a9305a2">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">textstat_frequency</span>(d)[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>), ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                feature frequency rank docfreq group
1                   the     34999    1      85   all
10                  our      9334   10      85   all
100               first       750  100      83   all
1000        investments        76  988      34   all
10000 service-connected         2 8696       2   all</code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>(d[</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">75</span>),</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">"the"</span>,<span class="st">"first"</span>,<span class="st">"investment"</span>,<span class="st">"defrauded"</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                     features
docs                   the first investment defrauded
  1946 Truman written 2141    21          9         0
  1965 Johnson spoken  283    14          0         0
  1984 Reagan spoken   209     8          1         0
  2009 Obama spoken    269     8          4         0</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>In Example <a href="#exm-freq"><span>10.5</span></a> we show how you can look at the content of the DTM. First, we show the overall term and document frequencies of each word, where we showcase words at different frequencies. Unsurprisingly, the word <em>the</em> tops both charts, but further down there are minor differences. In all cases, the highly frequent words are mostly functional words like <em>them</em> or <em>first</em>. More informative words such as <em>investments</em> are by their nature used much less often. Such term statistics are very useful to check for noise in the data and get a feeling of the kind of language that is used. Second, we take a look at the frequency of these same words in four speeches from Truman to Obama. All use words like <em>the</em> and <em>first</em>, but none of them talk about <em>defrauded</em> – which is not surprising, since it was only used once in all the speeches in the corpus.</p>
<p>However, the words that ranked around 1000 in the top frequency are still used in less than half of the documents. Since there are about 17000 even less frequent words in the corpus, you can imagine that most of the document-term matrix consists of zeros. The output also noted this <em>sparsity</em> in the first output above. In fact, R reports that the dtm is <span class="math inline">\(91\%\)</span> sparse, meaning 91% percent of all entries are zero. Python reports a similar figure, namely that there are only just under 150000 non-zero entries out of a possible <span class="math inline">\(8\times22219\)</span>, which boils down to a 92% sparse matrix.</p>
<p>Note that to display the matrix we turned it from a <em>sparse matrix</em> representation into a <em>dense matrix</em>. Briefly put, in a dense matrix, all entries are stored as a long list of numbers, including all the zeros. In a sparse matrix, only the non-zero entries and their location are stored. This conversion (using the function <code>as.matrix</code> and the method <code>todense</code> respectively), however, was only performed after selecting a small subset of the data. In general, it is very inefficient to store and work with the matrix in a <code>dense</code> format. For a reasonably large corpus with tens of thousands of documents and different words, this can quickly run to billions of numbers, which can cause problems even on modern computers and is, moreover, very inefficient. Because sparsity values are often higher than 99%, using a sparse matrix representation can easily reduce storage requirements by a hundred times, and in the process speed up calculations by reducing the number of entries that need to be inspected. Both <em>quanteda</em> and <code>scikit-learn</code>store DTMs as sparse matrices by default, and most analysis tools are able to deal with sparse matrices very efficiently (see, however, Section <a href="chapter11.html#sec-workflow"><span>11.4.1</span></a> for problems with machine learning on sparse matrices in R).</p>
<p>A final note on the difference between Python and R in this example. The code in R is much simpler and produces nicer results since it also shows the words and the speech names. In Python, we wrote our own helper function to create the frequency statistics which is built into the R <em>quanteda</em> package. These differences between Python and R reflect a pattern that is true in many (but not all) cases: in Python libraries such as <code>numpy</code>and <code>scikit-learn</code>are setup to maximize performance, while in R a library such as <em>quanteda</em> or <em>tidyverse</em> is more geared towards ease of use. For that reason, the DTM in Python does not “remember” the actual words, it uses the index of each word, so it consumes less memory if you don’t need to use the actual words in e.g.&nbsp;a machine learning setup. R, on the other hand, stores the words and also the document IDs and metadata in the DFM object. This is easier to use if you need to look up a word or document, but it consumes (slightly) more memory.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Python: Why fit_transform?</strong> In Python, you don’t have a function that directly transforms text into a DTM. Instead, you create an <em>transformer</em> called a CountVectorizer, which can then be used to “vectorize” texts (turn it into a row of numbers) by counting how often each word occurs. This uses the <code>fit_transform</code> function which is offered by all <code>scikit-learn</code>transformers. It “fits” the model on the training data, which in this case means learning the vocabulary. It can then be used to transform other data into a DTM with the exact same columns, which is often required for algorithms. Because the feature names (the words themselves) are stored in the CountVectorizer rather than the document-term matrix, you generally need to keep both objects.</p>
</div>
</div>
</div>
</section>
<section id="sec-bagofwords" class="level3" data-number="10.1.3">
<h3 data-number="10.1.3" class="anchored" data-anchor-id="sec-bagofwords"><span class="header-section-number">10.1.3</span> The DTM as a “Bag of Words”</h3>
<p>As you can see already in these simple examples, the document-term matrix discards quite a lot of information from text. Specifically, it disregards the order or words in a text: “John fired Mary” and “Mary fired John” both result in the same DTM, even though the meaning of the sentences is quite different. For this reason, a DTM is often called a <em>bag of words</em>, in the sense that all words in the document are simply put in a big bag without looking at the sentences or context of these words.</p>
<p>Thus, the DTM can be said to be a specific and “lossy” representation of the text, that turns out to be quite useful for certain tasks: the frequent occurrence of words like “employment”, “great”, or “I” might well be good indicators that a text is about the economy, is positive, or contains personal expressions respectively. As we will see in the next chapter, the DTM representation can be used for many different text analyses, from dictionaries to supervised and unsupervised machine learning.</p>
<p>Sometimes, however, you need information that is encoded in the order of words. For example, in analyzing conflict coverage it might be quite important to know who attacks whom, not just that an attack took place. In the Section <a href="#sec-ngram"><span>10.3</span></a> we will look at some ways to create a richer matrix-representation by using word pairs. Although it is beyond the scope of this book, you can also use automatic syntactic analysis to take grammatical relations into account as well. As is always the case with automatic analyses, it is important to understand what information the computer is looking at, as the computer cannot find patterns in information that it doesn’t have.</p>
</section>
<section id="sec-wordcloud" class="level3" data-number="10.1.4">
<h3 data-number="10.1.4" class="anchored" data-anchor-id="sec-wordcloud"><span class="header-section-number">10.1.4</span> The (Unavoidable) Word Cloud</h3>
<p>One of the most famous text visualizations is without doubt the word cloud. Essentially, a word cloud is an image where each word is displayed in a size that is representative of its frequency. Depending on preference, word position and color can be random, depending on word frequency, or in a decorative shape.</p>
<p>Word clouds are often criticized since they are (sometimes) pretty but mostly not very informative. The core reason for that is that only a single aspect of the words is visualized (frequency), and simple word frequency is often not that informative: the most frequent words are generally uninformative “stop words” like “the” and “I”.</p>
<p>For example, Example <a href="#exm-wordcloud"><span>10.6</span></a> shows the word cloud for the state of the union speeches downloaded above. In R, this is done using the <em>quanteda</em> function <code>textplot_wordcloud</code>. In Python we need to work a little harder, since it only has the counts, not the actual words. So, we sum the DTM columns to get the frequency of each word, and combine that with the feature names (words) from the <code>CountVectorized</code> object <code>cv</code>. Then we can create the word cloud and give it the frequencies to use. Finally, we plot the cloud and remove the axes.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-wordcloud" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.6 </strong></span>Word cloud of the US State of the Union corpus</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-9-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-1" role="tab" aria-controls="tabset-9-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-9-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-2" role="tab" aria-controls="tabset-9-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-9-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-9-1-tab">
<div class="cell" data-hash="chapter10_cache/html/wordcloud-python_e6b9d61c2d091477875a3690a7108a50">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wordcloud(dfm, vectorizer, <span class="op">**</span>options):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    freq_dict <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">zip</span>(vectorizer.get_feature_names(), dfm.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>).tolist()[<span class="dv">0</span>])</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    wc <span class="op">=</span> WordCloud(<span class="op">**</span>options)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> wc.generate_from_frequencies(freq_dict)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>wc <span class="op">=</span> wordcloud(d, cv, background_color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>plt.imshow(wc)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-9-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-9-2-tab">
<div class="cell" data-hash="chapter10_cache/html/wordcloud-r_af36ca6637b20a99d70c0014a6d4f366">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">textplot_wordcloud</span>(d, <span class="at">max_words=</span><span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter10_files/figure-html/wordcloud-r-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The results from Python and R look different at first – for one thing, R is nice and round but Python has more colors! However, if you look at the cloud you can see both are not very meaningful: the largest words are all punctuation or words like “a”, “and”, or “the”. You have to look closely to find words like “federal” or “security” that give a hint on what the texts were actually about.</p>
</section>
</section>
<section id="sec-dtmselect" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="sec-dtmselect"><span class="header-section-number">10.2</span> Weighting and Selecting Documents and Terms</h2>
<p>So far, the DTMs you made in this chapter simply show the count of each word in each document. Many words, however, are not informative for many questions. This is especially apparent if you look at a <em>word cloud</em>, essentially a plot of the most frequent words in a <em>corpus</em> (set of documents).</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Vectors and a geometric interpretation of document-term matrices
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We said that a document is represented by a “vector” of numbers, where each number (for a document-term matrix) is the frequency of a specific word in that document. This term is also seen in the name for the tokenizer <code>scikit-learn</code>: a <em>vectorizer</em> or function to turn texts into vectors.</p>
<p>The term <em>vector</em> here can be read as just a fancy word for a group of numbers. In this meaning, the term is also often used in R, where a column of a data frame is called a vector, and where functions that can be called on a whole vector at once are called <em>vectorized</em>.</p>
<p>More generally, however, a vector in geometry is a point (or line from the origin) in an <span class="math inline">\(n\)</span>-dimensional space, where <span class="math inline">\(n\)</span> is the length or dimensionality of the vector. This is also a very useful interpretation for vectors in text analysis: the dimensionality of the space is the number of unique words (columns) in the document-term matrix, and each document is a point in that <span class="math inline">\(n\)</span>-dimensional space.</p>
<p>In that interpretation, various geometric distances between documents can be calculated as an indicator for how similar two documents are. Techniques that reduce the number of columns in the matrix (such as clustering or topic modeling) can then be seen as dimensionality reduction techniques since they turn the DTM into a matrix with lower dimensionality (while hopefully retaining as much of the relevant information as possible).</p>
</div>
</div>
</div>
<p>More formally, a document-term matrix can be seen as a representation of data points about documents: each document (row) is represented as a vector containing the count per word (column). Although it is a simplification compared to the original text, an unfiltered document-term matrix contains a lot of relevant information. For example, if a president uses the word “terrorism” more often than the word “economy”, that could be an indication of their policy priorities.</p>
<p>However, there is also a lot of <em>noise</em> crowding out this <em>signal</em>: as seen in the word cloud in the previous section the most frequent words are generally quite uninformative. The same holds for words that hardly occur in any document (but still require a column to be represented) and noisy “words” such as punctuation or technical artifacts like HTML code.</p>
<p>This section will discuss a number of techniques for cleaning a corpus or document-term matrix in order to minimize the amount of noise: removing stop words, cleaning punctuation and other artifacts, and trimming and weighting. As a running example in this section, we will use a collection of tweets from US president Donald Trump. Example <a href="#exm-trumptweets"><span>10.7</span></a> shows how to load these tweets into a data frame containing the ID and text of the tweets. As you can see, this dataset contains a lot of non-textual features such as hyperlinks and hash tags as well as regular punctuation and stop words. Before we can start analyzing this data, we need to decide on and perform multiple cleaning steps such as detailed below.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-trumptweets" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.7 </strong></span>Top words used in Trump Tweets</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-10-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-1" role="tab" aria-controls="tabset-10-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-10-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-2" role="tab" aria-controls="tabset-10-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-10-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-10-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://cssbook.net/d/trumptweets.csv"</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>tweets <span class="op">=</span> pd.read_csv(url, usecols<span class="op">=</span>[<span class="st">"status_id"</span>, <span class="st">"text"</span>], index_col<span class="op">=</span><span class="st">"status_id"</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>tweets.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                text
status_id                                                           
x1864367186        Read a great interview with Donald Trump that ...
x9273573134835712  Congratulations to Evan Lysacek for being nomi...
x29014512646       I was on The View this morning. We talked abou...
x7483813542232064  Tomorrow night's episode of The Apprentice del...
x5775731054        Donald Trump Partners with TV1 on New Reality ...</code></pre>
</div>
</div>
</div>
<div id="tabset-10-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-10-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>url <span class="ot">=</span> <span class="st">"https://cssbook.net/d/trumptweets.csv"</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>tweets <span class="ot">=</span> <span class="fu">read_csv</span>(url, </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">col_types=</span><span class="fu">cols_only</span>(<span class="at">text=</span><span class="st">"c"</span>, <span class="at">status_id=</span><span class="st">"c"</span>)) </span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tweets)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 2
  status_id          text                                                       
  &lt;chr&gt;              &lt;chr&gt;                                                      
1 x1864367186        Read a great interview with Donald Trump that appeared in …
2 x9273573134835712  Congratulations to Evan Lysacek for being nominated SI spo…
3 x29014512646       I was on The View this morning. We talked about The Appren…
4 x7483813542232064  Tomorrow night's episode of The Apprentice delivers excite…
5 x5775731054        Donald Trump Partners with TV1 on New Reality Series Entit…
6 x14785576859340800 I'll be appearing on Larry King Live for his final show, T…</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Please note that although tweets are perhaps overused as a source of scientific information, we use them here because they nicely exemplify issues around non-textual elements such as hyperlinks. See Chapter <a href="chapter12.html"><span>12</span></a> for information on how to use the Twitter and other APIs to collect your own data.</p>
<section id="sec-stopwords" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="sec-stopwords"><span class="header-section-number">10.2.1</span> Removing stopwords</h3>
<p>A first step in cleaning a DTM is often <em>stop word removal</em>. Words such as “a” and “the” are often called stop words, i.e.&nbsp;words that do not tell us much about the content. Both <em>quanteda</em> and <code>scikit-learn</code>include built-in lists of stop words, making it very easy to remove the most common words. Example <a href="#exm-stopwords"><span>10.8</span></a> shows the result of specifying “English” stop words to be removed for both packages.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-stopwords" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.8 </strong></span>Simple stop word removal</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-11-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-1" role="tab" aria-controls="tabset-11-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-11-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-2" role="tab" aria-controls="tabset-11-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-11-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-11-1-tab">
<div class="cell" data-hash="chapter10_cache/html/stopwords-python_538e6024bb0fecc635349e1cf91839f7">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    stop_words<span class="op">=</span>stopwords.words(<span class="st">"english"</span>), tokenizer<span class="op">=</span>mytokenizer.tokenize</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> cv.fit_transform(tweets.text)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>wc <span class="op">=</span> wordcloud(d, cv, background_color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(wc)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-11-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-11-2-tab">
<div class="cell" data-hash="chapter10_cache/html/stopwords-r_ed985cd1de6023a3cda20f0514c73904">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">corpus</span>(tweets) <span class="sc">%&gt;%</span> </span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>(<span class="at">remove_punct=</span>T) <span class="sc">%&gt;%</span> </span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>() <span class="sc">%&gt;%</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_remove</span>(<span class="fu">stopwords</span>(<span class="st">"english"</span>))</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">textplot_wordcloud</span>(d, <span class="at">max_words=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter10_files/figure-html/stopwords-r-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Note, however, that it might seem easy to list words like “a” and “and”, but as it turns out there is no single well-defined list of stop words, and (as always) the best choice depends on your data and your research question.</p>
<p>Linguistically, stop words are generally function words or closed word classes such as determiner or pronoun, with closed classes meaning that while you can coin new nouns, you can’t simply invent new determiners or prepositions. However, there are many different stop word lists around which make different choices and are compatible with different kinds of preprocessing. The Python word cloud in Example <a href="#exm-stopwords"><span>10.8</span></a> shows a nice example of the importance of matching stopwords with the used tokenization: a central “word” in the cloud is the contraction <em>’s</em>. We are using the NLTK tokenizer, which splits <em>’s</em> from the word it was attached to, but the <code>scikit-learn</code>stop word list does not include that term. So, it is important to make sure that the words created by the tokenization match the way that words appear in the stop word list.</p>
<p>As an example of the substantive choices inherent in using a stop word lists, consider the word “will”. As an auxiliary verb, this is probably indeed a stop word: for most substantive questions, there is no difference whether you will do something or simply do it. However, “will” can also be a noun (a testament) and a name (e.g.&nbsp;Will Smith). Simply dropping such words from the corpus can be problematic; see Section <a href="#sec-nlp"><span>10.3.4</span></a> for ways of telling nouns and verbs apart for more fine-grained filtering.</p>
<p>Moreover, some research questions might actually be interested in certain stop words. If you are interested in references to the future or specific modalities, the word might actually be a key indicator. Similarly, if you are studying self-expression on Internet forums, social identity theory, or populist rhetoric, words like “I”, “us” and “them” can actually be very informative.</p>
<p>For this reason, it is always a good idea to understand and inspect what stop word list you are using, and use a different one or customize it as needed <span class="citation" data-cites="nothman18">(see also <a href="references.html#ref-nothman18" role="doc-biblioref">Nothman, Qin, and Yurchak 2018</a>)</span>. Example <a href="#exm-stopwords2"><span>10.9</span></a> shows how you can inspect and customize stop word lists. For more details on which lists are available and what choices these lists make, see the package documentation for the <em>stopwords</em> package in Python (part of NLTK) and R (part of quanteda)</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-stopwords2" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.9 </strong></span>Inspecting and Customizing stop word lists</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-12-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-12-1" role="tab" aria-controls="tabset-12-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-12-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-12-2" role="tab" aria-controls="tabset-12-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-12-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-12-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>mystopwords <span class="op">=</span> [<span class="st">"go"</span>, <span class="st">"to"</span>] <span class="op">+</span> stopwords.words(<span class="st">"english"</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(mystopwords)<span class="sc">}</span><span class="ss"> stopwords:"</span> <span class="ss">f"</span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(mystopwords[:<span class="dv">5</span>])<span class="sc">}</span><span class="ss">..."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>181 stopwords:go, to, i, me, my...</code></pre>
</div>
</div>
</div>
<div id="tabset-12-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-12-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>mystopwords <span class="ot">=</span> <span class="fu">stopwords</span>(<span class="st">"english"</span>, </span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">source=</span><span class="st">"snowball"</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>mystopwords <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"go"</span>, <span class="st">"one"</span>, mystopwords)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">"Now {length(mystopwords)} stopwords:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Now 177 stopwords:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>mystopwords[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "go"  "one" "i"   "me"  "my" </code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-punctuation" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="sec-punctuation"><span class="header-section-number">10.2.2</span> Removing Punctuation and Noise</h3>
<p>Next to stop words, text often contains punctuation and other things that can be considered “noise” for most research questions. For example, it could contain emoticons or emoji, Twitter hashtags or at-mentions, or HTML tags or other annotations.</p>
<p>In both Python and R, we can use regular expressions to remove (parts of) words. As explained above in Section <a href="chapter09.html#sec-regular"><span>9.2</span></a>, regular expressions are a powerful way to specify (sequences of) characters which are to be kept or removed. You can use this, for example, to remove things like punctuation, emoji, or HTML tags. This can be done either before or after tokenizing (splitting the text into words): in other words, we can clean the raw texts or the individual words (tokens).</p>
<p>In general, if you only want to keep or remove certain words, it is often easiest to do so after tokenization using a regular expression to select the words to keep or remove. If you want to remove parts of words (e.g.&nbsp;to remove the leading “#” in hashtags) it is easiest to do that before tokenization, that is, as a preprocessing step before the tokenization. Similarly, if you want to remove a term that would be split by the tokenization (such as hyperlinks), if can be better to remove them before the tokenization occurs.</p>
<p>Example <a href="#exm-noise"><span>10.10</span></a> shows how we can use regular expressions to remove noise in Python and R. For clarity, it shows the result of each step, it shows the result of each processing step on a single tweet that exemplifies many of the problems described above. To better understand the tokenization process, we print the tokens in that tweet separated by a vertical bar (<code>|</code>). As a first cleaning step, we will use a regular expression to remove hyperlinks and HTML entities like <code>&amp;amp;</code> from the untokenized texts. Since both hyperlinks and HTML entities are split over multiple tokens, it would be hard to remove them after tokenization.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-noise" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.10 </strong></span>Cleaning a single tweet at the text and token level</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-13-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-13-1" role="tab" aria-controls="tabset-13-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-13-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-13-2" role="tab" aria-controls="tabset-13-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-13-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-13-1-tab">
<div class="cell" data-hash="chapter10_cache/html/noise1-python_ba06998d2344958870e7212aff32bf84">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="bu">id</span> <span class="op">=</span> <span class="st">"x263687274812813312"</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>one_tweet <span class="op">=</span> tweets.text.values[tweets.index <span class="op">==</span> <span class="bu">id</span>][<span class="dv">0</span>]</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Raw:</span><span class="ch">\n</span><span class="sc">{</span>one_tweet<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Raw:
Part 1 of my @jimmyfallon interview discussing my $5M offer to Obama, ...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>tweet_tokens <span class="op">=</span> mytokenizer.tokenize(one_tweet)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Tokenized:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Tokenized:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" | "</span>.join(tweet_tokens))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Part | of | my | jimmyfallon | interview | discussing | my | 5M | offer | t...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>one_tweet <span class="op">=</span> re.sub(<span class="vs">r"\bhttps?://\S*|&amp;\w+;"</span>, <span class="st">""</span>, one_tweet)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>tweet_tokens <span class="op">=</span> mytokenizer.tokenize(one_tweet)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After pre-processing:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After pre-processing:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" | "</span>.join(tweet_tokens))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Part | of | my | jimmyfallon | interview | discussing | my | 5M | offer | t...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>tweet_tokens <span class="op">=</span> [</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    t.lower()</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> tweet_tokens</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> (</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>        t.lower() <span class="kw">in</span> stopwords.words(<span class="st">"english"</span>) <span class="kw">or</span> regex.match(<span class="vs">r"\P</span><span class="sc">{LETTER}</span><span class="vs">"</span>, t)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"After pruning tokens:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>After pruning tokens:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" | "</span>.join(tweet_tokens))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>part | jimmyfallon | interview | discussing | offer | obama | trump | tower...</code></pre>
</div>
</div>
</div>
<div id="tabset-13-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-13-2-tab">
<div class="cell" data-hash="chapter10_cache/html/noise1-r_bf0fc03b32a78687fe0d90414d421b86">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>id<span class="ot">=</span><span class="st">"x263687274812813312"</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>single_tweet <span class="ot">=</span> tweets<span class="sc">$</span>text[tweets<span class="sc">$</span>status_id <span class="sc">==</span> id]</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(single_tweet)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Part 1 of my @jimmyfallon interview discussing my $5M offer to Obama, ..."</code></pre>
</div>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>tweet_tokens <span class="ot">=</span> <span class="fu">tokens</span>(single_tweet)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"After tokenizing:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "After tokenizing:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(tweet_tokens, <span class="at">collapse=</span><span class="st">" | "</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Part | 1 | of | my | @jimmyfallon | interview | discussing | my | $ | ..."</code></pre>
</div>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>single_tweet <span class="ot">=</span> single_tweet  <span class="sc">%&gt;%</span> </span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">str_remove_all</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">bhttps?://</span><span class="sc">\\</span><span class="st">S*|&amp;</span><span class="sc">\\</span><span class="st">w+;"</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>tweet_tokens <span class="ot">=</span> <span class="fu">tokens</span>(single_tweet)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"After pre-processing:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "After pre-processing:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(tweet_tokens, <span class="at">collapse=</span><span class="st">" | "</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Part | 1 | of | my | @jimmyfallon | interview | discussing | my | $ | ..."</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>tweet_tokens <span class="ot">=</span> tweet_tokens <span class="sc">%&gt;%</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_tolower</span>()  <span class="sc">%&gt;%</span> </span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_remove</span>(<span class="fu">stopwords</span>(<span class="st">"english"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_keep</span>(<span class="st">"^</span><span class="sc">\\</span><span class="st">p{LETTER}"</span>, <span class="at">valuetype=</span><span class="st">"regex"</span>)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"After pruning:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "After pruning:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(tweet_tokens, <span class="at">collapse=</span><span class="st">" | "</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "part | interview | discussing | offer | obama | tower | atrium | tweet..."</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Regular expressions are explained fully in Section <a href="chapter09.html#sec-regular"><span>9.2</span></a>, so we will keep the explanation short: the bar <code>|</code> splits the pattern in two parts, i.e.&nbsp;it will match if it finds either of the subpatterns. The first pattern looks for the literal text <code>http</code>, followed by an optional <code>s</code> and the sequence <code>://</code>. Then, it takes all non-whitespace characters it finds, i.e.&nbsp;the pattern ends at the next whitespace or end of the text. The second pattern looks for an ampersand (<code>&amp;</code>) followed by one or more letters (<code>\\w+</code>), followed by a semicolon (<code>;</code>). This matches HTML escapes like <code>&amp;amp;</code> for an ampersand.</p>
<p>In the next step, we process the tokenized text to remove every token that is either a stopword or does not start with a letter. In Python, this is done by using a list comprehension (<code>[process(item) for item in list]</code>) for tokenizing each document; and a nested list comprehension for filtering each token in each document. In R this is not needed as the <code>tokens_\*</code> functions are <em>vectorized</em>, that is, they directly run over all the tokens.</p>
<p>Comparing R and Python, we see that the different tokenization functions mean that <code>#trump</code> is removed in R (since it is a token that does not start with a letter), but in Python the tokenization splits the <code>#</code> from the name and the resulting token <code>trump</code> is kept. If we would have used a different tokenizer for Python (e.g.&nbsp;the <code>WhitespaceTokenizer</code>) this would have been different again. This underscores the importance of inspecting and understanding the results of the specific tokenizer used, and to make sure that subsequent steps match these tokenization choices. Concretely, with the <code>TreebankWordtokenizer</code> we would have had to also remove hashtags at the text level rather than the token level.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-tagcloud" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.11 </strong></span>Cleaning the whole corpus and making a tag cloud</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-14-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-14-1" role="tab" aria-controls="tabset-14-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-14-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-14-2" role="tab" aria-controls="tabset-14-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-14-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-14-1-tab">
<div class="cell" data-hash="chapter10_cache/html/tagcloud-python_4b82046d065f798ce9bc06a93b175417">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> do_nothing(x):</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>tokenized <span class="op">=</span> [</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    WhitespaceTokenizer().tokenize(text) <span class="cf">for</span> text <span class="kw">in</span> tweets.text.values</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    [t.lower() <span class="cf">for</span> t <span class="kw">in</span> tokens <span class="cf">if</span> regex.match(<span class="st">"#"</span>, t)] <span class="cf">for</span> tokens <span class="kw">in</span> tokenized</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(tokenizer<span class="op">=</span>do_nothing, lowercase<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>dtm_emoji <span class="op">=</span> cv.fit_transform(tokens)</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>wc <span class="op">=</span> wordcloud(dtm_emoji, cv, background_color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>plt.imshow(wc)</span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-14-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-14-2-tab">
<div class="cell" data-hash="chapter10_cache/html/tagcloud-r_91faee03f18ba51b1d9b2f1c22ff5fc2">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>dfm_cleaned <span class="ot">=</span> tweets <span class="sc">%&gt;%</span> </span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">corpus</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>()  <span class="sc">%&gt;%</span> </span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_keep</span>(<span class="st">"^#"</span>, <span class="at">valuetype=</span><span class="st">"regex"</span>)  <span class="sc">%&gt;%</span> </span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>()</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>colors <span class="ot">=</span> RColorBrewer<span class="sc">::</span><span class="fu">brewer.pal</span>(<span class="dv">8</span>, <span class="st">"Dark2"</span>)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a><span class="fu">textplot_wordcloud</span>(dfm_cleaned, <span class="at">max_words=</span><span class="dv">100</span>, </span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_size =</span> <span class="dv">1</span>, <span class="at">max_size=</span><span class="dv">4</span>, <span class="at">random_order=</span><span class="cn">TRUE</span>,</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">random_color=</span> <span class="cn">TRUE</span>, <span class="at">color=</span>colors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter10_files/figure-html/tagcloud-r-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>As a final example, Example <a href="#exm-tagcloud"><span>10.11</span></a> shows how to filter tokens for the whole corpus, but rather than removing hashtags it keeps only the hashtags to produce a tag cloud. In R, this is mostly a pipeline of <em>quanteda</em> functions to create the corpus, tokenize, keep only hashtags, and create a DFM. To spice up the output we use the <em>RColorBrewer</em> package to set random colors for the tags. In Python, you can see that we now have a nested list comprehension, where the outer loop iterates over the texts and the inner loop iterates over the tokens in each text. Next, we make a <code>do_nothing</code> function for the vectorizer since the results are already tokenized. Note that we need to disable lowercasing as otherwise it will try to call <code>.lower()</code> on the token lists.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Lambda functions in Python.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Sometimes, we need to define a function that is very simple and that we need only once. An example for such a throwaway function is <code>do_nothing</code> in Example <a href="#exm-tagcloud"><span>10.11</span></a>. Instead of defining a reusable function with the <code>def</code> keyword and then to call it by its name when we need it later, we can therefore also directly define an unnamed function when we need it with the <code>lambda</code> keyword. The syntax is simple: <code>lambda argument: returnvalue</code>. A function that maps a value onto itself can therefore be written as <code>lambda x: x</code>. In Example <a href="#exm-tagcloud"><span>10.11</span></a>, instead of defining a named function, we could therefore also simply write <code>v = CountVectorizer(tokenizer=lambda x: x, lowercase=False)</code>. The advantages are that it saves you two lines of code here and you don’t clutter your environment with functions you do not intend to re-use anyway. The disadvantage is that it may be less clear what is happening, at least for people not familiar with lambda functions.</p>
</div>
</div>
</div>
</section>
<section id="sec-trimdtm" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="sec-trimdtm"><span class="header-section-number">10.2.3</span> Trimming a DTM</h3>
<p>The techniques above both drop terms from the DTM based on specific choices or patterns. It can also be beneficial to trim a DTM by removing words that occur very infrequently or overly frequently. For the former, the reason is that if a word only occurs in a very small percentage of documents it is unlikely to be very informative. Overly frequent words, for example occurring in more than half or 75% of all documents, function basically like stopwords for this corpus. In many cases, this can be a result of the selection strategy. If we select all tweets containing “Trump”, the word Trump itself is no longer informative about their content. It can also be that some words are used as standard phrases, for example “fellow Americans” in state of the union speeches. If every president in the corpus uses those terms, they are no longer informative about differences between presidents.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-trimming" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.12 </strong></span>Trimming a Document-Term Matrix</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-15-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-15-1" role="tab" aria-controls="tabset-15-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-15-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-15-2" role="tab" aria-controls="tabset-15-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-15-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-15-1-tab">
<div class="cell" data-hash="chapter10_cache/html/trimming-python_9bb5e3b9cc4095b0fdabb709f2cc5d75">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"# of words before trimming: </span><span class="sc">{</span>d<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># of words before trimming: 45912</code></pre>
</div>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>cv_trim <span class="op">=</span> CountVectorizer(</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    stop_words<span class="op">=</span>stopwords.words(<span class="st">"english"</span>),</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>mytokenizer.tokenize,</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    max_df<span class="op">=</span><span class="fl">0.75</span>,</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    min_df<span class="op">=</span><span class="fl">0.005</span>,</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>d_trim <span class="op">=</span> cv_trim.fit_transform(tweets.text)</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  after trimming: </span><span class="sc">{</span>d_trim<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  after trimming: 294</code></pre>
</div>
</div>
</div>
<div id="tabset-15-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-15-2-tab">
<div class="cell" data-hash="chapter10_cache/html/trimming-r_e24e4555fc1beed9285a486b40145800">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">"# of words before trimming: {ncol(d)}"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># of words before trimming: 44386</code></pre>
</div>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>d_trim <span class="ot">=</span> <span class="fu">dfm_trim</span>(d, <span class="at">min_docfreq =</span> <span class="fl">0.005</span>, </span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">max_docfreq =</span> <span class="fl">0.75</span>,</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">docfreq_type =</span> <span class="st">"prop"</span>)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">"# of word after trimming: {ncol(d_trim)}"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># of word after trimming: 301</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Example <a href="#exm-trimming"><span>10.12</span></a> shows how you can use the <em>relative document frequency</em> to trim a DTM in Python and R. We keep only words with a document frequency of between 0.5% and 75%.</p>
<p>Although these are reasonable numbers every choice depends on the corpus and the research question, so it can be a good idea to check which words are dropped.</p>
<p>Note that dropping words that occur almost never should normally not influence the results that much, since those words do not occur anyway. However, trimming a DTM to e.g.&nbsp;at least 1% document frequency often radically reduces the number of words (columns) in the DTM. Since many algorithms have to assign weights or parameters to each word, this can provide a significant improvement in computing speed or memory use.</p>
</section>
<section id="sec-dtmweight" class="level3" data-number="10.2.4">
<h3 data-number="10.2.4" class="anchored" data-anchor-id="sec-dtmweight"><span class="header-section-number">10.2.4</span> Weighting a DTM</h3>
<p>The DTMs created above all use the raw frequencies as cell values. It can also be useful to weight the words so more informative words have a higher weight than less informative ones. A common technique for this is <em>tf<span class="math inline">\(\cdot\)</span>idf</em> weighting. This stands for <em>term frequency <span class="math inline">\(\cdot\)</span> inverse document frequency</em> and weights each occurrence by its raw frequency (term frequency) corrected for how often it occurs in all documents (inverse document frequency). In a formula, the most common implementation of this weight is given as follows:</p>
<p><span class="math inline">\(tf\cdot idf(t,d)=tf(t,d)\cdot idf(t)=f_{t,d}\cdot -\log \frac{n_t}{N}\)</span></p>
<p>Where <span class="math inline">\(f_{t,d}\)</span> is the frequency of term <span class="math inline">\(t\)</span> in document <span class="math inline">\(d\)</span>, <span class="math inline">\(N\)</span> is the total number of documents, and <span class="math inline">\(n_t\)</span> is the number of documents in which term <span class="math inline">\(t\)</span> occurs. In other words, the term frequency is weighted by the negative log of the fraction of documents in which that term occurs. Since <span class="math inline">\(\log(1)\)</span> is zero, terms that occur in every document are disregarded, and in general the less frequent a term is, the higher the weight will be.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-tfidf" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.13 </strong></span>Tf<span class="math inline">\(\cdot\)</span>Idf weighting</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-16-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-16-1" role="tab" aria-controls="tabset-16-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-16-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-16-2" role="tab" aria-controls="tabset-16-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-16-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-16-1-tab">
<div class="cell" data-hash="chapter10_cache/html/tfidf-python_ffde093e64a48dba4dbaa677975aceca">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer(</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>mytokenizer.tokenize, sublinear_tf<span class="op">=</span><span class="va">True</span></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>d_w <span class="op">=</span> tfidf_vectorizer.fit_transform(sotu[<span class="st">"text"</span>])</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> [</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    tfidf_vectorizer.vocabulary_[x]</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> [<span class="st">"the"</span>, <span class="st">"for"</span>, <span class="st">"them"</span>, <span class="st">"submit"</span>, <span class="st">"sizes"</span>]</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>d_w[[[<span class="dv">0</span>], [<span class="dv">25</span>], [<span class="dv">50</span>], [<span class="dv">75</span>]], indices].todense()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>matrix([[0.04443888, 0.03107326, 0.01989122, 0.        , 0.        ],
        [0.05722359, 0.04138723, 0.02782201, 0.01429464, 0.        ],
        [0.0548183 , 0.04440376, 0.02332654, 0.        , 0.        ],
        [0.04866919, 0.03859479, 0.02234111, 0.        , 0.        ]])</code></pre>
</div>
</div>
</div>
<div id="tabset-16-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-16-2-tab">
<div class="cell" data-hash="chapter10_cache/html/tfidf-r_3af9f5b23b00d4f16a9fcc6105879230">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>d_tf <span class="ot">=</span> <span class="fu">corpus</span>(sotu) <span class="sc">%&gt;%</span> </span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_tfidf</span>(<span class="at">scheme_tf=</span><span class="st">"prop"</span>, <span class="at">smoothing=</span><span class="dv">1</span>)</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>(</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>  d_tf[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">75</span>),</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">"the"</span>,<span class="st">"first"</span>,<span class="st">"investment"</span>,<span class="st">"defrauded"</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                     features
docs                         the        first   investment defrauded
  1946 Truman written 0.02084698 0.0002080106 1.103379e-04         0
  1965 Johnson spoken 0.01746801 0.0008790725 0.000000e+00         0
  1984 Reagan spoken  0.01132996 0.0004411759 6.825554e-05         0
  2009 Obama spoken   0.01208793 0.0003657038 2.263162e-04         0</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>tf<span class="math inline">\(\cdot\)</span>idf weighting is a fairly common technique and can improve the results of subsequent analyses such as supervised machine learning. As such, it is no surprise that it is easy to apply this in both Python and R, as shown in Example <a href="#exm-tfidf"><span>10.13</span></a>. This example uses the same data as Example <a href="#exm-sotu"><span>10.4</span></a> above, so you can compare the resulting weighted values with the results reported there. As you can see, the tf<span class="math inline">\(\cdot\)</span>idf weighting in both languages have roughly the same effect: very frequent terms such as <em>the</em> are made less important compared to less frequent words such as <em>submit</em>. For example, in the raw frequencies for the 1965 Johnson speech, <em>the</em> occurred 355 times compared to <em>submit</em> only once. In the weighted matrix, the weight for <em>submit</em> is four times as low as the weight for <em>the</em>.</p>
<p>There are two more things to note if you compare the examples from R and Python. First, to make the two cases somewhat comparable we have to use two options for R, namely to set the term frequency to proportional (<code>scheme_tf='prop'</code>), and to add smoothing to the document frequencies (<code>smooth=1</code>). Without those options, the counts for the first columns would all be zero (since they occur in all documents, and <span class="math inline">\(\log \frac{85}{85}=0\)</span>), and the other counts would be greater than one since they would only be weighted, not normalized.</p>
<p>Even with those options the results are still different (in details if not in proportions), mainly because R normalizes the frequencies before weighting, while Python normalizes after the weighting. Moreover, Python by default uses L2 normalization, meaning that the length of the document vectors will be one, while R uses L1 normalization, that is, the row sums are one (before weighting). Both R and Python have various parameters to control these choices which are explained in their respective help pages. However, although the differences in absolute values look large, the relative effect of making more frequent terms less important is the same, and the specific weighting scheme and options will probably not matter that much for the final results. However, it is always good to be aware of the specific options available and try out which work best for your specific research question.</p>
</section>
</section>
<section id="sec-ngram" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="sec-ngram"><span class="header-section-number">10.3</span> Advanced Representation of Text</h2>
<p>The examples above all created document-term matrices where each column actually represents a word. There is more information in a text, however, than pure word counts. The phrases: <em>the movie was not good, it was in fact quite bad</em> and <em>the movie was not bad, in fact it was quite good</em> have exactly the same word frequencies, but are quite different in meaning. Similarly, <em>the new kings of York</em> and <em>the kings of New York</em> refer to very different people.</p>
<p>Of course, in the end which aspect of the meaning of a text is important depends on your research question: if you want to know the sentiment about the movie, it is important to take a word like “not” into account; but if you are interested in the topic or genre of the review, or the extremity of the language used, this might not be relevant.</p>
<p>The core idea of this section is that in many cases this information can be captured in a DTM by having the columns represent different information than just words, for example word combinations or groups of related words. This is often called <em>feature engineering</em>, as we are using our domain expertise to find the right features (columns, independent variables) to capture the relevant meaning for our research question. If we are using other columns than words it is also technically more correct to use the name <em>document-feature matrix</em>, as <em>quanteda</em> does, but we will stick to the most common name here and simply continue using the name DTM.</p>
<section id="sec-ngrams" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="sec-ngrams"><span class="header-section-number">10.3.1</span> <span class="math inline">\(n\)</span>-grams</h3>
<p>The first feature we will discuss are n-grams. The simplest case is a bigram (or 2-gram), where each feature is a pair of adjacent words. The example used above, <em>the movie was not bad</em>, will yield the following bigrams: <em>the-movie</em>, <em>movie-was</em>, <em>was-not</em>, and <em>not-bad</em>. Each of those bigrams is then treated as a feature, that is, a DTM would contain one column for each word pair.</p>
<p>As you can see in this example, we can now see the difference between <em>not-bad</em> and <em>not-good</em>. The downside of using n-grams is that there are many more unique word pairs than unique words, so the resulting DTM will have many more columns. Moreover, there is a bigger <em>data scarcity problem</em>, as each of those pairs will be less frequent, making it more difficult to find sufficient examples of each to generalize over.</p>
<p>Although bigrams are the most frequent use case, trigrams (3-grams) and (rarely) higher-order n-grams can also be used. As you can imagine, this will create even bigger DTMs and worse data scarcity problems, so even more attention must be paid to feature selection and/or trimming.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-ngram" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.14 </strong></span>Generating n-grams</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-17-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-17-1" role="tab" aria-controls="tabset-17-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-17-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-17-2" role="tab" aria-controls="tabset-17-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-17-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-17-1-tab">
<div class="cell" data-hash="chapter10_cache/html/ngram-python_3e6fc57b43f5f9dfd548cd6b25186cbc">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>), tokenizer<span class="op">=</span>mytokenizer.tokenize)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>cv.fit_transform([<span class="st">"This is a test"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;1x9 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 9 stored elements in Compressed Sparse Row format&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>cv.get_feature_names()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['a', 'a test', 'is', 'is a', 'is a test', 'test', 'this', 'this is', 'this is a']</code></pre>
</div>
</div>
</div>
<div id="tabset-17-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-17-2-tab">
<div class="cell" data-hash="chapter10_cache/html/ngram-r_30a7bb7487cdc54bdeda083002b63635">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>text <span class="ot">=</span> <span class="st">"This is a test"</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tokens</span>(text) <span class="sc">%&gt;%</span> </span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_tolower</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_ngrams</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tokens consisting of 1 document.
text1 :
[1] "this"      "is"        "a"         "test"      "this_is"   "is_a"     
[7] "a_test"    "this_is_a" "is_a_test"</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Example <a href="#exm-ngram"><span>10.14</span></a> shows how n-grams can be created and used in Python and R. In Python, you can pass the <code>ngram_range=(n, m)</code> option to the vectorizer, while R has a <code>tokens_ngrams(n:m)</code> function. Both will post-process the tokens to create all n-grams in the range of n to m. In this example, we are asking for unigrams (i.e., the words themselves), bigrams and trigrams of a simple example sentence. Both languages produce the same output, with R separating the words with an underscore while Python uses a simple space.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-ngram2" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.15 </strong></span>Words and bigrams containing “government”</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-18-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-18-1" role="tab" aria-controls="tabset-18-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-18-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-18-2" role="tab" aria-controls="tabset-18-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-18-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-18-1-tab">
<div class="cell" data-hash="chapter10_cache/html/ngram2-python_04905f1efe36fed4fed01caad675f7d6">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>    ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>), tokenizer<span class="op">=</span>mytokenizer.tokenize, stop_words<span class="op">=</span><span class="st">"english"</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>dfm <span class="op">=</span> cv.fit_transform(sotu.text.values)</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>ts <span class="op">=</span> termstats(dfm, cv)</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>ts.<span class="bu">filter</span>(like<span class="op">=</span><span class="st">"government"</span>, axis<span class="op">=</span><span class="dv">0</span>).head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                     frequency  docfreq
government                1493       84
federal government         278       56
governments                188       50
local governments          104       28
government 's               79       27
governmental                41       19
local government            32       16
government spending         30       20
self-government             26       20
government programs         23       17</code></pre>
</div>
</div>
</div>
<div id="tabset-18-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-18-2-tab">
<div class="cell" data-hash="chapter10_cache/html/ngram2-r_8f6b4806262f030de243b6bf56ac7781">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>sotu_tokens <span class="ot">=</span> <span class="fu">corpus</span>(sotu) <span class="sc">%&gt;%</span> </span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>(<span class="at">remove_punct=</span>T)  <span class="sc">%&gt;%</span>  </span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_remove</span>(<span class="fu">stopwords</span>(<span class="st">"english"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_tolower</span>()</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>dfm_bigram <span class="ot">=</span> sotu_tokens <span class="sc">%&gt;%</span> </span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_ngrams</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>()</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a><span class="fu">textstat_frequency</span>(dfm_bigram) <span class="sc">%&gt;%</span> </span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(feature, <span class="st">"government"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">12</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 feature frequency rank docfreq group
10            government      1424   10      84   all
198   federal_government       265  198      56   all
318          governments       188  318      50   all
650    local_governments       104  648      28   all
976         government's        71  972      25   all
1204     government_must        55 1195      28   all
1449      government_can        44 1433      26   all
1552        governmental        41 1537      19   all
1947    local_government        32 1919      16   all
2183 government_spending        28 2135      19   all
2271     self-government        26 2259      20   all
2645 government_programs        22 2589      17   all</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Example <a href="#exm-ngram2"><span>10.15</span></a> shows how you can generate n-grams for a whole corpus. In this case, we create a DTM of the state of the union matrix with all bigrams included. A glance at the frequency table for all words containing <em>government</em> shows that, besides the word itself and its plural and possessive forms, the bigrams include compound words (federal and local government), phrases with the government as subject (the government can and must), and nouns for which the government is an adjective (government spending and government programs).</p>
<p>You can imagine that including all these words as features will add many possibilities for analysis of the DTM which would not be possible in a normal bag-of-words approach. The terms local and federal government can be quite important to understand policy positions, but for e.g.&nbsp;sentiment analysis a bigram like <em>not good</em> would also be insightful (but make sure “not” is not on your stop word list!).</p>
</section>
<section id="sec-collocations" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="sec-collocations"><span class="header-section-number">10.3.2</span> Collocations</h3>
<p>A special case of n-grams are collocations. In the strict corpus linguistic sense of the word, collocations are pairs of words that occur more frequently than expected based on their underlying occurrence. For example, the phrase <em>crystal clear</em> presumably occurs much more often than would be expected by chance given how often <em>crystal</em> and <em>clear</em> occur separately. Collocations are important for text analysis since they often have a specific meaning, for example because they refer to names such as <em>New York</em> or disambiguate a term like <em>sound</em> in <em>sound asleep</em>, a <em>sound proposal</em>, or <em>loud sound</em>.</p>
<p>Example <a href="#exm-colloc"><span>10.16</span></a> shows how to identify the most “surprising” collocations using R and Python. For Python, we use the <em>gensim</em> package which we will also use for topic modeling in Section <a href="chapter11.html#sec-unsupervised"><span>11.5</span></a>. This package has a <code>Phrases</code> class which can identify the bigrams in a list of tokens. In R, we use the <code>textstat_collocations</code> function from <em>quanteda</em>. These packages each use a different implementation: <em>gensim</em> uses pointwise mutual information, i.e.&nbsp;how much information about finding the second word does seeing the first word give you? Quanteda estimates an interaction parameter in a loglinear model. Nonetheless, both methods give very similar results, with Saddam Hussein, the Iron Curtain, Al Qaida, and red tape topping the list for each.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-colloc" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.16 </strong></span>Identifying and applying collocations in the US State of the Union.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-19-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-19-1" role="tab" aria-controls="tabset-19-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-19-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-19-2" role="tab" aria-controls="tabset-19-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-19-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-19-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>tokenized_texts <span class="op">=</span> [mytokenizer.tokenize(t) <span class="cf">for</span> t <span class="kw">in</span> sotu.text]</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>    [t.lower() <span class="cf">for</span> t <span class="kw">in</span> tokens <span class="cf">if</span> <span class="kw">not</span> regex.search(<span class="st">"\P</span><span class="sc">{letter}</span><span class="st">"</span>, t)]</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tokens <span class="kw">in</span> tokenized_texts</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>phrases_model <span class="op">=</span> Phrases(tokens, min_count<span class="op">=</span><span class="dv">10</span>, scoring<span class="op">=</span><span class="st">"npmi"</span>, threshold<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>score_dict <span class="op">=</span> phrases_model.export_phrases()</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> pd.DataFrame(score_dict.items(), columns<span class="op">=</span>[<span class="st">"phrase"</span>, <span class="st">"score"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>scores.sort_values(<span class="st">"score"</span>, ascending<span class="op">=</span><span class="va">False</span>).head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             phrase     score
232    iron_curtain  0.977816
423  saddam_hussein  0.975395
438        al_qaida  0.963029
358        red_tape  0.952860
410    persian_gulf  0.951335</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>phraser <span class="op">=</span> Phraser(phrases_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>tokens_phrases <span class="op">=</span> [phraser[doc] <span class="cf">for</span> doc <span class="kw">in</span> tokens]</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(tokenizer<span class="op">=</span><span class="kw">lambda</span> x: x, lowercase<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>dtm <span class="op">=</span> cv.fit_transform(tokens_phrases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>termstats(dtm, cv).<span class="bu">filter</span>(like<span class="op">=</span><span class="st">"hussein"</span>, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                frequency  docfreq
saddam_hussein         29        5</code></pre>
</div>
</div>
</div>
<div id="tabset-19-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-19-2-tab">
<div class="cell" data-hash="chapter10_cache/html/colloc-r_526aeb048f42d5e2f7e3dedd6a753b5e">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>sotu_tokens <span class="ot">=</span> <span class="fu">corpus</span>(sotu)  <span class="sc">%&gt;%</span> </span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>(<span class="at">remove_punct=</span>T) <span class="sc">%&gt;%</span> </span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_tolower</span>()</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>colloc <span class="ot">=</span> sotu_tokens <span class="sc">%&gt;%</span> </span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">textstat_collocations</span>(<span class="at">min_count=</span><span class="dv">10</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>()</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>colloc <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="sc">-</span>lambda)  <span class="sc">%&gt;%</span> <span class="fu">head</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 6
  collocation    count count_nested length lambda     z
  &lt;chr&gt;          &lt;int&gt;        &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
1 saddam hussein    26            0      2   15.2 10.3 
2 iron curtain      11            0      2   15.2  9.85
3 al qaida          37            0      2   14.6 10.1 
4 red tape          22            0      2   13.5 15.1 
5 persian gulf      31            0      2   12.9 18.5 
6 line-item veto    10            0      2   12.9  8.81</code></pre>
</div>
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>collocations <span class="ot">=</span> colloc  <span class="sc">%&gt;%</span> </span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(lambda <span class="sc">&gt;</span> <span class="dv">8</span>)  <span class="sc">%&gt;%</span>  </span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(collocation)  <span class="sc">%&gt;%</span>  </span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">phrase</span>()</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>dfm <span class="ot">=</span> sotu_tokens <span class="sc">%&gt;%</span> </span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens_compound</span>(collocations) <span class="sc">%&gt;%</span> </span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>()</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a><span class="fu">textstat_frequency</span>(dfm) <span class="sc">%&gt;%</span> </span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(feature, <span class="st">"hussein"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            feature frequency rank docfreq group
2186 saddam_hussein        26 2120       5   all
8529      hussein's         3 7341       2   all</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The next block demonstrates how to use these collocations in further processing. In R, we filter the collocations list on <span class="math inline">\(lambda&gt;8\)</span> and use the <code>tokens_compound</code> function to compound bigrams from that list. As you can see in the term frequencies filtered on “Hussein”, the regular terms (apart from the possessive) are removed and the compounded term now has 26 occurrences. For Python, we use the <code>PhraseTransformer</code> class, which is an adaptation of the <code>Phrases</code> class to the <code>scikit-learn</code>methodology. After setting a standard threshold of 0.7, we can use <code>fit_transform</code> to change the tokens. The term statistics again show how the individual terms are now replaced by their compound.</p>
</section>
<section id="sec-wordembeddings" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="sec-wordembeddings"><span class="header-section-number">10.3.3</span> Word Embeddings</h3>
<p>A recent addition to the text analysis toolbox are <em>word embeddings</em>. Although it is beyond the scope of this book to give a full explanation of the algorithms behind word embeddings, they are relatively easy to understand and use at an intuitive level.</p>
<p>The first core idea behind word embeddings is that the meaning of a word can be expressed using a relatively small <em>embedding vector</em>, generally consisting of around 300 numbers which can be interpreted as dimensions of meaning. The second core idea is that these embedding vectors can be derived by scanning the context of each word in millions and millions of documents.</p>
<p>These embedding vectors can then be used as features or DTM columns for further analysis. Using embedding vectors instead of word frequencies has the advantages of strongly reducing the dimensionality of the DTM: instead of (tens of) thousands of columns for each unique word we only need hundreds of columns for the embedding vectors. This means that further processing can be more efficient as fewer parameters need to be fit, or conversely that more complicated models can be used without blowing up the parameter space. Another advantage is that a model can also give a result for words it never saw before, as these words most likely will have an embedding vector and so can be fed into the model. Finally, since words with similar meanings should have similar vectors, a model fit on embedding vectors gets a “head start” since the vectors for words like “great” and “fantastic” will already be relatively close to each other, while all columns in a normal DTM are treated independently.</p>
<p>The assumption that words with similar meanings have similar vectors can also be used directly to extract synonyms. This can be very useful, for example for (semi-)automatically expanding a dictionary for a concept. Example <a href="#exm-embedding"><span>10.17</span></a> shows how to download and use pre-trained embedding vectors to extract synonyms. First, we download a very small subset of the pre-trained Glove embedding vectors<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, wrapping the download call in a condition to only download it when needed.</p>
<p>Then, for Python, we use the excellent support from the <em>gensim</em> package to load the embeddings into a <code>KeyedVectors</code> object. Although not needed for the rest of the example, we create a <em>Pandas</em> data frame from the internal embedding values so the internal structure becomes clear: each row is a word, and the columns (in this case 50) are the different (semantic) dimensions that characterize that word according to the embeddings model. This data frame is sorted on the first dimension, which shows that negative values on that dimension are related to various sports. Next, we switch back to the <code>KeyedVectors</code> object to get the most similar words to the word <em>fraud</em>, which is apparently related to similar words like <em>bribery</em> and <em>corruption</em> but also to words like <em>charges</em> and <em>alleged</em>. These similarities are a good way to (semi-)automatically expand a dictionary: start from a small list of words, find all words that are similar to those words, and if needed manually curate that list. Finally, we use the embeddings to solve the “analogies” that famously showcase the geometric nature of these vectors: if you take the vector for <em>king</em>, subtract the vector for <em>man</em> and add that for <em>woman</em>, the closest word to the resulting vector is <em>queen</em>. Amusingly, it turns out that soccer is a female form of football, probably showing the American cultural origin of the source material.</p>
<p>For R, there was less support from existing packages so we decided to use the opportunity to show both the conceptual simplicity of embeddings vectors and the power of matrix manipulation in R. Thus, we directly read in the word vector file which has a head line and then on each line a word followed by its 50 values. This is converted to a matrix with the row names showing the word, which we normalize to (Euclidean) length of one for each vector for easier processing. To determine similarity, we take the cosine distance between the vector representing a word with all other words in the matrix. As you might remember from algebra, the cosine distance is the dot product between the vectors normalized to have length one (just like Pearson’s product–moment correlation is the dot product between the vectors normalized to z-scores per dimension). Thus, we can simply multiply the normalized target vector with the normalized matrix to get the similarity scores. These are then sorted, renamed, and the top values are taken using the basic functions from Chapter <a href="chapter06.html"><span>6</span></a>. Finally, analogies are solved by simply adding and subtracting the vectors as explained above, and then listing the closest words to the resulting vector (excluding the words in the analogy itself).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-embedding" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.17 </strong></span>Using word embeddings for finding similar and analogous words.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-20-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-20-1" role="tab" aria-controls="tabset-20-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-20-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-20-2" role="tab" aria-controls="tabset-20-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-20-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-20-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the model if needed</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>glove_fn <span class="op">=</span> <span class="st">"glove.6B.50d.10k.w2v.txt"</span></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="ss">f"https://cssbook.net/d/</span><span class="sc">{</span>glove_fn<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(glove_fn):</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>    urllib.request.urlretrieve(url, glove_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>('glove.6B.50d.10k.w2v.txt', &lt;http.client.HTTPMessage object at 0x7f5a27d26740&gt;)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the vectors</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>wv <span class="op">=</span> KeyedVectors.load_word2vec_format(glove_fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>wvdf <span class="op">=</span> pd.DataFrame(wv.vectors, index<span class="op">=</span>wv.index_to_key)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>wvdf.sort_values(<span class="dv">0</span>, ascending<span class="op">=</span><span class="va">False</span>).head()</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Find similar terms</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                0        1        2   ...        47        48        49
airbus      2.5966 -0.53562  0.41422  ...  1.505000 -0.066982  0.133350
spacecraft  2.5187  0.74418  1.66480  ... -0.065039 -0.235400 -0.401210
fiat        2.2865 -1.14970  0.48850  ...  0.173990  0.398950  0.042601
naples      2.2656 -0.10631 -1.27220  ... -0.857650  0.211240 -0.279690
di          2.2441 -0.60324 -1.46890  ... -1.229200  0.805580  0.775450

[5 rows x 50 columns]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> term, similarity <span class="kw">in</span> wv.most_similar(<span class="st">"fraud"</span>)[:<span class="dv">5</span>]:</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>term<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Find analogous terms</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>charges: 0.86
bribery: 0.86
alleged: 0.84
corruption: 0.83
allegations: 0.82</code></pre>
</div>
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analogy(a, b, c):</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> wv.most_similar(positive<span class="op">=</span>[b, c], negative<span class="op">=</span>[a])</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">"king"</span>, <span class="st">"boy"</span>, <span class="st">"father"</span>, <span class="st">"pete"</span>, <span class="st">"football"</span>]</span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x <span class="kw">in</span> words:</span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> analogy(<span class="st">"man"</span>, x, <span class="st">"woman"</span>)</span>
<span id="cb127-8"><a href="#cb127-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Man is to </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss"> as woman is to </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Man is to king as woman is to queen
Man is to boy as woman is to girl
Man is to father as woman is to mother
Man is to pete as woman is to barbara
Man is to football as woman is to soccer</code></pre>
</div>
</div>
</div>
<div id="tabset-20-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-20-2-tab">
<div class="cell" data-hash="chapter10_cache/html/embeddings0-r_e5e1bfb0196d0d2e898f7db14a3b1c35">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>glove_fn <span class="ot">=</span> <span class="st">"glove.6B.50d.10k.w2v.txt"</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">=</span> <span class="fu">glue</span>(<span class="st">"https://cssbook.net/d/{glove_fn}"</span>)</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">file.exists</span>(glove_fn)) </span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">download.file</span>(url, glove_fn)</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>wv_tibble <span class="ot">=</span> <span class="fu">read_delim</span>(glove_fn, <span class="at">skip=</span><span class="dv">1</span>,</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">delim=</span><span class="st">" "</span>, <span class="at">quote=</span><span class="st">""</span>, </span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">col_names =</span> <span class="fu">c</span>(<span class="st">"word"</span>, <span class="fu">paste0</span>(<span class="st">"d"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>)))</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>wv <span class="ot">=</span> <span class="fu">as.matrix</span>(wv_tibble[<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(wv) <span class="ot">=</span> wv_tibble<span class="sc">$</span>word</span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a>wv <span class="ot">=</span> wv <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">rowSums</span>(wv<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>wv[<span class="fu">order</span>(wv[,<span class="dv">1</span>])[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>], <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   d1         d2          d3         d4           d5
20003      -0.4402265 0.07209431 -0.02397687 0.18428984  0.001802660
basketball -0.4234652 0.23817458 -0.09346347 0.17270343 -0.001520135
collegiate -0.4232457 0.23873925 -0.28741579 0.02797958 -0.066008001
volleyball -0.4217268 0.18378662 -0.26229465 0.31409226 -0.124286069
ncaa       -0.4131240 0.14502199 -0.06088206 0.17017979 -0.157397324</code></pre>
</div>
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>wvector <span class="ot">=</span> <span class="cf">function</span>(wv, word) wv[word,,drop<span class="ot">=</span>F]</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>wv_similar <span class="ot">=</span> <span class="cf">function</span>(wv, target, <span class="at">n=</span><span class="dv">5</span>) {</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>  similarities <span class="ot">=</span> wv <span class="sc">%*%</span> <span class="fu">t</span>(target)</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>  similarities <span class="sc">%&gt;%</span> </span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as_tibble</span>(<span class="at">rownames =</span> <span class="st">"word"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rename</span>(<span class="at">similarity=</span><span class="dv">2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">arrange</span>(<span class="sc">-</span>similarity) <span class="sc">%&gt;%</span> </span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(<span class="at">n=</span>n)  </span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a><span class="fu">wv_similar</span>(wv, <span class="fu">wvector</span>(wv, <span class="st">"fraud"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 2
  word       similarity
  &lt;chr&gt;           &lt;dbl&gt;
1 fraud           1    
2 charges         0.859
3 bribery         0.856
4 alleged         0.842
5 corruption      0.830</code></pre>
</div>
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>wv_analogy <span class="ot">=</span> <span class="cf">function</span>(wv, a, b, c) {</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">=</span> (<span class="fu">wvector</span>(wv, b) </span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>            <span class="sc">+</span> <span class="fu">wvector</span>(wv, c) </span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>            <span class="sc">-</span> <span class="fu">wvector</span>(wv, a))</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>  matches <span class="ot">=</span> <span class="fu">wv_similar</span>(wv, result) <span class="sc">%&gt;%</span> </span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(<span class="sc">!</span>word <span class="sc">%in%</span> <span class="fu">c</span>(a,b,c))</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>  matches<span class="sc">$</span>word[<span class="dv">1</span>]</span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>words<span class="ot">=</span><span class="fu">c</span>(<span class="st">"king"</span>,<span class="st">"boy"</span>,<span class="st">"father"</span>,<span class="st">"pete"</span>,<span class="st">"football"</span>)</span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (x <span class="cf">in</span> words) {</span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">wv_analogy</span>(wv, <span class="st">"man"</span>, x, <span class="st">"woman"</span>)</span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">glue</span>(<span class="st">"Man is to {x} as woman is to: {y}"</span>))</span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Man is to king as woman is to: queen
Man is to boy as woman is to: girl
Man is to father as woman is to: mother
Man is to pete as woman is to: barbara
Man is to football as woman is to: soccer</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-nlp" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="sec-nlp"><span class="header-section-number">10.3.4</span> Linguistic Preprocessing</h3>
<p>A final technique to be discussed here is the use of linguistic preprocessing steps to enrich and filter a DTM. So far, all techniques discussed here are language independent. However, there are also many language-specific tools for automatically enriching text developed by computational linguistics communities around the world. Two techniques will be discussed here as they are relatively widely available for many languages and easy and quick to apply: <em>Part-of-speech tagging</em> and <em>lemmatizing</em>.</p>
<p>In <em>part-of-speech tagging</em> or POS-tagging, each word is enriched with information on its function in the sentence: verb, noun, determiner etc. For most languages, this can be determined with very high accuracy, although sometimes text can be ambiguous: in one famous example, the word flies in <em>fruit flies</em> is generally a noun (fruit flies are a type of fly), but it can also be a verb (if fruit could fly). Although there are different sets of POS tags used by different tools, there is broad agreement on the core set of tags listed in Table <a href="#tbl-postags"><span>10.1</span></a>.</p>
<div id="tbl-postags" class="anchored">
<table class="table">
<caption>Table&nbsp;10.1: Overview of part-of-speech (POS) tags.</caption>
<thead>
<tr class="header">
<th>Part of speech</th>
<th>Example</th>
<th>UDPipe/Spacy Tag</th>
<th>Penn Treebank Tag</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noun</td>
<td>apple</td>
<td>NOUN</td>
<td>NN, NNS</td>
</tr>
<tr class="even">
<td>Proper Name</td>
<td>Carlos</td>
<td>PROPN</td>
<td>NNP</td>
</tr>
<tr class="odd">
<td>Verb</td>
<td>write</td>
<td>VERB</td>
<td>VB, VBD, VBP, ..</td>
</tr>
<tr class="even">
<td>Auxiliary verb</td>
<td>be, have</td>
<td>AUX</td>
<td>(same as verb)</td>
</tr>
<tr class="odd">
<td>Adjective</td>
<td>quick</td>
<td>ADJ</td>
<td>JJ, JJR, JJS</td>
</tr>
<tr class="even">
<td>Adverb</td>
<td>quickly</td>
<td>ADV</td>
<td>RB</td>
</tr>
<tr class="odd">
<td>Pronoun</td>
<td>I, him</td>
<td>PRON</td>
<td>PRP</td>
</tr>
<tr class="even">
<td>Adposition</td>
<td>of, in</td>
<td>ADP</td>
<td>IN</td>
</tr>
<tr class="odd">
<td>Determiner</td>
<td>the, a</td>
<td>DET</td>
<td>DT</td>
</tr>
</tbody>
</table>
</div>
<p>POS tags are useful since they allow us for example to analyze only the <em>nouns</em> if we care about the things that are discussed, only the <em>verbs</em> if we care about actions that are described, or only the <em>adjectives</em> if we care about the characteristics given to a noun. Moreover, knowing the POS tag of a word can help disambiguate it. For example, like as a verb (I like books) is generally positive, but like as a preposition (a day like no other) has no clear sentiment attached.</p>
<p><em>Lemmatizing</em> is a technique for reducing each word to its root or <em>lemma</em> (plural: lemmata). For example, the lemma of the verb <em>reads</em> is (to) <em>read</em> and the lemma of the noun <em>books</em> is <em>book</em>. Lemmatizing is useful since for most of our research questions we do not care about these different conjugations of the same word. By lemmatizing the texts, we do not need to include all conjugations in a dictionary, and it reduces the dimensionality of the DTM – and thus also the data scarcity.</p>
<p>Note that lemmatizing is related to a technique called <em>stemming</em>, which removes known suffixes (endings) from words. For example, for English it will remove the “s” from both reads and books. Stemming is much less sophisticated than lemmatizing, however, and will trip over irregular conjugations (e.g.&nbsp;<em>are</em> as a form of to be) and regular word endings that look like conjugations (e.g.&nbsp;<em>virus</em> will be stemmed to <em>viru</em>). English has relatively simple conjugations and stemming can produce adequate results. For morphologically richer languages such as German or French, however, it is strongly advised to use lemmatizing instead of stemming. Even for English we would generally advise lemmatization since it is so easy nowadays and will yield better results than stemming.</p>
<p>For Example <a href="#exm-udpipe"><span>10.18</span></a>, we use the <em>UDPipe</em> natural language processing toolkit <span class="citation" data-cites="udpipe">(<a href="references.html#ref-udpipe" role="doc-biblioref">Straka and Straková 2017</a>)</span>, a “Pipeline” that parses text into “Universal Dependencies”, a representation of the syntactic structure of the text. For R, we can immediately call the <code>udpipe</code> function from the package of the same name. This parses the given text and returns the result as a data frame with one token (word) per row, and the various features in the columns. For Python, we need to take some more steps ourselves. First, we download the English models if they aren’t present. Second, we load the model and create a pipeline with all default settings, and use that to parse the same sentence. Finally, we use the <em>conllu</em> package to read the results into a form that can be turned into a data frame.</p>
<p>In both cases, the resulting tokens clearly show some of the potential advantages of linguistic processing: the lemma column shows that it correctly deals with irregular verbs and plural forms. Looking at the upos (universal part-of-speech) column, John is recognized as a proper name (PROPN), bought as a verb, and knives as a noun. Finally, the <code>head_token_id</code> and <code>dep_rel</code> columns represent the syntactic information in the sentence: “Bought” (token 2) is the root of the sentence, and “John” is the subject (nsubj) while “knives” is the object of the buying.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>('english-ewt-ud-2.4-190531.udpipe', &lt;http.client.HTTPMessage object at 0x7f5a27f11810&gt;)</code></pre>
</div>
</div>
<div id="exm-udpipe" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.18 </strong></span>Using UDPipe to analyze a sentence</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-21-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-21-1" role="tab" aria-controls="tabset-21-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-21-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-21-2" role="tab" aria-controls="tabset-21-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-21-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-21-1-tab">
<div class="cell" data-hash="chapter10_cache/html/udpipe-python_6e5072586d79c9c8d9b35d636d3f6479">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>udpipe_model <span class="op">=</span> <span class="st">"english-ewt-ud-2.4-190531.udpipe"</span></span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Model.load(udpipe_model)</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline(m, <span class="st">"tokenize"</span>, Pipeline.DEFAULT, Pipeline.DEFAULT, <span class="st">"conllu"</span>)</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"John bought new knives"</span></span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>tokenlist <span class="op">=</span> conllu.parse(pipeline.process(text))</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(tokenlist[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   id    form  lemma   upos  ... head deprel  deps                   misc
0   1    John   John  PROPN  ...    2  nsubj  None                   None
1   2  bought    buy   VERB  ...    0   root  None                   None
2   3     new    new    ADJ  ...    4   amod  None                   None
3   4  knives  knife   NOUN  ...    2    obj  None  {'SpacesAfter': '\n'}

[4 rows x 10 columns]</code></pre>
</div>
</div>
</div>
<div id="tabset-21-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-21-2-tab">
<div class="cell" data-hash="chapter10_cache/html/udpipe-r_0af297bf0c703b285da8199ca9283002">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="fu">udpipe</span>(<span class="st">"John bought new knives"</span>, <span class="st">"english"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(token_id<span class="sc">:</span>upos, head_token_id<span class="sc">:</span>dep_rel)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  token_id  token lemma  upos head_token_id dep_rel
1        1   John  John PROPN             2   nsubj
2        2 bought   buy  VERB             0    root
3        3    new   new   ADJ             4    amod
4        4 knives knife  NOUN             2     obj</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The syntactic relations can be useful if you need to differentiate between who is doing something and whom it was done to. For example, one of the authors of this book used syntactic relations to analyze conflict coverage, where there is an important difference between attacking and getting attacked <span class="citation" data-cites="clause">(<a href="references.html#ref-clause" role="doc-biblioref">Van Atteveldt et al. 2017</a>)</span>. However, in most cases you probably don’t need this information and analyzing dependency graphs is relatively complex. We would advise you to almost always consider lemmatizing and tagging your texts, as lemmatizing is simply so much better than stemming (especially for languages other than English), and the part-of-speech can be very useful for analyzing different aspects of a text.</p>
<p>If you only need the lemmatizer and tagger, you can speed up processing by setting <code>udpipe(.., parser='none')</code> (R) or setting the third argument to Pipeline (the parser) to <code>Pipeline.NONE</code> (Python). Example <a href="#exm-nouncloud"><span>10.19</span></a> shows how this can be used to extract only the nouns from the most recent state of the union speeches, create a DTM with these nouns, and then visualize them as a word cloud. As you can see, these words (such as student, hero, childcare, healthcare, and terrorism), are much more indicative of the topic of a text than the general words used earlier. In the next chapter we will show how you can further analyze these data, for example by analyzing usage patterns per person or over time, or using an unsupervised topic model to cluster words into topics.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-nouncloud" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.19 </strong></span>Nouns used in the most recent State of the Union addresses</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-22-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-22-1" role="tab" aria-controls="tabset-22-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-22-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-22-2" role="tab" aria-controls="tabset-22-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-22-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-22-1-tab">
<div class="cell" data-hash="chapter10_cache/html/nouncloud-python_200f668fb50b25d2dc5550cea8124996">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_nouns(text):</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> conllu.parse(pipeline.process(text))</span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sentence <span class="kw">in</span> result:</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> token <span class="kw">in</span> sentence:</span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> token[<span class="st">"upos"</span>] <span class="op">==</span> <span class="st">"NOUN"</span>:</span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a>                <span class="cf">yield</span> token[<span class="st">"lemma"</span>]</span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>parser <span class="op">=</span> Pipeline.NONE</span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline(m, <span class="st">"tokenize"</span>, Pipeline.DEFAULT, Pipeline.NONE, <span class="st">"conllu"</span>)</span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [<span class="bu">list</span>(get_nouns(text)) <span class="cf">for</span> text <span class="kw">in</span> sotu.text[<span class="op">-</span><span class="dv">5</span>:]]</span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(tokenizer<span class="op">=</span><span class="kw">lambda</span> x: x, lowercase<span class="op">=</span><span class="va">False</span>, max_df<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a>dtm_verbs <span class="op">=</span> cv.fit_transform(tokens)</span>
<span id="cb140-14"><a href="#cb140-14" aria-hidden="true" tabindex="-1"></a>wc <span class="op">=</span> wordcloud(dtm_verbs, cv, background_color<span class="op">=</span><span class="st">"white"</span>)</span>
<span id="cb140-15"><a href="#cb140-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(wc)</span>
<span id="cb140-16"><a href="#cb140-16" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-22-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-22-2-tab">
<div class="cell" data-hash="chapter10_cache/html/nouncloud-r_7043dd6b0146cf4d9beb0f66dee1f8c2">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="ot">=</span> sotu <span class="sc">%&gt;%</span> </span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">top_n</span>(<span class="dv">5</span>, Date) <span class="sc">%&gt;%</span> </span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">udpipe</span>(<span class="st">"english"</span>, <span class="at">parser=</span><span class="st">"none"</span>)</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>nouns <span class="ot">=</span> tokens <span class="sc">%&gt;%</span> </span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(upos <span class="sc">==</span> <span class="st">"NOUN"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(doc_id)  <span class="sc">%&gt;%</span> </span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">text=</span><span class="fu">paste</span>(lemma, <span class="at">collapse=</span><span class="st">" "</span>))</span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a>nouns <span class="sc">%&gt;%</span> </span>
<span id="cb141-9"><a href="#cb141-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">corpus</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb141-10"><a href="#cb141-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>() <span class="sc">%&gt;%</span></span>
<span id="cb141-11"><a href="#cb141-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb141-12"><a href="#cb141-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_trim</span>(<span class="at">max_docfreq=</span><span class="fl">0.7</span>,<span class="at">docfreq_type=</span><span class="st">"prop"</span>)<span class="sc">%&gt;%</span></span>
<span id="cb141-13"><a href="#cb141-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">textplot_wordcloud</span>(<span class="at">max_words=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter10_files/figure-html/nouncloud-r-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>As an alternative to UDPipe, you can also use Spacy, which is another free and popular natural language toolkit. It is written in Python, but the <em>spacyr</em> package offers an easy way to use it from R. For R users, installation of <em>spacyr</em> on MacOS and Linux is easy, but note that on Windows there are some additional steps, see <a href="https://cran.r-project.org/web/packages/spacyr/readme/README.html">cran.r-project.org/web/packages/spacyr/readme/README.html</a> for more details.</p>
<p>Example <a href="#exm-spacy"><span>10.20</span></a> shows how you can use Spacy to analyze the proverb “all roads lead to Rome” in Spanish. In the first block, the Spanish language model is downloaded (this is only needed once). The second block loads the language model and parses the sentence. You can see that the output is quite similar to UDPipe, but one additional feature is the inclusion of <em>Named Entity Recognition</em>: Spacy can automatically identify persons, locations, organizations and other entities. In this example, it identifies “Rome” as a location. This can be very useful to extract e.g.&nbsp;all persons from a newspaper corpus automatically. Note that in R, you can use the <em>quanteda</em> function <code>as.tokens</code> to directly use the Spacy output in quanteda.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-spacy" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.20 </strong></span>Using Spacy to analyze a Spanish sentence.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-23-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-23-1" role="tab" aria-controls="tabset-23-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-23-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-23-2" role="tab" aria-controls="tabset-23-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-23-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-23-1-tab">
<p>First, download the model using the command below and restart python: (in jupyter, try <code>!python</code> or <code>!python3</code> instead of plain <code>python</code>)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download es_core_news_sm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-23-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-23-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb143"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Only needed once</span></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="fu">spacy_install</span>()</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Only needed for languages other than English:</span></span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a><span class="fu">spacy_download_langmodel</span>(<span class="st">"es_core_news_sm"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-24-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-24-1" role="tab" aria-controls="tabset-24-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-24-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-24-2" role="tab" aria-controls="tabset-24-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-24-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-24-1-tab">
<div class="cell" data-hash="chapter10_cache/html/spacy-python_4a029343a0411208c34afba1cfee362d">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"es_core_news_sm"</span>)</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> nlp(<span class="st">"Todos los caminos llevan a Roma"</span>)</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">dict</span>(</span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>            i<span class="op">=</span>t.i,</span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a>            word<span class="op">=</span>t.text,</span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>            lemma<span class="op">=</span>t.lemma_,</span>
<span id="cb144-9"><a href="#cb144-9" aria-hidden="true" tabindex="-1"></a>            head<span class="op">=</span>t.head,</span>
<span id="cb144-10"><a href="#cb144-10" aria-hidden="true" tabindex="-1"></a>            dep<span class="op">=</span>t.dep_,</span>
<span id="cb144-11"><a href="#cb144-11" aria-hidden="true" tabindex="-1"></a>            ner<span class="op">=</span>t.ent_type_,</span>
<span id="cb144-12"><a href="#cb144-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb144-13"><a href="#cb144-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> tokens</span>
<span id="cb144-14"><a href="#cb144-14" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb144-15"><a href="#cb144-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   i     word   lemma     head    dep  ner
0  0    Todos    todo      los    det     
1  1      los      el  caminos    det     
2  2  caminos  camino   llevan  nsubj     
3  3   llevan  llevar   llevan   ROOT     
4  4        a       a     Roma   case     
5  5     Roma    Roma   llevan    obj  LOC</code></pre>
</div>
</div>
</div>
<div id="tabset-24-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-24-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="co"># I couldn't get this to work properly in the renv environment</span></span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="fu">spacy_initialize</span>(<span class="st">"es_core_news_sm"</span>)</span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="fu">spacy_parse</span>(<span class="st">"Todos los caminos llevan a Roma"</span>)</span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a><span class="co"># To close spacy (or switch languages), use:</span></span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spacy_finalize</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>As you can see, nowadays there are a number of good and relatively easy to use linguistic toolkits that can be used. Especially <em>Stanza</em> <span class="citation" data-cites="stanza">(<a href="references.html#ref-stanza" role="doc-biblioref">Qi et al. 2020</a>)</span> is also a very good and flexible toolkit with support for multiple (human) languages and good integration especially with Python. If you want to learn more about natural language processing, the book <em>Speech and Language Processing</em> by Jurafsky and Martin is a very good starting point [<span class="citation" data-cites="jurafsky">Jurafsky and Martin (<a href="references.html#ref-jurafsky" role="doc-biblioref">2009</a>)</span>]<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
</section>
</section>
<section id="which-preprocessing-to-use" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="which-preprocessing-to-use"><span class="header-section-number">10.4</span> Which Preprocessing to Use?</h2>
<p>This chapter has shown how to create a DTM and especially introduced a number of different steps that can be used to clean and preprocess the DTM before analysis. All of these steps are used by text analysis practitioners and in the relevant literature. However, no study ever uses all of these steps on top of each other. This of courses raises the question of how to know which preprocessing steps to use for your research question.</p>
<p>First, there are a number of things that you should (almost) always do. If your data contains noise such as boilerplate language, HTML artifacts, etc., you should generally strip these out before proceeding. Second, text almost always has an abundance of uninformative (stop) words and a very long tail of very rare words. Thus, it is almost always a good idea to use a combination of stop word removal, trimming based on document frequency, and/or <code>tf.idf</code> weighting. Note that when using a stop word list, you should always manually inspect and/or fine-tune the word list to make sure it matches your domain and research question.</p>
<p>The other steps such as n-grams, collocations, and tagging and lemmatization are more optional but can be quite important depending on the specific research. For this (and for choosing a specific combination of trimming and weighting), it is always good to know your domain well, look at the results, and think whether you think they make sense. Using the example given above, bigrams can make more sense for sentiment analysis (since <em>not good</em> is quite different from <em>good</em>), but for analyzing the topic of texts it may be less important.</p>
<p>Ultimately, however, many of these questions have no good theoretical answer, and the only way to find a good preprocessing “pipeline” for your research question is to try many different options and see which works best. This might feel like “cheating” from a social science perspective, since it is generally frowned upon to just test many different statistical models and report on what works best. There is a difference, however, between substantive statistical modeling where you actually want to understand the mechanisms, and technical processing steps where you just want the best possible measurement of an underlying variable (presumably to be used in a subsequent substantive model). <span class="citation" data-cites="mousetrap">Lin (<a href="references.html#ref-mousetrap" role="doc-biblioref">2015</a>)</span> uses the analogy of the mouse trap and the human condition: in engineering you want to make the best possible mouse trap, while in social science we want to understand the human condition. For the mouse trap, it is OK if it is a black box for which we have no understanding of how it works, as long as we are sure that it does work. For the social science model, this is not the case as it is exactly the inner workings we are interested in.</p>
<p>Technical (pre)processing steps such as those reviewed in this chapter are primarily engineering devices: we don’t really care how something like <code>tfc.idf</code> works, as long as it produces the best possible measurement of the variables we need for our analysis. In other words, it is an engineering challenge, not a social science research question. As a consequence, the key criterion by which to judge these steps is validity, not explainability. Thus, it is fine to try out different options, as long as you validate the results properly. If you have many different choices to evaluate against some metric such as performance on a subsequent prediction task, using the split-half or cross-validation techniques discussed in chapter Chapter <a href="chapter08.html"><span>8</span></a> are also relevant here to avoid biasing the evaluation.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-quanteda" class="csl-entry" role="doc-biblioentry">
Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. <span>“Quanteda: An r Package for the Quantitative Analysis of Textual Data.”</span> <em>Journal of Open Source Software</em> 3 (30): 774. <a href="https://doi.org/10.21105/joss.00774">https://doi.org/10.21105/joss.00774</a>.
</div>
<div id="ref-jurafsky" class="csl-entry" role="doc-biblioentry">
Jurafsky, Daniel, and James H Martin. 2009. <em>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition (2nd Ed.)</em>. Prentice Hall.
</div>
<div id="ref-mousetrap" class="csl-entry" role="doc-biblioentry">
Lin, Jimmy. 2015. <span>“On Building Better Mousetraps and Understanding the Human Condition: Reflections on Big Data in the Social Sciences.”</span> <em>The ANNALS of the American Academy of Political and Social Science</em> 659 (1): 33–47.
</div>
<div id="ref-nothman18" class="csl-entry" role="doc-biblioentry">
Nothman, Joel, Hanmin Qin, and Roman Yurchak. 2018. <span>“Stop Word Lists in Free Open-Source Software Packages.”</span> In <em>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</em>, 7–12.
</div>
<div id="ref-stanza" class="csl-entry" role="doc-biblioentry">
Qi, Peng, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. <span>“Stanza: A <span>Python</span> Natural Language Processing Toolkit for Many Human Languages.”</span> In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>. <a href="https://nlp.stanford.edu/pubs/qi2020stanza.pdf">https://nlp.stanford.edu/pubs/qi2020stanza.pdf</a>.
</div>
<div id="ref-udpipe" class="csl-entry" role="doc-biblioentry">
Straka, Milan, and Jana Straková. 2017. <span>“Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe.”</span> In <em>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</em>, 88–99. Vancouver, Canada: Association for Computational Linguistics. <a href="http://www.aclweb.org/anthology/K/K17/K17-3009.pdf">http://www.aclweb.org/anthology/K/K17/K17-3009.pdf</a>.
</div>
<div id="ref-clause" class="csl-entry" role="doc-biblioentry">
Van Atteveldt, W., T. Sheafer, S. R. Shenhav, and Y. Fogel-Dror. 2017. <span>“Clause Analysis: Using Syntactic Information to Automatically Extract Source, Subject, and Predicate from Texts with an Application to the 2008–2009 <span>Gaza</span> <span>War</span>.”</span> <em>Political Analysis</em> 25 (2): 207–22.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The full embedding models can be downloaded from https://nlp.stanford.edu/projects/glove/. To make the file easier to download, we took only the 10000 most frequent words of the smallest embeddings file (the 50 dimension version of the 6B tokens model). For serious applications you probably want to download the larger files, in our experience the 300 dimension version usually gives good results. Note that the files on that site are in a slightly different format which lacks the initial header line, so if you want to use other vectors for the examples here you can convert them with the <code>glove2word2vec</code> function in the <em>gensim</em> package. For R, you can also simply omit the <code>skip=1</code> argument as apart from the header line the formats are identical.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See <a href="https://web.stanford.edu/~jurafsky/slp3/">web.stanford.edu/~jurafsky/slp3/</a> for their draft of a new edition, which is (at the time of writing) free to download.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter09.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Processing text</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter11.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic analysis of text</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>