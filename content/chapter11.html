<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="An open access computational social science textbook giving a practical introduction to the analysis of texts, networks, and images with code examples in Python and R">

<title>Computational Analysis of Communication - 11&nbsp; Automatic analysis of text</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/chapter12.html" rel="next">
<link href="../content/chapter10.html" rel="prev">
<link href="../content/img/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="twitter:title" content="Computational Analysis of Communication - 11&nbsp; Automatic analysis of text">
<meta name="twitter:description" content="An open access computational social science textbook giving a practical introduction to the analysis of texts, networks, and images with code examples in Python and R">
<meta name="twitter:image" content="https://cssbook.net/content/img/cover.jpg">
<meta name="twitter:creator" content="@vanatteveldt">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic analysis of text</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Computational Analysis of Communication</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Table of Contents</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter01.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter02.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Getting started: Fun with data and visualizations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter03.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Programming concepts for data analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter04.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">How to write code</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter05.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">From file to data frame and back</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter06.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Wrangling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter07.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Exploratory data analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter08.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Statistical Modeling and Supervised Machine Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter09.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Processing text</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter10.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Text as data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter11.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic analysis of text</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter12.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Scraping online data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter13.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Network Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter14.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Multimedia data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter15.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Scaling up and distributing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter16.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Where to go next</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-deciding" id="toc-sec-deciding" class="nav-link active" data-scroll-target="#sec-deciding"><span class="toc-section-number">11.1</span>  Deciding on the Right Method</a></li>
  <li><a href="#sec-reviewdataset" id="toc-sec-reviewdataset" class="nav-link" data-scroll-target="#sec-reviewdataset"><span class="toc-section-number">11.2</span>  Obtaining a Review Dataset</a></li>
  <li><a href="#sec-dictionary" id="toc-sec-dictionary" class="nav-link" data-scroll-target="#sec-dictionary"><span class="toc-section-number">11.3</span>  Dictionary Approaches to Text Analysis</a></li>
  <li><a href="#sec-supervised" id="toc-sec-supervised" class="nav-link" data-scroll-target="#sec-supervised"><span class="toc-section-number">11.4</span>  Supervised Text Analysis: Automatic Classification and Sentiment Analysis</a>
  <ul class="collapse">
  <li><a href="#sec-workflow" id="toc-sec-workflow" class="nav-link" data-scroll-target="#sec-workflow"><span class="toc-section-number">11.4.1</span>  Putting Together a Workflow</a></li>
  <li><a href="#sec-bestclassifier" id="toc-sec-bestclassifier" class="nav-link" data-scroll-target="#sec-bestclassifier"><span class="toc-section-number">11.4.2</span>  Finding the Best Classifier</a></li>
  <li><a href="#sec-usingmodel" id="toc-sec-usingmodel" class="nav-link" data-scroll-target="#sec-usingmodel"><span class="toc-section-number">11.4.3</span>  Using the Model</a></li>
  <li><a href="#sec-deeplearning" id="toc-sec-deeplearning" class="nav-link" data-scroll-target="#sec-deeplearning"><span class="toc-section-number">11.4.4</span>  Deep Learning</a></li>
  </ul></li>
  <li><a href="#sec-unsupervised" id="toc-sec-unsupervised" class="nav-link" data-scroll-target="#sec-unsupervised"><span class="toc-section-number">11.5</span>  Unsupervised Text Analysis: Topic Modeling</a>
  <ul class="collapse">
  <li><a href="#sec-lda" id="toc-sec-lda" class="nav-link" data-scroll-target="#sec-lda"><span class="toc-section-number">11.5.1</span>  Latent Dirichlet Allocation (LDA)</a></li>
  <li><a href="#sec-ldafit" id="toc-sec-ldafit" class="nav-link" data-scroll-target="#sec-ldafit"><span class="toc-section-number">11.5.2</span>  Fitting an LDA Model</a></li>
  <li><a href="#sec-ldainspect" id="toc-sec-ldainspect" class="nav-link" data-scroll-target="#sec-ldainspect"><span class="toc-section-number">11.5.3</span>  Analyzing Topic Model Results</a></li>
  <li><a href="#sec-ldavalidate" id="toc-sec-ldavalidate" class="nav-link" data-scroll-target="#sec-ldavalidate"><span class="toc-section-number">11.5.4</span>  Validating and Inspecting Topic Models</a></li>
  <li><a href="#sec-beyondlda" id="toc-sec-beyondlda" class="nav-link" data-scroll-target="#sec-beyondlda"><span class="toc-section-number">11.5.5</span>  Beyond LDA</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-chap-text" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic analysis of text</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Update planned: R Tidymodels and tidytext
</div>
</div>
<div class="callout-body-container callout-body">
<p>At the time of writing this chapter, <code>quanteda</code> and <code>caret</code> were our packages of choice for machine learning and text analysis in R, even though (as noted below) we felt that especially supervised text analysis was much better supported in Python than in R. We now think that the <code>tidymodels</code> and <code>tidytext</code> packages are a better choice for text analysis, especially for students who have just learned to work with the <code>tidyverse</code> data wrangling package. For this reason, we are planning to rewrite this chapter using those packages. See <a href="https://v2.cssbook.net/content/chapter11.html">the draft updated version of this chapter</a> and/or the relevant github issues for <a href="https://github.com/vanatteveldt/cssbook/issues/5">tidytext</a> and <a href="https://github.com/vanatteveldt/cssbook/issues/5">tidymodels</a> for more information.</p>
</div>
</div>
<!--# 
Edit history 
- Bugfix: txt_* -> text_* 
- Adapt to new package version: keras.preprocessing -> keras_preprocessing 
- eli5 adapt to non-jupyter
-->
<div class="cell">

</div>
<div class="cell">

</div>
<p><strong>Abstract.</strong> In this chapter, we discuss different approaches to the automatic analysis of text; or automated content analysis. We combine techniques from earlier chapters, such as transforming texts into a matrix of term frequencies and machine learning. In particular, we describe three different approaches (dictionary-based analyses, supervised machine learning, unsupervised machine learning). The chapter provides guidance on how to conduct such analyses, and also on how to decide which of the approaches is most suitable for which types of question.</p>
<p><strong>Keywords.</strong> dictionary approaches, supervised machine learning, unsupervised machine learning, topic models, automated content analysis, sentiment analysis</p>
<p><strong>Objectives:</strong></p>
<ul>
<li>Understand different approaches to automatic analysis of text</li>
<li>Be able to decide on whether to use a dictionary approach, supervised machine learning, or unsupervised machine learning</li>
<li>Be able to use these techniques</li>
</ul>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Packages used in this chapter
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This chapter uses the basic text and data handling that were described in <a href="chapter10.html"><span>Chapter&nbsp;10</span></a> (<em>tidyverse</em>, <em>readtext</em>, and <em>quanteda</em> for R, <em>pandas</em> and <em>nltk</em> for Python). For supervised text analysis, we use <em>quanteda.textmodels</em> in R, and <em>sklearn</em> and <em>keras</em> in Python. For topic models we use <em>topicmodels</em> (R) and <em>gensim</em> (Python). You can install these packages with the code below if needed (see <a href="chapter01.html#sec-installing"><span>Section&nbsp;1.4</span></a> for more details):</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install nltk scikit<span class="op">-</span>learn pandas </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install gensim eli5 keras keras_preprocessing tensorflow</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(<span class="st">"tidyverse"</span>, <span class="st">"readtext"</span>, </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"quanteda"</span>, <span class="st">"quanteda.textmodels"</span>, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"topicmodels"</span>, <span class="st">"keras"</span>, <span class="st">"topicdoc"</span>, <span class="st">"MLmetrics"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>After installing, you need to import (activate) the packages every session:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># General packages and dictionary analysis</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tarfile</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> bz2</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> eli5</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> TreebankWordTokenizer</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Supervised text classification</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline, Pipeline</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> eli5</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.sentiment <span class="im">import</span> vader</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.sentiment.vader <span class="im">import</span> SentimentIntensityAnalyzer</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Deep learning with Keras</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Input, GlobalMaxPooling1D, Conv1D, Embedding</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Model</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> RMSprop</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras_preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.keyedvectors <span class="im">import</span> KeyedVectors</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Topic Modeling</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> matutils</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.ldamodel <span class="im">import</span> LdaModel</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.coherencemodel <span class="im">import</span> CoherenceModel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># General packages and dictionary analysis</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glue)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readtext)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Supervised text classification</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda.textmodels)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MLmetrics)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Deep learning with Keras</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Topic Modeling</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(topicmodels)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(topicdoc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>In earlier chapters, you learned about both supervised and unsupervised machine learning as well about dealing with texts. This chapter brings together these elements and discusses how to combine them to automatically analyze large corpora of texts. After presenting guidelines for choosing an appropriate approach in <a href="#sec-deciding"><span>Section&nbsp;11.1</span></a> and downloading an example dataset in <a href="#sec-reviewdataset"><span>Section&nbsp;11.2</span></a>, we discuss multiple techniques in detail. We begin with a very simple top-down approach in <a href="#sec-dictionary"><span>Section&nbsp;11.3</span></a>, in which we count occurrences of words from an <em>a priori</em> defined list of words. In <a href="#sec-supervised"><span>Section&nbsp;11.4</span></a>, we still use pre-defined categories that we want to code, but let the machine “learn” the rules of the coding itself. Finally, in <a href="#sec-unsupervised"><span>Section&nbsp;11.5</span></a>, we employ a bottom-up approach in which we do not use any <em>a priori</em> defined lists or coding schemes, but inductively extract topics from our data.</p>
<section id="sec-deciding" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="sec-deciding"><span class="header-section-number">11.1</span> Deciding on the Right Method</h2>
<p>When thinking about the computational analysis of texts, it is important to realize that there is no method that is <em>the one</em> to do so. While there are good choices and bad choices, we also cannot say that one method is necessarily and always superior to another. Some methods are more fashionable than others. For instance, there has been a growing interest in topic models (see <a href="#sec-unsupervised"><span>Section&nbsp;11.5</span></a>) in the past few years. There are indeed very good applications for such models, they are also sometimes applied to research questions and/or data where they make much less sense. As always, the choice of method should follow the research question and not the other way round. We therefore caution you about reading <a href="#sec-chap-text"><span>Chapter&nbsp;11</span></a> selectively because you want, for instance, to learn about supervised machine learning or about unsupervised topic models. Instead, you should be aware of very different approaches to make an informed decision on what to use when.</p>
<p><span class="citation" data-cites="Boumans2016">Boumans and Trilling (<a href="references.html#ref-Boumans2016" role="doc-biblioref">2016</a>)</span> provide useful guidelines for this. They place automatic text analysis approaches on a continuum from deductive (or top-down) to inductive (or bottom-up). At the deductive end of the spectrum, they place dictionary approaches (<a href="#sec-dictionary"><span>Section&nbsp;11.3</span></a>). Here, the researcher has strong <em>a priori</em> (theoretical) assumptions (for instance, which topics exist in a news data set; or which words are positive or negative) and can compile lists of words or rules based on these assumptions. The computer then only needs to execute these rules. At the inductive end of the spectrum, in contrast, lie approaches such as topic models (<a href="#sec-unsupervised"><span>Section&nbsp;11.5</span></a>) where little or no <em>a priori</em> assumptions are made, and where we exploratively look for patterns in the data. Here, we typically do not know which topics exist in advance. Supervised approaches (<a href="#sec-supervised"><span>Section&nbsp;11.4</span></a>) can be placed in between: here, we do define categories <em>a priori</em> (we do know which topics exist, and given an article, we know to which topic it belongs), but we do not have any set of rules: we do not know which words to look for or which exact rules to follow. These rules are to be “learned” by the computer from the data.</p>
<p>Before we get into the details and implementations, let us discuss some use cases of the three main approaches for the computational analysis of text: dictionary (or rule-based) approaches, supervised machine learning, and unsupervised machine learning.</p>
<p>Dictionary approaches excel under three conditions. First, the variable we want to code is <em>manifest and concrete</em> rather than <em>latent and abstract</em>: names of actors, specific physical objects, specific phrases, etc., rather than feelings, frames, or topics. Second, all synonyms to be included must be known beforehand. And third, the dictionary entries must not have multiple meanings. For instance, coding for how often gun control is mentioned in political speeches fits these criteria. There are only so many ways to talk about it, and it is rather unlikely that speeches about other topics contain a phrase like “gun control”. Similarly, if we want to find references to Angela Merkel, Donald Trump, or any other well-known politician, we can just directly search for their names – even though problems arise when people have very common surnames and are referred to by their surnames only.</p>
<p>Sadly, most interesting concepts are more complex to code. Take a seemingly straightforward problem: distinguishing whether a news article is about the economy or not. This is really easy to do for humans: there may be some edge cases, but in general, people rarely need longer than a few seconds to grasp whether an article is about the economy rather than about sports, culture, etc. Yet, many of these articles won’t directly state that they are about the economy by explicitly using the word “economy”.</p>
<p>We may think of extending our dictionary not only with <code>econom.+</code> (a regular expression that includes economists, economic, and so on), but also come up with other words like “stock exchange”, “market”, “company.” Unfortunately, we will quickly run into a problem that we also faced when we discussed the precision-recall trade-off in <a href="chapter08.html#sec-validation"><span>Section&nbsp;8.5</span></a>: the more terms we add to our dictionary, the more false positives we will get: articles about the geographical space called “market”, about some celebrity being seen in “company” of someone else, and so on.</p>
<p>From this example, we can conclude that often (1) it is easy for humans to decide to which class a text belongs, but (2) it is very hard for humans to come up with a list of words (or rules) on which their judgment is based. Such a situation is perfect for applying supervised machine learning: after all, it won’t take us much time to annotate, say, 1000 articles based on whether they are about the economy or not (probably this takes less time than thoroughly fine tuning a list of words to include or exclude); and the difficult part, deciding on the exact rules underlying the decision to classify an article as economic is done by the computer in seconds. Supervised machine learning, therefore, has replaced dictionary approaches in many areas.</p>
<p>Both dictionary (or rule-based) approaches and supervised machine learning assume that you know in advance which categories (positive versus negative; sports versus economy versus politics; …) exist. The big strength of unsupervised approaches such as topic models is that you can also apply them without this knowledge. They therefore allow you to find patterns in data that you did not expect and can generate new insights. This makes them particularly suitable for explorative research questions. Using them for confirmatory tests, in contrast, is less defensible: after all, if we are interested in knowing whether, say, news site A published more about the economy than news site B, then it would be a bit weird to pretend not to know that the topic “economy” exists. Also practically, mapping the resulting topics that the topic model produces onto such <em>a priori</em> existing categories can be challenging.</p>
<p>Despite all differences, all approaches share one requirement: you need to “Validate. Validate. Validate” <span class="citation" data-cites="Grimmer2013">(<a href="references.html#ref-Grimmer2013" role="doc-biblioref">Grimmer and Stewart 2013</a>)</span>. Though it has been done in the past, simply applying a dictionary without comparing the performance to manual coding of the same concepts is not acceptable; neither is using a supervised machine learning classifier without doing the same; or blindly trusting a topic model without at least manually checking whether the scores the model assigns to documents really capture what the documents are about.</p>
</section>
<section id="sec-reviewdataset" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="sec-reviewdataset"><span class="header-section-number">11.2</span> Obtaining a Review Dataset</h2>
<p>For the sections on dictionary and supervised approaches we will use a dataset of movie reviews from the IMDB database <span class="citation" data-cites="aclimdb">(<a href="references.html#ref-aclimdb" role="doc-biblioref">Maas et al. 2011</a>)</span>. This dataset is published as a compressed set of folders, with separate folders for the train and test datasets and subfolders for positive and negative reviews. Lots of other review datasets are available online, for example for Amazon review data (<a href="https://jmcauley.ucsd.edu/data/amazon/">jmcauley.ucsd.edu/data/amazon/</a>).</p>
<p>The IMDB dataset we will use is a relatively large file and it requires bit of processing, so it is smart to <em>cache</em> the data rather than downloading and processing it every time you need it. This is done in <a href="#exm-reviewdata">Example&nbsp;<span>11.1</span></a>, which also serves as a nice example of how to download and process files. Both R and Python follow the same basic pattern. First, we check whether the cached file exists, and if it does we read the data from that file. For R, we use the standard <em>RDS</em> format, while for Python we use a compressed <em>pickle</em> file. The format of the data is also slightly different, following the convention for each language: In R we use the data frame returned by <code>readtext</code>, which can read files from a folder or zip archive and return a data frame containing one text per row. In Python, we have separate lists for the train and test datasets and for the full texts and labels: <code>text_train</code> are the training texts and <code>y_train</code> are the corresponding labels.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-reviewdata" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.1 </strong></span>Downloading and caching IMDB review data.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"reviewdata.pickle.bz2"</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(filename):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Using cached file </span><span class="sc">{</span>filename<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> bz2.BZ2File(filename, <span class="st">"r"</span>) <span class="im">as</span> zipfile:</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> pickle.load(zipfile)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    text_train, text_test, y_train, y_test <span class="op">=</span> data</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    url <span class="op">=</span> <span class="st">"https://cssbook.net/d/aclImdb_v1.tar.gz"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Downloading from </span><span class="sc">{</span>url<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    fn, _headers <span class="op">=</span> urllib.request.urlretrieve(url, filename<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> tarfile.<span class="bu">open</span>(fn, mode<span class="op">=</span><span class="st">"r:gz"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    text_train, text_test <span class="op">=</span> [], []</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> [], []</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f <span class="kw">in</span> t.getmembers():</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> re.match(<span class="st">"aclImdb/(\w+)/(pos|neg)/"</span>, f.name)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> m:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># skip folder names, other categories</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        dataset, label <span class="op">=</span> m.groups()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> t.extractfile(f).read().decode(<span class="st">"utf-8"</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> dataset <span class="op">==</span> <span class="st">"train"</span>:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            text_train.append(text)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            y_train.append(label)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> dataset <span class="op">==</span> <span class="st">"test"</span>:</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            text_test.append(text)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>            y_test.append(label)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> text_train, text_test, y_train, y_test</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Saving to </span><span class="sc">{</span>filename<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> bz2.BZ2File(filename, <span class="st">"w"</span>) <span class="im">as</span> zipfile:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        pickle.dump(data, zipfile)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading from https://cssbook.net/d/aclImdb_v1.tar.gz
Saving to reviewdata.pickle.bz2</code></pre>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell" data-hash="chapter11_cache/html/data-r_96bedd7d1a63be109bf786b566775552">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>filename <span class="ot">=</span> <span class="st">"reviewdata.rds"</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">file.exists</span>(filename)) {</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">"Using cached data"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    reviewdata<span class="ot">=</span> <span class="fu">readRDS</span>(filename)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">"Downloading data"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    fn <span class="ot">=</span> <span class="st">"aclImdb_v1.tar.gz"</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    url <span class="ot">=</span> <span class="fu">glue</span>(<span class="st">"https://cssbook.net/d/{fn}"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">download.file</span>(url, fn)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">untar</span>(fn)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    reviewdata <span class="ot">=</span> <span class="fu">readtext</span>(</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">file.path</span>(<span class="st">"aclImdb"</span>, <span class="st">"*"</span>, <span class="st">"*"</span>, <span class="st">"*.txt"</span>), </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">docvarsfrom =</span> <span class="st">"filepaths"</span>, <span class="at">dvsep=</span><span class="st">"[/</span><span class="sc">\\</span><span class="st">]"</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">docvarnames=</span><span class="fu">c</span>(<span class="st">"i"</span>,<span class="st">"dataset"</span>,<span class="st">"label"</span>,<span class="st">"fn"</span>))</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unlink</span>(<span class="fu">c</span>(<span class="st">"aclImdb"</span>, fn), <span class="at">recursive=</span><span class="cn">TRUE</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    reviewdata <span class="ot">=</span> reviewdata <span class="sc">%&gt;%</span> </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>      <span class="fu">filter</span>(label <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"pos"</span>, <span class="st">"neg"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(<span class="sc">-</span>i) <span class="sc">%&gt;%</span> </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>      <span class="fu">corpus</span>()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">saveRDS</span>(reviewdata, filename)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Downloading data"</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>If the cached data file does not exist yet, the file is downloaded from the Internet. In R, we then extract the file and call <code>readtext</code> on the resulting folder. This automatically creates columns for the subfolders, so in this case for the dataset and label. After this, we remove the download file and the extracted folder, clean up the <code>reviewdata</code>, and save it to the <code>reviewdata.rds</code> file. In Python, we can extract files from the downloaded file directly, so we do not need to explicitly extract it. We loop over all files in the archive, and use a regular expression to select only text files and extract the label and dataset name (see <a href="chapter09.html#sec-regular"><span>Section&nbsp;9.2</span></a> for more information about regular expressions). Then, we extract the text from the archive, and add the text and the label to the appropriate list. Finally, the data is saved as a compressed pickle file, so the next time we run this cell it does not need to download the file again.</p>
</section>
<section id="sec-dictionary" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="sec-dictionary"><span class="header-section-number">11.3</span> Dictionary Approaches to Text Analysis</h2>
<p>A straightforward way to automatically analyze text is to compile a list of terms you are interested in and simply count how often they occur in each document. For example, if you are interested in finding out whether mentions of political parties in news articles change over the years, you only need to compile a list of all party names and write a small script to count them.</p>
<p>Historically, this is how sentiment analysis was done. Example <a href="#exm-sentsimple"><span>11.2</span></a> shows how to do a simple sentiment analysis based on a list of positive and negative words. The logic is straightforward: you count how often each positive word occurs in a text, you do the same for the negative words, and then determine which occur more often.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-sentsimple" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.2 </strong></span>Different approaches to a simple dictionary-based sentiment analysis: counting and summing all words using a for-loop over all reviews (Python) versus constructing a term-document matrix and looking up the words in there (R). Note that both approaches would be possible in either language.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="cell" data-hash="chapter11_cache/html/sentsimple-python_52824e69b2aef9b05b28c4e0e62eccc3">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>poswords <span class="op">=</span> <span class="st">"https://cssbook.net/d/positive.txt"</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>negwords <span class="op">=</span> <span class="st">"https://cssbook.net/d/negative.txt"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> <span class="bu">set</span>(requests.get(poswords).text.split(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>neg <span class="op">=</span> <span class="bu">set</span>(requests.get(negwords).text.split(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>sentimentdict <span class="op">=</span> {word: <span class="op">+</span><span class="dv">1</span> <span class="cf">for</span> word <span class="kw">in</span> pos}</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>sentimentdict.update({word: <span class="op">-</span><span class="dv">1</span> <span class="cf">for</span> word <span class="kw">in</span> neg})</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>mytokenizer <span class="op">=</span> TreebankWordTokenizer()</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># For speed, we only take the first 100 reviews</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> review <span class="kw">in</span> text_train[:<span class="dv">100</span>]:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> mytokenizer.tokenize(review)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we look up each word in the sentiment dict</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and assign its value (with default 0)</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    scores.append(<span class="bu">sum</span>(sentimentdict.get(word, <span class="dv">0</span>) <span class="cf">for</span> word <span class="kw">in</span> words))</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[-3, -4, 1, 3, -2, -7, -6, 9, 7, 7, 10, 5, -1, 2, 7, -4, 2, 21, 1, -1, 2, -...</code></pre>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="cell" data-hash="chapter11_cache/html/sentsimple-r_e537a15bc7d1e11df8ff4f9ff87ffb5b">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>poswords <span class="ot">=</span> <span class="st">"https://cssbook.net/d/positive.txt"</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>negwords <span class="ot">=</span> <span class="st">"https://cssbook.net/d/negative.txt"</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>pos <span class="ot">=</span> <span class="fu">scan</span>(poswords, <span class="at">what=</span><span class="st">"list"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>neg <span class="ot">=</span> <span class="fu">scan</span>(negwords, <span class="at">what=</span><span class="st">"list"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>sentimentdict <span class="ot">=</span> <span class="fu">dictionary</span>(<span class="fu">list</span>(<span class="at">pos=</span>pos, <span class="at">neg=</span>neg))</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># For speed, we only take the first 100 reviews</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>scores <span class="ot">=</span> <span class="fu">corpus_sample</span>(reviewdata, <span class="dv">100</span>)  <span class="sc">%&gt;%</span> </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>() <span class="sc">%&gt;%</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_lookup</span>(sentimentdict) <span class="sc">%&gt;%</span> </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">convert</span>(<span class="at">to=</span><span class="st">"data.frame"</span>)  <span class="sc">%&gt;%</span> </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sent =</span> pos <span class="sc">-</span> neg)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                            doc_id pos neg sent
1 test/pos/12431_8.txt/12431_8.txt   9  10   -1
2  train/neg/3385_1.txt/3385_1.txt  25  13   12
3  train/neg/4824_2.txt/4824_2.txt   3   4   -1
4  train/neg/3605_1.txt/3605_1.txt   5   8   -3
5   test/neg/8667_3.txt/8667_3.txt  20   9   11
6   test/neg/9270_4.txt/9270_4.txt   8  11   -3</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>As you may already realize, there are a lot of downsides to this approach. Most notably, our bag-of-words approach does not allow us to account for negation: “not good” will be counted as positive. Relatedly, we cannot handle modifiers such as “very good”. Also, all words are either positive or negative, while “great” should be more positive than “good”. More advanced dictionary-based sentiment analysis packages like Vader <span class="citation" data-cites="Hutto2014">(<a href="references.html#ref-Hutto2014" role="doc-biblioref">Hutto and Gilbert 2014</a>)</span> or SentiStrength <span class="citation" data-cites="Thelwall2012">(<a href="references.html#ref-Thelwall2012" role="doc-biblioref">Thelwall, Buckley, and Paltoglou 2012</a>)</span> include such functionalities. Yet, as we will discuss in Section <a href="#sec-supervised"><span>11.4</span></a>, also these off-the-shelf packages perform very poorly in many sentiment analysis tasks, especially outside of the domains they were developed for. Dictionary-based sentiment analysis has been shown to be problematic when analyzing news content [e.g. <span class="citation" data-cites="Gonzalez-Bailon2015">Gonzalez-Bailon and Paltoglou (<a href="references.html#ref-Gonzalez-Bailon2015" role="doc-biblioref">2015</a>)</span>;@ Boukes2019]. They are problematic when accuracy at the sentence level is important, but may be satisfactory with longer texts for comparatively easy tasks such as movie review classification <span class="citation" data-cites="Reagan2017">(<a href="references.html#ref-Reagan2017" role="doc-biblioref">Reagan et al. 2017</a>)</span>, where there is clear ground truth data and the genre convention implies that the whole text is evaluative and evaluates one object (the film).</p>
<p>Still, there are many use cases where dictionary approaches work very well. Because your list of words can contain anything, not just positive or negative words, dictionary approaches have been used, for instance, to measure the use of racist words or swearwords in online fora <span class="citation" data-cites="Tulkens2016">(e.g., <a href="references.html#ref-Tulkens2016" role="doc-biblioref">Tulkens et al. 2016</a>)</span>. Dictionary approaches are simple to understand and straightforward, which can be a good argument for using them when it is important that the method is no black-box but fully transparent even without technical knowledge. Especially when the dictionary already exists or is easy to create, it is also a very cheap method. However, this is at the expense of their limitation to only performing well when measuring easy to operationalize concepts. To put it bluntly: it’s great for measuring the visibility of parties or organizations in the news, but it’s not good for measuring concepts such as emotions or frames.</p>
<p>What gave dictionary approaches a bit of a bad name is that many researchers applied them without validating them. This is especially problematic when a dictionary is applied in a slightly different domain than that for which it was originally made.</p>
<p>If you want to use a dictionary-based approach, we advise the following procedure:</p>
<ul>
<li>Construct a dictionary based on theoretical considerations and by closely reading a sample of example texts.
<ul>
<li>Code some articles manually and compare with the automated coding.</li>
<li>Improve your dictionary and check again.</li>
<li>Manually code a validation dataset of sufficient size. The required size depends a bit on how balanced your data is – if one code occurs very infrequently, you will need more data.</li>
<li>Calculate the agreement. You could use standard intercoder reliability measures used in manual content analysis, but we would also advise you to calculate precision and recall (see Section <a href="chapter08.html#sec-validation"><span>8.5</span></a>).</li>
</ul></li>
</ul>
<p>Very extensive dictionaries will have a high recall (it becomes increasingly unlikely that you “miss” a relevant document), but often suffer from low precision (more documents will contain one of the words even though they are irrelevant). Vice versa, a very short dictionary will often be very precise, but miss a lot of documents. It depends on your research question where the right balance lies, but to substantially interpret your results, you need to be able to quantify the performance of your dictionary-based approach.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
How many documents do you need to calculate agreement with human annotators?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To determine the number of documents one needs to determine the agreement between a human and a machine, one can follow the same standards that are recommended for traditional manual content analysis.</p>
<p>For instance, <span class="citation" data-cites="Krippendorff2004">Krippendorff (<a href="references.html#ref-Krippendorff2004" role="doc-biblioref">2004</a>)</span> provides a convenience table to look up the required sample sizes for determining the agreement between two human coders (p.&nbsp;240). <span class="citation" data-cites="Riffe2019">Riffe et al. (<a href="references.html#ref-Riffe2019" role="doc-biblioref">2019</a>)</span> provide similar suggestions (p.&nbsp;114). In short, the sample size depends on the level of statistical significance the researcher deems acceptable as well as on the distribution of the data. In an extreme case, if only 5 out of 100 items are to be coded as <span class="math inline">\(x\)</span>, then in a sample of 20 items, such an item may not even occur. In order to determine agreement between the automated method and a human, we suggest that sample sizes that one would also use for the calculation of agreement between human coders are used. For specific calculations, we refer to content analysis books such as the two referenced here. To give a very rough ballpark figure (that shouldn’t replace a careful calculation!), roughly 100 to 200 items will cover many scenarios (assuming a small amount of reasonably balanced classes).</p>
</div>
</div>
</div>
</section>
<section id="sec-supervised" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="sec-supervised"><span class="header-section-number">11.4</span> Supervised Text Analysis: Automatic Classification and Sentiment Analysis</h2>
<p>For many applications, there are good reasons to use the dictionary approach presented in the previous section. First, it is intuitively understandable and results can – in principle – even be verified by hand, which can be an advantage when transparency or communicability is of high importance. Second, it is very easy to use. But as we have discussed in <a href="#sec-deciding"><span>Section&nbsp;11.1</span></a>, dictionary approaches in general perform less well the more abstract, non-manifest, or complex a concept becomes. In the next section, we will make the case that topics, but also sentiment, in fact, are quite a complex concepts that are often hard to capture with dictionaries (or at least, crafting a custom dictionary would be difficult). For instance, while “positive” and “negative” seem straightforward categories at first sight, the more we think about it, the more apparent it becomes how context-dependent it actually is: in a dataset about the economy and stock market returns, “increasing” may indicate something positive, in a dataset about unemployment rates the same word would be something negative. Thus, machine learning can be a more appropriate technique for such tasks.</p>
<section id="sec-workflow" class="level3" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="sec-workflow"><span class="header-section-number">11.4.1</span> Putting Together a Workflow</h3>
<p>With the knowledge we gained in previous chapters, it is not difficult to set up a supervised machine learning classifier to automatically determine, for instance, the topic of a news article.</p>
<p>Let us recap the building blocks that we need. In <a href="chapter08.html"><span>Chapter&nbsp;8</span></a>, you learned how to use different classifiers, how to evaluate them, and how to choose the best settings. However, in these examples, we used numerical data as features; now, we have text. In <a href="chapter10.html"><span>Chapter&nbsp;10</span></a>, you learned how to turn text into numerical features. And that’s all we need to get started!</p>
<p>Typical examples for supervised machine learning in the analysis of communication include the classification of topics <span class="citation" data-cites="Scharkow2011">(e.g., <a href="references.html#ref-Scharkow2011" role="doc-biblioref">Scharkow 2011</a>)</span>, frames <span class="citation" data-cites="Burscher2014">(e.g., <a href="references.html#ref-Burscher2014" role="doc-biblioref">Burscher et al. 2014</a>)</span>, user characteristics such as gender or ideology, or sentiment.</p>
<p>Let us consider the case of sentiment analysis in more detail. Classical sentiment analysis is done with a dictionary approach: you take a list of positive words, a list of negative words, and count which occur more frequently. Additionally, one may attach a weight to each word, such that “perfect” gets a higher weight than “good”, for instance. An obvious drawback is that these pure bag-of-words approaches cannot cope with negation (“not good”) and intensifiers (“very good”), which is why extensions have been developed that take these (and other features, such as punctuation) into account <span class="citation" data-cites="Thelwall2012 Hutto2014 DeSmedt2012">(<a href="references.html#ref-Thelwall2012" role="doc-biblioref">Thelwall, Buckley, and Paltoglou 2012</a>; <a href="references.html#ref-Hutto2014" role="doc-biblioref">Hutto and Gilbert 2014</a>; <a href="references.html#ref-DeSmedt2012" role="doc-biblioref">De Smedt, Daelemans, and Smedt 2012</a>)</span>.</p>
<p>But while available off-the-shelf packages that implement these extended dictionary-based methods are very easy to use (in fact, they spit out a sentiment score with one single line of code), it is questionable how well they work in practice. After all, “sentiment” is not exactly a clear, manifest concept for which we can enumerate a list of words. It has been shown that results obtained with multiple of these packages correlate very poorly with each other and with human annotations <span class="citation" data-cites="Boukes2019 Chan2021">(<a href="references.html#ref-Boukes2019" role="doc-biblioref">Boukes et al. 2019</a>; <a href="references.html#ref-Chan2021" role="doc-biblioref">Chan et al. in press</a>)</span>.</p>
<p>Consequently, it has been suggested that it is better to use supervised machine learning to automatically code the sentiment of texts <span class="citation" data-cites="Gonzalez-Bailon2015 vermeer2019seeing">(<a href="references.html#ref-Gonzalez-Bailon2015" role="doc-biblioref">Gonzalez-Bailon and Paltoglou 2015</a>; <a href="references.html#ref-vermeer2019seeing" role="doc-biblioref">Vermeer et al. 2019</a>)</span>. However, you may need to annotate documents from your own dataset: training a classifier on, for instance, movie reviews and then using it to predict sentiment in political texts violates the assumption that training set, test set, and the unlabeled data that are to be classified are (at least in principle and approximately) drawn from the same population.</p>
<p>To illustrate the workflow, we will use the ACL IMDB dataset, a large dataset that consists of a training dataset of 25000 movie reviews (of which 12500 are positive and 12500 are negative) and an equally sized test dataset <span class="citation" data-cites="aclimdb">(<a href="references.html#ref-aclimdb" role="doc-biblioref">Maas et al. 2011</a>)</span>. It can be downloaded at <a href="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></p>
<p>These data do not come in one file, but rather in a set of text files that are sorted in different folders named after the dataset to which they belong (<code>test</code> or <code>train</code>) and their label (<code>pos</code> and <code>neg</code>). This means that we cannot simply use a pre-defined function to read them, but we need to think of a way of reading the content into a data structure that we can use. This data was loaded in <a href="#exm-reviewdata">Example&nbsp;<span>11.1</span></a> above.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Sparse versus dense matrices in Python and R
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In a document-term matrix, you would typically find a lot of zeros: most words do <em>not</em> appear in any given document. For instance, the reviews in the IMDB dataset contain more than 100000 unique words. Hence, the matrix has more than 100000 columns. Yet, most reviews only consist of a couple of hundred words. As a consequence, more than 99% of the cells in the table contain a zero. In a sparse matrix, we do not store all these zeros, but only store the values for cells that actually contain a value. This drastically reduces the memory needed. But even if you have a huge amount of memory, this does not solve the issue: in R, the number of cells in a matrix is limited to 2147483647. It is therefore impossible to store a matrix with 100000 features and 25000 documents as a dense matrix. Unfortunately, many models that you can run via <em>caret</em> in R will convert your sparse document-term matrix to a dense matrix, and hence are effectively only usable for very small datasets. An alternative is using the <em>quanteda</em> package, which does use sparse matrices throughout. However, at the time of writing this book, quanteda only provides a very limited number of models. As all of these problems do not arise in <em>scikit-learn</em>, you may want to consider using Python for many text classification tasks.</p>
</div>
</div>
</div>
<p>Let us now train our first classifier. We choose a Naïve Bayes classifier with a simple count vectorizer (<a href="#exm-imdbbaseline">Example&nbsp;<span>11.3</span></a>). In the Python example, pay attention to the fitting of the vectorizer: we fit on the training data <em>and</em> transform the training data with it, but we only transform the test data <em>without re-fitting the vectorizer</em>. Fitting, here, includes the decision about which words to include (by definition, words that are not present in the training data are not included; but we could also choose additional constraints, such as excluding very rare or very common words), but also assigning an (internally used) identifier (variable name) to each word. If we fit the classifier again, these would not be compatible any more. In R, the same is achieved in a slightly different way: two term-document matrices are created independently, before they are matched in such a way that only the features that are present in the training matrix are retained in the test matrix.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A word that is not present in the training data, but is present in the test data, is thus ignored. If you want to use the information such out-of-vocabulary words can entail (e.g., they may be synonyms), consider using a word embedding approach (see <a href="chapter10.html#sec-wordembeddings"><span>Section&nbsp;10.3.3</span></a>)</p>
</div>
</div>
</div>
<p>We do not necessarily expect this first model to be the best classifier we can come up with, but it provides us with a reasonable baseline. In fact, even without any further adjustments, it works reasonably well: precision is higher for positive reviews and recall is higher for negative reviews (classifying a positive review as negative happens twice as much as the reverse), but none of the values is concerningly low.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-imdbbaseline" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.3 </strong></span>Training a Naïve Bayes classifier with simple word counts as features</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="cell" data-hash="chapter11_cache/html/imdbbaseline-python_04abfca35c0f8f816b89ca70bfab7bbf">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(stop_words<span class="op">=</span><span class="st">"english"</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> vectorizer.fit_transform(text_train)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> vectorizer.transform(text_test)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>nb <span class="op">=</span> MultinomialNB()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>nb.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MultinomialNB()</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> nb.predict(X_test)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>rep <span class="op">=</span> metrics.classification_report(y_test, y_pred)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rep)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

         neg       0.79      0.88      0.83     12500
         pos       0.86      0.76      0.81     12500

    accuracy                           0.82     25000
   macro avg       0.82      0.82      0.82     25000
weighted avg       0.82      0.82      0.82     25000</code></pre>
</div>
</div>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<div class="cell" data-hash="chapter11_cache/html/imdbbaseline-r_b0095c2b7a31ebfbddd833d5476c7758">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>dfm_train <span class="ot">=</span> reviewdata <span class="sc">%&gt;%</span> </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">corpus_subset</span>(dataset <span class="sc">==</span> <span class="st">"train"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>() <span class="sc">%&gt;%</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_trim</span>(<span class="at">min_docfreq=</span><span class="fl">0.01</span>, <span class="at">docfreq_type=</span><span class="st">"prop"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>dfm_test <span class="ot">=</span> reviewdata <span class="sc">%&gt;%</span> </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">corpus_subset</span>(dataset <span class="sc">==</span> <span class="st">"test"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>() <span class="sc">%&gt;%</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_match</span>(<span class="fu">featnames</span>(dfm_train))</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>myclassifier <span class="ot">=</span> <span class="fu">textmodel_nb</span>(dfm_train, </span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">docvars</span>(dfm_train, <span class="st">"label"</span>))</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>predicted <span class="ot">=</span> <span class="fu">predict</span>(myclassifier,<span class="at">newdata=</span>dfm_test)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>actual <span class="ot">=</span> <span class="fu">docvars</span>(dfm_test, <span class="st">"label"</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>results <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (label <span class="cf">in</span> <span class="fu">c</span>(<span class="st">"pos"</span>, <span class="st">"neg"</span>)) {</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>  results[[label]] <span class="ot">=</span> <span class="fu">tibble</span>(</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">Precision=</span><span class="fu">Precision</span>(actual, predicted, label),</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">Recall=</span><span class="fu">Recall</span>(actual, predicted, label),</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">F1=</span><span class="fu">F1_Score</span>(actual, predicted, label))</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(results, <span class="at">.id=</span><span class="st">"label"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  label Precision Recall    F1
  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
1 pos       0.825  0.794 0.809
2 neg       0.801  0.832 0.816</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-bestclassifier" class="level3" data-number="11.4.2">
<h3 data-number="11.4.2" class="anchored" data-anchor-id="sec-bestclassifier"><span class="header-section-number">11.4.2</span> Finding the Best Classifier</h3>
<p>Let us start by comparing the two simple classifiers we know (Naïve Bayes and Logistic Regression (see <a href="chapter08.html#sec-nb2dnn"><span>Section&nbsp;8.3</span></a>) and the two vectorizers that transform our texts into two numerical representations that we know: word counts and <code>tf.idf</code> scores (see <a href="chapter10.html"><span>Chapter&nbsp;10</span></a>).</p>
<p>We can also tune some things in the vectorizer, such as filtering out stopwords, or specifying a minimum number (or proportion) of documents in which a word needs to occur in order to be included, or the maximum number (or proportion) of documents in which it is allowed to occur. For instance, it could make sense to say that a word that occurs in less than <span class="math inline">\(n=5\)</span> documents is probably a spelling mistake or so unusual that it just unnecessarily bloats our feature matrix; and on the other hand, a word that is so common that it occurs in more than 50% of all documents is so common that it does not help us to distinguish between different classes.</p>
<p>We can try all of these things out by hand by just re-running the code from <a href="#exm-imdbbaseline">Example&nbsp;<span>11.3</span></a> and only changing the line in which the vectorizer is specified and the line in which the classifier is specified. However, copy-pasting essentially the same code is generally not a good idea, as it makes your code unnecessary long and increases the likelihood of errors creeping in when you, for instance, need to apply the same changes to multiple copies of the code. A more elegant approach is outlined in <a href="#exm-basiccomparisons">Example&nbsp;<span>11.4</span></a>: We define a function that gives us a short summary of only the output we are interested in, and then use a for-loop to iterate over all configurations we want to evaluate, fit them and call the function we defined before. In fact, with 23 lines of code, we manage to compare four different models, while we already needed 15 lines (in <a href="#exm-imdbbaseline">Example&nbsp;<span>11.3</span></a>) to evaluate only one model.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-basiccomparisons" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.4 </strong></span>An example of a custom function to give a brief overview of the performance of four simple vectorizer-classifier combinations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> short_classification_report(y_test, y_pred):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"    </span><span class="ch">\t</span><span class="st">Precision</span><span class="ch">\t</span><span class="st">Recall"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label <span class="kw">in</span> <span class="bu">set</span>(y_pred):</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        pr <span class="op">=</span> metrics.precision_score(y_test, y_pred, pos_label<span class="op">=</span>label)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        re <span class="op">=</span> metrics.recall_score(y_test, y_pred, pos_label<span class="op">=</span>label)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">:</span><span class="ch">\t</span><span class="sc">{</span>pr<span class="sc">:0.2f}</span><span class="ch">\t\t</span><span class="sc">{</span>re<span class="sc">:0.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="chapter11_cache/html/basiccomparisons-python_2016a9f323b9671244a1555277d66820">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>configs <span class="op">=</span> [</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"NB-count"</span>, CountVectorizer(min_df<span class="op">=</span><span class="dv">5</span>, max_df<span class="op">=</span><span class="fl">0.5</span>), MultinomialNB()),</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"NB-TfIdf"</span>, TfidfVectorizer(min_df<span class="op">=</span><span class="dv">5</span>, max_df<span class="op">=</span><span class="fl">0.5</span>), MultinomialNB()),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LR-Count"</span>,</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        CountVectorizer(min_df<span class="op">=</span><span class="dv">5</span>, max_df<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>),</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LR-TfIdf"</span>,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        TfidfVectorizer(min_df<span class="op">=</span><span class="dv">5</span>, max_df<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>),</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, vectorizer, classifier <span class="kw">in</span> configs:</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> vectorizer.fit_transform(text_train)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> vectorizer.transform(text_test)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    classifier.fit(X_train, y_train)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> classifier.predict(X_test)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    short_classification_report(y_test, y_pred)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>NB-count
MultinomialNB()
        Precision   Recall
pos:    0.87        0.77
neg:    0.79        0.88


NB-TfIdf
MultinomialNB()
        Precision   Recall
pos:    0.87        0.78
neg:    0.80        0.88


LR-Count
LogisticRegression(solver='liblinear')
        Precision   Recall
pos:    0.87        0.85
neg:    0.85        0.87


LR-TfIdf
LogisticRegression(solver='liblinear')
        Precision   Recall
pos:    0.89        0.88
neg:    0.88        0.89</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The output of this little example already gives us quite a bit of insight into how to tackle our specific classification tasks: first, we see that a <span class="math inline">\(tf\cdot idf\)</span> classifier seems to be slightly but consistently superior to a count classifier (this is often, but not always the case). Second, we see that the logistic regression performs better than the Naïve Bayes classifier (again, this is often, but not always, the case). In particular, in our case, the logistic regression improved on the excessive misclassification of positive reviews as negative, and achieves a very balanced performance.</p>
<p>There may be instances where one nevertheless may want to use a Count Vectorizer with a Naïve Bayes classifier instead (especially if it is too computationally expensive to estimate the other model), but for now, we may settle on the best performing combination, logistic regression with a <code>tf.idf</code> vectorizer. You could also try fitting a Support Vector Machine instead, but we have little reason to believe that our data isn’t linearly separable, which means that there is little reason to believe that the SVM will perform better. Given the good performance we already achieved, we decide to stick to the logistic regression for now.</p>
<p>We can now go as far as we like, include more models, use crossvalidation and gridsearch (see <a href="chapter08.html#sec-crossvalidation"><span>Section&nbsp;8.5.3</span></a>), etc. However, our workflow now consists of <em>two</em> steps: fitting/transforming our input data using a vectorizer, and fitting a classifier. To make things easier, in scikit-learn, both steps can be combined into a so-called pipe. <a href="#exm-basicpipe">Example&nbsp;<span>11.5</span></a> shows how the loop in <a href="#exm-basiccomparisons">Example&nbsp;<span>11.4</span></a> can be re-written using pipes (the result stays the same).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-basicpipe" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.5 </strong></span>Instead of fitting vectorizer and classifier separately, they can be combined in a pipeline.</p>
<div class="cell" data-hash="chapter11_cache/html/basicpipe-python_89e2606af81fc435e465ddcaa6e8f4c6">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, vectorizer, classifier <span class="kw">in</span> configs:</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    pipe <span class="op">=</span> make_pipeline(vectorizer, classifier)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    pipe.fit(text_train, y_train)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> pipe.predict(text_test)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    short_classification_report(y_test, y_pred)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>NB-count
Pipeline(steps=[('countvectorizer', CountVectorizer(max_df=0.5, min_df=5)),
                ('multinomialnb', MultinomialNB())])
        Precision   Recall
pos:    0.87        0.77
neg:    0.79        0.88


NB-TfIdf
Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_df=0.5, min_df=5)),
                ('multinomialnb', MultinomialNB())])
        Precision   Recall
pos:    0.87        0.78
neg:    0.80        0.88


LR-Count
Pipeline(steps=[('countvectorizer', CountVectorizer(max_df=0.5, min_df=5)),
                ('logisticregression', LogisticRegression(solver='liblinear'))])
        Precision   Recall
pos:    0.87        0.85
neg:    0.85        0.87


LR-TfIdf
Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_df=0.5, min_df=5)),
                ('logisticregression', LogisticRegression(solver='liblinear'))])
        Precision   Recall
pos:    0.89        0.88
neg:    0.88        0.89</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Such a pipeline lends itself very well to performing a gridsearch. <a href="#exm-gridsearchlogreg">Example&nbsp;<span>11.6</span></a> gives you an example. With <code>LogisticRegression?</code> and <code>TfIdfVectorizer?</code>, we can get a list of all possible hyperparameters that we may want to tune. For instance, these could be the minimum and maximum frequency for words to be included or whether we want to use only unigrams (single words) or also bigrams (combinations of two words, see <a href="chapter10.html#sec-ngram"><span>Section&nbsp;10.3</span></a>). For the Logistic Regression, it may be the regularization hyperparameter C, which applies a penalty for too complex models. We can put all values for these parameters that we want to consider in a dictionary, with a descriptive key (i.e., a string with the step of the pipeline followed by two underscores and the name of the hyperparameter) and a list of all values we want to consider as the corresponding value.</p>
<p>The gridsearch procedure will then estimate all combinations of all values, using cross-validation (see <a href="chapter08.html#sec-validation"><span>Section&nbsp;8.5</span></a>). In our example, we have <span class="math inline">\(2 x 2 x 2 x 2 x 3 = 24\)</span> different models, and <span class="math inline">\(24 models x 5 folds = 120\)</span> models to estimate. Hence, it may take you some time to run the code.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-gridsearchlogreg" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.6 </strong></span>A gridsearch to find the best hyperparameters for a pipeline consisting of a vectorizer and a classifier. Note that we can tune any parameter that either the vectorizer or the classifier accepts as an input, not only the four hyperparameters we chose in this example.</p>
<div class="cell" data-hash="chapter11_cache/html/gridsearchlogreg-python_747ffb3fd74d0827f490a48c008643ef">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    steps<span class="op">=</span>[</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"vectorizer"</span>, TfidfVectorizer()),</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"classifier"</span>, LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>)),</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> {</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vectorizer__ngram_range"</span>: [(<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">2</span>)],</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vectorizer__max_df"</span>: [<span class="fl">0.5</span>, <span class="fl">1.0</span>],</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vectorizer__min_df"</span>: [<span class="dv">0</span>, <span class="dv">5</span>],</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"classifier__C"</span>: [<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="dv">100</span>],</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>search <span class="op">=</span> GridSearchCV(</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>pipeline, n_jobs<span class="op">=-</span><span class="dv">1</span>, param_grid<span class="op">=</span>grid, scoring<span class="op">=</span><span class="st">"accuracy"</span>, cv<span class="op">=</span><span class="dv">5</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>search.fit(text_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GridSearchCV(cv=5,
             estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),
                                       ('classifier',
                                        LogisticRegression(solver='liblinear'))]),
             n_jobs=-1,
             param_grid={'classifier__C': [0.01, 1, 100],
                         'vectorizer__max_df': [0.5, 1.0],
                         'vectorizer__min_df': [0, 5],
                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},
             scoring='accuracy')</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best parameters: </span><span class="sc">{</span>search<span class="sc">.</span>best_params_<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters: {'classifier__C': 100, 'vectorizer__max_df': 0.5, 'vectori...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> search.predict(text_test)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(short_classification_report(y_test, pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Precision   Recall
pos:    0.90        0.90
neg:    0.90        0.90
None</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<p>We see that we could further improve our model to precision and recall values of 0.90, by excluding extremely infrequent and extremely frequent words, including both unigrams and bigrams (which, we may speculate, help us to account for the “not good” versus “not”, “good” problem), and changing the default penalty of <span class="math inline">\(C=1\)</span> to <span class="math inline">\(C=100\)</span>.</p>
<p>Let us, just for the sake of it, compare the performance of our model with an off-the-shelf sentiment analysis package, in this case Vader <span class="citation" data-cites="Hutto2014">(<a href="references.html#ref-Hutto2014" role="doc-biblioref">Hutto and Gilbert 2014</a>)</span>. For any text, it will directly estimate sentiment scores (more specifically, a positivity score, a negativity score, a neutrality score, and a compound measure that combines them), without any need to have training data. However, as Example <a href="#exm-vader"><span>11.7</span></a> shows, such a method is clearly inferior to a supervised machine learning approach. While in almost all cases (except for <span class="math inline">\(n=11\)</span> cases), Vader was able to make a choice (getting scores of 0 is a notorious problem in very short texts), precision and recall are clearly worse than even the simple baseline model we started with, and much worse than those of the final model we finished with. In fact, we miss half (!) of the negative reviews. There are probably very few applications in the analysis of communication in which we would find this acceptable. It is important to highlight that this is not because the off-the-shelf package we chose is a particularly bad one (on the contrary, it is actually comparatively good), but because of the inherent limitations of dictionary-based sentiment analysis.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-vader" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.7 </strong></span>For the sake of comparison, we calculate how an off-the-shelf sentiment analysis package would have performed in this task</p>
<div class="cell" data-hash="chapter11_cache/html/vader-python_0358c37c584639ba1faf143ee2021d7c">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">"vader_lexicon"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True

[nltk_data] Downloading package vader_lexicon to
[nltk_data]     /home/runner/nltk_data...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>analyzer <span class="op">=</span> SentimentIntensityAnalyzer()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> []</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> review <span class="kw">in</span> text_test:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    sentiment <span class="op">=</span> analyzer.polarity_scores(review)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sentiment[<span class="st">"compound"</span>] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        pred.append(<span class="st">"pos"</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> sentiment[<span class="st">"compound"</span>] <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        pred.append(<span class="st">"neg"</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        pred.append(<span class="st">"dont know"</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(metrics.confusion_matrix(y_test, pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[    0     0     0]
 [    6  6706  5788]
 [    5  1748 10747]]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(metrics.classification_report(y_test, pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

   dont know       0.00      0.00      0.00         0
         neg       0.79      0.54      0.64     12500
         pos       0.65      0.86      0.74     12500

    accuracy                           0.70     25000
   macro avg       0.48      0.47      0.46     25000
weighted avg       0.72      0.70      0.69     25000</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<p>We need to keep in mind, though, that with this dataset, we chose one of the easiest sentiment analysis tasks: a set of long, rather formal texts (compared to informal short social media messages), that evaluate exactly one entity (one film), and that are not ambiguous at all. Many applications that communication scientists are interested in are much less straightforward. Therefore, however tempting it may be to use an off-the-shelf package, doing so requires a thorough test based on at least some human-annotated data.</p>
</section>
<section id="sec-usingmodel" class="level3" data-number="11.4.3">
<h3 data-number="11.4.3" class="anchored" data-anchor-id="sec-usingmodel"><span class="header-section-number">11.4.3</span> Using the Model</h3>
<p>So far, we have focused on training and evaluating models, almost forgetting why we were doing this in the first place: to use them to predict the label for new data that we did not annotate.</p>
<p>Of course, we could always re-train the model when we need to use it – but that has two downsides: first, as you may have seen, it may actually take considerable time to train it, and second, you need to have the training data available, which may be a problem both in terms of storage space and of copyright and/or privacy if you want to share your classifier with others.</p>
<p>Therefore, it makes sense to save both our classifier and our vectorizer to a file, so that we can reload them later (Example <a href="#exm-reuse"><span>11.8</span></a>). Keep in mind that you have to re-use <em>both</em> – after all, the columns of your feature matrix will be different (and hence, completely useless for the classifier) when fitting a new vectorizer. Therefore, as you see, you do not do any fitting any longer, and only use the <code>.transform()</code> method of the (already fitted) vectorizer and the <code>.predict()</code> method of the (already fitted) classifier.</p>
<p>In R, you have no vectorizer you could save – but because in contrast to Python, both your DTM and your classifier include the feature names, it suffices to save the classifier only (using <code>saveRDS(myclassifier, "myclassifier.rds")</code>) and using on a new DTM later on. You do need to remember, though, how you constructed the DTM (e.g., which preprocessing steps you took), to make sure that the features are comparable.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-reuse" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.8 </strong></span>Saving and loading a vectorizer and a classifier</p>
<div class="cell" data-hash="chapter11_cache/html/reuse-python_949b9952ad69f9e4acf9e55e2f4b517d">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a vectorizer and train a classifier</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(min_df<span class="op">=</span><span class="dv">5</span>, max_df<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> vectorizer.fit_transform(text_train)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>classifier.fit(X_train, y_train)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Save them to disk</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LogisticRegression(solver='liblinear')</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"myvectorizer.pkl"</span>, mode<span class="op">=</span><span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    pickle.dump(vectorizer, f)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"myclassifier.pkl"</span>, mode<span class="op">=</span><span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    joblib.dump(classifier, f)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Later on, re-load this classifier and apply:</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>new_texts <span class="op">=</span> [<span class="st">"This is a great movie"</span>, <span class="st">"I hated this one."</span>, <span class="st">"What an awful fail"</span>]</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"myvectorizer.pkl"</span>, mode<span class="op">=</span><span class="st">"rb"</span>) <span class="im">as</span> f:</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    myvectorizer <span class="op">=</span> pickle.load(f)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"myclassifier.pkl"</span>, mode<span class="op">=</span><span class="st">"rb"</span>) <span class="im">as</span> f:</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    myclassifier <span class="op">=</span> joblib.load(f)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>new_features <span class="op">=</span> myvectorizer.transform(new_texts)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> myclassifier.predict(new_features)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> review, label <span class="kw">in</span> <span class="bu">zip</span>(new_texts, pred):</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>review<span class="sc">}</span><span class="ss">' is probably '</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>'This is a great movie' is probably 'pos'.
'I hated this one.' is probably 'neg'.
'What an awful fail' is probably 'neg'.</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Another thing that we might want to do is to get a better idea of the features that the model uses to arrive at its prediction; in our example, what actually characterizes the best and the worst reviews. Example <a href="#exm-eli5"><span>11.9</span></a> shows how this can be done in one line of code using <em>eli5</em> – a package that aims to “<em>e</em>xplain [the model] <em>l</em>ike <em>I</em>’m <em>5</em> years old”. Here, we re-use the <code>pipe</code> we constructed earlier to provide both the vectorizer and the classifier to <em>eli5</em> – if we had only provided the classifier, then the feature names would have been internal identifiers (which are meaningless to us) rather than human-readable words.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-eli5" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.9 </strong></span>Using eli5 to get the most predictive features</p>
<div class="cell" data-hash="chapter11_cache/html/eli5-python_4d5c8c9263af4a00a357b3124bb25bcb">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    TfidfVectorizer(min_df<span class="op">=</span><span class="dv">5</span>, max_df<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>),</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>pipe.fit(text_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_df=0.5, min_df=5)),
                ('logisticregression', LogisticRegression(solver='liblinear'))])</code></pre>
</div>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(eli5.format_as_text(eli5.explain_weights(pipe)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Explained as: linear model

Features with largest coefficients.
Caveats:
1. Be careful with features which are not
   independent - weights don't show their importance.
2. If scale of input features is different then scale of coefficients
   will also be different, making direct comparison between coefficient values
   incorrect.
3. Depending on regularization, rare features sometimes may have high
   coefficients; this doesn't mean they contribute much to the
   classification result for most examples.

y='pos' top features
Weight  Feature  
------  ---------
+7.173  great    
+6.101  excellent
+5.055  best     
+4.791  perfect  
+4.698  wonderful
+4.181  amazing  
+4.170  well     
… 13660 more positive …
… 13567 more negative …
-4.050  dull     
-4.070  no       
-4.176  horrible 
-4.247  poorly   
-4.406  worse    
-4.599  nothing  
-4.695  terrible 
-5.337  poor     
-5.733  boring   
-6.315  waste    
-6.349  awful    
-7.347  bad      
-9.059  worst    </code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<p>We can also use eli5 to explain how the classifier arrived at a prediction for a specific document, by using different shades of green and red to explain how much different features contributed to the classification, and in which direction (Example <a href="#exm-eli5b"><span>11.10</span></a>).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-eli5b" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.10 </strong></span>Using eli5 to explain a prediction ## Python code</p>
<div class="cell" data-hash="chapter11_cache/html/eli5b-python_a1afba05fd02039627fa14549bc7d7ce">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># WvA: This doesn't work outside a notebook, should probably call other functions </span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co"># eli5.show_prediction(classifier, text_test[0], vec=vectorizer, targets=["pos"])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-deeplearning" class="level3" data-number="11.4.4">
<h3 data-number="11.4.4" class="anchored" data-anchor-id="sec-deeplearning"><span class="header-section-number">11.4.4</span> Deep Learning</h3>
<p>Deep learning models were introduced in <a href="#sec-deeplearning"><span>Section&nbsp;11.4.4</span></a> as a (relatively) new class of models in supervised machine learning. Using the Python <em>keras</em> package you can define various model architectures such as Convolutional or Recurrent Neural Networks. Although it is beyond the scope of this chapter to give a detailed treatment of building and training deep learning models, in this section we do give an example of using a Convolutional Neural Network using pre-trained word embeddings. We would urge anyone who is interested in machine learning for text analysis to continue studying deep learning, probably starting with the excellent book by <span class="citation" data-cites="goldberg2017">Goldberg (<a href="references.html#ref-goldberg2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>Impressively, in R you can now also use the <em>keras</em> package to train deep learning models, as shown in the example. Similar to how <em>spacyr</em> works (<a href="chapter10.html#sec-nlp"><span>Section&nbsp;10.3.4</span></a>), the R package actually installs and calls Python behind the screens using the <em>reticulate</em> package. Although the resulting models are relatively similar, it is less easy to build and debug the models in R because most of the documentation and community examples are written in Python. Thus in the end, we probably recommend people who want to dive into deep learning should choose Python rather than R.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-rnndata" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.11 </strong></span>Loading Dutch Sentiment Data <span class="citation" data-cites="vanatteveldt2021">(from <a href="references.html#ref-vanatteveldt2021" role="doc-biblioref">Van Atteveldt, Van der Velden, and Boukes 2021</a>)</span></p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-2" role="tab" aria-controls="tabset-6-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://cssbook.net/d/dutch_sentiment.csv"</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>h.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      id  value                                            lemmata
0  10007      0  Rabobank voorspellen flink stijging hypotheekr...
1  10027      0  D66 willen reserve provincie aanspreken voor g...
2  10037      1                             UWV dit jaar veel baan
3  10059      1                  proost op geslaagd beursgang bols
4  10099      0         helft werknemer gaan na 65ste met pensioen</code></pre>
</div>
</div>
</div>
<div id="tabset-6-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-2-tab">
<div class="cell" data-hash="chapter11_cache/html/rnndata-r_e4d1e8d8dcc1c1beec6f33b7492228f5">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>url<span class="ot">=</span><span class="st">"https://cssbook.net/d/dutch_sentiment.csv"</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">read_csv</span>(url)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 3
     id value lemmata                                             
  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                               
1 10007     0 Rabobank voorspellen flink stijging hypotheekrente  
2 10027     0 D66 willen reserve provincie aanspreken voor groei  
3 10037     1 UWV dit jaar veel baan                              
4 10059     1 proost op geslaagd beursgang bols                   
5 10099     0 helft werknemer gaan na 65ste met pensioen          
6 10101     1 Europa groeien voorzichtig dankzij laag energieprijs</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-rnnmodel" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.12 </strong></span>Deep Learning: Defining a Recursive Neural Network</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-7-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-1" role="tab" aria-controls="tabset-7-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-7-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-2" role="tab" aria-controls="tabset-7-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-7-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-7-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize texts</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(num_words<span class="op">=</span><span class="dv">9999</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(h.lemmata)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>word_index <span class="op">=</span> tokenizer.word_index</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> tokenizer.texts_to_sequences(h.lemmata)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> pad_sequences(sequences, maxlen<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare embeddings layer</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>fn <span class="op">=</span> <span class="st">"w2v_320d_trimmed"</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(fn):</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    url <span class="op">=</span> <span class="ss">f"https://cssbook.net/d/</span><span class="sc">{</span>fn<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Downloading embeddings from </span><span class="sc">{</span>url<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    urllib.request.urlretrieve(url, fn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading embeddings from https://cssbook.net/d/w2v_320d_trimmed
('w2v_320d_trimmed', &lt;http.client.HTTPMessage object at 0x7fbaa3f84820&gt;)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> KeyedVectors.load_word2vec_format(fn)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>emb_matrix <span class="op">=</span> np.zeros((<span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span>, embeddings.vector_size))</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, i <span class="kw">in</span> tokenizer.word_index.items():</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> embeddings:</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>        emb_matrix[i] <span class="op">=</span> embeddings[word]</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> Embedding(</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    emb_matrix.shape[<span class="dv">0</span>],</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    emb_matrix.shape[<span class="dv">1</span>],</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    input_length<span class="op">=</span>tokens.shape[<span class="dv">1</span>],</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    weights<span class="op">=</span>[emb_matrix],</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure the CNN model</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>sequence_input <span class="op">=</span> Input(shape<span class="op">=</span>(tokens.shape[<span class="dv">1</span>],), dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>seq <span class="op">=</span> embedding_layer(sequence_input)</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Conv1D(filters<span class="op">=</span><span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(seq)</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> GlobalMaxPooling1D()(m)</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(m)</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"tanh"</span>)(m)</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Model(sequence_input, preds)</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>m.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1000)]            0         
                                                                 
 embedding (Embedding)       (None, 1000, 320)         2176640   
                                                                 
 conv1d (Conv1D)             (None, 998, 128)          123008    
                                                                 
 global_max_pooling1d (Globa  (None, 128)              0         
 lMaxPooling1D)                                                  
                                                                 
 dense (Dense)               (None, 64)                8256      
                                                                 
 dense_1 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 2,307,969
Trainable params: 2,307,969
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
</div>
<div id="tabset-7-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-7-2-tab">
<div class="cell" data-hash="chapter11_cache/html/rnnmodel-r_d46599257d47a5ddf47e2d4cfd7023a3">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>text_vectorization <span class="ot">=</span> <span class="fu">layer_text_vectorization</span>(</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">max_tokens=</span><span class="dv">10000</span>, <span class="at">output_sequence_length=</span><span class="dv">50</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="fu">adapt</span>(text_vectorization, d<span class="sc">$</span>lemmata)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>input <span class="ot">=</span> <span class="fu">layer_input</span>(<span class="at">shape=</span><span class="dv">1</span>, <span class="at">dtype =</span> <span class="st">"string"</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>output <span class="ot">=</span> input <span class="sc">%&gt;%</span> </span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text_vectorization</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> <span class="dv">10000</span> <span class="sc">+</span> <span class="dv">1</span>, </span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>                  <span class="at">output_dim =</span> <span class="dv">16</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_1d</span>(<span class="at">filters=</span><span class="dv">128</span>, <span class="at">kernel_size=</span><span class="dv">3</span>, </span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">activation=</span><span class="st">"relu"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_global_max_pooling_1d</span>() <span class="sc">%&gt;%</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">"relu"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">"tanh"</span>)</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model</span>(input, output)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_1"
________________________________________________________________________________
 Layer (type)                  Output Shape               Param #    Trainable  
================================================================================
 input_2 (InputLayer)          [(None, 1)]                0          Y          
 text_vectorization (TextVecto  (None, 50)                0          Y          
 rization)                                                                      
 embedding_1 (Embedding)       (None, 50, 16)             160016     Y          
 conv1d_1 (Conv1D)             (None, 48, 128)            6272       Y          
 global_max_pooling1d_1 (Globa  (None, 128)               0          Y          
 lMaxPooling1D)                                                                 
 dense_3 (Dense)               (None, 64)                 8256       Y          
 dense_2 (Dense)               (None, 1)                  65         Y          
================================================================================
Total params: 174,609
Trainable params: 174,609
Non-trainable params: 0
________________________________________________________________________________</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-rnn" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.13 </strong></span>Deep Learning: Training and Testing the model</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-8-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-1" role="tab" aria-controls="tabset-8-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-8-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-2" role="tab" aria-controls="tabset-8-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-8-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-8-1-tab">
<div class="cell" data-hash="chapter11_cache/html/rnn-python_795792f4cda9414b47aa1bd357fb3d53">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into train and test</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> tokens[:<span class="dv">4000</span>]</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> tokens[<span class="dv">4000</span>:]</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>train_labels <span class="op">=</span> h.value[:<span class="dv">4000</span>]</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> h.value[<span class="dv">4000</span>:]</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model (note: remove 'verbose=0' to see progress)</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>m.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mean_absolute_error"</span>, optimizer<span class="op">=</span>RMSprop(learning_rate<span class="op">=</span><span class="fl">0.004</span>))</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.asarray([[x] <span class="cf">for</span> x <span class="kw">in</span> train_labels])</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>m.fit(train_data, labels, epochs<span class="op">=</span><span class="dv">5</span>, batch_size<span class="op">=</span><span class="dv">128</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;keras.callbacks.History object at 0x7fbaa6830fa0&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> m.predict(test_data, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Bin output into -1, 0, 1</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="fl">0.3</span> <span class="cf">else</span> (<span class="dv">0</span> <span class="cf">if</span> x[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="op">-</span><span class="fl">0.3</span> <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> x <span class="kw">in</span> output]</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> [x <span class="op">==</span> y <span class="cf">for</span> (x, y) <span class="kw">in</span> <span class="bu">zip</span>(pred, test_labels)]</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> <span class="bu">sum</span>(correct) <span class="op">/</span> <span class="bu">len</span>(pred)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.570198105081826</code></pre>
</div>
</div>
</div>
<div id="tabset-8-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-8-2-tab">
<div class="cell" data-hash="chapter11_cache/html/rnn-r_ddfd2f6a4c5e3f2a3a188c110452dea4">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into train and test</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>d_train <span class="ot">=</span> d <span class="sc">%&gt;%</span> <span class="fu">slice_sample</span>(<span class="at">n=</span><span class="dv">4000</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>d_test <span class="ot">=</span> d <span class="sc">%&gt;%</span> <span class="fu">anti_join</span>(d_train)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="fu">compile</span>(model, <span class="at">loss =</span> <span class="st">"binary_crossentropy"</span>,</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">optimizer =</span> <span class="st">"adam"</span>, <span class="at">metrics =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="fu">fit</span>(model, d_train<span class="sc">$</span>lemmata, d_train<span class="sc">$</span>value,</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">10</span>, <span class="at">batch_size =</span> <span class="dv">512</span>, </span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span>)</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Validate against test data</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>eval<span class="ot">=</span><span class="fu">evaluate</span>(model, d_test<span class="sc">$</span>lemmata, d_test<span class="sc">$</span>value)</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">glue</span>(<span class="st">"Accuracy: {eval['accuracy']}"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.439707159996033</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>First, <a href="#exm-rnndata">Example&nbsp;<span>11.11</span></a> loads a dataset described by <span class="citation" data-cites="vanatteveldt2021">Van Atteveldt, Van der Velden, and Boukes (<a href="references.html#ref-vanatteveldt2021" role="doc-biblioref">2021</a>)</span>, which consists of Dutch economic news headlines with a sentiment value. Next, in <a href="#exm-rnnmodel">Example&nbsp;<span>11.12</span></a> a model is defined consisting of several layers, corresponding roughly to <a href="chapter08.html#fig-cnn">Figure&nbsp;<span>8.7</span></a>. First, an <em>embedding</em> layer transforms the textual input into a semantic vector for each word. Next, the <em>convolutional</em> layer defines features (filters) over windows of vectors, which are then pooled in the <em>max-pooling</em> layer. This results in a vector of detected features for each document, which are then used in a regular (hidden) <em>dense</em> layer followed by an output layer.</p>
<p>Finally, in <a href="#exm-rnn">Example&nbsp;<span>11.13</span></a> we train the model on 4000 documents and test it against the remaining documents. The Python model, which uses pre-trained word embeddings (the <code>w2v_320d</code> file downloaded at the top), achieves a mediocre accuracy of about 56% (probably due to the low number of training sentences). The R model, which trains the embedding layer as part of the model, performs more poorly at 44% as this model is even more dependent on large training data to properly estimate the embedding layer.</p>
</section>
</section>
<section id="sec-unsupervised" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="sec-unsupervised"><span class="header-section-number">11.5</span> Unsupervised Text Analysis: Topic Modeling</h2>
<p>In <a href="chapter07.html#sec-clustering"><span>Section&nbsp;7.3</span></a>, we discussed how clustering techniques can be used to find patterns in data, such as which cases or respondents are most similar. Similarly, especially in survey research it is common to use factor analysis to discover (or confirm) variables that form a scale.</p>
<p>In essence, the idea behind these techniques is similar: by understanding the regularities in the data (which cases or variables behave similarly), you can describe the relevant information in the data with fewer data points. Moreover, assuming that the regularities capture interesting information and the deviations from these regularities are mostly uninteresting noise, these clusters of cases or variables can actually be substantively informative.</p>
<p>Since a document-term matrix (DTM) is “just” a matrix, you can also apply these clustering techniques to the DTM to find groups of words or documents. You can therefore use any of the techniques we described in <a href="chapter07.html"><span>Chapter&nbsp;7</span></a>, and in particular clustering techniques such as <span class="math inline">\(k\)</span>-means clustering (see <a href="chapter07.html#sec-clustering"><span>Section&nbsp;7.3</span></a>) to group documents that use similar words together.</p>
<p>It can be very instructive to do this, and we encourage you to play around with such techniques. However, in recent years, a set of models called <em>topic models</em> have become especially popular for the unsupervised analysis of texts. Very much what like what you would do with other unsupervised techniques, also in topic modeling, you group words and documents into “topics”, consisting of words and documents that co-vary. If you see the word “agriculture” in a news article, there is a good chance you might find words such as “farm” or “cattle”, and there is a lower chance you will find a word like “soldier”. In other words, the words “agriculture” and “farm” generally occur in the same kind of documents, so they can be said to be part of the same topic. Similarly, two documents that share a lot of words are probably about the same topic, and if you know what topic a document is on (e.g., an agricultural topic), you are better able to guess what words might occur in that document (e.g., “cattle”).</p>
<p>Thus, we can formulate the goal of topic modeling as: given a corpus, find a set of <span class="math inline">\(n\)</span> topics, consisting of specific words and/or documents, that minimize the mistakes we would make if we try to reconstruct the corpus from the topics. This is similar to regression where we try to find a line that minimizes the prediction error.</p>
<p>In early research on document clustering, a technique called Latent Semantic Analysis (LSA) essentially used a factor analysis technique called Singular Value Decomposition (see <a href="chapter07.html#sec-pcasvd"><span>Section&nbsp;7.3.3</span></a>) on the DTM. This has yielded promising results in information retrieval (i.e., document search) and studying human memory and language use. However, it has a number of drawbacks including factor loadings that can be difficult to interpret substantively and is not a good way of dealing with words that can have multiple meanings <span class="citation" data-cites="lsa">(<a href="references.html#ref-lsa" role="doc-biblioref">Landauer et al. 2013</a>)</span>.</p>
<section id="sec-lda" class="level3" data-number="11.5.1">
<h3 data-number="11.5.1" class="anchored" data-anchor-id="sec-lda"><span class="header-section-number">11.5.1</span> Latent Dirichlet Allocation (LDA)</h3>
<p>The most widely used technique for topic modeling is Latent Dirichlet Allocation <span class="citation" data-cites="blei03">(LDA, <a href="references.html#ref-blei03" role="doc-biblioref">D. M. Blei, Ng, and Jordan 2003</a>)</span>. Although the goal of LDA is the same as for clustering techniques, it starts from the other end with what is called a <em>generative model</em>. A generative model is a (simplified) formal model of how the data is assumed to have been generated. For example, if we would have a standard regression model predicting income based on age and education level, the implicit generative model is that to determine someone’s income, you take their age and education level, multiply them both by their regression parameters, and then add the intercept and some random error. Of course, we know that’s not actually how most companies determine wages, but it can be a useful starting point to analyze, e.g., labor market discrimination.</p>
<p>The generative model behind LDA works as follows. Assume that you are a journalist writing a 500 word news item. First, you would choose one or more <em>topics</em> to write about, for example 70% healthcare and 30% economy. Next, for each word in the item, you randomly pick one of these topics based on their respective weight. Finally, you pick a random word from the words associated with that topic, where again each word has a certain probability for that topic. For example, “hospital” might have a high probability for healthcare while “effectiveness” might have a lower probability but could still occur.</p>
<p>As said, we know (or at least strongly suspect) that this is not how journalists actually write their stories. However, this generative model helps understand the substantive interpretation of topics. Moreover, LDA is a <em>mixture model</em>, meaning it allows for each document to be about multiple topics, and for each word to occur in multiple topics. This matches with the fact that in many cases, our documents are in fact about multiple topics, from a news article about the economic effects of the COVID virus to an open survey answer containing multiple reasons for supporting a certain candidate. Additionally, since topic assignment is based on what other words occur in a document, the word “pupil” could be assigned either to a “biology” topic or to an “education” topic, depending on whether the document talks about eyes and lenses or about teachers and classrooms.</p>
<div id="fig-lda" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/lda.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;11.1: Latent Dirichlet Allocation in ``Plate Model’’ notation (source: Blei et al, 2003)</figcaption><p></p>
</figure>
</div>
<p>Figure <a href="#fig-lda"><span>11.1</span></a> is a more formal notation of the same generative model. Starting from the left, for each document you pick a set of topics <span class="math inline">\(\Theta\)</span>. This set of topics is drawn from a <em>Dirichlet distribution</em> which itself has a parameter <span class="math inline">\(\alpha\)</span> (see note). Next, for each word you select a single topic <span class="math inline">\(z\)</span> from the topics in that document. Finally, you pick an actual word <span class="math inline">\(w\)</span> from the words in that topic, again controlled by a parameter <span class="math inline">\(\beta\)</span>.</p>
<p>Now, if we know which words and documents are in which topics, we can start generating the documents in the corpus. In reality, of course, we have the reverse situation: we know the documents, and we want to know the topics. Thus, the task of LDA is to find the parameters that have the highest chance of generating these documents. Since only the word frequencies are observed, this is a latent variable model where we want to find the most likely values for the (latent) topic <span class="math inline">\(z\)</span> for each word in each document.</p>
<p>Unfortunately, there is no simple analytic solution to calculate these topic assignments like there is for OLS regression. Thus, like other more complicated statistical models such as multilevel regression, we need to use an iterative estimation that progressively optimizes the assignment to improve the fit until it converges.</p>
<p>An estimation method that is often used for LDA is Gibbs sampling. Simply put, this starts with a random assignment of topics to words. Then, in each iteration, it reconsiders each word and recomputes what likely topics for that word are given the other topics in that document and the topics in which that word occurs in other documents. Thus, if a document already contains a number of words placed in a certain topic, a new word is more likely to be placed in that topic as well. After enough iterations, this converges to a solution.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The Dirichlet Distribution and its Hyperparameters
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The Dirichlet distribution can be seen as a distribution over multinomial distributions, that is, every draw from a Dirichlet distribution results in a multinomial distribution. An easy way to visualize this is to see the Dirichlet distribution as a bag of dice. You draw a die from the bag, and each die is a distribution over the numbers one to six.</p>
<p>This distribution is controlled by a parameter called alpha (<span class="math inline">\(\alpha\)</span>), which is often called a <em>hyperparameter</em> because it is a parameter that controls how other parameters (the actual topic distributions) are estimated, similar to, e.g., the learning speed in many machine learning models. This alpha hyperparameter controls what kind of dice there are in the bag. A high alpha means that the dice are generally fair, i.e., give a uniform multinomial distribution. For topic models, this means that documents will in general contain an even spread of multiple topics. A low alpha means that each die is unfair in the sense of having a strong preference for some number(s), as if these numbers are weighted. You can then draw a die that prefers ones, or a die that prefers sixes. For topic models this means that each document tends to have one or two dominant topics. Finally, alpha can be symmetric (meaning dice are unfair, but randomly, so in the end each topic has the same chance) or asymmetric (they are still unfair, and now also favor some topics more than others). This would correspond to some topics being more likely to occur in all documents.</p>
<p>In our experience, most documents actually do have one or two dominant topics, and some topics are actually more prevalent across many documents then others – especially if you consider that procedural words and boilerplate also need to be fit into a topic unless they are filtered out beforehand. Thus, we would generally recommend a relatively low and asymmetric alpha, and in fact <em>gensim</em> provides an algorithm to find, based on the data, an alpha that corresponds to this recommendation (by specifying <code>alpha='auto'</code>). In R, we would recommend picking a lower alpha than the default value, probably around <span class="math inline">\(\alpha=5/K\)</span>, and optionally try using an asymmetric alpha if you find some words that occur across multiple topics.</p>
<p>To get a more intuitive understanding of the effects of alpha, please see <a href="https://cssbook.net/lda">cssbook.net/lda</a> for additional material and visualizations.</p>
</div>
</div>
</div>
</section>
<section id="sec-ldafit" class="level3" data-number="11.5.2">
<h3 data-number="11.5.2" class="anchored" data-anchor-id="sec-ldafit"><span class="header-section-number">11.5.2</span> Fitting an LDA Model</h3>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-lda" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.14 </strong></span>LDA Topic Model of Obama’s State of the Union speeches.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-9-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-1" role="tab" aria-controls="tabset-9-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-9-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-2" role="tab" aria-controls="tabset-9-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-9-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-9-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://cssbook.net/d/sotu.csv"</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>sotu <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>p_obama <span class="op">=</span> sotu[sotu.President <span class="op">==</span> <span class="st">"Obama"</span>].text.<span class="bu">str</span>.split(<span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>).explode()</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> CountVectorizer(min_df<span class="op">=</span><span class="fl">0.01</span>, stop_words<span class="op">=</span><span class="st">"english"</span>)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>dtm <span class="op">=</span> cv.fit_transform(p_obama)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>dtm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> matutils.Sparse2Corpus(dtm, documents_columns<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">enumerate</span>(cv.get_feature_names()))</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>lda <span class="op">=</span> LdaModel(</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    corpus, id2word<span class="op">=</span>vocab, num_topics<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">123</span>, alpha<span class="op">=</span><span class="st">"asymmetric"</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"Topic </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>: [w <span class="cf">for</span> (w, tw) <span class="kw">in</span> words]</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (n, words) <span class="kw">in</span> lda.show_topics(formatted<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Topic 0    Topic 1    Topic 2  ...    Topic 7     Topic 8     Topic 9
0   america   american       year  ...    america     america         new
1  american       jobs  americans  ...  americans         tax        jobs
2    people      years       know  ...       hard        work        just
3       new       like     people  ...     people      states        help
4     right        let        new  ...      years   americans     country
5     world         ve   american  ...     nation  businesses    families
6      help  americans      years  ...       work         god        make
7   economy     people       like  ...         ve       bless       world
8       let       year         ve  ...      world      united  government
9      know       home       make  ...       time      people        fact

[10 rows x 10 columns]</code></pre>
</div>
</div>
</div>
<div id="tabset-9-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-9-2-tab">
<div class="cell" data-hash="chapter11_cache/html/lda1-r_a7c95397a3009f53400912c459aa6226">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>url <span class="ot">=</span> <span class="st">"https://cssbook.net/d/sotu.csv"</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>sotu <span class="ot">=</span> <span class="fu">read_csv</span>(url) </span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>p_obama <span class="ot">=</span> sotu <span class="sc">%&gt;%</span> </span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(President <span class="sc">==</span> <span class="st">"Obama"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">corpus</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">corpus_reshape</span>(<span class="st">"paragraphs"</span>)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>dfm <span class="ot">=</span> p_obama <span class="sc">%&gt;%</span> </span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokens</span>(<span class="at">remove_punct=</span>T) <span class="sc">%&gt;%</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_remove</span>(<span class="fu">stopwords</span>(<span class="st">"english"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dfm_trim</span>(<span class="at">min_docfreq=</span>.<span class="dv">01</span>,<span class="at">docfreq_type =</span> <span class="st">"prop"</span>)</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>dfm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 738 documents, 746 features (97.02% sparse) and 6 docvars.
         features
docs      speaker mr vice president members congress first united around come
  text1.1       1  1    1         1       1        1     1      1      1    0
  text1.2       0  0    0         0       0        0     0      0      0    1
  text1.3       0  0    0         0       0        0     0      0      0    0
  text1.4       0  0    0         0       0        0     0      1      0    0
  text1.5       0  0    0         0       0        0     0      0      0    0
  text1.6       0  0    0         0       0        0     0      0      0    0
[ reached max_ndoc ... 732 more documents, reached max_nfeat ... 736 more features ]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>lda <span class="ot">=</span> dfm <span class="sc">%&gt;%</span> </span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">convert</span>(<span class="at">to =</span> <span class="st">"topicmodels"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">LDA</span>(<span class="at">k=</span><span class="dv">10</span>,<span class="at">control=</span><span class="fu">list</span>(<span class="at">seed=</span><span class="dv">123</span>, <span class="at">alpha =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>))</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="fu">terms</span>(lda, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Topic 1     Topic 2     Topic 3     Topic 4       Topic 5     Topic 6   
 [1,] "college"   "care"      "us"        "can"         "years"     "energy"  
 [2,] "new"       "health"    "nation"    "make"        "first"     "change"  
 [3,] "workers"   "still"     "people"    "congress"    "economy"   "new"     
 [4,] "education" "families"  "can"       "republicans" "back"      "clean"   
 [5,] "help"      "americans" "states"    "democrats"   "time"      "world"   
 [6,] "job"       "like"      "one"       "work"        "home"      "power"   
 [7,] "every"     "new"       "america"   "take"        "crisis"    "america" 
 [8,] "kids"      "need"      "together"  "pay"         "financial" "can"     
 [9,] "small"     "must"      "united"    "cuts"        "two"       "economy" 
[10,] "schools"   "job"       "president" "banks"       "plan"      "research"
      Topic 7      Topic 8    Topic 9    Topic 10    
 [1,] "people"     "country"  "world"    "jobs"      
 [2,] "get"        "time"     "american" "year"      
 [3,] "day"        "future"   "war"      "years"     
 [4,] "now"        "american" "security" "new"       
 [5,] "know"       "america"  "people"   "last"      
 [6,] "see"        "people"   "troops"   "tax"       
 [7,] "tax"        "done"     "us"       "million"   
 [8,] "americans"  "work"     "america"  "$"         
 [9,] "government" "get"      "new"      "businesses"
[10,] "american"   "now"      "nations"  "america"   </code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Example <a href="#exm-lda"><span>11.14</span></a> shows how you can fit an LDA model in Python or R. As example data, we use Obama’s State of the Union Speeches using the corpus introduced in Chapter <a href="chapter10.html"><span>10</span></a>. Since such a speech generally touches on many different topics, we choose to first split by paragraph as these will be more semantically coherent (for Obama, at least). In R, we use the <code>corpus_reshape</code> function to split the paragraphs, while in Python we use <em>pandas</em>’ <code>str.split</code>, which creates a list or paragraphs for each text, which we then convert into a paragraph per row using <code>explode</code>. Converting this to a DTM we get a reasonably sized matrix of 738 paragraphs and 746 unique words.</p>
<p>Next, we fit the actual LDA model using the package <em>gensim</em> (Python) and <em>topicmodels</em> (R). Before we can do this, we need to convert the DTM format into a format accepted by that package. For Python, this is done using the <code>Sparse2Corpus</code> helper function while in R this is done with the <em>quanteda</em> <code>convert</code> function. Then, we fit the model, asking for 10 topics to be identified in these paragraphs. There are three things to note in this line. First, we specify a <em>random seed</em> of 123 to make sure the analysis is replicable. Second, we specify an “asymmetric” of <code>1/1:10</code>, meaning the first topic has alpha 1, the second 0.5, etc. (in R). In Python, instead of using the default of <code>alpha='symmetric'</code>, we set <code>alpha='asymmetric'</code>, which uses the formula <span class="math inline">\(\frac{1}{topic\_index + \sqrt{num\_topics}}\)</span> to determine the priors. At the cost of a longer estimation time, we can even specify <code>alpha='auto'</code>, which will learn an asymmetric prior from the data. See the note on hyperparameters for more information. Third, for Python we also need to specify the vocabulary names since these are not included in the DTM.</p>
<p>The final line generates a data frame of top words per topic for first inspection (which in Python requires separating the words from their weights in a list comprehension and converting it to a data frame for easy viewing). As you can see, most topics are interpretable and somewhat coherent: for example, topic 1 seems to be about education and jobs, while topic 2 is health care. You also see that the word “job” occurs in multiple topics (presumably because unemployment was a pervasive concern during Obama’s tenure). Also, some topics like topic 3 are relatively more difficult to interpret from this table. A possible reason for this is that not every paragraph actually has policy content. For example, the first paragraph of his first State of the Union was: <em>Madam Speaker, Mr.&nbsp;Vice President, Members of Congress, the First Lady of the United States – she’s around here somewhere</em>. None of these words really fit a “topic” in the normal meaning of that term, but all of these words need to be assigned a topic in LDA. Thus, you often see “procedural” or “boilerplate” topics such as topic 3 occurring in LDA outputs.</p>
<p>Finally, note that we showed the R results here. As <em>gensim</em> uses a different estimation algorithm (and <code>scikit-learn</code>uses a different tokenizer and stopword list), results will not be identical, but should be mostly similar.</p>
</section>
<section id="sec-ldainspect" class="level3" data-number="11.5.3">
<h3 data-number="11.5.3" class="anchored" data-anchor-id="sec-ldainspect"><span class="header-section-number">11.5.3</span> Analyzing Topic Model Results</h3>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-ldaresults" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.15 </strong></span>Analyzing and inspecting LDA results.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-10-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-1" role="tab" aria-controls="tabset-10-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-10-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-2" role="tab" aria-controls="tabset-10-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-10-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-10-1-tab">
<div class="cell" data-hash="chapter11_cache/html/ldaresults1-python_cc6d47b618f04e1896ceccb5f28e6c6c">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>topics <span class="op">=</span> pd.DataFrame(</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">dict</span>(lda.get_document_topics(doc, minimum_probability<span class="op">=</span><span class="fl">0.0</span>))</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> doc <span class="kw">in</span> corpus</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>meta <span class="op">=</span> sotu.iloc[p_obama.index].drop(columns<span class="op">=</span>[<span class="st">"text"</span>]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>tpd <span class="op">=</span> pd.concat([meta, topics], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>tpd.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  FirstName President        Date  ...         7         8         9
0    Barack     Obama  2009-02-24  ...  0.007036  0.006406  0.005879
1    Barack     Obama  2009-02-24  ...  0.005277  0.004805  0.004410
2    Barack     Obama  2009-02-24  ...  0.002436  0.002218  0.002035
3    Barack     Obama  2009-02-24  ...  0.004222  0.003844  0.003528
4    Barack     Obama  2009-02-24  ...  0.002639  0.002402  0.002205

[5 rows x 16 columns]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> docid <span class="kw">in</span> [<span class="dv">622</span>, <span class="dv">11</span>, <span class="dv">322</span>]:</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>docid<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">list</span>(p_obama)[docid]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>622: I intend to protect a free and open Internet, extend its reach to ever...
11: Because of this plan, there are teachers who can now keep their jobs an...
322: I want every American looking for work to have the same opportunity as...</code></pre>
</div>
</div>
</div>
<div id="tabset-10-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-10-2-tab">
<div class="cell" data-hash="chapter11_cache/html/ldaresults1-r_413d847bcc4231e5e9a5a4ea983df438">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>topics <span class="ot">=</span> <span class="fu">posterior</span>(lda)<span class="sc">$</span>topics <span class="sc">%&gt;%</span> </span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename_all</span>(<span class="sc">~</span><span class="fu">paste0</span>(<span class="st">"Topic_"</span>, .))</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>meta <span class="ot">=</span> <span class="fu">docvars</span>(p_obama) <span class="sc">%&gt;%</span> </span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(President<span class="sc">:</span>Date) <span class="sc">%&gt;%</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">doc_id=</span><span class="fu">docnames</span>(p_obama),<span class="at">.before=</span><span class="dv">1</span>)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>tpd <span class="ot">=</span> <span class="fu">bind_cols</span>(meta, topics) </span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tpd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   doc_id President       Date     Topic_1     Topic_2     Topic_3     Topic_4
1 text1.1     Obama 2009-02-24 0.013920249 0.013920249 0.681016299 0.013958435
2 text1.2     Obama 2009-02-24 0.010817618 0.529660710 0.383715945 0.010826304
3 text1.3     Obama 2009-02-24 0.225158932 0.004360997 0.004365404 0.004358067
4 text1.4     Obama 2009-02-24 0.008349675 0.008345439 0.439462311 0.008356367
5 text1.5     Obama 2009-02-24 0.089106811 0.098659225 0.330131825 0.004954681
6 text1.6     Obama 2009-02-24 0.012717996 0.012718835 0.506051395 0.012766016
      Topic_5     Topic_6     Topic_7     Topic_8     Topic_9    Topic_10
1 0.109552840 0.013940244 0.013928915 0.013938857 0.111895624 0.013928289
2 0.010817139 0.010816612 0.010825601 0.010849458 0.010849549 0.010821065
3 0.387834833 0.004366069 0.356468915 0.004375268 0.004354753 0.004356762
4 0.008367663 0.008355544 0.008349396 0.493717715 0.008349213 0.008346677
5 0.060645762 0.004977742 0.060033550 0.185271671 0.112512468 0.053706265
6 0.392025793 0.012724883 0.012792718 0.012739463 0.012741326 0.012721575</code></pre>
</div>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (id <span class="cf">in</span> <span class="fu">c</span>(<span class="st">"text7.73"</span>, <span class="st">"text5.1"</span>, <span class="st">"text2.12"</span>)) {</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>  text <span class="ot">=</span> <span class="fu">as.character</span>(p_obama)[id]</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">glue</span>(<span class="st">"{id}: {text}"</span>))</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "text7.73: So the question for those of us here tonight is how we, all ..."
[1] "text5.1: Mr. Speaker, Mr. Vice President, members of Congress, fellow ..."
[1] "text2.12: And tonight, I'd like to talk about how together, we can del..."</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Example <a href="#exm-ldaresults"><span>11.15</span></a> shows how you can combine the LDA results (topics per document) with the original document metadata. This could be your starting point for substantive analyses of the results, for example to investigate relations between topics or between, e.g., time or partisanship and topic use.</p>
<p>You can also use this to find specific documents for reading. For example, we noted above that topic 3 is difficult to interpret. As you can see in the table in Example <a href="#exm-ldaresults"><span>11.15</span></a> (which is sorted by value of topic 3), most of the high scoring documents are the first paragraph in each speech, which do indeed contain the “Madam speaker” boilerplate noted above. The other three documents are all calls for bipartisanship and support. As you can see from this example, carefully inspecting the top documents for each topic is very helpful for making sense of the results.</p>
</section>
<section id="sec-ldavalidate" class="level3" data-number="11.5.4">
<h3 data-number="11.5.4" class="anchored" data-anchor-id="sec-ldavalidate"><span class="header-section-number">11.5.4</span> Validating and Inspecting Topic Models</h3>
<p>As we saw in the previous subsection, running a topic model is relatively easy. However, that doesn’t mean that the resulting topic model will always be useful. As with all text analysis techniques, <em>validation</em> is the key to good analysis: are you measuring what you want to measure? And how do you know?</p>
<p>For topic modeling (and arguably for all text analysis), the first step after fitting a model is inspecting the results and establishing face validity. Top words per topic such as those listed above are a good place to start, but we would really encourage you to also look at the top documents per topic to better understand how words are used in context. Also, it is good to inspect the relationships between topics and look at documents that load high on multiple topics to understand the relationship.</p>
<p>If the only goal is to get an explorative understanding of the corpus, for example as a first step before doing a dictionary analysis or manual coding, just face validity is probably sufficient. For a more formal validation, however, it depends on the reason for using topic modeling.</p>
<p>If you are using topic modeling in a true unsupervised sense, i.e., without a predefined analytic schema in mind, it is difficult to assess whether the model measures what you want to measure – because the whole point is that you don’t know what you want to measure. That said, however, you can have the general criteria that the model needs to achieve <em>coherence</em> and <em>interpretability</em>, meaning that words and documents that share a topic are also similar semantically.</p>
<p>In their excellent paper on the topic, <span class="citation" data-cites="chang09">Chang et al. (<a href="references.html#ref-chang09" role="doc-biblioref">2009</a>)</span> propose two formal tasks to judge this using manual (or crowd) coding: in <em>word intrusion</em>, a coder is asked to pick the “odd one out” from a list where one other word is mixed in a group of topic words. In <em>topic intrusion</em>, the coder is presented with a document and a set of topics that occur in the document, and is asked to spot the one topic that was not present according to the model. In both tasks, if the coder is unable to identify the intruding word or topic, apparently the model does not fit our intuitive notion of “aboutness” or semantic similarity. Perhaps their most interesting finding is that goodness-of-fit measures like perplexity<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> are actually not good predictors of the interpretability of the resulting models.</p>
<p>If you are using topic models in a more confirmatory manner, that is, if you wish the topics to match some sort of predefined categorization, you should use regular gold standard techniques for validation: code a sufficiently large random sample of documents with your predefined categories, and test whether the LDA topics match those categories. In general, however, in such cases it is a better idea to use a dictionary or supervised analysis technique as topic models often do not exactly capture our categories. After all, unsupervised techniques mainly excel in bottom-up and explorative analyses (Section <a href="#sec-deciding"><span>11.1</span></a>).</p>
</section>
<section id="sec-beyondlda" class="level3" data-number="11.5.5">
<h3 data-number="11.5.5" class="anchored" data-anchor-id="sec-beyondlda"><span class="header-section-number">11.5.5</span> Beyond LDA</h3>
<p>This chapter focused on regular or “vanilla” LDA topic modeling. Since the seminal publication, however, a large amount of variations and extensions on LDA have been proposed. These include, for example, <em>Dynamic Topic Models</em> (which incorporate time; <span class="citation" data-cites="dynamiclda">D. M. Blei and Lafferty (<a href="references.html#ref-dynamiclda" role="doc-biblioref">2006</a>)</span>) and <em>Correlated Topic Models</em> (which explicitly model correlation between topics; <span class="citation" data-cites="correlatedlda">D. Blei and Lafferty (<a href="references.html#ref-correlatedlda" role="doc-biblioref">2006</a>)</span>). Although it is beyond the scope of this book to describe these models in detail, the interested reader is encouraged to learn more about these models.</p>
<p>Especially noteworthy are <em>Structural Topic Models</em> (R package <em>stm</em>; <span class="citation" data-cites="stm">Roberts et al. (<a href="references.html#ref-stm" role="doc-biblioref">2014</a>)</span>), which allow you to model covariates as topic or word predictors. This allows you, for example, to model topic shifts over time or different words for the same topic based on, e.g., Republican or Democrat presidents.</p>
<p>Python users should check out <em>Hierarchical Topic Modeling</em> <span class="citation" data-cites="hierarchicallda">(<a href="references.html#ref-hierarchicallda" role="doc-biblioref">Griffiths et al. 2004</a>)</span>. In hierarchical topic modeling, rather than the researcher specifying a fixed number of topics, the model returns a hierarchy of topics from few general topics to a large number of specific topics, allowing for a more flexible exploration and analysis of the data.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
How many topics?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>With topic modeling, the most important researcher choices are the <em>number of topics</em> and the value of <em>alpha</em>. These choices are called <em>hyperparameters</em>, since they determine how the model parameters (e.g.&nbsp;words per topic) are found.</p>
<p>There is no good theoretical solution to determine the “right” number of topics for a given corpus and research question. Thus, a sensible approach can be to ask the computer to try many models, and see which works best. Unfortunately, because this is an unsupervised (inductive) method, there is no single metric that determines how good a topic model is.</p>
<p>There are a number of such metrics proposed in the literature, of which we will introduce two. <em>Perplexity</em> is a score of how well the LDA model can fit (predict) the actual word distribution (or in other words: how “perplexed” the model is seeing the corpus). <em>Coherence</em> is a measure of how semantically coherent the topics are by checking how often the top token co-occurs in documents in each topic <span class="citation" data-cites="mimno11">(<a href="references.html#ref-mimno11" role="doc-biblioref">Mimno et al. 2011</a>)</span>.</p>
<p>The code below shows how these can be calculated for a range of topic counts, and the same code could be used for trying different values of <em>alpha</em>. For both measures, lower values are better, and both essentially keep decreasing as you add more topics. What you are looking for is the <em>inflection point</em> (or “elbow point”) where it goes from a steep decrease to a more gradual decrease. For coherence, this seems to be at 10 topics, while for perplexity this is at 20 topics.</p>
<p>There are two very important caveats to make here, however. First, these metrics are no substitute for human validation and the best model according to these metrics is not always the most interpretable or coherent model. In our experience, most metrics give a higher topic count that would be optimal from an interpretability perspective, but of course that also depends on how we operationalize interpretability. Nonetheless, these topic numbers are probably more indicative of a range of counts that should be inspected manually, rather than giving a definitive answer.</p>
<p>Second, the code below was written so it is easy to understand and quick to run. For real use in a research project, it is advised to include a broader range of topic counts and also vary the <span class="math inline">\(\alpha\)</span>. Moreover, it is smart to run each count multiple times so you get an indication of the variance as well as a single point (it is quite likely that the local minimum for coherence at <span class="math inline">\(k=10\)</span> is an outlier that will disappear if more runs are averaged). Finally, especially for a goodness-of-fit measure like perplexity it is better to split the data into a training and test set (see Section <a href="#sec-workflow"><span>11.4.1</span></a> for more details).</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-11-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-1" role="tab" aria-controls="tabset-11-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-11-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-2" role="tab" aria-controls="tabset-11-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-11-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-11-1-tab">
<div class="cell" data-hash="chapter11_cache/html/ldacoherence-python_9f5214351cf332392effc94105dd30b0">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> []</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>, <span class="dv">30</span>]:</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> LdaModel(</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>        corpus,</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>        num_topics<span class="op">=</span>k,</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>        id2word<span class="op">=</span>vocab,</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">123</span>,</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="st">"asymmetric"</span>,</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>    perplexity <span class="op">=</span> m.log_perplexity(corpus)</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>    coherence <span class="op">=</span> CoherenceModel(</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>m, corpus<span class="op">=</span>corpus, coherence<span class="op">=</span><span class="st">"u_mass"</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>    ).get_coherence()</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>    result.append(<span class="bu">dict</span>(k<span class="op">=</span>k, perplexity<span class="op">=</span>perplexity, coherence<span class="op">=</span>coherence))</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> pd.DataFrame(result)</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>result.plot(x<span class="op">=</span><span class="st">"k"</span>, y<span class="op">=</span>[<span class="st">"perplexity"</span>, <span class="st">"coherence"</span>])</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter11_files/figure-html/ldacoherence-python-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-11-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-11-2-tab">
<div class="cell" data-hash="chapter11_cache/html/ldacoherence-r_24ff9ed08a2ed34489e5513e69880faa">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>results <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">=</span> <span class="fu">convert</span>(dfm, <span class="at">to=</span><span class="st">"topicmodels"</span>)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>, <span class="dv">30</span>)) {</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    alpha <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span>((<span class="dv">1</span><span class="sc">:</span>k)<span class="sc">+</span><span class="fu">sqrt</span>(k))</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    lda <span class="ot">=</span> <span class="fu">LDA</span>(dtm,<span class="at">k=</span>k,<span class="at">control=</span><span class="fu">list</span>(<span class="at">seed=</span><span class="dv">99</span>,<span class="at">alpha=</span>alpha))</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    results[[<span class="fu">as.character</span>(k)]] <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">perplexity=</span><span class="fu">perplexity</span>(lda),</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">coherence=</span><span class="fu">mean</span>(<span class="fu">topic_coherence</span>(lda,dtm)))</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(results, <span class="at">.id=</span><span class="st">"k"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">k=</span><span class="fu">as.numeric</span>(k)) <span class="sc">%&gt;%</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>k) <span class="sc">%&gt;%</span> </span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>k, <span class="at">y=</span>value)) <span class="sc">+</span> </span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Number of topics"</span>) <span class="sc">+</span> </span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(name <span class="sc">~</span> ., <span class="at">scales=</span><span class="st">"free"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter11_files/figure-html/ldacoherence-r-3.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="cell">

</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-dynamiclda" class="csl-entry" role="doc-biblioentry">
Blei, David M, and John D Lafferty. 2006. <span>“Dynamic Topic Models.”</span> In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 113–20.
</div>
<div id="ref-blei03" class="csl-entry" role="doc-biblioentry">
Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. <span>“Latent Dirichlet Allocation.”</span> <em>Journal of Machine Learning Research</em> 3 (Jan): 993–1022.
</div>
<div id="ref-correlatedlda" class="csl-entry" role="doc-biblioentry">
Blei, David, and John Lafferty. 2006. <span>“Correlated Topic Models.”</span> <em>Advances in Neural Information Processing Systems</em> 18: 147.
</div>
<div id="ref-Boukes2019" class="csl-entry" role="doc-biblioentry">
Boukes, Mark, Bob van de Velde, Theo Araujo, and Rens Vliegenthart. 2019. <span>“<span class="nocase">What’s the Tone? Easy Doesn’t Do It: Analyzing Performance and Agreement Between Off-the-Shelf Sentiment Analysis Tools</span>.”</span> <em>Communication Methods and Measures</em> 00 (00): 1–22. <a href="https://doi.org/10.1080/19312458.2019.1671966">https://doi.org/10.1080/19312458.2019.1671966</a>.
</div>
<div id="ref-Boumans2016" class="csl-entry" role="doc-biblioentry">
Boumans, Jelle W., and Damian Trilling. 2016. <span>“<span class="nocase">Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars</span>.”</span> <em>Digital Journalism</em> 4 (1): 8–23. <a href="https://doi.org/10.1080/21670811.2015.1096598">https://doi.org/10.1080/21670811.2015.1096598</a>.
</div>
<div id="ref-Burscher2014" class="csl-entry" role="doc-biblioentry">
Burscher, Björn, Daan Odijk, Rens Vliegenthart, Maarten de Rijke, and Claes H. de Vreese. 2014. <span>“<span class="nocase">Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis</span>.”</span> <em>Communication Methods and Measures</em> 8 (3): 190–206. <a href="https://doi.org/10.1080/19312458.2014.937527">https://doi.org/10.1080/19312458.2014.937527</a>.
</div>
<div id="ref-Chan2021" class="csl-entry" role="doc-biblioentry">
Chan, Chung-hong, Joseph Bajjalieh, Loretta Auvil, Hartmut Wessler, Scott Althaus, Kasper Welbers, Wouter van Atteveldt, and Marc Jungblut. in press. <span>“Four Best Practices for Measuring News Sentiment Using <span>‘Off-the-Shelf’</span> Dictionaries: A Large-Scale p-Hacking Experiment.”</span> <em>Computational Communication Research</em>, in press.
</div>
<div id="ref-chang09" class="csl-entry" role="doc-biblioentry">
Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. 2009. <span>“Reading Tea Leaves: How Humans Interpret Topic Models.”</span> In <em>Advances in Neural Information Processing Systems</em>, 288–96.
</div>
<div id="ref-DeSmedt2012" class="csl-entry" role="doc-biblioentry">
De Smedt, Tom, W Daelemans, and Tom De Smedt. 2012. <span>“<span class="nocase">Pattern for Python</span>.”</span> <em>The Journal of Machine Learning Research</em> 13: 2063–67. <a href="http://dl.acm.org/citation.cfm?id=2343710">http://dl.acm.org/citation.cfm?id=2343710</a>.
</div>
<div id="ref-goldberg2017" class="csl-entry" role="doc-biblioentry">
Goldberg, Yoav. 2017. <em>Neural Network Models for Natural Language Processing</em>. Morgan &amp; Claypool.
</div>
<div id="ref-Gonzalez-Bailon2015" class="csl-entry" role="doc-biblioentry">
Gonzalez-Bailon, S., and G. Paltoglou. 2015. <span>“<span class="nocase">Signals of Public Opinion in Online Communication: A Comparison of Methods and Data Sources</span>.”</span> <em>The ANNALS of the American Academy of Political and Social Science</em> 659 (1): 95–107. <a href="https://doi.org/10.1177/0002716215569192">https://doi.org/10.1177/0002716215569192</a>.
</div>
<div id="ref-hierarchicallda" class="csl-entry" role="doc-biblioentry">
Griffiths, Thomas L, Michael I Jordan, Joshua B Tenenbaum, and David M Blei. 2004. <span>“Hierarchical Topic Models and the Nested Chinese Restaurant Process.”</span> In <em>Advances in Neural Information Processing Systems</em>, 17–24.
</div>
<div id="ref-Grimmer2013" class="csl-entry" role="doc-biblioentry">
Grimmer, J., and B. M. Stewart. 2013. <span>“Text as Data: <span>The</span> Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.”</span> <em>Political Analysis</em> 21 (3): 267–97. <a href="https://doi.org/10.1093/pan/mps028">https://doi.org/10.1093/pan/mps028</a>.
</div>
<div id="ref-Hutto2014" class="csl-entry" role="doc-biblioentry">
Hutto, Clayton J, and Eric Gilbert. 2014. <span>“Vader: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text.”</span> In <em>Eighth International AAAI Conference on Weblogs and Social Media</em>.
</div>
<div id="ref-Krippendorff2004" class="csl-entry" role="doc-biblioentry">
Krippendorff, Klaus. 2004. <em>Content Analysis: An Introduction to Its Methodology</em>. 2nd ed. Thousand Oaks, CA: SAGE.
</div>
<div id="ref-lsa" class="csl-entry" role="doc-biblioentry">
Landauer, Thomas K, Danielle S McNamara, Simon Dennis, and Walter Kintsch. 2013. <em>Handbook of Latent Semantic Analysis</em>. Psychology Press.
</div>
<div id="ref-aclimdb" class="csl-entry" role="doc-biblioentry">
Maas, Andrew L., Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. <span>“Learning Word Vectors for Sentiment Analysis.”</span> In <em>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, 142–50. Portland, Oregon, USA: Association for Computational Linguistics. <a href="http://www.aclweb.org/anthology/P11-1015">http://www.aclweb.org/anthology/P11-1015</a>.
</div>
<div id="ref-mimno11" class="csl-entry" role="doc-biblioentry">
Mimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. <span>“Optimizing Semantic Coherence in Topic Models.”</span> In <em>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</em>, 262–72.
</div>
<div id="ref-Reagan2017" class="csl-entry" role="doc-biblioentry">
Reagan, Andrew J., Christopher M. Danforth, Brian Tivnan, Jake Ryland Williams, and Peter Sheridan Dodds. 2017. <span>“<span class="nocase">Sentiment analysis methods for understanding large-scale texts: a case for using continuum-scored words and word shift graphs</span>.”</span> <em>EPJ Data Science</em> 6 (1). <a href="https://doi.org/10.1140/epjds/s13688-017-0121-9">https://doi.org/10.1140/epjds/s13688-017-0121-9</a>.
</div>
<div id="ref-Riffe2019" class="csl-entry" role="doc-biblioentry">
Riffe, Daniel, Stephen Lacy, Brendan R. Watson, and Frederick Fico. 2019. <em>Analyzing Media Messages: Using Quantitative Content Analysis in Research</em>. 4th ed. New York, NY: Routledge.
</div>
<div id="ref-stm" class="csl-entry" role="doc-biblioentry">
Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G Rand. 2014. <span>“Structural Topic Models for Open-Ended Survey Responses.”</span> <em>American Journal of Political Science</em> 58 (4): 1064–82.
</div>
<div id="ref-Scharkow2011" class="csl-entry" role="doc-biblioentry">
Scharkow, Michael. 2011. <span>“<span class="nocase">Thematic content analysis using supervised machine learning: An empirical evaluation using German online news</span>.”</span> <em>Quality <span>&amp;</span> Quantity</em> 47 (2): 761–73. <a href="https://doi.org/10.1007/s11135-011-9545-7">https://doi.org/10.1007/s11135-011-9545-7</a>.
</div>
<div id="ref-Thelwall2012" class="csl-entry" role="doc-biblioentry">
Thelwall, Mike, Kevan Buckley, and Georgios Paltoglou. 2012. <span>“Sentiment Strength Detection for the Social Web.”</span> <em>Journal of the American Society for Information Science and Technology</em> 63 (1): 163–73. <a href="https://doi.org/10.1002/asi.21662">https://doi.org/10.1002/asi.21662</a>.
</div>
<div id="ref-Tulkens2016" class="csl-entry" role="doc-biblioentry">
Tulkens, Stéphan, Lisa Hilte, Elise Lodewyckx, Ben Verhoeven, and Walter Daelemans. 2016. <span>“<span class="nocase">A Dictionary-based Approach to Racism Detection in Dutch Social Media</span>.”</span> <em>Proceedings of the Workshop on Text Analytics for Cybersecurity and Online Safety (TA-COS 2016)</em>, 11–17. <a href="http://www.clips.ua.ac.be/bibliography/a-dictionary-based-approach-to-racism-detection-in-dutch-social-media">http://www.clips.ua.ac.be/bibliography/a-dictionary-based-approach-to-racism-detection-in-dutch-social-media</a>.
</div>
<div id="ref-vanatteveldt2021" class="csl-entry" role="doc-biblioentry">
Van Atteveldt, Wouter, Mariken ACG Van der Velden, and Mark Boukes. 2021. <span>“The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms.”</span> <em>Communication Methods and Measures</em> 15 (2): 121–40.
</div>
<div id="ref-vermeer2019seeing" class="csl-entry" role="doc-biblioentry">
Vermeer, Susan A. M., Theo Araujo, Stefan F. Bernritter, and Guda van Noort. 2019. <span>“<span class="nocase">Seeing the wood for the trees: How machine learning can help firms in identifying relevant electronic word-of-mouth in social media</span>.”</span> <em>International Journal of Research in Marketing</em> 36 (3): 492–508. <a href="https://doi.org/10.1016/j.ijresmar.2019.01.010">https://doi.org/10.1016/j.ijresmar.2019.01.010</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Perplexity is a measure to compare and evaluate topic models using log-likelihood in order to estimate how well a model predicts a sample. See the note ‘how many topics’ below for example code on how to compute perplexity<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../content/chapter10.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Text as data</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/chapter12.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Scraping online data</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>