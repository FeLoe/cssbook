<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computational Analysis of Communication - 14&nbsp; Multimedia data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/chapter15.html" rel="next">
<link href="../content/chapter13.html" rel="prev">
<link href="../img/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Multimedia data</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Computational Analysis of Communication</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Table of Contents</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter01.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter02.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Getting started: Fun with data and visualizations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter03.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Programming concepts for data analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter04.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">How to write code</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter05.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">From file to data frame and back</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter06.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Wrangling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter07.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Exploratory data analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter08.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Statistical Modeling and Supervised Machine Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter09.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Processing text</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter10.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Text as data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter11.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automatic analysis of text</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter12.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Scraping online data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter13.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Network Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter14.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Multimedia data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter15.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Scaling up and distributing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/chapter16.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Where to go next</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-beyond" id="toc-sec-beyond" class="nav-link active" data-scroll-target="#sec-beyond"><span class="toc-section-number">14.1</span>  Beyond Text Analysis: Images, Audio and Video</a></li>
  <li><a href="#sec-apivisions" id="toc-sec-apivisions" class="nav-link" data-scroll-target="#sec-apivisions"><span class="toc-section-number">14.2</span>  Using Existing Libraries and APIs</a></li>
  <li><a href="#sec-storing" id="toc-sec-storing" class="nav-link" data-scroll-target="#sec-storing"><span class="toc-section-number">14.3</span>  Storing, Representing, and Converting Images</a></li>
  <li><a href="#sec-cnn" id="toc-sec-cnn" class="nav-link" data-scroll-target="#sec-cnn"><span class="toc-section-number">14.4</span>  Image Classification</a>
  <ul class="collapse">
  <li><a href="#sec-shallow" id="toc-sec-shallow" class="nav-link" data-scroll-target="#sec-shallow"><span class="toc-section-number">14.4.1</span>  Basic Classification with Shallow Algorithms</a></li>
  <li><a href="#sec-deep" id="toc-sec-deep" class="nav-link" data-scroll-target="#sec-deep"><span class="toc-section-number">14.4.2</span>  Deep Learning for Image Analysis</a></li>
  <li><a href="#sec-tuning" id="toc-sec-tuning" class="nav-link" data-scroll-target="#sec-tuning"><span class="toc-section-number">14.4.3</span>  Re-using an Open Source CNN</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-chap-image" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Multimedia data</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell">

</div>
<div class="cell">

</div>
<p><strong>Abstract.</strong> Digitally collected data often does not only contain texts, but also audio, images, and videos. Instead of using only textual features as we did in previous chapters, we can also use pixel values to analyze images. First, we will see how to use existing libraries, commercial services or APIs to conduct multimedia analysis (i.e., optical character recognition, speech-to-text or object recognition). Then we will show how to store, represent, and convert image data in order to use it as an input in our computational analysis. We will focus on image analysis using machine learning classification techniques based on deep learning, and will explain how to build (or fine-tune) a Convolutional Neural Network (CNN) by ourselves.</p>
<p><strong>Keywords.</strong> image, audio, video, multimedia, image classification, deep learning</p>
<p><strong>Objectives:</strong></p>
<ul>
<li>Learn how to transform multimedia data into useful inputs for computational analysis</li>
<li>Understand how to conduct deep learning to automatic classification of images</li>
</ul>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Packages used in this chapter
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This chapter uses <em>tesseract</em> (generic) and Google ‘Cloud Speech’ API (<em>googleLanguageR</em> and <em>google-cloud-language</em> in Python) to convert images or audio files into text. We will use <em>PIL</em> (Python) and <em>imagemagic</em> (generic) to convert pictures as inputs; and <em>Tensorflow</em> and <em>keras</em> (both in Python and R) to build and fine-tune CNNs.</p>
<p>You can install these and other auxiliary packages with the code below if needed (see Section <a href="chapter01.html#sec-installing"><span>1.4</span></a> for more details):</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install Pillow requests numpy sklearn</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip3 install tensorflow keras</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">c</span>(<span class="st">"magick"</span>, <span class="st">"glue"</span>,<span class="st">"lsa"</span>,</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tidyverse"</span>,<span class="st">"dslabs"</span>,<span class="st">"randomForest"</span>,<span class="st">"caret"</span>,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tensorflow"</span>,<span class="st">"keras"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>After installing, you need to import (activate) the packages every session:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.applications.resnet50 <span class="im">import</span> ResNet50</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(magick)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lsa)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<section id="sec-beyond" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="sec-beyond"><span class="header-section-number">14.1</span> Beyond Text Analysis: Images, Audio and Video</h2>
<p>A book about the <em>computational analysis of communication</em> would be incomplete without a chapter dedicated to analyzing visual data. In fact, if you think of the possible contents derived from social, cultural and political dynamics in the current digital landscape, you will realize that written content is only a limited slice of the bigger cake. Humans produce much more oral content than text messages, and are more agile in deciphering sounds and visual content. Digitalization of social and political life, as well as the explosion of self-generated digital content in the web and social media, have provoked an unprecedented amount of multimedia content that deserve to be included in many types of research.</p>
<p>Just imagine a collection of digital recorded radio stations, or the enormous amount of pictures produced every day on Instagram, or even the millions of videos of social interest uploaded on Youtube. These are definitely goldmines for social researchers who traditionally used manual techniques to analyze just a very small portion of this multimedia content. However, it is also true that computational techniques to analyze audio, images or video are still little developed in social sciences given the difficulty of application for non-computational practitioners and the novelty of the discoveries in fields such as computer vision.</p>
<p>This section gives a brief overview of different formats of multimedia files. We explain how to generate useful inputs into our pipeline to perform computational analysis.</p>
<p>You are probably already familiar with digital formats of images (.jpg, .bmp, .gif, etc.), audio (.mp3, .wav, .wma, flac, etc.) or video (.avi, .mov, .wmv, .flv, etc.), which is the very first step to use these contents as input. However, similar to the case of texts you will need to do some preprocessing to put these formats into good shape and get a proper mathematical representation of the content.</p>
<p>In the case of audio, there are many useful computational approaches to do research over these contents: from voice recognition, audio sentiment analysis or sound classification, to automatic generation of music. Recent advances in the field of artificial intelligence have created a prosperous and diversified field with multiple academic and commercial applications. Nevertheless, computational social scientists can obtain great insights just by using specific applications such as speech-to-text transformation and then apply text analytics (already explained in chapters 9, 10, and 11) to the results. As you will see in Section <a href="#sec-apivisions"><span>14.2</span></a>, there are some useful libraries in R and Python to use pre-trained models to transcribe voice in different languages.</p>
<p>Even when this approach is quite limited (just a small portion of the audio analytics world) and constrained (we will not address how to create the models), it will show how a specific, simple and powerful application of the automatic analysis of audio inputs can help answering many social questions (e.g., what are the topics of a natural conversation, what are the sentiments expressed in the scripts of radio news pieces, or which actors are named in oral speeches of any political party). In fact, automated analysis of audio can enable new research questions, different from those typically applied to text analysis. This is the case of the research by <span class="citation" data-cites="knox2021dynamic">Knox and Lucas (<a href="references.html#ref-knox2021dynamic" role="doc-biblioref">2021</a>)</span>, who used a computational approach over audio data from the Supreme Court Oral Arguments (407 arguments and 153 hours of audio, comprising over 66000 justice utterances and 44 million moments) to demonstrate that some crucial information such as the skepticism of legal arguments was transmitted by vocal delivery (e.g., speech tone), something indecipherable to text analysis. Or we could also mention the work by <span class="citation" data-cites="dietrich2019pitch">Dietrich, Hayes, and O’BRIEN (<a href="references.html#ref-dietrich2019pitch" role="doc-biblioref">2019</a>)</span> who computationally analyzed the vocal pitch of more than 70000 Congressional floor audio speeches and found that female members of the Congress spoke with greater <em>emotional intensity</em> when talking about women.</p>
<p>On the other hand, applying computational methods to video input is probably the most challenging task in spite of the recent and promising advances in computer vision. For the sake of space, we will not cover specific video analytics in this chapter, but it is important to let you know that most of the computational analysis of video is based on the inspection of image and audio contents. With this standard approach you need to specify which key frames you are going to extract from the video (for example take a still image every 1000 frames) and then apply computer vision techniques (such as object detection) to those independent images. Check for example version 3 of the object detection architecture <em>You Only Look Once Take</em> (YOLOv3)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> created by <span class="citation" data-cites="yolov3">Redmon and Farhadi (<a href="references.html#ref-yolov3" role="doc-biblioref">2018</a>)</span>, which uses a pre-trained Convolutional Neural Network (CNN) (see Section <a href="#sec-cnn"><span>14.4</span></a>) to locate objects within the video (Figure <a href="#fig-yolo"><span>14.1</span></a>). To answer many social science questions you might complement this frame-to-frame image analysis with an analysis of audio features. In any case, this approach will not cover some interesting aspects of the video such as the camera frame shots and movements, or the editing techniques, which certainly give more content information.</p>
<div id="fig-yolo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch15_yolo.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;14.1: A screen shot of a real-time video analyzed by YOLOv3 on its website https://pjreddie.com/darknet/yolo/</figcaption><p></p>
</figure>
</div>
</section>
<section id="sec-apivisions" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="sec-apivisions"><span class="header-section-number">14.2</span> Using Existing Libraries and APIs</h2>
<p>In the following sections we will show you how to deal with multimedia contents from scratch, with special attention to image classification using state-of-the-art libraries. However, it might be a good idea to begin by using existing libraries that directly implement multimedia analyses or by connecting to commercial services to deploy classification tasks remotely using their APIs. There is a vast variety of available libraries and APIs, which we cannot cover in this book, but we will briefly mention some of them that may be useful in the computational analysis of communication.</p>
<p>One example in the field of visual analytics is the <em>optical character recognition</em> (OCR). It is true that you can train your own models to deploy multi-class classification and predict every letter, number or symbol in an image, but it will be a task that will take you a lot of effort. Instead, there are specialized libraries in both R and Python such as <em>tesseract</em> that deploy this task in seconds with high accuracy. It is still possible that you will have to apply some pre-processing to the input images in order to get them in good shape. This means that you may need to use packages such as <em>PIL</em> or <em>Magick</em> to improve the quality of the image by cropping it or by reducing the background noise. In the case of PDF files you will have to convert them first into images and then apply OCR.</p>
<p>In the case of more complex audio and image documents you can use more sophisticated services provided by private companies (e.g., Google, Amazon, Microsoft, etc.). These commercial services have already deployed their own machine learning models with very good results. Sometimes you can even customize some of their models, but as a rule their internal features and configuration are not transparent to the user. Moreover, these services offer friendly APIs and, usually, a free quota to deploy your first exercises.</p>
<p>To work with audio files, many social researchers might need to convert long conversations, radio programs, or interviews to plain text. For this propose, <em>Google Cloud</em> offer the service <em>Speech-to-Text</em><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that remotely transcribes the audio to a text format supporting multiple languages (more than 125!). With this service you can remotely use the advanced deep learning models created by Google Platform from your own local computer (you must have an account and connect with the proper packages such as <em>googleLanguageR</em> or <em>google-cloud-language</em> in Python).</p>
<p>If you apply either OCR to images or Speech-to-Text recognition to audio content you will have juicy plain text to conduct NLP, sentiment analysis, topic modelling, among other techniques (see Chapter 11). Thus, it is very likely that you will have to combine different libraries and services to perform a complete computational pipeline, even jumping from R to Python, and vice versa!</p>
<p>Finally, we would like to mention the existence of the commercial services of <em>autotaggers</em>, such as Google’s Cloud Vision, Microsoft’s Computer Vision or Amazon’s Recognition. For example, if you connect to the services of Amazon’s Recognition you can not only detect and classify images, but also conduct sentiment analysis over faces or predict sensitive contents within the images. As in the case of Google Cloud, you will have to obtain commercially sold credentials to be able to connect to Amazon’s Recognition API (although you get a free initial “quota” of API access calls before you are required to pay for usage). This approach has two main advantages. The first is the access to a very well trained and validated model (continuously re-trained) over millions of images and with the participation of thousands of coders. The second is the scalability because you can store and analyze images at scale at a very good speed using cloud computing services.</p>
<div id="fig-refugees" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch15_refugees.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;14.2: A photograph of refugees on a lifeboat, used as an input for Amazon’s Recognition API. The commercial service detects in the pictures classes such as clothing, apparel, human, person, life jacket or vest.</figcaption><p></p>
</figure>
</div>
<p>As an example, you can use Amazon’s Recognition to detect objects in a news photograph of refugees in a lifeboat (Figure <a href="#fig-refugees"><span>14.2</span></a>) and you will obtain a set of accurate labels: <em>Clothing</em> (99.95%), <em>Apparel</em> (99.95%), <em>Human</em> (99.70%), <em>Person</em> (99.70%), <em>Life jacket</em> (99.43%) and <em>Vest</em> (99.43%). With a lower confidence you will also find labels such as <em>Coat</em> (67.39%) and <em>People</em> (66.78%). This example also highlights the need for validation, and the difficulty of grasping complex concepts in automated analyses: while all of these labels are arguably correct, it is safe to say that they fail to actually grasp the essence of the picture and the social context. One may even go as far as saying that – knowing the picture is about refugees – some of these labels, were they given by a human to describe the picture, would sound pretty cynical.</p>
<p>In Section <a href="#sec-cnn"><span>14.4</span></a> we will use this very same image (stored as <code>myimg2_RGB</code>) to detect objects using a classification model trained with an open-access database of images (ImageNet). You will find that there are some different predictions in both methods, but especially that the time to conduct the classification is shorter in the commercial service, since we don’t have to train or choose a model. As you may imagine, you can neither modify the commercial models nor have access to their internal details, which is a strong limitation if you want to build your own customized classification system.</p>
</section>
<section id="sec-storing" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="sec-storing"><span class="header-section-number">14.3</span> Storing, Representing, and Converting Images</h2>
<p>In this section we will focus on learning how to store, represent, and convert images for further computational analysis. For a more exhaustive discussion of the computational analysis of images, see <span class="citation" data-cites="williams2020images">Williams, Casas, and Wilkerson (<a href="references.html#ref-williams2020images" role="doc-biblioref">2020</a>)</span>.</p>
<p>To perform basic image manipulation we have to: (i) load images and transform their shape when it is necessary (by cropping or resizing), and (ii) create a mathematical representation of the image (normally derived from its size, colors and pixel intensity) such as a three-dimensional matrix (x, y, color channel) or a flattened vector. You have some useful libraries in Python and R (<em>pil</em> and <em>imagemagik</em>, respectively) to conduct research in these initial stages, but you will also find that more advanced libraries in computer vision will include functions or modules for pre-processing images. At this point you can work either locally or remotely, but keep in mind that images can be heavy files and if you are working with thousands of files you will probably need to store or process them in the cloud (see Section <a href="chapter15.html#sec-cloudcomputing"><span>15.2</span></a>).</p>
<p>You can load any image as an object into your workspace as we show in Example <a href="#exm-loadimg"><span>14.1</span></a>. In this case we load two pictures of refugees published by mainstream media in Europe (see <span class="citation" data-cites="amores2019visual">Amores, Calderón, and Stanek (<a href="references.html#ref-amores2019visual" role="doc-biblioref">2019</a>)</span>), one is a JPG and the other is a PNG file. For this basic loading step we used the <code>open</code> function of the <code>Image</code> module in <em>pil</em> and <code>image_read</code> function in <em>imagemagik</em>. The JPG image file is a <span class="math inline">\(805\times 453\)</span> picture with the color model <em>RGB</em> and the PNG is a <span class="math inline">\(1540\times 978\)</span> picture with the color model <em>RGBA</em>. As you may notice the two objects have different formats, sizes and color models, which means that there is little analysis you can do if you don’t create a standard mathematical representation of both.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-loadimg" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.1 </strong></span>Loading JPG and PNG pictures as objects</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>myimg1 <span class="op">=</span> Image.<span class="bu">open</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    requests.get(<span class="st">"https://cssbook.net/d/259_3_32_15.jpg"</span>, stream<span class="op">=</span><span class="va">True</span>).raw</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>myimg2 <span class="op">=</span> Image.<span class="bu">open</span>(</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    requests.get(<span class="st">"https://cssbook.net/d/298_5_52_15.png"</span>, stream<span class="op">=</span><span class="va">True</span>).raw</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(myimg1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=805x453 at 0x7FC66DB...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(myimg2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1540x978 at 0x7FC66DB...</code></pre>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>myimg1 <span class="ot">=</span> <span class="fu">image_read</span>(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"https://cssbook.net/d/259_3_32_15.jpg"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>myimg2 <span class="ot">=</span> <span class="fu">image_read</span>(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"https://cssbook.net/d/298_5_52_15.png"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="fu">image_info</span>(myimg1), <span class="fu">image_info</span>(myimg2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 7
  format width height colorspace matte filesize density
  &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  
1 JPEG     805    453 sRGB       FALSE    75275 72x72  
2 PNG     1540    978 sRGB       TRUE   2752059 57x57  </code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="fig-pixel" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch15_pixel.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;14.3: Representation of the matrix data structure of a RGB image in which each pixel contains information for the intensity of each color component.</figcaption><p></p>
</figure>
</div>
<p>The good news when working with digital images is that the concept of <code>pixel</code> (picture element) will help you to understand the basic mathematical representation behind computational analysis of images. A rectangular grid of pixels is represented by a dot matrix which in turn generates a <code>bitmap image</code> or <code>raster graphic</code>. The dot matrix data structure is a basic but powerful representation of the images since we can conduct multiple simple and advanced operations with the matrices. Specifically, each dot in the matrix is a number that contains information about the intensity of each pixel (that commonly ranges from 0 to 255) also known as bit or color depth (figure <a href="#fig-pixel"><span>14.3</span></a>). This means that the numerical representation of a pixel can have 256 different values, 0 being the darkest tone of a given color and 255 the lightest. Keep in mind that if you divide the pixel values by 255 you will have a 0–1 scale to represent the intensity.</p>
<p>In a black-and-white picture we will only have one color (gray-scale), with the darker points representing the black and the lighter ones the white. The mathematical representation will be a single matrix or a two-dimensional array in which the number of rows and columns will correspond to the dimensions of the image. For instance in a <span class="math inline">\(224 \times 224\)</span> black-and-white picture we will have 50176 integers (0–255 scales) representing each pixel intensity.</p>
<p>In Example <a href="#exm-imagel"><span>14.2</span></a> we convert our original JPG picture to gray-scale and then create an object with the mathematical representation (a <span class="math inline">\(453 \times 805\)</span> matrix).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-imagel" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.2 </strong></span>Converting images to gray-scale and creating a two-dimensional array</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="cell" data-hash="chapter14_cache/html/imagel-python_83e3fd328c99b761bb1ef7a3c7a727f5">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>myimg1_L <span class="op">=</span> myimg1.convert(<span class="st">"L"</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(myimg1_L))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'PIL.Image.Image'&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>myimg1_L_array <span class="op">=</span> np.array(myimg1_L)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(myimg1_L_array))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(myimg1_L_array.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(453, 805)</code></pre>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="cell" data-hash="chapter14_cache/html/imagel-r_55991f3482c9c7f10864105a4f2600f0">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>myimg1_L <span class="ot">=</span> <span class="fu">image_convert</span>(myimg1, <span class="at">colorspace =</span> <span class="st">"gray"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">class</span>(myimg1_L))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "magick-image"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>myimg1_L_array <span class="ot">=</span> <span class="fu">as.integer</span>(myimg1_L[[<span class="dv">1</span>]])</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">class</span>(myimg1_L_array))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "array"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(myimg1_L_array))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 453 805   1</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>By contrast, color images will have multiple color channels that depend on the color model you chose. One standard color model is the three-channel RGB (<em>red</em>, <em>green</em> and <em>blue</em>), but you can find other variations in the chosen colors and the number of channels such as: RYB (<em>red</em>, <em>yellow</em> and <em>blue</em>), RGBA (<em>red</em>, <em>green</em>, <em>blue</em> and <em>alpha</em><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> ) or CMYK (<em>cyan</em>, <em>magenta</em>, <em>yellow</em> and <em>key</em><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>). Importantly, while schemes used for printing such as CMYK are <em>substractive</em> (setting all colors to their highest value results in black, setting them to their lowest value results in white), schemes used for computer and television screens (such as RGB) are <em>additive</em>: setting all of the colors to their maximal value results in white (pretty much the opposite as what you got with your paintbox in primary school).</p>
<p>We will mostly use RGB in this book since it is the most used representation in the state-of-the-art literature in computer vision given that normally these color channels yield more accurate models. RGB’s mathematical representation will be a three-dimensional matrix or a collection of three two-dimensional arrays (one for each color) as we showed in figure <a href="#fig-pixel"><span>14.3</span></a>. Then an RGB <span class="math inline">\(224 \times 224\)</span> picture will have 50176 pixel intensities for each of the three colors, or in other words a total of 150528 integers!</p>
<p>Now, in Example <a href="#exm-imagergb"><span>14.3</span></a> we convert our original JPG file to a RGB object and then create a new object with the mathematical representation (a <span class="math inline">\(453 \times 805 \times 3\)</span> matrix).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-imagergb" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.3 </strong></span>Converting images to RGB color model and creating three two-dimensional arrays</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>myimg1_RGB <span class="op">=</span> myimg1.convert(<span class="st">"RGB"</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(myimg1_RGB))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'PIL.Image.Image'&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_array <span class="op">=</span> np.array(myimg1_RGB)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(myimg1_RGB_array))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(myimg1_RGB_array.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(453, 805, 3)</code></pre>
</div>
</div>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>myimg1_RGB <span class="ot">=</span> <span class="fu">image_convert</span>(myimg1, <span class="at">colorspace =</span> <span class="st">"RGB"</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">class</span>(myimg1_RGB))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "magick-image"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_array <span class="ot">=</span> <span class="fu">as.integer</span>(myimg1_RGB[[<span class="dv">1</span>]])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">class</span>(myimg1_RGB_array))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "array"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(myimg1_RGB_array))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 453 805   3</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Instead of pixels, there are other ways to store digital images. One of them is the <em>vector graphics</em>, with formats such as .ai, .eps, .svg or .drw. Differently to bitmap images, they don’t have a grid of dots but a set of <em>paths</em> (lines, triangles, square, curvy shapes, etc.) that have a start and end point, so simple and complex images are created with paths. The great advantage of this format is that images do not get “pixelated” when you enlarge them because the paths can easily be transformed while remaining smooth. However, to obtain the standard mathematical representation of images you can convert the vector graphics to raster graphics (the way back is a bit more difficult and often only possible by approximation).</p>
<p>Sometimes you need to convert your image to a specific size. For example, in the case of image classification this is a very important step since all the input images of the model must have the same size. For this reason, one of the most common tasks in the preprocessing stage is to change the dimensions of the image in order to adjust width and height to a specific size. In Example <a href="#exm-resize"><span>14.4</span></a> we use the <code>resize</code> method provided by <em>pil</em> and the <code>image_scale</code> function in <em>imagemagik</em> to reduce the first of our original pictures in RGB (<code>myimg1_RGB</code>) to 25% . Notice that we first obtain the original dimensions of the photograph (i.e.&nbsp;<code>myimg1_RGB.width</code> or <code>image_info(myimg1_RGB)['width'][[1]]</code>) and then multiply it by 0.25 in order to obtain the new size which is the argument required by the functions.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-resize" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.4 </strong></span>Resize to 25% and visualize a picture</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-2" role="tab" aria-controls="tabset-6-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<div class="cell" data-hash="chapter14_cache/html/resize-python_297c33999163963b17801a08602f26d1">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize and visalize myimg1. Reduce to 25%</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="bu">int</span>(myimg1_RGB.width <span class="op">*</span> <span class="fl">0.25</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>height <span class="op">=</span> <span class="bu">int</span>(myimg1_RGB.height <span class="op">*</span> <span class="fl">0.25</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_25 <span class="op">=</span> myimg1_RGB.resize((width, height))</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(myimg1_RGB_25)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/resize-python-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-6-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-2-tab">
<div class="cell" data-hash="chapter14_cache/html/resize-r_7381090b1592ad3a9c1f4be84c5a4af0">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Resize and visalize myimg1. Reduce to 25%</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_25 <span class="ot">=</span> <span class="fu">image_scale</span>(myimg1_RGB,</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="fu">image_info</span>(myimg1_RGB)[<span class="st">"width"</span>][[<span class="dv">1</span>]]<span class="sc">*</span><span class="fl">0.25</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myimg1_RGB_25)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/resize-r-3.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Now, using the same functions of the latter example, we specify in Example <a href="#exm-resize2"><span>14.5</span></a> how to resize the same picture to <span class="math inline">\(224 \times 244\)</span>, which is one of the standard dimensions in computer vision.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-resize2" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.5 </strong></span>Resize to <span class="math inline">\(224 \times 224\)</span> and visualize a picture</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-7-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-1" role="tab" aria-controls="tabset-7-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-7-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-2" role="tab" aria-controls="tabset-7-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-7-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-7-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize to 224 x 224</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_224 <span class="op">=</span> myimg1_RGB.resize((<span class="dv">224</span>, <span class="dv">224</span>))</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(myimg1_RGB_224)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/resize2-python-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-7-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-7-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Resize and visalize myimg1. Resize to 224 x 224</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The ! is used to specify an exact width and height</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_224 <span class="ot">=</span> <span class="fu">image_scale</span>(myimg1_RGB,</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>                             <span class="st">"!224x!224"</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myimg1_RGB_224)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/resize2-r-3.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>You may have noticed that the new image has now the correct width and height but that it looks deformed. The reason is that the original picture was not squared and our order was to force it to fit into a <span class="math inline">\(224 \times 224\)</span> square, losing its original aspect. There are different alternatives to solving this issue, but probably the most extended is to <em>crop</em> the original image to create a squared picture. As you can see in Example <a href="#exm-crop"><span>14.6</span></a> we can create a function that first determines the orientation of the picture (vertical versus horizontal) and then cut the margins (up and down if it is vertical; and left and right if it is horizontal) to create a square. After applying this ad hoc function <code>crop</code> to the original image we can resize again to obtain a non-distorted <span class="math inline">\(224 \times 224\)</span> image.</p>
<p>Of course you are now losing part of the picture information, so you may think of other alternatives such as filling a couple of sides with blank pixels (or <code>padding</code>) in order to create the square by adding information instead of removing it.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-crop" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.6 </strong></span>Function to crop the image to create a square and the resize the picture</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-8-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-1" role="tab" aria-controls="tabset-8-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-8-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-2" role="tab" aria-controls="tabset-8-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-8-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-8-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crop and resize to 224 x 224</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adapted from Webb, Casas &amp; Wilkerson (2020)</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> crop(img):</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    height <span class="op">=</span> img.height</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    width <span class="op">=</span> img.width</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    hw_dif <span class="op">=</span> <span class="bu">abs</span>(height <span class="op">-</span> width)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    hw_halfdif <span class="op">=</span> hw_dif <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    crop_leftright <span class="op">=</span> width <span class="op">&gt;</span> height</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> crop_leftright:</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        y0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        y1 <span class="op">=</span> height</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        x0 <span class="op">=</span> <span class="dv">0</span> <span class="op">+</span> hw_halfdif</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        x1 <span class="op">=</span> width <span class="op">-</span> hw_halfdif</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        y0 <span class="op">=</span> <span class="dv">0</span> <span class="op">+</span> hw_halfdif</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        y1 <span class="op">=</span> height <span class="op">-</span> hw_halfdif</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>        x0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>        x1 <span class="op">=</span> width</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img.crop((x0, y0, x1, y1))</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_crop <span class="op">=</span> crop(myimg1_RGB)</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_crop_224 <span class="op">=</span> myimg1_RGB_crop.resize((<span class="dv">224</span>, <span class="dv">224</span>))</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>plt.imshow(myimg1_RGB_crop_224)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/crop-python-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-8-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-8-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Crop and resize to 224 x 224</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Create function</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>crop <span class="ot">=</span> <span class="cf">function</span>(img) {</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    width <span class="ot">=</span> <span class="fu">image_info</span>(img)[<span class="st">"width"</span>][[<span class="dv">1</span>]]</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    height <span class="ot">=</span> <span class="fu">image_info</span>(img)[<span class="st">"height"</span>][[<span class="dv">1</span>]]</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (width <span class="sc">&gt;</span> height) {</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span> (<span class="fu">image_crop</span>(img, </span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>                <span class="fu">sprintf</span>(<span class="st">"%dx%d+%d"</span>, height,</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>                    height, (width<span class="sc">-</span>height)<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    }   <span class="cf">else</span>  {</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span> (<span class="fu">image_crop</span>(img,</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>                <span class="fu">sprintf</span>(<span class="st">"%sx%s+%s+%s"</span>, width,</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        width, (width<span class="sc">-</span>width), (height<span class="sc">-</span>width)<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_crop <span class="ot">=</span> <span class="fu">crop</span>(myimg1_RGB)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_crop_224 <span class="ot">=</span> <span class="fu">image_scale</span>(myimg1_RGB_crop, <span class="st">"!224x!224"</span>)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myimg1_RGB_crop_224)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/crop-r-3.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>You can also adjust the orientation of the image, flip it, or change its background, among other commands. These techniques might be useful for creating extra images in order to enlarge the training set in image classification (see Section <a href="#sec-cnn"><span>14.4</span></a>). This is called <em>data augmentation</em> and consists of duplicating the initial examples on which the model was trained and altering them so that the algorithm can be more robust and generalize better. In Example <a href="#exm-rotate"><span>14.7</span></a> we used the <code>rotate</code> method in <em>pil</em> and <code>image_rotate</code> function in <em>imagemagik</em> to rotate 45 degrees the above resized image <code>myimg1_RGB_224</code> to see how easily we can get an alternative picture with similar information to include in an augmented training set.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-rotate" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.7 </strong></span>Rotating a picture 45 degrees</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-9-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-1" role="tab" aria-controls="tabset-9-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-9-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-2" role="tab" aria-controls="tabset-9-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-9-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-9-1-tab">
<div class="cell" data-hash="chapter14_cache/html/rotate-python_e0db45ee20b3694e25cfc8cd59ee99fb">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Rotate 45 degrees</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_224_rot <span class="op">=</span> myimg1_RGB_224.rotate(<span class="op">-</span><span class="dv">45</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(myimg1_RGB_224_rot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/rotate-python-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-9-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-9-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Rotate 45 degrees</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_224_rot <span class="ot">=</span> <span class="fu">image_rotate</span>(</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    myimg1_RGB_224, <span class="dv">45</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myimg1_RGB_224_rot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/rotate-r-3.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Finally, the numerical representation of visual content can help us to <em>compare</em> pictures in order to find similar or even duplicate images. Let’s take the case of RGB images which in Example <a href="#exm-imagergb"><span>14.3</span></a> we showed how to transform to a three two-dimensional array. If we now convert the three-dimensional matrix of the image into a flattened vector we can use this simpler numerical representation to estimate similarities. Specifically, as we do in Example <a href="#exm-flatten"><span>14.8</span></a>, we can take the vectors of two <em>flattened images</em> of resized <span class="math inline">\(15 \times 15\)</span> images to ease computation (<code>img_vect1</code> and <code>img_vect2</code>) and use the <em>cosine similarity</em> to estimate how akin those images are. We stacked the two vectors in a matrix and then used the <code>cosine_similarity</code> function of the <code>metrics</code> module of the <em>sklearn</em> package in Python and the <code>cosine</code> function of the <em>lsa</em> package in R.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-flatten" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.8 </strong></span>Comparing two flattened vectors to detect similarities between images</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-10-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-1" role="tab" aria-controls="tabset-10-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-10-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-2" role="tab" aria-controls="tabset-10-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-10-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-10-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create two 15x15 small images to compare</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># image1</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_crop_15 <span class="op">=</span> myimg1_RGB_crop_224.resize((<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># image2</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>myimg2_RGB <span class="op">=</span> myimg2.convert(<span class="st">"RGB"</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>myimg2_RGB_array <span class="op">=</span> np.array(myimg2_RGB)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>myimg2_RGB_crop <span class="op">=</span> crop(myimg2_RGB)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>myimg2_RGB_crop_224 <span class="op">=</span> myimg2_RGB_crop.resize((<span class="dv">224</span>, <span class="dv">224</span>))</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>myimg2_RGB_crop_15 <span class="op">=</span> myimg2_RGB_crop_224.resize((<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>img_vect1 <span class="op">=</span> np.array(myimg1_RGB_crop_15).flatten()</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>img_vect2 <span class="op">=</span> np.array(myimg2_RGB_crop_15).flatten()</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> np.row_stack((img_vect1, img_vect2))</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>sim_mat <span class="op">=</span> cosine_similarity(matrix)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>sim_mat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>array([[1.        , 0.86455477],
       [0.86455477, 1.        ]])</code></pre>
</div>
</div>
</div>
<div id="tabset-10-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-10-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Create two 15x15 small images to compare</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co">#image1</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>myimg1_RGB_crop_15 <span class="ot">=</span> <span class="fu">image_scale</span>(myimg1_RGB_crop_224, <span class="dv">15</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>img_vect1 <span class="ot">=</span> <span class="fu">as.integer</span>(myimg1_RGB_crop_15[[<span class="dv">1</span>]])</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>img_vect1 <span class="ot">=</span> <span class="fu">as.vector</span>(img_vect1)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="co">#image2</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>myimg2_RGB <span class="ot">=</span> <span class="fu">image_convert</span>(myimg2, <span class="at">colorspace =</span> <span class="st">"RGB"</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>myimg2_RGB_crop <span class="ot">=</span> <span class="fu">crop</span>(myimg2_RGB)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>myimg2_RGB_crop_15 <span class="ot">=</span> <span class="fu">image_scale</span>(myimg2_RGB_crop, <span class="dv">15</span>)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>img_vect2 <span class="ot">=</span> <span class="fu">as.integer</span>(myimg2_RGB_crop_15[[<span class="dv">1</span>]])</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="co">#drop the extra channel for comparision</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>img_vect2 <span class="ot">=</span> img_vect2[,,<span class="sc">-</span><span class="dv">4</span>] </span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>img_vect2 <span class="ot">=</span> <span class="fu">as.vector</span>(img_vect2)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>matrix <span class="ot">=</span> <span class="fu">cbind</span>(img_vect1, img_vect2)</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="fu">cosine</span>(img_vect1, img_vect2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]
[1,] 0.8993252</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cosine</span>(matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          img_vect1 img_vect2
img_vect1 1.0000000 0.8993252
img_vect2 0.8993252 1.0000000</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>As you can see in the resulting matrix when the images are compared with themselves (that would be the case of an exact duplicate) they obtain a value of 1. Similar images would obtain values under 1 but still close to it, while dissimilar images would obtain low values.</p>
</section>
<section id="sec-cnn" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="sec-cnn"><span class="header-section-number">14.4</span> Image Classification</h2>
<p>The implementation of computational image classification can help to answer many scientific questions, from testing some traditional hypotheses to opening new fields of interest in social science research. Just think about the potential of detecting at scale <em>who</em> appears in news photographs or what are the facial <em>emotions</em> expressed in the profiles of a social network. Moreover, imagine you can automatically label whether an image contains a certain action or not. For example, this is the case of <span class="citation" data-cites="williams2020images">Williams, Casas, and Wilkerson (<a href="references.html#ref-williams2020images" role="doc-biblioref">2020</a>)</span> who conducted a binary classification of pictures related to the <em>Black Lives Matter</em> movement in order to model if a picture was a protest or not, which can help to understand the extent to which the media covered a relevant social and political issue.</p>
<p>There are many other excellent examples of how you can adopt image classification tasks to answer specific research questions in social sciences such as those of <span class="citation" data-cites="horiuchi2012should">Horiuchi, Komatsu, and Nakaya (<a href="references.html#ref-horiuchi2012should" role="doc-biblioref">2012</a>)</span> who detected smiles in images of politicians to estimate the effects of facial appearance on election outcomes; or the work by <span class="citation" data-cites="peng2018same">Peng (<a href="references.html#ref-peng2018same" role="doc-biblioref">2018</a>)</span> who used automated recognition of facial traits in American politicians to investigating the bias of media portrayals.</p>
<p>In this section, we will learn how to conduct computational image classification which is probably the most extended computer vision application in communication and social sciences (see Table <a href="#tbl-visionlingo"><span>14.1</span></a> for some terminology). We will first discuss how to apply a <em>shallow</em> algorithm and then a deep-learning approach, given a labelled data set.</p>
<div id="tbl-visionlingo" class="anchored">
<table class="table">
<caption>Table&nbsp;14.1: Some computer vision concepts used in computational analysis of communication</caption>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Computer vision lingo</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>bitmap</td>
<td>Format to store digital images using a rectangular grid of points of colors. Also called “raster image”.</td>
</tr>
<tr class="even">
<td>pixel</td>
<td>Stands for “picture element” and is the smallest point of a bitmap image</td>
</tr>
<tr class="odd">
<td>color model</td>
<td>Mathematical representation of colors in a picture. The standard in computer vision is RGB, but there are others such as RYB, RGBA or CMYK.</td>
</tr>
<tr class="even">
<td>vector graphic</td>
<td>Format to store digital images using lines and curves formed by points.</td>
</tr>
<tr class="odd">
<td>data augmentation</td>
<td>Technique to increase the training set of images by creating new ones base on the modification of some of the originals (cropping, rotating, etc.)</td>
</tr>
<tr class="even">
<td>image classification</td>
<td>Machine learning task to predict a class of an image based on a model. State-of-the-art image classification is conducted with Convolutional Neural Networks (CNN). Related tasks are object detection and image segmentation.</td>
</tr>
<tr class="odd">
<td>activation function</td>
<td>Parameter of a CNN that defines the output of a layer given the inputs of the previous layer. Some usual activation functions in image classification are sigmoid, softmax, or RELU.</td>
</tr>
<tr class="even">
<td>loss function</td>
<td>Parameter of a CNN which accounts for the difference between the prediction and the target variable (confidence in the prediction). A common one in image classification is the cross entropy loss.</td>
</tr>
<tr class="odd">
<td>optimization</td>
<td>Parameter of a CNN that updates weights and biases in order to reduce the error. Some common optimizers in image classification are Stochastic Gradient Descent and ADAM.</td>
</tr>
<tr class="even">
<td>transfer learning</td>
<td>Using trained layers of other CNN architectures to fine tune a new model investing less resources (e.g.&nbsp;training data).</td>
</tr>
</tbody>
</table>
</div>
<p>Technically, in an image classification task we train a model with examples (e.g., a corpus of pictures with labels) in order to predict the category of any given new sample. It is the same logic used in supervised text classification explained in Section <a href="chapter11.html#sec-supervised"><span>11.4</span></a> but using images instead of texts. For example, if we show many pictures of cats and houses the algorithm would learn the constant features in each and will tell you with some degree of confidence if a new picture contains either a cat or a house. It is the same with letters, numbers, objects or faces, and you can apply either binary or multi-class classification. Just think when your vehicle registration plate is recognized by a camera or when your face is automatically labelled in pictures posted on Facebook.</p>
<p>Beyond image classification we have other specific tasks in computer vision such as <em>object detection</em> or <em>semantic segmentation</em> (Figure 14.4). To conduct object detection we first have to locate all the possible objects contained in a picture by predicting a bounding box (i.e., the four points corresponding to the vertical and horizontal coordinates of the center of the object), which is normally a regression task. Once the bounding boxes are placed around the objects, we must apply multi-class classification as explained earlier. In the case of semantic segmentation, instead of classifying objects, we classify each pixel of the image according to the class of the object the pixel belongs to, which means that different objects of the same class might not be distinguished. See <span class="citation" data-cites="geron2019hands">Géron (<a href="references.html#ref-geron2019hands" role="doc-biblioref">2019</a>)</span> for a more detailed explanation and graphical examples of object detection versus image segmentation.</p>
<div id="fig-location" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch15_location.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;14.4: Object detection (left) versus semantic segmentation (right). Source: <span class="citation" data-cites="geron2019hands">Géron (<a href="references.html#ref-geron2019hands" role="doc-biblioref">2019</a>)</span></figcaption><p></p>
</figure>
</div>
<p>It is beyond the scope of this book to address the implementation of object detection or semantic segmentation, but we will focus on how to conduct basic image classification in state-of-the-art libraries in R and Python. As you may have imagined we will need some already-labelled images to have a proper training set. It is also out of the scope of this chapter to collect and annotate the images, which is the reason why we will mostly rely on pre-existing image databases (i.e., MINST or Fashion MINST) and pre-trained models (i.e., CNN architectures).</p>
<section id="sec-shallow" class="level3" data-number="14.4.1">
<h3 data-number="14.4.1" class="anchored" data-anchor-id="sec-shallow"><span class="header-section-number">14.4.1</span> Basic Classification with Shallow Algorithms</h3>
<p>In Chapter <a href="chapter08.html"><span>8</span></a> we introduced you to the exciting world of machine learning and in Section <a href="chapter11.html#sec-supervised"><span>11.4</span></a> we introduced the <em>supervised</em> approach to classify texts. Most of the discussed models used so-called <em>shallow</em> algorithms such as Naïve Bayes or Support Vector Machines rather than the various large neural network models called <em>deep learning</em>. As we will see in the next section, deep neural networks are nowadays the best option for complex tasks in image classification. However, we will now explain how to conduct simple multi-class classification of images that contain numbers with a shallow algorithm.</p>
<p>Let us begin by training a model to recognize numbers using 70000 small images of digits handwritten from the Modified National Institute of Standards and Technology (MNIST) dataset (<span class="citation" data-cites="lecun1998gradient">(<a href="references.html#ref-lecun1998gradient" role="doc-biblioref">LeCun et al. 1998</a>)</span>). This popular training corpus contains gray-scale examples of numbers written by American students and workers and it is usually employed to test machine learning models (60000 for training and 10000 for testing). The image sizes are <span class="math inline">\(28 \times 28\)</span>, which generates 784 features for each image, with pixels values from white to black represented by a 0–255 scales. In Figure <a href="#fig-numbers"><span>14.5</span></a> you can observe the first 10 handwritten numbers used in both training and test set.</p>
<div id="fig-numbers" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch15_numbers.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;14.5: First 10 handwritten digits from the training and test set of the MNIST.</figcaption><p></p>
</figure>
</div>
<p>You can download the MNIST images from its project web page<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, but many libraries also offer this dataset. In Example <a href="#exm-mnist"><span>14.9</span></a> we use the <code>read_mnist</code> function from the <em>dslabs</em> package (Data Science Labs) in R and the <code>fetch_openml</code> function from the <em>sklearn</em> package (<code>datasets</code> module) in Python to read and load a <code>mnist</code> object into our workspace. We then create the four necessary objects (<code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>) to generate a ML model and print the first numbers in training and test sets and check they coincide with those in <a href="#fig-numbers"><span>14.5</span></a>.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-mnist" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.9 </strong></span>Loading MNIST dataset and preparing training and test sets</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-11-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-1" role="tab" aria-controls="tabset-11-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-11-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-2" role="tab" aria-controls="tabset-11-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-11-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-11-1-tab">
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> fetch_openml(<span class="st">"mnist_784"</span>, version<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> mnist[<span class="st">"data"</span>], mnist[<span class="st">"target"</span>]</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.astype(np.uint8)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test <span class="op">=</span> X[:<span class="dv">60000</span>], X[<span class="dv">60000</span>:]</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> y[:<span class="dv">60000</span>], y[<span class="dv">60000</span>:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Numbers in training set= "</span>, y_train[<span class="dv">0</span>:<span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Numbers in training set=  0    5
1    0
2    4
3    1
4    9
5    2
6    1
7    3
8    1
9    4
Name: class, dtype: uint8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Numbers in test set= "</span>, y_test[<span class="dv">0</span>:<span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Numbers in test set=  60000    7
60001    2
60002    1
60003    0
60004    4
60005    1
60006    4
60007    9
60008    5
60009    9
Name: class, dtype: uint8</code></pre>
</div>
</div>
</div>
<div id="tabset-11-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-11-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">=</span> <span class="fu">read_mnist</span>()</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>X_train <span class="ot">=</span> mnist<span class="sc">$</span>train<span class="sc">$</span>images</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">=</span> <span class="fu">factor</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>labels)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>X_test <span class="ot">=</span> mnist<span class="sc">$</span>test<span class="sc">$</span>images</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">=</span> <span class="fu">factor</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>labels)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Numbers in training set = "</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Numbers in training set = "</code></pre>
</div>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">factor</span>(y_train[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]), <span class="at">max.levels =</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 5 0 4 1 9 2 1 3 1 4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Numbers in test set = "</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Numbers in test set = "</code></pre>
</div>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">factor</span>(y_test[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]), <span class="at">max.levels =</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 7 2 1 0 4 1 4 9 5 9</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Once we are ready to model the numbers we choose one of the shallow algorithms explained in Section <a href="chapter08.html#sec-nb2dnn"><span>8.3</span></a> to deploy a binary or multi-class image classification task. In the case of binary, we should select a number of reference (for instance “3”) and then create the model of that number against all the others (to answer questions such as “What’s the probability of this digit of being number 3?”). On the other hand, if we choose multi-class classification our model can predict any of the ten numbers (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) included in our examples.</p>
<p>Now, we used the basic concepts of the Random Forest algorithm (see <a href="chapter08.html#sec-randomforest"><span>Section&nbsp;8.3.4</span></a>) to create and fit a model with 100 trees (<code>forest_clf</code>). In Example <a href="#exm-multiclass"><span>14.10</span></a> we use again the <em>randomForest</em> package in R and <em>sklearn</em> package in Python to estimate a model for the ten classes using the corpus of 60000 images (classes were similarly balanced, $$9–11% each). As we do in the examples, you can check the predictions for the first ten images of the test set (<code>X_test</code>), which correctly correspond to the right digits, and also check the (<code>predictions</code>) for the whole test set and then get some metrics of the model. The accuracy is over 0.97 which means the classification task is performed very well.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-multiclass" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.10 </strong></span>Modeling the handwritten digits with RandomForest and predicting some outcomes</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-12-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-12-1" role="tab" aria-controls="tabset-12-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-12-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-12-2" role="tab" aria-controls="tabset-12-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-12-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-12-1-tab">
<div class="cell" data-hash="chapter14_cache/html/multiclass-python_2c8e64057739e009e0554d6f9625c72e">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>forest_clf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>forest_clf.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>RandomForestClassifier(random_state=42)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(forest_clf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>RandomForestClassifier(random_state=42)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Predict the first 10 numbers of our set:"</span>, forest_clf.predict(X_test[:<span class="dv">10</span>])</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predict the first 10 numbers of our set: [7 2 1 0 4 1 4 9 5 9]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> forest_clf.predict(X_test)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Overall Accuracy: "</span>, accuracy_score(y_test, predictions))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overall Accuracy:  0.9705</code></pre>
</div>
</div>
</div>
<div id="tabset-12-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-12-2-tab">
<div class="cell" data-hash="chapter14_cache/html/multiclass-r_d253fa35b280017b23cbbd5ae27085e1">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Multiclass classification with RandomForest</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>rf_clf <span class="ot">=</span> <span class="fu">randomForest</span>(X_train, y_train, <span class="at">ntree=</span><span class="dv">100</span>)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>rf_clf</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
 randomForest(x = X_train, y = y_train, ntree = 100) 
               Type of random forest: classification
                     Number of trees: 100
No. of variables tried at each split: 28

        OOB estimate of  error rate: 3.43%
Confusion matrix:
     0    1    2    3    4    5    6    7    8    9 class.error
0 5845    2    7    3    6    5   22    0   29    4  0.01316900
1    0 6645   34   12   11    5    5   13   10    7  0.01438742
2   26   11 5764   32   28    3   15   37   34    8  0.03256126
3    8    5   84 5820    8   81    6   42   53   24  0.05072582
4    9    9   14    2 5646    4   22   10   19  107  0.03355015
5   20    9   12   87    7 5176   38    7   36   29  0.04519461
6   27   12    5    3   11   29 5814    0   17    0  0.01757350
7    8   22   53    9   35    1    0 6057   16   64  0.03320032
8   12   31   33   47   28   57   21    6 5552   64  0.05110238
9   19   10   13   68   93   23    2   51   44 5626  0.05429484</code></pre>
</div>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(rf_clf, X_test[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1  2  3  4  5  6  7  8  9 10 
 7  2  1  0  4  1  4  9  5  9 
Levels: 0 1 2 3 4 5 6 7 8 9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">=</span> <span class="fu">predict</span>(rf_clf, X_test)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>cm <span class="ot">=</span> <span class="fu">confusionMatrix</span>(predictions, y_test)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cm<span class="sc">$</span>overall[<span class="st">"Accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy 
    0.97 </code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>This approach based on shallow algorithms seems to work pretty well for simple images, but has a lot of limitations for more complex images such as figures or real pictures. After all, the more complex the image and the more abstract the concept, the less likely it is that one can expect a direct relationship between a pixel color and the classification. In the next section we introduce the use of deep learning in image classification which is nowadays a more accurate approach for complex tasks.</p>
</section>
<section id="sec-deep" class="level3" data-number="14.4.2">
<h3 data-number="14.4.2" class="anchored" data-anchor-id="sec-deep"><span class="header-section-number">14.4.2</span> Deep Learning for Image Analysis</h3>
<p>Even though they require heavy computations, Deep Neural Networks (DNN) are nowadays the best way to conduct image classification because their performance is normally higher than shallow algorithms. The reason is that we broaden the learning process using intermediate hidden layers, so each of these layers can learn different patterns or aspects of the image with different levels of abstraction: e.g., from detecting lines or contours in the first layers to catching higher feature representation of an image (such as the color of skin, the shapes of the eyes or the noses) in the next layers. In Section <a href="chapter08.html#sec-neural"><span>8.3.5</span></a> and Section <a href="chapter11.html#sec-deeplearning"><span>11.4.4</span></a> we introduced the general concepts of a DNN (such as perceptrons, layers, hidden layers, back or forward propagation, and output functions), and now we will cover some common architectures for image analysis.</p>
<p>One of the simplest DNNs architectures is the Multilayer Perceptron (MLP) which contains one input layer, one or many hidden layers, and one output layer (all of them <em>fully</em> connected and with bias neurons except for the output layer). Originally in a MLP the signals propagate from the inputs to the outputs (in one direction), which we call a feedforward neural network (FNN), but using Gradient Decent as an optimizer we can apply <em>backpropagation</em> (automatically computing the gradients of the network’s errors in two stages: one forward and one backward) and then obtain a more efficient training.</p>
<p>We can use MLPs for binary and multi-class classification. In the first case, we normally use a single output neuron with the <em>sigmoid</em> or <em>logistic</em> activation function (probability from 0 to 1) (see <a href="chapter08.html#sec-logreg"><span>Section&nbsp;8.3.2</span></a>); and in the second case we will need one output neuron per class with the <em>softmax</em> activation function (probabilities from 0 to 1 for each class but they must add up to 1 if the classes are exclusive. This is the function used in multinomial logistic regression). To predict probabilities, in both cases we will need a <em>loss</em> function and the one that is normally recommended is the <em>cross entropy loss</em> or simply <em>log loss</em>.</p>
<p>The state-of-the-art library for neural networks in general and for computer vision in particular is <em>TensorFlow</em><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> (originally created by Google and later publicly released) and the high-level Deep Learning API <em>Keras</em>, although you can find other good implementation packages such as <em>PyTorch</em> (created by Facebook), which has many straightforward functionalities and has also become popular in recent years (see for example the image classification tasks for social sciences conducted in <em>PyTorch</em> by <span class="citation" data-cites="williams2020images">Williams, Casas, and Wilkerson (<a href="references.html#ref-williams2020images" role="doc-biblioref">2020</a>)</span>). All these packages have current versions for both R and Python.</p>
<p>Now, let’s train an MLP to build an image classifier to recognize fashion items using the Fashion MNIST dataset<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. This dataset contains 70000 (60000 for training and 10000 for test) gray scale examples (<span class="math inline">\(28\times 28\)</span>) of ten different classes that include ankle boots, bags, coats, dresses, pullovers, sandals, shirts, sneakers, t-shirts/tops and trousers (Figure <a href="#fig-fashion"><span>14.6</span></a>). If you compare this dataset with the MINST, you will find that figures of fashion items are more complex than handwritten digits, which normally generates a lower accuracy in supervised classification.</p>
<div id="fig-fashion" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch15_fashion.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;14.6: Examples of Fashion MNIST items.</figcaption><p></p>
</figure>
</div>
<p>You can use <em>Keras</em> to load the Fashion MNIST. In Example <a href="#exm-fashion"><span>14.11</span></a> we load the complete dataset and create the necessary objects for modeling (<code>X_train_full</code>, <code>y_train_full</code>, <code>X_test</code>, <code>y_test</code>). In addition we rescaled all the input features from 0–255 to 0–1 by dividing them by 255 in order to apply Gradient Decent. Then, we obtained three sets with <span class="math inline">\(28\times 28\)</span> arrays: 60000 in the training, and 10000 in the test. We could also generate here a validation set (e.g., <code>X_valid</code> and <code>y_valid</code>) with a given amount of records extracted from the training set (e.g., 5000), but as you will later see <em>Keras</em> allows us to automatically generate the validation set as a proportion of the training set (e.g., 0.1, which would be 6000 records in our example) when fitting the model (check the importance to work with a validation set to avoid over-fitting, explained in Section <a href="chapter08.html#sec-train"><span>8.5.2</span></a>).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-fashion" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.11 </strong></span>Loading Fashion MNIST dataset and preparing training, test and validation sets</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-13-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-13-1" role="tab" aria-controls="tabset-13-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-13-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-13-2" role="tab" aria-controls="tabset-13-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-13-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-13-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>fashion_mnist <span class="op">=</span> keras.datasets.fashion_mnist</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>(X_train, y_train), (X_test, y_test) <span class="op">=</span> fashion_mnist.load_data()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz

 8192/29515 [=======&gt;......................] - ETA: 0s
29515/29515 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz

    8192/26421880 [..............................] - ETA: 0s
  802816/26421880 [..............................] - ETA: 1s
 4202496/26421880 [===&gt;..........................] - ETA: 0s
14106624/26421880 [===============&gt;..............] - ETA: 0s
25247744/26421880 [===========================&gt;..] - ETA: 0s
26421880/26421880 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz

5148/5148 [==============================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz

   8192/4422102 [..............................] - ETA: 0s
 925696/4422102 [=====&gt;........................] - ETA: 0s
4422102/4422102 [==============================] - 0s 0us/step</code></pre>
</div>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">"T-shirt/top"</span>, <span class="st">"Trouser"</span>, <span class="st">"Pullover"</span>,</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Dress"</span>, <span class="st">"Coat"</span>, <span class="st">"Sandal"</span>, <span class="st">"Shirt"</span>,</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Sneaker"</span>, <span class="st">"Bag"</span>, <span class="st">"Ankle boot"</span>]</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, X_test.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(60000, 28, 28) (10000, 28, 28)</code></pre>
</div>
</div>
</div>
<div id="tabset-13-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-13-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>fashion_mnist <span class="ot">&lt;-</span> <span class="fu">dataset_fashion_mnist</span>()</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(X_train, y_train) <span class="sc">%&lt;-%</span> fashion_mnist<span class="sc">$</span>train</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(X_test, y_test) <span class="sc">%&lt;-%</span> fashion_mnist<span class="sc">$</span>test</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>class_names <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"T-shirt/top"</span>,<span class="st">"Trouser"</span>,</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Pullover"</span>,<span class="st">"Dress"</span>, <span class="st">"Coat"</span>, <span class="st">"Sandal"</span>,<span class="st">"Shirt"</span>,</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sneaker"</span>, <span class="st">"Bag"</span>,<span class="st">"Ankle boot"</span>)</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>X_train <span class="ot">&lt;-</span> X_train <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> y_test <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(X_train))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 60000    28    28</code></pre>
</div>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">dim</span>(X_test))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10000    28    28</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The next step is to design the architecture of our model. There are three ways to create the models in <em>Keras</em> (<em>sequential</em>, <em>functional</em>, or <em>subclassing</em>), but there are thousands of ways to configure a deep neural network. In the case of this MLP, we have to include first an input layer with the <code>input_shape</code> equal to the image dimension (<span class="math inline">\(28\times 28\)</span> for 784 neurons). At the top of the MLP you will need a output layer with 10 neurons (the number of possible outcomes in our multi-class classification task) and a <em>softmax</em> activation function for the final probabilities for each class.</p>
<p>In Example <a href="#exm-mlp"><span>14.12</span></a> we use the <em>sequential</em> model to design our MLP layer by layer including the above-mentioned input and output layers. In the middle, there are many options for the configuration of the <em>hidden</em> layers: number of layers, number of neurons, activation functions, etc. As we know that each hidden layer will help to model different patterns of the image, it would be fair to include at least two of them with different numbers of neurons (significantly reducing this number in the second one) and transmit its information using the <em>relu</em> activation function. What we actually do is create an object called <code>model</code> which saves the proposed architecture. We can use the method <code>summary</code> to obtain a clear representation of the created neural network and the number of parameters of the model (266610 in this case!).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-mlp" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.12 </strong></span>Creating the architecture of the MLP with Keras</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-14-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-14-1" role="tab" aria-controls="tabset-14-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-14-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-14-2" role="tab" aria-controls="tabset-14-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-14-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-14-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.models.Sequential(</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>        keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>]),</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(<span class="dv">300</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(<span class="dv">100</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>),</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 784)               0         
                                                                 
 dense (Dense)               (None, 300)               235500    
                                                                 
 dense_1 (Dense)             (None, 100)               30100     
                                                                 
 dense_2 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
</div>
<div id="tabset-14-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-14-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="fu">layer_flatten</span>(<span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">28</span>, <span class="dv">28</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="fu">layer_dense</span>(<span class="at">units=</span><span class="dv">300</span>, <span class="at">activation=</span><span class="st">"relu"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="fu">layer_dense</span>(<span class="at">units=</span><span class="dv">100</span>, <span class="at">activation=</span><span class="st">"relu"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="fu">layer_dense</span>(<span class="at">units=</span><span class="dv">10</span>, <span class="at">activation=</span><span class="st">"softmax"</span>)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_1"
________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     
================================================================================
 flatten_1 (Flatten)                (None, 784)                     0           
 dense_5 (Dense)                    (None, 300)                     235500      
 dense_4 (Dense)                    (None, 100)                     30100       
 dense_3 (Dense)                    (None, 10)                      1010        
================================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
________________________________________________________________________________</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The next steps will be to <code>compile</code>, <code>fit</code>, and <code>evaluate</code> the model, similarly to what you have already done in previous exercises of this book. In Example <a href="#exm-model"><span>14.13</span></a> we first include the parameters (loss, optimizer, and metrics) of the compilation step and fit the model, which might take some minutes (or even hours depending on your dataset, the architecture of you DNN and, of course, your computer).</p>
<p>When fitting the model you have to separate your training set into phases or <em>epochs</em>. A good rule of thumb to choose the optimal number of epochs is to stop a few iterations after the test loss stops improving<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> (here we chose five epochs for the example). You will also have to set the proportion of the training set that will become the validation set (in this case 0.1). In addition, you can use the parameter <code>verbose</code> to choose whether to see the progress (1 for progress bar and 2 for one line per epoch) or not (0 for silent) of the training process. By using the method <code>evaluate</code> you can then obtain the final loss and accuracy, which in this case is 0.84 (but you can reach up 0.88 if you fit it with 25 epochs!).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-model" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.13 </strong></span>Compiling fitting and evaluating the model for the MLP</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-15-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-15-1" role="tab" aria-controls="tabset-15-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-15-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-15-2" role="tab" aria-controls="tabset-15-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-15-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-15-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"sgd"</span>,</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">5</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_split<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Evaluation: "</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Evaluation: </code></pre>
</div>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.evaluate(X_test, y_test, verbose<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.4696454107761383, 0.8309999704360962]</code></pre>
</div>
</div>
</div>
<div id="tabset-15-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-15-2-tab">
<div class="cell" data-hash="chapter14_cache/html/model-r_443f43f1dbcb48202366cb84a85d7bf7">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(<span class="at">optimizer =</span> <span class="st">"sgd"</span>, <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">"accuracy"</span>),</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">loss =</span> <span class="st">"sparse_categorical_crossentropy"</span>)</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>history <span class="ot">=</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(X_train, y_train,<span class="at">validation_split=</span><span class="fl">0.1</span>, <span class="at">epochs=</span><span class="dv">5</span>, <span class="at">verbose=</span><span class="dv">0</span>)</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(history<span class="sc">$</span>metrics)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$loss
[1] 0.7314489 0.4865300 0.4420610 0.4155026 0.3949344

$accuracy
[1] 0.7610000 0.8308148 0.8448704 0.8543148 0.8609074

$val_loss
[1] 0.5390297 0.5013266 0.4185368 0.4302060 0.4073453

...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>score <span class="ot">=</span> model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(X_test, y_test, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Evaluation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Evaluation"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(score)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     loss  accuracy 
1872.3418    0.0823 </code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Finally, you can use the model to predict the classes of any new image (using <code>predict_classes</code>). In Example <a href="#exm-predict"><span>14.14</span></a> we used the model to predict the classes of the first six elements of the test set. If you go back to <a href="#fig-fashion"><span>14.6</span></a> you can compare these predictions (“ankle boot”, “pullover”, “trouser”, “trouser”, “shirt”, and “tTrouser”) with the actual first six images of the test set, and see how accurate our model was.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-predict" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.14 </strong></span>Predicting classes using the MLP</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-16-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-16-1" role="tab" aria-controls="tabset-16-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-16-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-16-2" role="tab" aria-controls="tabset-16-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-16-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-16-1-tab">
<div class="cell" data-hash="chapter14_cache/html/predict-python_a0179b1c80b86141ee38b3031d3b5a74">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> X_test[:<span class="dv">6</span>]</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.argmax(model.predict(X_new, verbose<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>class_pred <span class="op">=</span> [class_names[i] <span class="cf">for</span> i <span class="kw">in</span> y_pred]</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>class_pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['Ankle boot', 'Pullover', 'Trouser', 'Trouser', 'Shirt', 'Trouser']</code></pre>
</div>
</div>
</div>
<div id="tabset-16-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-16-2-tab">
<div class="cell" data-hash="chapter14_cache/html/predict-r_b4163935e60d59e563e78fece44cecc3">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>img <span class="ot">=</span> X_test[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, , , drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>class_pred <span class="ot">=</span> model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(img, <span class="at">verbose=</span><span class="dv">0</span>) <span class="sc">%&gt;%</span> <span class="fu">k_argmax</span>()</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>class_pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tf.Tensor([9 2 1 1 6 1], shape=(6), dtype=int64)</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Using the above-described concepts and code you may try to train a new MLP using color images of ten classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck) using the CIFAR-10 and CIFAR-100 datasets<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>!</p>
</section>
<section id="sec-tuning" class="level3" data-number="14.4.3">
<h3 data-number="14.4.3" class="anchored" data-anchor-id="sec-tuning"><span class="header-section-number">14.4.3</span> Re-using an Open Source CNN</h3>
<p>Training complex images such as photographs is normally a more sophisticated task if we compare it to the examples included in the last sections. On the one hand, it might not be a good idea to build a deep neural network from scratch as we did in section <a href="#sec-deep"><span>14.4.2</span></a> to train a MLP. This means that you can re-use some lower layers of other DNNs and deploy <em>transfer learning</em> to save time with less training data. On the other hand, we should also move from traditional MLPs to other kinds of DNNs such as Convolutional Neural Networks (CNNs) which are nowadays the state-of-the-art approach in computer vision. Moreover, to get good results we should also build or explore different CNNs architectures that can produce more accurate predictions in image classification. In this section we will show how to re-use an open source CNN architecture and will suggest an example of how to fine-tune an existing CNN for a social science problem.</p>
<p>As explained in Section <a href="chapter08.html#sec-cnnbasis"><span>8.4.1</span></a> a CNN is a specific type of DNN that has had great success in complex visual tasks (image classification, object detection or semantic segmentation) and voice recognition<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. Instead of using <em>fully connected</em> layers like in a typical MLP, a CNN uses only <em>partially connected</em> layers inspired on how “real” neurons connect in the visual cortex: some neurons only react to stimuli located in a limited <em>receptive field</em>. In other words, in a CNN every neuron is connected to some neurons of the previous layer (and not to all of them), which significantly reduces the amount of information transmitted to the next layer and helps the DNN to detect complex patterns. Surprisingly, this reduction in the number of parameters and weights involved in the model works better for larger and more complex images, different from those shown in MNIST.</p>
<p>Building a CNN is quite similar to a MLP, except for the fact that you will have to work with <em>convolutional</em> and <em>pooling</em> layers. The convolutional layers include a <em>bias term</em> and are the most important blocks of a CNN because they establish the specific connections among the neurons. In simpler words: a given neuron of a high-level layer is connected only to a rectangular group of neurons (the receptive field) of the low-level layer and not to all of them<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. For more technical details of the basis of a CNN you can go to specific literature such as <span class="citation" data-cites="geron2019hands">Géron (<a href="references.html#ref-geron2019hands" role="doc-biblioref">2019</a>)</span>.</p>
<p>Instead of building a CNN from scratch, there are many pre-trained and open-source architectures that have been optimized for image classification. Besides a stack of convolutional and pooling layers, these architectures normally include some fully connected layers and a regular output layer for prediction (just like in MLPs). We can mention here some of these architectures: LeNet-5, AlexNet, GoogLeNet, VGGNet, ResNet, Xception or SENet<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. All these CNNs have been previously tested in image classification with promising results, but you still have to look at the internal composition of each of them and their metrics to choose the most appropriate for you. You can implement and train most of them from scratch either in <em>keras</em> or <em>PyTorch</em>, or you can just use them directly or even fine-tune the pre-trained model in order to save time.</p>
<p>Let’s use the pre-trained model of a Residual Network (ResNet) with 50 layers, also known as <em>ResNet50</em>, to show you how to deploy a multi-class classifier over pictures. The ResNet architecture (also with 34, 101 and 152 layers) is based on residual learning and uses <em>skip connections</em>, which means that the input layer not only feeds the next layer but this signal is also added to the output of another high-level layer. This allows you to have a much deeper network and in the case of ResNet152 it has achieved a top-five error rate of 3.6%. As we do in Example <a href="#exm-resnet50"><span>14.15</span></a>, you can easily import into your workspace a ResNet50 architecture and include the pre-trained weights of a model trained with ImageNet (uncomment the second line of the code to visualize the complete model!).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-resnet50" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.15 </strong></span>Loading a visualizing the ResNet50 architecture</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-17-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-17-1" role="tab" aria-controls="tabset-17-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-17-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-17-2" role="tab" aria-controls="tabset-17-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-17-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-17-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>model_rn50 <span class="op">=</span> tf.keras.applications.resnet50.ResNet50(weights<span class="op">=</span><span class="st">"imagenet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5

     8192/102967424 [..............................] - ETA: 0s
   827392/102967424 [..............................] - ETA: 6s
  4202496/102967424 [&gt;.............................] - ETA: 3s
  8396800/102967424 [=&gt;............................] - ETA: 2s
 13885440/102967424 [===&gt;..........................] - ETA: 2s
 18636800/102967424 [====&gt;.........................] - ETA: 2s
 25198592/102967424 [======&gt;.......................] - ETA: 1s
 33562624/102967424 [========&gt;.....................] - ETA: 1s
 41951232/102967424 [===========&gt;..................] - ETA: 1s
 47448064/102967424 [============&gt;.................] - ETA: 1s
 53379072/102967424 [==============&gt;...............] - ETA: 0s
 58728448/102967424 [================&gt;.............] - ETA: 0s
 67117056/102967424 [==================&gt;...........] - ETA: 0s
 75505664/102967424 [====================&gt;.........] - ETA: 0s
 83894272/102967424 [=======================&gt;......] - ETA: 0s
 92282880/102967424 [=========================&gt;....] - ETA: 0s
102967424/102967424 [==============================] - 2s 0us/step</code></pre>
</div>
</div>
</div>
<div id="tabset-17-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-17-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>model_resnet50 <span class="ot">=</span> <span class="fu">application_resnet50</span>(<span class="at">weights=</span><span class="st">"imagenet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>ImageNet is a corpus of labelled images based on the WordNet hierarchy. ResNet uses a subset of ImageNet with &nbsp;1000 examples for each of the 1000 classes for a total corpus of roughly 1350000 pictures (1200000 for training, 100000 for test, and 50000 for validation).</p>
<p>In Example <a href="#exm-newimages"><span>14.16</span></a> we crop a part of our second example picture of refugees arriving at the European coast (<code>myimg2_RGB</code>) in order to get just the sea landscape. With the created <code>model_resnet50</code> we then ask for up to three predictions of the class of the photograph in Example <a href="#exm-resnetpredictions"><span>14.17</span></a>.</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-newimages" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.16 </strong></span>Cropping an image to get a picture of a see landscape</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-18-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-18-1" role="tab" aria-controls="tabset-18-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-18-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-18-2" role="tab" aria-controls="tabset-18-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-18-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-18-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_color_image(image):</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image, interpolation<span class="op">=</span><span class="st">"nearest"</span>)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>picture1 <span class="op">=</span> np.array(myimg2_RGB) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>picture2 <span class="op">=</span> np.array(myimg2_RGB) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> np.array([picture1, picture2])</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>see <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>]</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>refugees <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.35</span>, <span class="fl">0.8</span>, <span class="fl">0.95</span>]</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>tf_images <span class="op">=</span> tf.image.crop_and_resize(</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>    images, [see, refugees], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">224</span>, <span class="dv">224</span>]</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>plot_color_image(tf_images[<span class="dv">0</span>])</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/newimages-python-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-18-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-18-2-tab">
<div class="cell" data-hash="chapter14_cache/html/newimages-r_6d49f14d657b76f807a2c1573481d5a8">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>picture1 <span class="ot">=</span> <span class="fu">image_crop</span>(myimg2_RGB, <span class="st">"224x224+50+50"</span>)</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(picture1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/newimages-r-3.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>picture1 <span class="ot">=</span> <span class="fu">as.integer</span>(picture1[[<span class="dv">1</span>]])</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="co">#drop the extra channel for comparision</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>picture1 <span class="ot">=</span> picture1[,,<span class="sc">-</span><span class="dv">4</span>] </span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>picture1 <span class="ot">=</span> <span class="fu">array_reshape</span>(picture1, <span class="fu">c</span>(<span class="dv">1</span>, <span class="fu">dim</span>(picture1)))</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>picture1 <span class="ot">=</span> <span class="fu">imagenet_preprocess_input</span>(picture1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-resnetpredictions" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.17 </strong></span>Predicting the class of the first image</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-19-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-19-1" role="tab" aria-controls="tabset-19-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-19-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-19-2" role="tab" aria-controls="tabset-19-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-19-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-19-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tf.keras.applications.resnet50.preprocess_input(tf_images <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>Y_proba <span class="op">=</span> model_rn50.predict(inputs, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> tf.keras.applications.resnet50.decode_predictions(Y_proba, top<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json

 8192/35363 [=====&gt;........................] - ETA: 0s
35363/35363 [==============================] - 0s 0us/step</code></pre>
</div>
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(preds[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('n09421951', 'sandbar', 0.08357885), ('n09428293', 'seashore', 0.06147332...</code></pre>
</div>
</div>
</div>
<div id="tabset-19-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-19-2-tab">
<div class="cell" data-hash="chapter14_cache/html/resnetpredictions-r_f91b4567542b59a865555da99972a76c">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>preds1 <span class="ot">=</span> model_resnet50 <span class="sc">%&gt;%</span> <span class="fu">predict</span>(picture1)</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">imagenet_decode_predictions</span>(preds1, <span class="at">top =</span> <span class="dv">3</span>)[[<span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  class_name class_description      score
1  n09421951           sandbar 0.07926155
2  n04347754         submarine 0.04810233
3  n02066245        grey_whale 0.04798760</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>As you can see in the Python and R outputs<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>, the best guess of the model is a <em>sandbar</em>, which is very close to the real picture that contains sea water, mountains and sky. However, it seems that the model is confusing sand with sea. Other results in the Python model are <em>seashore</em> and <em>cliff</em>, which are also very close to real sea landscape. Nevertheless, in the case of the R prediction the model detects a <em>submarine</em> and a <em>gray whale</em>, which revels that predictions are not 100% accurate yet.</p>
<p>If we do the same with another part of that original picture and focus only on the group of refugees in a lifeboat arriving at the European coast, we will get a different result! In Example <a href="#exm-newimages2"><span>14.18</span></a> we crop again (<code>myimg2_RGB</code>) and get a new framed picture. Then in Example <a href="#exm-resnetpredictions2"><span>14.19</span></a> we re-run the prediction task using the model <em>ResNet50</em> trained with ImageNet and get a correct result: both predictions coincide to see a <em>lifeboat</em>, which is a good tag for the image we want to classify. Again, other lower-level predictions can seem accurate (<em>speedboat</em>) and totally inaccurate (<em>volcano</em>, <em>gray whale</em> or <em>amphibian</em>).</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-newimages2" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.18 </strong></span>Cropping an image to get a picture of refugees in a lifeboat</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-20-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-20-1" role="tab" aria-controls="tabset-20-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-20-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-20-2" role="tab" aria-controls="tabset-20-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-20-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-20-1-tab">
<div class="cell" data-hash="chapter14_cache/html/newimages2-python_a6a541e593264111dbd58ddd44e9be5c">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>plot_color_image(tf_images[<span class="dv">1</span>])</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/newimages2-python-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-20-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-20-2-tab">
<div class="cell" data-hash="chapter14_cache/html/newimages2-r_e75180093d7029ac5dd962b96c8370ea">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>picture2 <span class="ot">=</span> <span class="fu">image_crop</span>(myimg2_RGB, <span class="st">"224x224+1000"</span>)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(picture2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="chapter14_files/figure-html/newimages2-r-3.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>picture2 <span class="ot">=</span> <span class="fu">as.integer</span>(picture2[[<span class="dv">1</span>]])</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="co">#drop the extra channel for comparision</span></span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>picture2 <span class="ot">=</span> picture2[,,<span class="sc">-</span><span class="dv">4</span>] </span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>picture2 <span class="ot">=</span> <span class="fu">array_reshape</span>(picture2, <span class="fu">c</span>(<span class="dv">1</span>, <span class="fu">dim</span>(picture2)))</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>picture2 <span class="ot">=</span> <span class="fu">imagenet_preprocess_input</span>(picture2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-resnetpredictions2" class="theorem example">
<p><span class="theorem-title"><strong>Example 14.19 </strong></span>Predicting the class of the second image</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-21-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-21-1" role="tab" aria-controls="tabset-21-1" aria-selected="true">Python code</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-21-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-21-2" role="tab" aria-controls="tabset-21-2" aria-selected="false">R code</a></li></ul>
<div class="tab-content">
<div id="tabset-21-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-21-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(preds[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('n03662601', 'lifeboat', 0.19698665), ('n09472597', 'volcano', 0.10091315...</code></pre>
</div>
</div>
</div>
<div id="tabset-21-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-21-2-tab">
<div class="cell" data-hash="chapter14_cache/html/resnetpredictions2-r_3aa1cbd3e9d448c08f3fcff89fbb9516">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>preds2 <span class="ot">=</span> model_resnet50 <span class="sc">%&gt;%</span> <span class="fu">predict</span>(picture2)</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a><span class="fu">imagenet_decode_predictions</span>(preds2, <span class="at">top =</span> <span class="dv">3</span>)[[<span class="dv">1</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  class_name class_description      score
1  n03662601          lifeboat 0.39761335
2  n04273569         speedboat 0.11085796
3  n02704792         amphibian 0.06916222</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>These examples show you how to use an open-source and pre-trained CNN that has 1000 classes and has been trained on images that we do not have control of. However, you may want to build your own classifier with your own training data, but using part of an existing architecture. This is called fine-tuning and you can follow a good example in social science in <span class="citation" data-cites="williams2020images">Williams, Casas, and Wilkerson (<a href="references.html#ref-williams2020images" role="doc-biblioref">2020</a>)</span> in which the authors reuse RestNet18 to build binary and multi-class classifiers adding their own data examples over the pre-trained CNN<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>So far we have covered the main techniques, methods, and services to analyze multimedia data, specifically images. It is up to you to choose which library or service to use, and you will find most of them in R and Python, using the basic concepts explained in this chapter. If you are interested in deepening your understanding of multimedia analysis, we encourage you explore this emerging and exciting field of expertise given the enormous importance it will no doubt have in the near future.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-amores2019visual" class="csl-entry" role="doc-biblioentry">
Amores, Javier J, Carlos Arcila Calderón, and Mikolaj Stanek. 2019. <span>“Visual Frames of Migrants and Refugees in the Main Western European Media.”</span> <em>Economics &amp; Sociology</em> 12 (3): 147–61.
</div>
<div id="ref-dietrich2019pitch" class="csl-entry" role="doc-biblioentry">
Dietrich, Bryce J, Matthew Hayes, and DIANA Z O’BRIEN. 2019. <span>“Pitch Perfect: Vocal Pitch and the Emotional Intensity of Congressional Speech.”</span> <em>American Political Science Review</em> 113 (4): 941–62.
</div>
<div id="ref-geron2019hands" class="csl-entry" role="doc-biblioentry">
Géron, Aurélien. 2019. <em>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems</em>. O’Reilly Media.
</div>
<div id="ref-horiuchi2012should" class="csl-entry" role="doc-biblioentry">
Horiuchi, Yusaku, Tadashi Komatsu, and Fumio Nakaya. 2012. <span>“Should Candidates Smile to Win Elections? An Application of Automated Face Recognition Technology.”</span> <em>Political Psychology</em> 33 (6): 925–33.
</div>
<div id="ref-knox2021dynamic" class="csl-entry" role="doc-biblioentry">
Knox, Dean, and Christopher Lucas. 2021. <span>“A Dynamic Model of Speech for the Social Sciences.”</span> <em>American Political Science Review</em> 115 (2): 649–66.
</div>
<div id="ref-lecun1998gradient" class="csl-entry" role="doc-biblioentry">
LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324.
</div>
<div id="ref-peng2018same" class="csl-entry" role="doc-biblioentry">
Peng, Yilang. 2018. <span>“Same Candidates, Different Faces: Uncovering Media Bias in Visual Portrayals of Presidential Candidates with Computer Vision.”</span> <em>Journal of Communication</em> 68 (5): 920–41.
</div>
<div id="ref-yolov3" class="csl-entry" role="doc-biblioentry">
Redmon, Joseph, and Ali Farhadi. 2018. <span>“YOLOv3: An Incremental Improvement.”</span> <em>arXiv</em>.
</div>
<div id="ref-williams2020images" class="csl-entry" role="doc-biblioentry">
Williams, Nora Webb, Andreu Casas, and John D Wilkerson. 2020. <span>“Images as Data for Social Science Research: An Introduction to Convolutional Neural Nets for Image Classification.”</span> <em>Elements in Quantitative and Computational Methods for the Social Sciences</em>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>https://pjreddie.com/darknet/yolo/<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://cloud.google.com/speech-to-text<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Alpha refers to the opacity of each pixel.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Key refers to <em>black</em>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>http://yann.lecun.com/exdb/mnist/<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>We will deploy *TensorFlow**2* in our exercises.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>https://github.com/zalandoresearch/fashion-mnist<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>The train loss/accuracy will gradually be better and better. And the test loss/accuracy as well, in the beginning. But then, at some point train loss/acc improves but test loss/acc stops getting better. If we keep training the model for more epochs, we are just overfitting on the train set, which of course we do not want to. Specifically, we do not want to simply stop at the iteration where we got the best loss/acc for the test set, because then we are overfitting on the test set. Hence practitioners often let it run for a few more epochs after hitting the best loss/acc for the test set. Then, a final check on the validation set will really tell us how well we do out of sample.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>https://www.cs.toronto.edu/ kriz/cifar.html<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>CNNs have also a great performance in natural language processing.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>If the input layer (in the case of color images there are three sublayers, one per color channel) and the convolutional layers are of different sizes we can apply techniques such as <em>zero padding</em> (adding zeros around the inputs) or spacing out the receptive fields (each shift from one receptive field to the other will be a <em>stride</em>). In order to transmit the weights from the receptive fields to the neurons, the convolutional layer will automatically generate some <em>filters</em> to create <em>features maps</em>, which are the areas of the input that mostly activate those filters. Additionally, by creating subsamples of the inputs, the pooling layers will reduce the number of parameters, the computational effort of the network and the risk of overfitting. The pooling layers <em>aggregates</em> the inputs using a standard arithmetic function such as minimum, maximum or mean.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>The description of technical details of all of these architectures is beyond the scope of this book, but besides the specific scientific literature of each architecture, some packages such as <em>keras</em> usually include basic documentation.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Outputs in Python and R might differ a little bit since the cropping of the new images were similar but not identical.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>The examples are provided in Python with the package <em>PyTorch</em>, which is quite friendly if you are already familiar to <em>Keras</em>.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../content/chapter13.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Network Data</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/chapter15.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Scaling up and distributing</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>